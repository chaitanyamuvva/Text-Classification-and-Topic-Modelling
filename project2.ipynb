{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting GPU\n",
    "\n",
    "Please uncomment this code, if your system has GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "3riNCT-IlbQw",
    "outputId": "cdf1752f-bb99-437e-bedf-19d8f8016056"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# if device_name == '/device:GPU:0':\n",
    "#     print('Found GPU at: {}'.format(device_name))\n",
    "# else:\n",
    "#     raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIEPmscVlkS0",
    "outputId": "5a32a980-b869-4e3c-9380-aeb9cf9add03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuRkd0uFfFb3",
    "outputId": "e3cdb21d-48f7-4cb2-8c41-fb718ad06077"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "HX3PqUmhSkjs"
   },
   "outputs": [],
   "source": [
    "# !pip3 install transformers==2.4.1\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1ezIPoffk8L",
    "outputId": "b7d339e2-3bd9-4a8e-d259-ca4277548e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 54,731\n",
      "\n",
      "Number of testing records: 19,678\n",
      "\n",
      "Number of df_train_1000 records: 1,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "df_train = pd.read_csv(\".//assignment1_data/axcs_train.csv\", delimiter=',')\n",
    "df_test = pd.read_csv(\".//assignment1_data/axcs_test.csv\", delimiter=',')\n",
    "df_train_1000 = df_train[:1000]\n",
    "\n",
    "print('Number of training records: {:,}\\n'.format(df_train.shape[0]))\n",
    "print('Number of testing records: {:,}\\n'.format(df_test.shape[0]))\n",
    "print('Number of df_train_1000 records: {:,}\\n'.format(df_train_1000.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "2kev0A2yhP1Q",
    "outputId": "352a4b7a-80e7-4647-ac9d-af86362ae883"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>InfoTheory</th>\n",
       "      <th>CompVis</th>\n",
       "      <th>Math</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46286</th>\n",
       "      <td>no-14043816</td>\n",
       "      <td>arxiv.org/abs/1404.3816</td>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>A Kalman filter powered by $\\mathcal{H}^2$-mat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A Kalman filter powered by matrices for quasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24468</th>\n",
       "      <td>no-12014500</td>\n",
       "      <td>arxiv.org/abs/1201.4500</td>\n",
       "      <td>2012-01-21</td>\n",
       "      <td>Requirements and the baseline plan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Requirements and the baseline plan For each s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48598</th>\n",
       "      <td>no-14065299</td>\n",
       "      <td>arxiv.org/abs/1406.5299</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>Properties and Complexity of Fan-Planarity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Properties and Complexity of Fan-Planarity In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49565</th>\n",
       "      <td>no-14075889</td>\n",
       "      <td>arxiv.org/abs/1407.5889</td>\n",
       "      <td>2014-07-22</td>\n",
       "      <td>Domain-partitioned element management systems ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Domain-partitioned element management systems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13719</th>\n",
       "      <td>no-10035633</td>\n",
       "      <td>arxiv.org/abs/1003.5633</td>\n",
       "      <td>2010-03-29</td>\n",
       "      <td>Performance Analysis of Best suited Adaptive E...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Performance Analysis of Best suited Adaptive ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                      URL        Date  \\\n",
       "46286  no-14043816  arxiv.org/abs/1404.3816  2014-04-15   \n",
       "24468  no-12014500  arxiv.org/abs/1201.4500  2012-01-21   \n",
       "48598  no-14065299  arxiv.org/abs/1406.5299  2014-06-20   \n",
       "49565  no-14075889  arxiv.org/abs/1407.5889  2014-07-22   \n",
       "13719  no-10035633  arxiv.org/abs/1003.5633  2010-03-29   \n",
       "\n",
       "                                                   Title  InfoTheory  CompVis  \\\n",
       "46286  A Kalman filter powered by $\\mathcal{H}^2$-mat...           0        0   \n",
       "24468                 Requirements and the baseline plan           0        0   \n",
       "48598         Properties and Complexity of Fan-Planarity           0        0   \n",
       "49565  Domain-partitioned element management systems ...           0        0   \n",
       "13719  Performance Analysis of Best suited Adaptive E...           0        0   \n",
       "\n",
       "       Math                                           Abstract  \n",
       "46286     1   A Kalman filter powered by matrices for quasi...  \n",
       "24468     0   Requirements and the baseline plan For each s...  \n",
       "48598     0   Properties and Complexity of Fan-Planarity In...  \n",
       "49565     0   Domain-partitioned element management systems...  \n",
       "13719     0   Performance Analysis of Best suited Adaptive ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "fu4gPX8ehkH8",
    "outputId": "9bdedd3c-ca36-4a93-86ba-4f0fae71b508"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>InfoTheory</th>\n",
       "      <th>CompVis</th>\n",
       "      <th>Math</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nested satisfiability</td>\n",
       "      <td>Nested satisfiability A special case of the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A note on digitized angles</td>\n",
       "      <td>A note on digitized angles We study the confi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Textbook examples of recursion</td>\n",
       "      <td>Textbook examples of recursion We discuss pro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theory and practice</td>\n",
       "      <td>Theory and practice The author argues to Sili...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Context-free multilanguages</td>\n",
       "      <td>Context-free multilanguages This article is a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title  \\\n",
       "0           Nested satisfiability   \n",
       "1      A note on digitized angles   \n",
       "2  Textbook examples of recursion   \n",
       "3             Theory and practice   \n",
       "4     Context-free multilanguages   \n",
       "\n",
       "                                            Abstract  InfoTheory  CompVis  \\\n",
       "0   Nested satisfiability A special case of the s...           0        0   \n",
       "1   A note on digitized angles We study the confi...           0        0   \n",
       "2   Textbook examples of recursion We discuss pro...           0        0   \n",
       "3   Theory and practice The author argues to Sili...           0        0   \n",
       "4   Context-free multilanguages This article is a...           0        0   \n",
       "\n",
       "   Math  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)[['Title','Abstract','InfoTheory','CompVis','Math']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VMml3aqRi4rf"
   },
   "outputs": [],
   "source": [
    "Abstracts = df_train.Abstract.values\n",
    "InfoTheory_labels = df_train.InfoTheory.values\n",
    "CompVis_labels = df_train.CompVis.values\n",
    "Math_labels = df_train.Math.values\n",
    "\n",
    "Abstracts_1000 = Abstracts[:1000]\n",
    "InfoTheory_labels_1000 = InfoTheory_labels[:1000]\n",
    "CompVis_labels_1000 = CompVis_labels[:1000]\n",
    "Math_labels_1000 = Math_labels[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_uNOfhKZaL1"
   },
   "source": [
    "## BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "03c4947af61a40608bc41f2975e557ed",
      "d35b61e91ba74271b21d068441cdf0fe",
      "0c66acd0f36d474193fe4bc20a6ef9ca",
      "a2fc0d2232ba4b07b43d136c5a0c341f",
      "2376c8ca1f454327b8a0abb521c5c1b9",
      "fa290d3d093d40c2ab0204869f8184a1",
      "b1b12dbaab0343268e03088d08a0645e",
      "a07dec1b19ce4ff1a4cf4c6f5e902663"
     ]
    },
    "id": "MSTOX0gQYgGf",
    "outputId": "087a0fce-c1f0-431e-bbe4-9f4891ec10c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Loading the BERT tokenizer.\n",
    "print('Loading BERT tokenizer')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6CLcrL-YjVP",
    "outputId": "18f6fe92-5330-4317-e09a-ddafb207aeed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x7fc686e4bca0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT with BERT tokenizer on all the InfoTheory Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMMeCHFyZszY",
    "outputId": "1c2f89d4-760d-435f-ef69-fe7230036ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Tokenized:  ['nest', '##ed', 'sat', '##is', '##fia', '##bility', 'a', 'special', 'case', 'of', 'the', 'sat', '##is', '##fia', '##bility', 'problem', ',', 'in', 'which', 'the', 'clauses', 'have', 'a', 'hierarchical', 'structure', ',', 'is', 'shown', 'to', 'be', 'sol', '##vable', 'in', 'linear', 'time', ',', 'assuming', 'that', 'the', 'clauses', 'have', 'been', 'represented', 'in', 'a', 'convenient', 'way', '.']\n",
      "Token IDs:  [9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Printing the original sentence\n",
    "print(' Original: ', Abstracts[0])\n",
    "\n",
    "# Printing the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(Abstracts[0]))\n",
    "\n",
    "# Printing the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(Abstracts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKUcZkOEedEx"
   },
   "source": [
    "### Sentences to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elhao80ye_YM",
    "outputId": "d04a5754-0c08-45d2-aed6-938f1a61b678"
   },
   "outputs": [],
   "source": [
    "# print('Max sentence length: ', max([len(tokenizer.tokenize(abs)) for abs in Abstracts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDJOkM4caO6j",
    "outputId": "57e2ff11-e1f8-45d6-cfbe-a1ded9409dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Token IDs: [101, 9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012, 102]\n",
      "--- 192.44011521339417 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Tokenizing all of the abstracts and map the tokenized words to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "for abstract in Abstracts:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745, #we got the max len 743 and after 2 special tokens\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)\n",
    "\n",
    "print('Original: ', Abstracts[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsvlKUYUejV1"
   },
   "source": [
    "\n",
    "### Padding & Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjvEUxfHayVz",
    "outputId": "dae04749-834f-4b3f-cadd-881366009aa3"
   },
   "outputs": [],
   "source": [
    "# print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XOMNBPcfgYU",
    "outputId": "7144d8da-8cc9-4657-9b7e-68d36246734d"
   },
   "outputs": [],
   "source": [
    "# !pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8owEsEF8g15m",
    "outputId": "82ec4142-5111-486c-ffbe-15cbe69897e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdoYzuHvhc24"
   },
   "source": [
    "### Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pMZeWjtChOt2"
   },
   "outputs": [],
   "source": [
    "#attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    # If a token ID is 0, setting the mask to 0.\n",
    "    # If a token ID is > 0, setting the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwubZnWkh9E4"
   },
   "source": [
    "### Training & Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UyV-7_ubh22O"
   },
   "outputs": [],
   "source": [
    "# Using train_test_split to split our data into train and validation sets for the training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using 90% for training and 10% for validation for both training data and attention masks\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, InfoTheory_labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, InfoTheory_labels,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jA6c0IzdiV8_"
   },
   "outputs": [],
   "source": [
    "# Converting all inputs and labels into torch tensors which is the required datatypefor our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U6cBiDosi3z9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for the validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRHS3u3Dj3Ah"
   },
   "source": [
    "### BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LcL6bPaGj4ue"
   },
   "outputs": [],
   "source": [
    "#!pip3 install ipywidgets\n",
    "#!pip3 install --upgrade jupyter_client\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "303f68b0545f4f6dbd847b8466ad9884",
      "653fc0c1edb9494f9cc14f23460b1a3f",
      "313a2a7cc85d4f2bae319610a7fe6b1e",
      "169c148e00e743ed951321231a4ee165",
      "edad10fc1a4345cab0417dd8e7466fdc",
      "6bf06e49cb364b91a5cc58d9d135707c",
      "cf6042b9870e4aa1a10f14ecd42b90ca",
      "84b17e634bad48238e462ec05527dbb4",
      "2563f0dd6c404fa4af0b403c77c6da7b",
      "88b6978b8daf4183b60c7e963aba5c7c",
      "91f3f1fd5e35402685489d74bcd441d3",
      "c06fb95a45514f3fbb990bf5c38ed2cf",
      "39362aafbb4a47669d3998ac28548d20",
      "2899efa168cf4c1da278861ee31a7b01",
      "ba3e86ff23a949108148016e8a6bf91b",
      "cb3d2b08ecf64defb9d819dda8334a9d"
     ]
    },
    "id": "J15zOsY2j_xy",
    "outputId": "f1e1e6f9-a6fa-4151-92da-6ba0e3ab15f4"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Loading BertForSequenceClassification, the pretrained BERT model. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Using the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KauVnxTXkEuv",
    "outputId": "621a4fde-367e-420b-b0b2-e6c3200e0800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Getting all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWaO6nRvk33H"
   },
   "source": [
    "### Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T_KL3kJOkj1o"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-fgl6OclCVC",
    "outputId": "82b28eab-8dc7-4a48-84b1-20f1027a2d88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7fc630e94850>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "#learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24ZmncZglLdG"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0o6sQVxYlHYz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WDA_rauulPkX"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAjyDCs4l5Gq",
    "outputId": "0aa2a69a-9144-4989-c60d-2ac0c6b557d6"
   },
   "outputs": [],
   "source": [
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "7yWtQ8nolTu2",
    "outputId": "eb234b11-f9cf-4aae-f04f-cc65880ad465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch     5  of  1,540.    Elapsed: 0:00:55.\n",
      "  Batch    10  of  1,540.    Elapsed: 0:01:23.\n",
      "  Batch    15  of  1,540.    Elapsed: 0:01:46.\n",
      "  Batch    20  of  1,540.    Elapsed: 0:02:14.\n",
      "  Batch    25  of  1,540.    Elapsed: 0:02:37.\n",
      "  Batch    30  of  1,540.    Elapsed: 0:02:59.\n",
      "  Batch    35  of  1,540.    Elapsed: 0:03:28.\n",
      "  Batch    40  of  1,540.    Elapsed: 0:04:48.\n",
      "  Batch    45  of  1,540.    Elapsed: 0:05:13.\n",
      "  Batch    50  of  1,540.    Elapsed: 0:05:41.\n",
      "  Batch    55  of  1,540.    Elapsed: 0:06:09.\n",
      "  Batch    60  of  1,540.    Elapsed: 0:06:34.\n",
      "  Batch    65  of  1,540.    Elapsed: 0:07:00.\n",
      "  Batch    70  of  1,540.    Elapsed: 0:07:35.\n",
      "  Batch    75  of  1,540.    Elapsed: 0:08:44.\n",
      "  Batch    80  of  1,540.    Elapsed: 0:09:08.\n",
      "  Batch    85  of  1,540.    Elapsed: 0:09:35.\n",
      "  Batch    90  of  1,540.    Elapsed: 0:10:07.\n",
      "  Batch    95  of  1,540.    Elapsed: 0:10:32.\n",
      "  Batch   100  of  1,540.    Elapsed: 0:11:11.\n",
      "  Batch   105  of  1,540.    Elapsed: 1:38:07.\n",
      "  Batch   110  of  1,540.    Elapsed: 1:59:15.\n",
      "  Batch   115  of  1,540.    Elapsed: 1:59:41.\n",
      "  Batch   120  of  1,540.    Elapsed: 2:00:09.\n",
      "  Batch   125  of  1,540.    Elapsed: 2:00:40.\n",
      "  Batch   130  of  1,540.    Elapsed: 2:01:13.\n",
      "  Batch   135  of  1,540.    Elapsed: 2:01:41.\n",
      "  Batch   140  of  1,540.    Elapsed: 2:03:05.\n",
      "  Batch   145  of  1,540.    Elapsed: 2:04:01.\n",
      "  Batch   150  of  1,540.    Elapsed: 2:04:28.\n",
      "  Batch   155  of  1,540.    Elapsed: 2:04:58.\n",
      "  Batch   160  of  1,540.    Elapsed: 2:05:32.\n",
      "  Batch   165  of  1,540.    Elapsed: 2:06:03.\n",
      "  Batch   170  of  1,540.    Elapsed: 2:06:35.\n",
      "  Batch   175  of  1,540.    Elapsed: 2:07:45.\n",
      "  Batch   180  of  1,540.    Elapsed: 2:08:49.\n",
      "  Batch   185  of  1,540.    Elapsed: 2:09:15.\n",
      "  Batch   190  of  1,540.    Elapsed: 2:09:45.\n",
      "  Batch   195  of  1,540.    Elapsed: 2:10:12.\n",
      "  Batch   200  of  1,540.    Elapsed: 2:10:46.\n",
      "  Batch   205  of  1,540.    Elapsed: 2:11:15.\n",
      "  Batch   210  of  1,540.    Elapsed: 2:12:22.\n",
      "  Batch   215  of  1,540.    Elapsed: 2:13:17.\n",
      "  Batch   220  of  1,540.    Elapsed: 2:13:45.\n",
      "  Batch   225  of  1,540.    Elapsed: 2:14:10.\n",
      "  Batch   230  of  1,540.    Elapsed: 2:14:38.\n",
      "  Batch   235  of  1,540.    Elapsed: 2:15:05.\n",
      "  Batch   240  of  1,540.    Elapsed: 2:15:30.\n",
      "  Batch   245  of  1,540.    Elapsed: 2:16:31.\n",
      "  Batch   250  of  1,540.    Elapsed: 2:17:24.\n",
      "  Batch   255  of  1,540.    Elapsed: 2:17:51.\n",
      "  Batch   260  of  1,540.    Elapsed: 2:18:18.\n",
      "  Batch   265  of  1,540.    Elapsed: 2:18:46.\n",
      "  Batch   270  of  1,540.    Elapsed: 2:19:12.\n",
      "  Batch   275  of  1,540.    Elapsed: 2:19:39.\n",
      "  Batch   280  of  1,540.    Elapsed: 2:20:35.\n",
      "  Batch   285  of  1,540.    Elapsed: 2:21:31.\n",
      "  Batch   290  of  1,540.    Elapsed: 2:21:57.\n",
      "  Batch   295  of  1,540.    Elapsed: 2:22:28.\n",
      "  Batch   300  of  1,540.    Elapsed: 2:22:55.\n",
      "  Batch   305  of  1,540.    Elapsed: 2:23:23.\n",
      "  Batch   310  of  1,540.    Elapsed: 2:23:51.\n",
      "  Batch   315  of  1,540.    Elapsed: 2:24:52.\n",
      "  Batch   320  of  1,540.    Elapsed: 2:25:51.\n",
      "  Batch   325  of  1,540.    Elapsed: 2:26:22.\n",
      "  Batch   330  of  1,540.    Elapsed: 2:26:48.\n",
      "  Batch   335  of  1,540.    Elapsed: 2:27:17.\n",
      "  Batch   340  of  1,540.    Elapsed: 2:27:44.\n",
      "  Batch   345  of  1,540.    Elapsed: 2:28:11.\n",
      "  Batch   350  of  1,540.    Elapsed: 2:29:17.\n",
      "  Batch   355  of  1,540.    Elapsed: 2:30:04.\n",
      "  Batch   360  of  1,540.    Elapsed: 2:30:30.\n",
      "  Batch   365  of  1,540.    Elapsed: 2:30:57.\n",
      "  Batch   370  of  1,540.    Elapsed: 2:31:23.\n",
      "  Batch   375  of  1,540.    Elapsed: 2:31:50.\n",
      "  Batch   380  of  1,540.    Elapsed: 2:32:20.\n",
      "  Batch   385  of  1,540.    Elapsed: 2:33:11.\n",
      "  Batch   390  of  1,540.    Elapsed: 2:34:07.\n",
      "  Batch   395  of  1,540.    Elapsed: 2:34:35.\n",
      "  Batch   400  of  1,540.    Elapsed: 2:35:02.\n",
      "  Batch   405  of  1,540.    Elapsed: 2:35:28.\n",
      "  Batch   410  of  1,540.    Elapsed: 2:35:56.\n",
      "  Batch   415  of  1,540.    Elapsed: 2:36:23.\n",
      "  Batch   420  of  1,540.    Elapsed: 2:37:23.\n",
      "  Batch   425  of  1,540.    Elapsed: 2:38:05.\n",
      "  Batch   430  of  1,540.    Elapsed: 2:38:31.\n",
      "  Batch   435  of  1,540.    Elapsed: 2:39:02.\n",
      "  Batch   440  of  1,540.    Elapsed: 2:39:28.\n",
      "  Batch   445  of  1,540.    Elapsed: 2:39:54.\n",
      "  Batch   450  of  1,540.    Elapsed: 2:40:22.\n",
      "  Batch   455  of  1,540.    Elapsed: 2:41:37.\n",
      "  Batch   460  of  1,540.    Elapsed: 2:42:14.\n",
      "  Batch   465  of  1,540.    Elapsed: 2:42:41.\n",
      "  Batch   470  of  1,540.    Elapsed: 2:43:09.\n",
      "  Batch   475  of  1,540.    Elapsed: 2:43:35.\n",
      "  Batch   480  of  1,540.    Elapsed: 2:44:02.\n",
      "  Batch   485  of  1,540.    Elapsed: 2:44:29.\n",
      "  Batch   490  of  1,540.    Elapsed: 2:45:28.\n",
      "  Batch   495  of  1,540.    Elapsed: 2:46:22.\n",
      "  Batch   500  of  1,540.    Elapsed: 2:46:53.\n",
      "  Batch   505  of  1,540.    Elapsed: 2:47:20.\n",
      "  Batch   510  of  1,540.    Elapsed: 2:47:46.\n",
      "  Batch   515  of  1,540.    Elapsed: 2:48:12.\n",
      "  Batch   520  of  1,540.    Elapsed: 2:48:40.\n",
      "  Batch   525  of  1,540.    Elapsed: 2:49:56.\n",
      "  Batch   530  of  1,540.    Elapsed: 2:50:32.\n",
      "  Batch   535  of  1,540.    Elapsed: 2:50:58.\n",
      "  Batch   540  of  1,540.    Elapsed: 2:51:25.\n",
      "  Batch   545  of  1,540.    Elapsed: 2:51:50.\n",
      "  Batch   550  of  1,540.    Elapsed: 2:52:17.\n",
      "  Batch   555  of  1,540.    Elapsed: 2:52:45.\n",
      "  Batch   560  of  1,540.    Elapsed: 2:53:53.\n",
      "  Batch   565  of  1,540.    Elapsed: 2:54:26.\n",
      "  Batch   570  of  1,540.    Elapsed: 2:54:54.\n",
      "  Batch   575  of  1,540.    Elapsed: 2:55:22.\n",
      "  Batch   580  of  1,540.    Elapsed: 2:55:47.\n",
      "  Batch   585  of  1,540.    Elapsed: 2:56:13.\n",
      "  Batch   590  of  1,540.    Elapsed: 2:56:48.\n",
      "  Batch   595  of  1,540.    Elapsed: 2:58:05.\n",
      "  Batch   600  of  1,540.    Elapsed: 2:58:34.\n",
      "  Batch   605  of  1,540.    Elapsed: 2:59:01.\n",
      "  Batch   610  of  1,540.    Elapsed: 2:59:27.\n",
      "  Batch   615  of  1,540.    Elapsed: 2:59:55.\n",
      "  Batch   620  of  1,540.    Elapsed: 3:00:22.\n",
      "  Batch   625  of  1,540.    Elapsed: 3:00:58.\n",
      "  Batch   630  of  1,540.    Elapsed: 3:02:11.\n",
      "  Batch   635  of  1,540.    Elapsed: 3:02:38.\n",
      "  Batch   640  of  1,540.    Elapsed: 3:03:04.\n",
      "  Batch   645  of  1,540.    Elapsed: 3:03:30.\n",
      "  Batch   650  of  1,540.    Elapsed: 3:03:57.\n",
      "  Batch   655  of  1,540.    Elapsed: 3:04:22.\n",
      "  Batch   660  of  1,540.    Elapsed: 3:05:00.\n",
      "  Batch   665  of  1,540.    Elapsed: 3:06:06.\n",
      "  Batch   670  of  1,540.    Elapsed: 3:06:32.\n",
      "  Batch   675  of  1,540.    Elapsed: 3:06:58.\n",
      "  Batch   680  of  1,540.    Elapsed: 3:07:24.\n",
      "  Batch   685  of  1,540.    Elapsed: 3:07:51.\n",
      "  Batch   690  of  1,540.    Elapsed: 3:08:18.\n",
      "  Batch   695  of  1,540.    Elapsed: 3:09:11.\n",
      "  Batch   700  of  1,540.    Elapsed: 3:09:59.\n",
      "  Batch   705  of  1,540.    Elapsed: 3:10:22.\n",
      "  Batch   710  of  1,540.    Elapsed: 3:10:46.\n",
      "  Batch   715  of  1,540.    Elapsed: 3:11:10.\n",
      "  Batch   720  of  1,540.    Elapsed: 3:11:35.\n",
      "  Batch   725  of  1,540.    Elapsed: 3:11:59.\n",
      "  Batch   730  of  1,540.    Elapsed: 3:12:32.\n",
      "  Batch   735  of  1,540.    Elapsed: 3:13:26.\n",
      "  Batch   740  of  1,540.    Elapsed: 3:13:49.\n",
      "  Batch   745  of  1,540.    Elapsed: 3:14:13.\n",
      "  Batch   750  of  1,540.    Elapsed: 3:14:45.\n",
      "  Batch   755  of  1,540.    Elapsed: 3:25:52.\n",
      "  Batch   760  of  1,540.    Elapsed: 3:26:26.\n",
      "  Batch   765  of  1,540.    Elapsed: 3:27:24.\n",
      "  Batch   770  of  1,540.    Elapsed: 3:28:32.\n",
      "  Batch   775  of  1,540.    Elapsed: 3:29:02.\n",
      "  Batch   780  of  1,540.    Elapsed: 3:29:27.\n",
      "  Batch   785  of  1,540.    Elapsed: 3:29:51.\n",
      "  Batch   790  of  1,540.    Elapsed: 3:30:15.\n",
      "  Batch   795  of  1,540.    Elapsed: 3:30:41.\n",
      "  Batch   800  of  1,540.    Elapsed: 3:31:11.\n",
      "  Batch   805  of  1,540.    Elapsed: 3:32:07.\n",
      "  Batch   810  of  1,540.    Elapsed: 3:32:33.\n",
      "  Batch   815  of  1,540.    Elapsed: 3:33:02.\n",
      "  Batch   820  of  1,540.    Elapsed: 3:33:38.\n",
      "  Batch   825  of  1,540.    Elapsed: 3:34:02.\n",
      "  Batch   830  of  1,540.    Elapsed: 3:34:30.\n",
      "  Batch   835  of  1,540.    Elapsed: 3:35:10.\n",
      "  Batch   840  of  1,540.    Elapsed: 3:36:01.\n",
      "  Batch   845  of  1,540.    Elapsed: 3:36:25.\n",
      "  Batch   850  of  1,540.    Elapsed: 3:37:01.\n",
      "  Batch   855  of  1,540.    Elapsed: 3:37:37.\n",
      "  Batch   860  of  1,540.    Elapsed: 3:38:07.\n",
      "  Batch   865  of  1,540.    Elapsed: 3:38:42.\n",
      "  Batch   870  of  1,540.    Elapsed: 3:40:18.\n",
      "  Batch   875  of  1,540.    Elapsed: 3:41:19.\n",
      "  Batch   880  of  1,540.    Elapsed: 3:41:47.\n",
      "  Batch   885  of  1,540.    Elapsed: 3:42:15.\n",
      "  Batch   890  of  1,540.    Elapsed: 3:42:44.\n",
      "  Batch   895  of  1,540.    Elapsed: 3:43:14.\n",
      "  Batch   900  of  1,540.    Elapsed: 3:43:51.\n",
      "  Batch   905  of  1,540.    Elapsed: 3:45:31.\n",
      "  Batch   910  of  1,540.    Elapsed: 3:46:11.\n",
      "  Batch   915  of  1,540.    Elapsed: 3:46:40.\n",
      "  Batch   920  of  1,540.    Elapsed: 3:47:17.\n",
      "  Batch   925  of  1,540.    Elapsed: 3:47:48.\n",
      "  Batch   930  of  1,540.    Elapsed: 3:48:16.\n",
      "  Batch   935  of  1,540.    Elapsed: 3:48:45.\n",
      "  Batch   940  of  1,540.    Elapsed: 3:50:36.\n",
      "  Batch   945  of  1,540.    Elapsed: 3:51:28.\n",
      "  Batch   950  of  1,540.    Elapsed: 3:51:57.\n",
      "  Batch   955  of  1,540.    Elapsed: 3:52:22.\n",
      "  Batch   960  of  1,540.    Elapsed: 3:52:48.\n",
      "  Batch   965  of  1,540.    Elapsed: 3:53:27.\n",
      "  Batch   970  of  1,540.    Elapsed: 3:54:01.\n",
      "  Batch   975  of  1,540.    Elapsed: 3:55:35.\n",
      "  Batch   980  of  1,540.    Elapsed: 3:57:01.\n",
      "  Batch   985  of  1,540.    Elapsed: 3:57:43.\n",
      "  Batch   990  of  1,540.    Elapsed: 3:58:09.\n",
      "  Batch   995  of  1,540.    Elapsed: 3:58:34.\n",
      "  Batch 1,000  of  1,540.    Elapsed: 3:59:01.\n",
      "  Batch 1,005  of  1,540.    Elapsed: 3:59:40.\n",
      "  Batch 1,010  of  1,540.    Elapsed: 4:00:46.\n",
      "  Batch 1,015  of  1,540.    Elapsed: 4:02:07.\n",
      "  Batch 1,020  of  1,540.    Elapsed: 4:02:32.\n",
      "  Batch 1,025  of  1,540.    Elapsed: 4:02:59.\n",
      "  Batch 1,030  of  1,540.    Elapsed: 4:03:25.\n",
      "  Batch 1,035  of  1,540.    Elapsed: 4:03:53.\n",
      "  Batch 1,040  of  1,540.    Elapsed: 4:04:28.\n",
      "  Batch 1,045  of  1,540.    Elapsed: 4:05:41.\n",
      "  Batch 1,050  of  1,540.    Elapsed: 4:07:32.\n",
      "  Batch 1,055  of  1,540.    Elapsed: 4:08:07.\n",
      "  Batch 1,060  of  1,540.    Elapsed: 4:08:40.\n",
      "  Batch 1,065  of  1,540.    Elapsed: 4:09:16.\n",
      "  Batch 1,070  of  1,540.    Elapsed: 4:09:55.\n",
      "  Batch 1,075  of  1,540.    Elapsed: 4:10:28.\n",
      "  Batch 1,080  of  1,540.    Elapsed: 4:11:21.\n",
      "  Batch 1,085  of  1,540.    Elapsed: 4:12:54.\n",
      "  Batch 1,090  of  1,540.    Elapsed: 4:13:24.\n",
      "  Batch 1,095  of  1,540.    Elapsed: 4:13:55.\n",
      "  Batch 1,100  of  1,540.    Elapsed: 4:14:27.\n",
      "  Batch 1,105  of  1,540.    Elapsed: 4:15:04.\n",
      "  Batch 1,110  of  1,540.    Elapsed: 4:15:37.\n",
      "  Batch 1,115  of  1,540.    Elapsed: 4:17:13.\n",
      "  Batch 1,120  of  1,540.    Elapsed: 4:18:23.\n",
      "  Batch 1,125  of  1,540.    Elapsed: 4:18:55.\n",
      "  Batch 1,130  of  1,540.    Elapsed: 4:19:28.\n",
      "  Batch 1,135  of  1,540.    Elapsed: 4:19:58.\n",
      "  Batch 1,140  of  1,540.    Elapsed: 4:20:27.\n",
      "  Batch 1,145  of  1,540.    Elapsed: 4:20:56.\n",
      "  Batch 1,150  of  1,540.    Elapsed: 4:22:40.\n",
      "  Batch 1,155  of  1,540.    Elapsed: 4:23:58.\n",
      "  Batch 1,160  of  1,540.    Elapsed: 4:24:24.\n",
      "  Batch 1,165  of  1,540.    Elapsed: 4:24:53.\n",
      "  Batch 1,170  of  1,540.    Elapsed: 4:25:21.\n",
      "  Batch 1,175  of  1,540.    Elapsed: 4:25:49.\n",
      "  Batch 1,180  of  1,540.    Elapsed: 4:26:17.\n",
      "  Batch 1,185  of  1,540.    Elapsed: 4:27:08.\n",
      "  Batch 1,190  of  1,540.    Elapsed: 4:28:30.\n",
      "  Batch 1,195  of  1,540.    Elapsed: 4:28:57.\n",
      "  Batch 1,200  of  1,540.    Elapsed: 4:29:28.\n",
      "  Batch 1,205  of  1,540.    Elapsed: 4:29:55.\n",
      "  Batch 1,210  of  1,540.    Elapsed: 4:30:22.\n",
      "  Batch 1,215  of  1,540.    Elapsed: 4:30:50.\n",
      "  Batch 1,220  of  1,540.    Elapsed: 4:32:08.\n",
      "  Batch 1,225  of  1,540.    Elapsed: 4:32:54.\n",
      "  Batch 1,230  of  1,540.    Elapsed: 4:33:22.\n",
      "  Batch 1,235  of  1,540.    Elapsed: 4:33:52.\n",
      "  Batch 1,240  of  1,540.    Elapsed: 4:34:17.\n",
      "  Batch 1,245  of  1,540.    Elapsed: 4:34:44.\n",
      "  Batch 1,250  of  1,540.    Elapsed: 4:35:11.\n",
      "  Batch 1,255  of  1,540.    Elapsed: 4:36:06.\n",
      "  Batch 1,260  of  1,540.    Elapsed: 4:37:04.\n",
      "  Batch 1,265  of  1,540.    Elapsed: 4:37:30.\n",
      "  Batch 1,270  of  1,540.    Elapsed: 4:37:57.\n",
      "  Batch 1,275  of  1,540.    Elapsed: 4:38:22.\n",
      "  Batch 1,280  of  1,540.    Elapsed: 4:38:50.\n",
      "  Batch 1,285  of  1,540.    Elapsed: 4:39:15.\n",
      "  Batch 1,290  of  1,540.    Elapsed: 4:40:13.\n",
      "  Batch 1,295  of  1,540.    Elapsed: 4:41:12.\n",
      "  Batch 1,300  of  1,540.    Elapsed: 4:41:37.\n",
      "  Batch 1,305  of  1,540.    Elapsed: 4:42:04.\n",
      "  Batch 1,310  of  1,540.    Elapsed: 4:42:35.\n",
      "  Batch 1,315  of  1,540.    Elapsed: 4:43:05.\n",
      "  Batch 1,320  of  1,540.    Elapsed: 4:43:40.\n",
      "  Batch 1,325  of  1,540.    Elapsed: 4:45:03.\n",
      "  Batch 1,330  of  1,540.    Elapsed: 4:45:35.\n",
      "  Batch 1,335  of  1,540.    Elapsed: 4:46:03.\n",
      "  Batch 1,340  of  1,540.    Elapsed: 4:46:29.\n",
      "  Batch 1,345  of  1,540.    Elapsed: 4:46:56.\n",
      "  Batch 1,350  of  1,540.    Elapsed: 4:47:22.\n",
      "  Batch 1,355  of  1,540.    Elapsed: 4:47:56.\n",
      "  Batch 1,360  of  1,540.    Elapsed: 4:49:19.\n",
      "  Batch 1,365  of  1,540.    Elapsed: 4:49:44.\n",
      "  Batch 1,370  of  1,540.    Elapsed: 4:50:11.\n",
      "  Batch 1,375  of  1,540.    Elapsed: 4:50:40.\n",
      "  Batch 1,380  of  1,540.    Elapsed: 4:51:12.\n",
      "  Batch 1,385  of  1,540.    Elapsed: 4:51:41.\n",
      "  Batch 1,390  of  1,540.    Elapsed: 4:52:12.\n",
      "  Batch 1,395  of  1,540.    Elapsed: 4:53:41.\n",
      "  Batch 1,400  of  1,540.    Elapsed: 4:54:09.\n",
      "  Batch 1,405  of  1,540.    Elapsed: 4:54:35.\n",
      "  Batch 1,410  of  1,540.    Elapsed: 4:55:00.\n",
      "  Batch 1,415  of  1,540.    Elapsed: 4:55:25.\n",
      "  Batch 1,420  of  1,540.    Elapsed: 4:55:51.\n",
      "  Batch 1,425  of  1,540.    Elapsed: 4:56:17.\n",
      "  Batch 1,430  of  1,540.    Elapsed: 4:57:12.\n",
      "  Batch 1,435  of  1,540.    Elapsed: 4:58:10.\n",
      "  Batch 1,440  of  1,540.    Elapsed: 4:58:38.\n",
      "  Batch 1,445  of  1,540.    Elapsed: 4:59:04.\n",
      "  Batch 1,450  of  1,540.    Elapsed: 4:59:33.\n",
      "  Batch 1,455  of  1,540.    Elapsed: 5:00:04.\n",
      "  Batch 1,460  of  1,540.    Elapsed: 5:00:29.\n",
      "  Batch 1,465  of  1,540.    Elapsed: 5:01:48.\n",
      "  Batch 1,470  of  1,540.    Elapsed: 5:02:26.\n",
      "  Batch 1,475  of  1,540.    Elapsed: 5:02:52.\n",
      "  Batch 1,480  of  1,540.    Elapsed: 5:03:19.\n",
      "  Batch 1,485  of  1,540.    Elapsed: 5:03:46.\n",
      "  Batch 1,490  of  1,540.    Elapsed: 5:04:11.\n",
      "  Batch 1,495  of  1,540.    Elapsed: 5:04:40.\n",
      "  Batch 1,500  of  1,540.    Elapsed: 5:05:25.\n",
      "  Batch 1,505  of  1,540.    Elapsed: 5:06:16.\n",
      "  Batch 1,510  of  1,540.    Elapsed: 5:06:42.\n",
      "  Batch 1,515  of  1,540.    Elapsed: 5:07:09.\n",
      "  Batch 1,520  of  1,540.    Elapsed: 5:07:35.\n",
      "  Batch 1,525  of  1,540.    Elapsed: 5:08:00.\n",
      "  Batch 1,530  of  1,540.    Elapsed: 5:08:26.\n",
      "  Batch 1,535  of  1,540.    Elapsed: 5:09:21.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 5:10:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:04:04\n",
      "\n",
      "Training complete!\n",
      "--- 18860.537353038788 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22120463061206913]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "1s1PSnGhlmoS"
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists for the test data\n",
    "sentences = df_test.Abstract.values\n",
    "labels = df_test.InfoTheory.values\n",
    "\n",
    "input_ids = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745,\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Setting the batch size  \n",
    "batch_size = 32  \n",
    "\n",
    "#DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test data\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "  logits = outputs[0]\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 3616 of 19678 (18.38%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df_test.InfoTheory.sum(), len(df_test.InfoTheory), (df_test.InfoTheory.sum() / len(df_test.InfoTheory) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.927\n"
     ]
    }
   ],
   "source": [
    "# Combining the predictions for every batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/BERT/InfoTheory/full/L16_B32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/BERT/InfoTheory/full/L16_B32/vocab.txt',\n",
       " './model_save/BERT/InfoTheory/full/L16_B32/special_tokens_map.json',\n",
       " './model_save/BERT/InfoTheory/full/L16_B32/added_tokens.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/BERT/InfoTheory/full/L16_B32'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"./model_save/BERT/InfoTheory/full/L16_B32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15454   608]\n",
      " [  829  2787]]\n",
      "Accuracy: 0.9269742860046752\n",
      "Macro Precision: 0.8850005567033825\n",
      "Macro Recall: 0.866443916025622\n",
      "Macro F1 score:0.875304551452492\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT L16_B32 on all the CompVis records using BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Token IDs: [101, 9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012, 102]\n",
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n",
      "--- 170.43667006492615 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Tokenizing all of the abstracts and map the tokenized words to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "for abstract in Abstracts:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745, #we got the max len 743 and after 2 special tokens\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)\n",
    "\n",
    "print('Original: ', Abstracts[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "\n",
    "#attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    # If a token ID is 0, setting the mask to 0.\n",
    "    # If a token ID is > 0, setting the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_test_split to split our data into train and validation sets for the training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using 90% for training and 10% for validation for both training data and attention masks\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, CompVis_labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, CompVis_labels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "\n",
    "# Converting all inputs and labels into torch tensors which is the required datatypefor our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for the validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of  1,540.    Elapsed: 0:00:24.\n",
      "  Batch    10  of  1,540.    Elapsed: 0:00:42.\n",
      "  Batch    15  of  1,540.    Elapsed: 0:01:01.\n",
      "  Batch    20  of  1,540.    Elapsed: 0:01:21.\n",
      "  Batch    25  of  1,540.    Elapsed: 0:01:43.\n",
      "  Batch    30  of  1,540.    Elapsed: 0:02:06.\n",
      "  Batch    35  of  1,540.    Elapsed: 0:02:28.\n",
      "  Batch    40  of  1,540.    Elapsed: 0:03:00.\n",
      "  Batch    45  of  1,540.    Elapsed: 0:03:21.\n",
      "  Batch    50  of  1,540.    Elapsed: 0:03:45.\n",
      "  Batch    55  of  1,540.    Elapsed: 0:04:07.\n",
      "  Batch    60  of  1,540.    Elapsed: 0:04:28.\n",
      "  Batch    65  of  1,540.    Elapsed: 0:04:51.\n",
      "  Batch    70  of  1,540.    Elapsed: 0:05:14.\n",
      "  Batch    75  of  1,540.    Elapsed: 0:05:39.\n",
      "  Batch    80  of  1,540.    Elapsed: 0:06:01.\n",
      "  Batch    85  of  1,540.    Elapsed: 0:06:21.\n",
      "  Batch    90  of  1,540.    Elapsed: 0:06:40.\n",
      "  Batch    95  of  1,540.    Elapsed: 0:07:00.\n",
      "  Batch   100  of  1,540.    Elapsed: 0:07:22.\n",
      "  Batch   105  of  1,540.    Elapsed: 0:07:43.\n",
      "  Batch   110  of  1,540.    Elapsed: 0:08:04.\n",
      "  Batch   115  of  1,540.    Elapsed: 0:08:23.\n",
      "  Batch   120  of  1,540.    Elapsed: 0:08:42.\n",
      "  Batch   125  of  1,540.    Elapsed: 0:09:02.\n",
      "  Batch   130  of  1,540.    Elapsed: 0:09:22.\n",
      "  Batch   135  of  1,540.    Elapsed: 0:09:42.\n",
      "  Batch   140  of  1,540.    Elapsed: 0:10:02.\n",
      "  Batch   145  of  1,540.    Elapsed: 0:10:21.\n",
      "  Batch   150  of  1,540.    Elapsed: 0:10:41.\n",
      "  Batch   155  of  1,540.    Elapsed: 0:11:01.\n",
      "  Batch   160  of  1,540.    Elapsed: 0:11:22.\n",
      "  Batch   165  of  1,540.    Elapsed: 0:11:45.\n",
      "  Batch   170  of  1,540.    Elapsed: 0:12:05.\n",
      "  Batch   175  of  1,540.    Elapsed: 0:12:25.\n",
      "  Batch   180  of  1,540.    Elapsed: 0:12:47.\n",
      "  Batch   185  of  1,540.    Elapsed: 0:13:08.\n",
      "  Batch   190  of  1,540.    Elapsed: 0:13:28.\n",
      "  Batch   195  of  1,540.    Elapsed: 0:13:48.\n",
      "  Batch   200  of  1,540.    Elapsed: 0:14:10.\n",
      "  Batch   205  of  1,540.    Elapsed: 0:14:33.\n",
      "  Batch   210  of  1,540.    Elapsed: 0:14:54.\n",
      "  Batch   215  of  1,540.    Elapsed: 0:15:15.\n",
      "  Batch   220  of  1,540.    Elapsed: 0:15:35.\n",
      "  Batch   225  of  1,540.    Elapsed: 0:15:57.\n",
      "  Batch   230  of  1,540.    Elapsed: 0:16:19.\n",
      "  Batch   235  of  1,540.    Elapsed: 0:16:41.\n",
      "  Batch   240  of  1,540.    Elapsed: 0:17:03.\n",
      "  Batch   245  of  1,540.    Elapsed: 0:17:25.\n",
      "  Batch   250  of  1,540.    Elapsed: 0:17:46.\n",
      "  Batch   255  of  1,540.    Elapsed: 0:18:08.\n",
      "  Batch   260  of  1,540.    Elapsed: 0:18:29.\n",
      "  Batch   265  of  1,540.    Elapsed: 0:18:51.\n",
      "  Batch   270  of  1,540.    Elapsed: 0:19:12.\n",
      "  Batch   275  of  1,540.    Elapsed: 0:19:34.\n",
      "  Batch   280  of  1,540.    Elapsed: 0:19:56.\n",
      "  Batch   285  of  1,540.    Elapsed: 0:20:17.\n",
      "  Batch   290  of  1,540.    Elapsed: 0:20:39.\n",
      "  Batch   295  of  1,540.    Elapsed: 0:21:00.\n",
      "  Batch   300  of  1,540.    Elapsed: 0:21:22.\n",
      "  Batch   305  of  1,540.    Elapsed: 0:21:44.\n",
      "  Batch   310  of  1,540.    Elapsed: 0:22:05.\n",
      "  Batch   315  of  1,540.    Elapsed: 0:22:27.\n",
      "  Batch   320  of  1,540.    Elapsed: 0:22:48.\n",
      "  Batch   325  of  1,540.    Elapsed: 0:23:11.\n",
      "  Batch   330  of  1,540.    Elapsed: 0:23:32.\n",
      "  Batch   335  of  1,540.    Elapsed: 0:23:54.\n",
      "  Batch   340  of  1,540.    Elapsed: 0:24:15.\n",
      "  Batch   345  of  1,540.    Elapsed: 0:24:37.\n",
      "  Batch   350  of  1,540.    Elapsed: 0:24:58.\n",
      "  Batch   355  of  1,540.    Elapsed: 0:25:19.\n",
      "  Batch   360  of  1,540.    Elapsed: 0:25:41.\n",
      "  Batch   365  of  1,540.    Elapsed: 0:26:02.\n",
      "  Batch   370  of  1,540.    Elapsed: 0:26:23.\n",
      "  Batch   375  of  1,540.    Elapsed: 0:26:45.\n",
      "  Batch   380  of  1,540.    Elapsed: 0:27:06.\n",
      "  Batch   385  of  1,540.    Elapsed: 0:27:27.\n",
      "  Batch   390  of  1,540.    Elapsed: 0:27:49.\n",
      "  Batch   395  of  1,540.    Elapsed: 0:28:10.\n",
      "  Batch   400  of  1,540.    Elapsed: 0:28:31.\n",
      "  Batch   405  of  1,540.    Elapsed: 0:28:52.\n",
      "  Batch   410  of  1,540.    Elapsed: 0:29:13.\n",
      "  Batch   415  of  1,540.    Elapsed: 0:29:34.\n",
      "  Batch   420  of  1,540.    Elapsed: 0:29:55.\n",
      "  Batch   425  of  1,540.    Elapsed: 0:30:19.\n",
      "  Batch   430  of  1,540.    Elapsed: 0:30:39.\n",
      "  Batch   435  of  1,540.    Elapsed: 0:31:01.\n",
      "  Batch   440  of  1,540.    Elapsed: 0:31:22.\n",
      "  Batch   445  of  1,540.    Elapsed: 0:31:43.\n",
      "  Batch   450  of  1,540.    Elapsed: 0:32:04.\n",
      "  Batch   455  of  1,540.    Elapsed: 0:32:25.\n",
      "  Batch   460  of  1,540.    Elapsed: 0:32:46.\n",
      "  Batch   465  of  1,540.    Elapsed: 0:33:07.\n",
      "  Batch   470  of  1,540.    Elapsed: 0:33:28.\n",
      "  Batch   475  of  1,540.    Elapsed: 0:33:49.\n",
      "  Batch   480  of  1,540.    Elapsed: 0:34:10.\n",
      "  Batch   485  of  1,540.    Elapsed: 0:34:31.\n",
      "  Batch   490  of  1,540.    Elapsed: 0:34:51.\n",
      "  Batch   495  of  1,540.    Elapsed: 0:35:13.\n",
      "  Batch   500  of  1,540.    Elapsed: 0:35:34.\n",
      "  Batch   505  of  1,540.    Elapsed: 0:35:56.\n",
      "  Batch   510  of  1,540.    Elapsed: 0:36:20.\n",
      "  Batch   515  of  1,540.    Elapsed: 0:36:41.\n",
      "  Batch   520  of  1,540.    Elapsed: 0:37:02.\n",
      "  Batch   525  of  1,540.    Elapsed: 0:37:23.\n",
      "  Batch   530  of  1,540.    Elapsed: 0:37:44.\n",
      "  Batch   535  of  1,540.    Elapsed: 0:38:05.\n",
      "  Batch   540  of  1,540.    Elapsed: 0:38:26.\n",
      "  Batch   545  of  1,540.    Elapsed: 0:38:47.\n",
      "  Batch   550  of  1,540.    Elapsed: 0:39:08.\n",
      "  Batch   555  of  1,540.    Elapsed: 0:39:28.\n",
      "  Batch   560  of  1,540.    Elapsed: 0:39:49.\n",
      "  Batch   565  of  1,540.    Elapsed: 0:40:09.\n",
      "  Batch   570  of  1,540.    Elapsed: 0:40:30.\n",
      "  Batch   575  of  1,540.    Elapsed: 0:40:51.\n",
      "  Batch   580  of  1,540.    Elapsed: 0:41:12.\n",
      "  Batch   585  of  1,540.    Elapsed: 0:41:32.\n",
      "  Batch   590  of  1,540.    Elapsed: 0:41:53.\n",
      "  Batch   595  of  1,540.    Elapsed: 0:42:14.\n",
      "  Batch   600  of  1,540.    Elapsed: 0:42:34.\n",
      "  Batch   605  of  1,540.    Elapsed: 0:42:55.\n",
      "  Batch   610  of  1,540.    Elapsed: 0:43:16.\n",
      "  Batch   615  of  1,540.    Elapsed: 0:43:36.\n",
      "  Batch   620  of  1,540.    Elapsed: 0:43:57.\n",
      "  Batch   625  of  1,540.    Elapsed: 0:44:17.\n",
      "  Batch   630  of  1,540.    Elapsed: 0:44:38.\n",
      "  Batch   635  of  1,540.    Elapsed: 0:44:58.\n",
      "  Batch   640  of  1,540.    Elapsed: 0:45:19.\n",
      "  Batch   645  of  1,540.    Elapsed: 0:45:39.\n",
      "  Batch   650  of  1,540.    Elapsed: 0:45:59.\n",
      "  Batch   655  of  1,540.    Elapsed: 0:46:20.\n",
      "  Batch   660  of  1,540.    Elapsed: 0:46:45.\n",
      "  Batch   665  of  1,540.    Elapsed: 0:47:04.\n",
      "  Batch   670  of  1,540.    Elapsed: 0:47:23.\n",
      "  Batch   675  of  1,540.    Elapsed: 0:47:42.\n",
      "  Batch   680  of  1,540.    Elapsed: 0:48:02.\n",
      "  Batch   685  of  1,540.    Elapsed: 0:48:21.\n",
      "  Batch   690  of  1,540.    Elapsed: 0:48:40.\n",
      "  Batch   695  of  1,540.    Elapsed: 0:48:59.\n",
      "  Batch   700  of  1,540.    Elapsed: 0:49:18.\n",
      "  Batch   705  of  1,540.    Elapsed: 0:49:36.\n",
      "  Batch   710  of  1,540.    Elapsed: 0:49:56.\n",
      "  Batch   715  of  1,540.    Elapsed: 0:50:14.\n",
      "  Batch   720  of  1,540.    Elapsed: 0:50:33.\n",
      "  Batch   725  of  1,540.    Elapsed: 0:50:52.\n",
      "  Batch   730  of  1,540.    Elapsed: 0:51:15.\n",
      "  Batch   735  of  1,540.    Elapsed: 0:51:37.\n",
      "  Batch   740  of  1,540.    Elapsed: 0:52:00.\n",
      "  Batch   745  of  1,540.    Elapsed: 0:52:20.\n",
      "  Batch   750  of  1,540.    Elapsed: 0:52:40.\n",
      "  Batch   755  of  1,540.    Elapsed: 0:53:01.\n",
      "  Batch   760  of  1,540.    Elapsed: 0:53:21.\n",
      "  Batch   765  of  1,540.    Elapsed: 0:53:41.\n",
      "  Batch   770  of  1,540.    Elapsed: 0:54:01.\n",
      "  Batch   775  of  1,540.    Elapsed: 0:54:21.\n",
      "  Batch   780  of  1,540.    Elapsed: 0:54:40.\n",
      "  Batch   785  of  1,540.    Elapsed: 0:55:00.\n",
      "  Batch   790  of  1,540.    Elapsed: 0:55:20.\n",
      "  Batch   795  of  1,540.    Elapsed: 0:55:40.\n",
      "  Batch   800  of  1,540.    Elapsed: 0:56:00.\n",
      "  Batch   805  of  1,540.    Elapsed: 0:56:19.\n",
      "  Batch   810  of  1,540.    Elapsed: 0:56:39.\n",
      "  Batch   815  of  1,540.    Elapsed: 0:56:58.\n",
      "  Batch   820  of  1,540.    Elapsed: 0:57:18.\n",
      "  Batch   825  of  1,540.    Elapsed: 0:57:38.\n",
      "  Batch   830  of  1,540.    Elapsed: 0:57:58.\n",
      "  Batch   835  of  1,540.    Elapsed: 0:58:18.\n",
      "  Batch   840  of  1,540.    Elapsed: 0:58:37.\n",
      "  Batch   845  of  1,540.    Elapsed: 0:58:57.\n",
      "  Batch   850  of  1,540.    Elapsed: 0:59:17.\n",
      "  Batch   855  of  1,540.    Elapsed: 0:59:37.\n",
      "  Batch   860  of  1,540.    Elapsed: 0:59:57.\n",
      "  Batch   865  of  1,540.    Elapsed: 1:00:17.\n",
      "  Batch   870  of  1,540.    Elapsed: 1:00:37.\n",
      "  Batch   875  of  1,540.    Elapsed: 1:00:57.\n",
      "  Batch   880  of  1,540.    Elapsed: 1:01:17.\n",
      "  Batch   885  of  1,540.    Elapsed: 1:01:37.\n",
      "  Batch   890  of  1,540.    Elapsed: 1:01:58.\n",
      "  Batch   895  of  1,540.    Elapsed: 1:02:17.\n",
      "  Batch   900  of  1,540.    Elapsed: 1:02:38.\n",
      "  Batch   905  of  1,540.    Elapsed: 1:02:57.\n",
      "  Batch   910  of  1,540.    Elapsed: 1:03:18.\n",
      "  Batch   915  of  1,540.    Elapsed: 1:03:38.\n",
      "  Batch   920  of  1,540.    Elapsed: 1:03:58.\n",
      "  Batch   925  of  1,540.    Elapsed: 1:04:18.\n",
      "  Batch   930  of  1,540.    Elapsed: 1:04:38.\n",
      "  Batch   935  of  1,540.    Elapsed: 1:04:58.\n",
      "  Batch   940  of  1,540.    Elapsed: 1:05:17.\n",
      "  Batch   945  of  1,540.    Elapsed: 1:05:37.\n",
      "  Batch   950  of  1,540.    Elapsed: 1:05:57.\n",
      "  Batch   955  of  1,540.    Elapsed: 1:06:17.\n",
      "  Batch   960  of  1,540.    Elapsed: 1:06:37.\n",
      "  Batch   965  of  1,540.    Elapsed: 1:06:57.\n",
      "  Batch   970  of  1,540.    Elapsed: 1:07:16.\n",
      "  Batch   975  of  1,540.    Elapsed: 1:07:36.\n",
      "  Batch   980  of  1,540.    Elapsed: 1:07:56.\n",
      "  Batch   985  of  1,540.    Elapsed: 1:08:16.\n",
      "  Batch   990  of  1,540.    Elapsed: 1:08:36.\n",
      "  Batch   995  of  1,540.    Elapsed: 1:08:55.\n",
      "  Batch 1,000  of  1,540.    Elapsed: 1:09:15.\n",
      "  Batch 1,005  of  1,540.    Elapsed: 1:09:35.\n",
      "  Batch 1,010  of  1,540.    Elapsed: 1:09:55.\n",
      "  Batch 1,015  of  1,540.    Elapsed: 1:10:15.\n",
      "  Batch 1,020  of  1,540.    Elapsed: 1:10:35.\n",
      "  Batch 1,025  of  1,540.    Elapsed: 1:10:55.\n",
      "  Batch 1,030  of  1,540.    Elapsed: 1:11:15.\n",
      "  Batch 1,035  of  1,540.    Elapsed: 1:11:35.\n",
      "  Batch 1,040  of  1,540.    Elapsed: 1:11:55.\n",
      "  Batch 1,045  of  1,540.    Elapsed: 1:12:15.\n",
      "  Batch 1,050  of  1,540.    Elapsed: 1:12:35.\n",
      "  Batch 1,055  of  1,540.    Elapsed: 1:12:55.\n",
      "  Batch 1,060  of  1,540.    Elapsed: 1:13:15.\n",
      "  Batch 1,065  of  1,540.    Elapsed: 1:13:38.\n",
      "  Batch 1,070  of  1,540.    Elapsed: 1:14:02.\n",
      "  Batch 1,075  of  1,540.    Elapsed: 1:14:22.\n",
      "  Batch 1,080  of  1,540.    Elapsed: 1:14:42.\n",
      "  Batch 1,085  of  1,540.    Elapsed: 1:15:02.\n",
      "  Batch 1,090  of  1,540.    Elapsed: 1:15:22.\n",
      "  Batch 1,095  of  1,540.    Elapsed: 1:15:42.\n",
      "  Batch 1,100  of  1,540.    Elapsed: 1:16:03.\n",
      "  Batch 1,105  of  1,540.    Elapsed: 1:16:23.\n",
      "  Batch 1,110  of  1,540.    Elapsed: 1:16:43.\n",
      "  Batch 1,115  of  1,540.    Elapsed: 1:17:03.\n",
      "  Batch 1,120  of  1,540.    Elapsed: 1:17:23.\n",
      "  Batch 1,125  of  1,540.    Elapsed: 1:17:43.\n",
      "  Batch 1,130  of  1,540.    Elapsed: 1:18:03.\n",
      "  Batch 1,135  of  1,540.    Elapsed: 1:18:23.\n",
      "  Batch 1,140  of  1,540.    Elapsed: 1:18:43.\n",
      "  Batch 1,145  of  1,540.    Elapsed: 1:19:03.\n",
      "  Batch 1,150  of  1,540.    Elapsed: 1:19:23.\n",
      "  Batch 1,155  of  1,540.    Elapsed: 1:19:43.\n",
      "  Batch 1,160  of  1,540.    Elapsed: 1:20:03.\n",
      "  Batch 1,165  of  1,540.    Elapsed: 1:20:24.\n",
      "  Batch 1,170  of  1,540.    Elapsed: 1:20:43.\n",
      "  Batch 1,175  of  1,540.    Elapsed: 1:21:04.\n",
      "  Batch 1,180  of  1,540.    Elapsed: 1:21:24.\n",
      "  Batch 1,185  of  1,540.    Elapsed: 1:21:43.\n",
      "  Batch 1,190  of  1,540.    Elapsed: 1:22:03.\n",
      "  Batch 1,195  of  1,540.    Elapsed: 1:22:23.\n",
      "  Batch 1,200  of  1,540.    Elapsed: 1:22:43.\n",
      "  Batch 1,205  of  1,540.    Elapsed: 1:23:03.\n",
      "  Batch 1,210  of  1,540.    Elapsed: 1:23:23.\n",
      "  Batch 1,215  of  1,540.    Elapsed: 1:23:43.\n",
      "  Batch 1,220  of  1,540.    Elapsed: 1:24:03.\n",
      "  Batch 1,225  of  1,540.    Elapsed: 1:24:23.\n",
      "  Batch 1,230  of  1,540.    Elapsed: 1:24:43.\n",
      "  Batch 1,235  of  1,540.    Elapsed: 1:25:03.\n",
      "  Batch 1,240  of  1,540.    Elapsed: 1:25:23.\n",
      "  Batch 1,245  of  1,540.    Elapsed: 1:25:43.\n",
      "  Batch 1,250  of  1,540.    Elapsed: 1:26:03.\n",
      "  Batch 1,255  of  1,540.    Elapsed: 1:26:23.\n",
      "  Batch 1,260  of  1,540.    Elapsed: 1:26:42.\n",
      "  Batch 1,265  of  1,540.    Elapsed: 1:27:02.\n",
      "  Batch 1,270  of  1,540.    Elapsed: 1:27:22.\n",
      "  Batch 1,275  of  1,540.    Elapsed: 1:27:42.\n",
      "  Batch 1,280  of  1,540.    Elapsed: 1:28:02.\n",
      "  Batch 1,285  of  1,540.    Elapsed: 1:28:22.\n",
      "  Batch 1,290  of  1,540.    Elapsed: 1:28:42.\n",
      "  Batch 1,295  of  1,540.    Elapsed: 1:29:02.\n",
      "  Batch 1,300  of  1,540.    Elapsed: 1:29:21.\n",
      "  Batch 1,305  of  1,540.    Elapsed: 1:29:41.\n",
      "  Batch 1,310  of  1,540.    Elapsed: 1:30:01.\n",
      "  Batch 1,315  of  1,540.    Elapsed: 1:30:21.\n",
      "  Batch 1,320  of  1,540.    Elapsed: 1:30:41.\n",
      "  Batch 1,325  of  1,540.    Elapsed: 1:31:00.\n",
      "  Batch 1,330  of  1,540.    Elapsed: 1:31:20.\n",
      "  Batch 1,335  of  1,540.    Elapsed: 1:31:40.\n",
      "  Batch 1,340  of  1,540.    Elapsed: 1:32:00.\n",
      "  Batch 1,345  of  1,540.    Elapsed: 1:32:20.\n",
      "  Batch 1,350  of  1,540.    Elapsed: 1:32:40.\n",
      "  Batch 1,355  of  1,540.    Elapsed: 1:33:00.\n",
      "  Batch 1,360  of  1,540.    Elapsed: 1:33:19.\n",
      "  Batch 1,365  of  1,540.    Elapsed: 1:33:39.\n",
      "  Batch 1,370  of  1,540.    Elapsed: 1:33:59.\n",
      "  Batch 1,375  of  1,540.    Elapsed: 1:34:19.\n",
      "  Batch 1,380  of  1,540.    Elapsed: 1:34:39.\n",
      "  Batch 1,385  of  1,540.    Elapsed: 1:34:59.\n",
      "  Batch 1,390  of  1,540.    Elapsed: 1:35:19.\n",
      "  Batch 1,395  of  1,540.    Elapsed: 1:35:39.\n",
      "  Batch 1,400  of  1,540.    Elapsed: 1:36:00.\n",
      "  Batch 1,405  of  1,540.    Elapsed: 1:36:21.\n",
      "  Batch 1,410  of  1,540.    Elapsed: 1:36:41.\n",
      "  Batch 1,415  of  1,540.    Elapsed: 1:37:01.\n",
      "  Batch 1,420  of  1,540.    Elapsed: 1:37:21.\n",
      "  Batch 1,425  of  1,540.    Elapsed: 1:37:41.\n",
      "  Batch 1,430  of  1,540.    Elapsed: 1:38:01.\n",
      "  Batch 1,435  of  1,540.    Elapsed: 1:38:21.\n",
      "  Batch 1,440  of  1,540.    Elapsed: 1:38:41.\n",
      "  Batch 1,445  of  1,540.    Elapsed: 1:39:01.\n",
      "  Batch 1,450  of  1,540.    Elapsed: 1:39:21.\n",
      "  Batch 1,455  of  1,540.    Elapsed: 1:39:41.\n",
      "  Batch 1,460  of  1,540.    Elapsed: 1:40:01.\n",
      "  Batch 1,465  of  1,540.    Elapsed: 1:40:21.\n",
      "  Batch 1,470  of  1,540.    Elapsed: 1:40:41.\n",
      "  Batch 1,475  of  1,540.    Elapsed: 1:41:01.\n",
      "  Batch 1,480  of  1,540.    Elapsed: 1:41:26.\n",
      "  Batch 1,485  of  1,540.    Elapsed: 1:41:47.\n",
      "  Batch 1,490  of  1,540.    Elapsed: 1:42:08.\n",
      "  Batch 1,495  of  1,540.    Elapsed: 1:42:30.\n",
      "  Batch 1,500  of  1,540.    Elapsed: 1:42:54.\n",
      "  Batch 1,505  of  1,540.    Elapsed: 1:43:15.\n",
      "  Batch 1,510  of  1,540.    Elapsed: 1:43:36.\n",
      "  Batch 1,515  of  1,540.    Elapsed: 1:43:57.\n",
      "  Batch 1,520  of  1,540.    Elapsed: 1:44:18.\n",
      "  Batch 1,525  of  1,540.    Elapsed: 1:44:38.\n",
      "  Batch 1,530  of  1,540.    Elapsed: 1:44:59.\n",
      "  Batch 1,535  of  1,540.    Elapsed: 1:45:22.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 1:45:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation took: 0:03:18\n",
      "\n",
      "Training complete!\n",
      "--- 6537.916715145111 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing testing data\n",
    "sentences = df_test.Abstract.values\n",
    "labels = df_test.CompVis.values\n",
    "\n",
    "input_ids_test = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745,\n",
    "                   )\n",
    "    \n",
    "    input_ids_test.append(encoded_sent)\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 2152 of 19678 (10.94%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df_test.CompVis.sum(), len(df_test.CompVis), (df_test.CompVis.sum() / len(df_test.CompVis) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.891\n"
     ]
    }
   ],
   "source": [
    "# Combining the predictions for every batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/BERT/CompVis/full/L16_B32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/BERT/CompVis/full/L16_B32/vocab.txt',\n",
       " './model_save/BERT/CompVis/full/L16_B32/special_tokens_map.json',\n",
       " './model_save/BERT/CompVis/full/L16_B32/added_tokens.json')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/BERT/CompVis/full/L16_B32'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"./model_save/BERT/CompVis/full/L16_B32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17526     0]\n",
      " [ 2152     0]]\n",
      "Accuracy: 0.8906392926110377\n",
      "Macro Precision: 0.44531964630551885\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4710783786689603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"./model_save/BERT/CompVis/full/L16_B32\")\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT L16_B32 on all the Math records using BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Token IDs: [101, 9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012, 102]\n",
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n",
      "--- 184.9595546722412 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Tokenizing all of the abstracts and map the tokenized words to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "for abstract in Abstracts:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745, #we got the max len 743 and after 2 special tokens\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)\n",
    "\n",
    "print('Original: ', Abstracts[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "\n",
    "#attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    # If a token ID is 0, setting the mask to 0.\n",
    "    # If a token ID is > 0, setting the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_test_split to split our data into train and validation sets for the training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using 90% for training and 10% for validation for both training data and attention masks\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Math_labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, Math_labels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "\n",
    "# Converting all inputs and labels into torch tensors which is the required datatypefor our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for the validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of  1,540.    Elapsed: 0:00:27.\n",
      "  Batch    10  of  1,540.    Elapsed: 0:00:48.\n",
      "  Batch    15  of  1,540.    Elapsed: 0:01:08.\n",
      "  Batch    20  of  1,540.    Elapsed: 0:01:27.\n",
      "  Batch    25  of  1,540.    Elapsed: 0:01:46.\n",
      "  Batch    30  of  1,540.    Elapsed: 0:02:05.\n",
      "  Batch    35  of  1,540.    Elapsed: 0:02:25.\n",
      "  Batch    40  of  1,540.    Elapsed: 0:02:47.\n",
      "  Batch    45  of  1,540.    Elapsed: 0:03:10.\n",
      "  Batch    50  of  1,540.    Elapsed: 0:03:31.\n",
      "  Batch    55  of  1,540.    Elapsed: 0:03:53.\n",
      "  Batch    60  of  1,540.    Elapsed: 0:04:14.\n",
      "  Batch    65  of  1,540.    Elapsed: 0:04:37.\n",
      "  Batch    70  of  1,540.    Elapsed: 0:05:00.\n",
      "  Batch    75  of  1,540.    Elapsed: 0:05:21.\n",
      "  Batch    80  of  1,540.    Elapsed: 0:05:44.\n",
      "  Batch    85  of  1,540.    Elapsed: 0:06:06.\n",
      "  Batch    90  of  1,540.    Elapsed: 0:06:27.\n",
      "  Batch    95  of  1,540.    Elapsed: 0:06:47.\n",
      "  Batch   100  of  1,540.    Elapsed: 0:07:07.\n",
      "  Batch   105  of  1,540.    Elapsed: 0:07:26.\n",
      "  Batch   110  of  1,540.    Elapsed: 0:07:46.\n",
      "  Batch   115  of  1,540.    Elapsed: 0:08:05.\n",
      "  Batch   120  of  1,540.    Elapsed: 0:08:25.\n",
      "  Batch   125  of  1,540.    Elapsed: 0:08:47.\n",
      "  Batch   130  of  1,540.    Elapsed: 0:09:11.\n",
      "  Batch   135  of  1,540.    Elapsed: 0:09:34.\n",
      "  Batch   140  of  1,540.    Elapsed: 0:09:57.\n",
      "  Batch   145  of  1,540.    Elapsed: 0:10:22.\n",
      "  Batch   150  of  1,540.    Elapsed: 0:10:55.\n",
      "  Batch   155  of  1,540.    Elapsed: 0:11:20.\n",
      "  Batch   160  of  1,540.    Elapsed: 0:11:42.\n",
      "  Batch   165  of  1,540.    Elapsed: 0:12:04.\n",
      "  Batch   170  of  1,540.    Elapsed: 0:12:29.\n",
      "  Batch   175  of  1,540.    Elapsed: 0:12:50.\n",
      "  Batch   180  of  1,540.    Elapsed: 0:13:11.\n",
      "  Batch   185  of  1,540.    Elapsed: 0:13:31.\n",
      "  Batch   190  of  1,540.    Elapsed: 0:13:57.\n",
      "  Batch   195  of  1,540.    Elapsed: 0:14:18.\n",
      "  Batch   200  of  1,540.    Elapsed: 0:14:37.\n",
      "  Batch   205  of  1,540.    Elapsed: 0:14:59.\n",
      "  Batch   210  of  1,540.    Elapsed: 0:15:20.\n",
      "  Batch   215  of  1,540.    Elapsed: 0:15:39.\n",
      "  Batch   220  of  1,540.    Elapsed: 0:15:58.\n",
      "  Batch   225  of  1,540.    Elapsed: 0:16:18.\n",
      "  Batch   230  of  1,540.    Elapsed: 0:16:37.\n",
      "  Batch   235  of  1,540.    Elapsed: 0:16:57.\n",
      "  Batch   240  of  1,540.    Elapsed: 0:17:19.\n",
      "  Batch   245  of  1,540.    Elapsed: 0:17:42.\n",
      "  Batch   250  of  1,540.    Elapsed: 0:18:04.\n",
      "  Batch   255  of  1,540.    Elapsed: 0:18:25.\n",
      "  Batch   260  of  1,540.    Elapsed: 0:18:56.\n",
      "  Batch   265  of  1,540.    Elapsed: 0:19:18.\n",
      "  Batch   270  of  1,540.    Elapsed: 0:19:39.\n",
      "  Batch   275  of  1,540.    Elapsed: 0:20:01.\n",
      "  Batch   280  of  1,540.    Elapsed: 0:20:20.\n",
      "  Batch   285  of  1,540.    Elapsed: 0:20:39.\n",
      "  Batch   290  of  1,540.    Elapsed: 0:20:58.\n",
      "  Batch   295  of  1,540.    Elapsed: 0:21:19.\n",
      "  Batch   300  of  1,540.    Elapsed: 0:21:38.\n",
      "  Batch   305  of  1,540.    Elapsed: 0:22:02.\n",
      "  Batch   310  of  1,540.    Elapsed: 0:22:22.\n",
      "  Batch   315  of  1,540.    Elapsed: 0:22:41.\n",
      "  Batch   320  of  1,540.    Elapsed: 0:23:06.\n",
      "  Batch   325  of  1,540.    Elapsed: 0:23:53.\n",
      "  Batch   330  of  1,540.    Elapsed: 0:24:21.\n",
      "  Batch   335  of  1,540.    Elapsed: 0:24:42.\n",
      "  Batch   340  of  1,540.    Elapsed: 0:25:07.\n",
      "  Batch   345  of  1,540.    Elapsed: 0:25:32.\n",
      "  Batch   350  of  1,540.    Elapsed: 0:25:53.\n",
      "  Batch   355  of  1,540.    Elapsed: 0:26:14.\n",
      "  Batch   360  of  1,540.    Elapsed: 0:26:36.\n",
      "  Batch   365  of  1,540.    Elapsed: 0:26:57.\n",
      "  Batch   370  of  1,540.    Elapsed: 0:27:18.\n",
      "  Batch   375  of  1,540.    Elapsed: 0:27:39.\n",
      "  Batch   380  of  1,540.    Elapsed: 0:28:00.\n",
      "  Batch   385  of  1,540.    Elapsed: 0:28:20.\n",
      "  Batch   390  of  1,540.    Elapsed: 0:28:42.\n",
      "  Batch   395  of  1,540.    Elapsed: 0:29:03.\n",
      "  Batch   400  of  1,540.    Elapsed: 0:29:24.\n",
      "  Batch   405  of  1,540.    Elapsed: 0:29:45.\n",
      "  Batch   410  of  1,540.    Elapsed: 0:30:07.\n",
      "  Batch   415  of  1,540.    Elapsed: 0:30:27.\n",
      "  Batch   420  of  1,540.    Elapsed: 0:30:48.\n",
      "  Batch   425  of  1,540.    Elapsed: 0:31:11.\n",
      "  Batch   430  of  1,540.    Elapsed: 0:31:32.\n",
      "  Batch   435  of  1,540.    Elapsed: 0:31:54.\n",
      "  Batch   440  of  1,540.    Elapsed: 0:32:16.\n",
      "  Batch   445  of  1,540.    Elapsed: 0:32:38.\n",
      "  Batch   450  of  1,540.    Elapsed: 0:32:58.\n",
      "  Batch   455  of  1,540.    Elapsed: 0:33:18.\n",
      "  Batch   460  of  1,540.    Elapsed: 0:33:39.\n",
      "  Batch   465  of  1,540.    Elapsed: 0:34:00.\n",
      "  Batch   470  of  1,540.    Elapsed: 0:34:23.\n",
      "  Batch   475  of  1,540.    Elapsed: 0:34:43.\n",
      "  Batch   480  of  1,540.    Elapsed: 0:35:03.\n",
      "  Batch   485  of  1,540.    Elapsed: 0:35:24.\n",
      "  Batch   490  of  1,540.    Elapsed: 0:35:44.\n",
      "  Batch   495  of  1,540.    Elapsed: 0:36:07.\n",
      "  Batch   500  of  1,540.    Elapsed: 0:36:27.\n",
      "  Batch   505  of  1,540.    Elapsed: 0:36:48.\n",
      "  Batch   510  of  1,540.    Elapsed: 0:37:08.\n",
      "  Batch   515  of  1,540.    Elapsed: 0:37:30.\n",
      "  Batch   520  of  1,540.    Elapsed: 0:37:49.\n",
      "  Batch   525  of  1,540.    Elapsed: 0:38:09.\n",
      "  Batch   530  of  1,540.    Elapsed: 0:38:29.\n",
      "  Batch   535  of  1,540.    Elapsed: 0:38:48.\n",
      "  Batch   540  of  1,540.    Elapsed: 0:39:08.\n",
      "  Batch   545  of  1,540.    Elapsed: 0:39:28.\n",
      "  Batch   550  of  1,540.    Elapsed: 0:39:48.\n",
      "  Batch   555  of  1,540.    Elapsed: 0:40:09.\n",
      "  Batch   560  of  1,540.    Elapsed: 0:40:29.\n",
      "  Batch   565  of  1,540.    Elapsed: 0:40:51.\n",
      "  Batch   570  of  1,540.    Elapsed: 0:41:14.\n",
      "  Batch   575  of  1,540.    Elapsed: 0:41:41.\n",
      "  Batch   580  of  1,540.    Elapsed: 0:42:02.\n",
      "  Batch   585  of  1,540.    Elapsed: 0:42:22.\n",
      "  Batch   590  of  1,540.    Elapsed: 0:42:45.\n",
      "  Batch   595  of  1,540.    Elapsed: 0:43:04.\n",
      "  Batch   600  of  1,540.    Elapsed: 0:43:30.\n",
      "  Batch   605  of  1,540.    Elapsed: 0:43:51.\n",
      "  Batch   610  of  1,540.    Elapsed: 0:44:11.\n",
      "  Batch   615  of  1,540.    Elapsed: 0:44:31.\n",
      "  Batch   620  of  1,540.    Elapsed: 0:44:50.\n",
      "  Batch   625  of  1,540.    Elapsed: 0:45:09.\n",
      "  Batch   630  of  1,540.    Elapsed: 0:45:31.\n",
      "  Batch   635  of  1,540.    Elapsed: 0:45:53.\n",
      "  Batch   640  of  1,540.    Elapsed: 0:46:16.\n",
      "  Batch   645  of  1,540.    Elapsed: 0:46:37.\n",
      "  Batch   650  of  1,540.    Elapsed: 0:46:56.\n",
      "  Batch   655  of  1,540.    Elapsed: 0:47:16.\n",
      "  Batch   660  of  1,540.    Elapsed: 0:47:37.\n",
      "  Batch   665  of  1,540.    Elapsed: 0:47:56.\n",
      "  Batch   670  of  1,540.    Elapsed: 0:48:15.\n",
      "  Batch   675  of  1,540.    Elapsed: 0:48:33.\n",
      "  Batch   680  of  1,540.    Elapsed: 0:48:53.\n",
      "  Batch   685  of  1,540.    Elapsed: 0:49:12.\n",
      "  Batch   690  of  1,540.    Elapsed: 0:49:31.\n",
      "  Batch   695  of  1,540.    Elapsed: 0:49:50.\n",
      "  Batch   700  of  1,540.    Elapsed: 0:50:12.\n",
      "  Batch   705  of  1,540.    Elapsed: 0:50:36.\n",
      "  Batch   710  of  1,540.    Elapsed: 0:50:57.\n",
      "  Batch   715  of  1,540.    Elapsed: 0:51:18.\n",
      "  Batch   720  of  1,540.    Elapsed: 0:51:37.\n",
      "  Batch   725  of  1,540.    Elapsed: 0:51:55.\n",
      "  Batch   730  of  1,540.    Elapsed: 0:52:14.\n",
      "  Batch   735  of  1,540.    Elapsed: 0:52:33.\n",
      "  Batch   740  of  1,540.    Elapsed: 0:52:51.\n",
      "  Batch   745  of  1,540.    Elapsed: 0:53:10.\n",
      "  Batch   750  of  1,540.    Elapsed: 0:53:28.\n",
      "  Batch   755  of  1,540.    Elapsed: 0:53:47.\n",
      "  Batch   760  of  1,540.    Elapsed: 0:54:06.\n",
      "  Batch   765  of  1,540.    Elapsed: 0:54:25.\n",
      "  Batch   770  of  1,540.    Elapsed: 0:54:43.\n",
      "  Batch   775  of  1,540.    Elapsed: 0:55:02.\n",
      "  Batch   780  of  1,540.    Elapsed: 0:55:25.\n",
      "  Batch   785  of  1,540.    Elapsed: 0:55:43.\n",
      "  Batch   790  of  1,540.    Elapsed: 0:56:03.\n",
      "  Batch   795  of  1,540.    Elapsed: 0:56:25.\n",
      "  Batch   800  of  1,540.    Elapsed: 0:56:45.\n",
      "  Batch   805  of  1,540.    Elapsed: 0:57:03.\n",
      "  Batch   810  of  1,540.    Elapsed: 0:57:22.\n",
      "  Batch   815  of  1,540.    Elapsed: 0:57:40.\n",
      "  Batch   820  of  1,540.    Elapsed: 0:57:59.\n",
      "  Batch   825  of  1,540.    Elapsed: 0:58:18.\n",
      "  Batch   830  of  1,540.    Elapsed: 0:58:36.\n",
      "  Batch   835  of  1,540.    Elapsed: 0:58:55.\n",
      "  Batch   840  of  1,540.    Elapsed: 0:59:14.\n",
      "  Batch   845  of  1,540.    Elapsed: 0:59:32.\n",
      "  Batch   850  of  1,540.    Elapsed: 0:59:51.\n",
      "  Batch   855  of  1,540.    Elapsed: 1:00:10.\n",
      "  Batch   860  of  1,540.    Elapsed: 1:00:30.\n",
      "  Batch   865  of  1,540.    Elapsed: 1:00:57.\n",
      "  Batch   870  of  1,540.    Elapsed: 1:02:01.\n",
      "  Batch   875  of  1,540.    Elapsed: 1:02:23.\n",
      "  Batch   880  of  1,540.    Elapsed: 1:02:43.\n",
      "  Batch   885  of  1,540.    Elapsed: 1:06:41.\n",
      "  Batch   890  of  1,540.    Elapsed: 1:57:12.\n",
      "  Batch   895  of  1,540.    Elapsed: 2:13:12.\n",
      "  Batch   900  of  1,540.    Elapsed: 2:15:23.\n",
      "  Batch   905  of  1,540.    Elapsed: 2:15:42.\n",
      "  Batch   910  of  1,540.    Elapsed: 2:16:10.\n",
      "  Batch   915  of  1,540.    Elapsed: 2:16:30.\n",
      "  Batch   920  of  1,540.    Elapsed: 2:16:50.\n",
      "  Batch   925  of  1,540.    Elapsed: 2:17:11.\n",
      "  Batch   930  of  1,540.    Elapsed: 2:17:30.\n",
      "  Batch   935  of  1,540.    Elapsed: 2:17:48.\n",
      "  Batch   940  of  1,540.    Elapsed: 2:18:06.\n",
      "  Batch   945  of  1,540.    Elapsed: 2:18:25.\n",
      "  Batch   950  of  1,540.    Elapsed: 2:18:43.\n",
      "  Batch   955  of  1,540.    Elapsed: 2:19:01.\n",
      "  Batch   960  of  1,540.    Elapsed: 2:19:19.\n",
      "  Batch   965  of  1,540.    Elapsed: 2:19:38.\n",
      "  Batch   970  of  1,540.    Elapsed: 2:19:56.\n",
      "  Batch   975  of  1,540.    Elapsed: 2:20:14.\n",
      "  Batch   980  of  1,540.    Elapsed: 2:20:32.\n",
      "  Batch   985  of  1,540.    Elapsed: 2:20:53.\n",
      "  Batch   990  of  1,540.    Elapsed: 2:21:16.\n",
      "  Batch   995  of  1,540.    Elapsed: 2:21:35.\n",
      "  Batch 1,000  of  1,540.    Elapsed: 2:21:53.\n",
      "  Batch 1,005  of  1,540.    Elapsed: 2:22:11.\n",
      "  Batch 1,010  of  1,540.    Elapsed: 2:22:30.\n",
      "  Batch 1,015  of  1,540.    Elapsed: 2:22:48.\n",
      "  Batch 1,020  of  1,540.    Elapsed: 2:23:06.\n",
      "  Batch 1,025  of  1,540.    Elapsed: 2:23:24.\n",
      "  Batch 1,030  of  1,540.    Elapsed: 2:23:42.\n",
      "  Batch 1,035  of  1,540.    Elapsed: 2:24:01.\n",
      "  Batch 1,040  of  1,540.    Elapsed: 2:24:21.\n",
      "  Batch 1,045  of  1,540.    Elapsed: 2:24:39.\n",
      "  Batch 1,050  of  1,540.    Elapsed: 2:25:00.\n",
      "  Batch 1,055  of  1,540.    Elapsed: 2:25:20.\n",
      "  Batch 1,060  of  1,540.    Elapsed: 2:25:40.\n",
      "  Batch 1,065  of  1,540.    Elapsed: 2:26:00.\n",
      "  Batch 1,070  of  1,540.    Elapsed: 2:26:18.\n",
      "  Batch 1,075  of  1,540.    Elapsed: 2:26:37.\n",
      "  Batch 1,080  of  1,540.    Elapsed: 2:26:55.\n",
      "  Batch 1,085  of  1,540.    Elapsed: 2:27:16.\n",
      "  Batch 1,090  of  1,540.    Elapsed: 2:27:35.\n",
      "  Batch 1,095  of  1,540.    Elapsed: 2:27:54.\n",
      "  Batch 1,100  of  1,540.    Elapsed: 2:28:15.\n",
      "  Batch 1,105  of  1,540.    Elapsed: 2:28:35.\n",
      "  Batch 1,110  of  1,540.    Elapsed: 2:28:55.\n",
      "  Batch 1,115  of  1,540.    Elapsed: 2:29:14.\n",
      "  Batch 1,120  of  1,540.    Elapsed: 2:29:34.\n",
      "  Batch 1,125  of  1,540.    Elapsed: 2:29:53.\n",
      "  Batch 1,130  of  1,540.    Elapsed: 2:30:14.\n",
      "  Batch 1,135  of  1,540.    Elapsed: 2:30:35.\n",
      "  Batch 1,140  of  1,540.    Elapsed: 2:30:56.\n",
      "  Batch 1,145  of  1,540.    Elapsed: 2:31:15.\n",
      "  Batch 1,150  of  1,540.    Elapsed: 2:31:33.\n",
      "  Batch 1,155  of  1,540.    Elapsed: 2:31:52.\n",
      "  Batch 1,160  of  1,540.    Elapsed: 2:32:12.\n",
      "  Batch 1,165  of  1,540.    Elapsed: 2:32:31.\n",
      "  Batch 1,170  of  1,540.    Elapsed: 2:32:51.\n",
      "  Batch 1,175  of  1,540.    Elapsed: 2:33:09.\n",
      "  Batch 1,180  of  1,540.    Elapsed: 2:33:33.\n",
      "  Batch 1,185  of  1,540.    Elapsed: 2:33:51.\n",
      "  Batch 1,190  of  1,540.    Elapsed: 2:34:14.\n",
      "  Batch 1,195  of  1,540.    Elapsed: 2:34:33.\n",
      "  Batch 1,200  of  1,540.    Elapsed: 2:34:53.\n",
      "  Batch 1,205  of  1,540.    Elapsed: 2:35:12.\n",
      "  Batch 1,210  of  1,540.    Elapsed: 2:35:30.\n",
      "  Batch 1,215  of  1,540.    Elapsed: 2:35:48.\n",
      "  Batch 1,220  of  1,540.    Elapsed: 2:36:06.\n",
      "  Batch 1,225  of  1,540.    Elapsed: 2:36:25.\n",
      "  Batch 1,230  of  1,540.    Elapsed: 2:36:43.\n",
      "  Batch 1,235  of  1,540.    Elapsed: 2:37:01.\n",
      "  Batch 1,240  of  1,540.    Elapsed: 2:37:19.\n",
      "  Batch 1,245  of  1,540.    Elapsed: 2:37:38.\n",
      "  Batch 1,250  of  1,540.    Elapsed: 2:37:56.\n",
      "  Batch 1,255  of  1,540.    Elapsed: 2:38:15.\n",
      "  Batch 1,260  of  1,540.    Elapsed: 2:38:33.\n",
      "  Batch 1,265  of  1,540.    Elapsed: 2:38:51.\n",
      "  Batch 1,270  of  1,540.    Elapsed: 2:39:09.\n",
      "  Batch 1,275  of  1,540.    Elapsed: 2:39:27.\n",
      "  Batch 1,280  of  1,540.    Elapsed: 2:39:46.\n",
      "  Batch 1,285  of  1,540.    Elapsed: 2:40:06.\n",
      "  Batch 1,290  of  1,540.    Elapsed: 2:40:24.\n",
      "  Batch 1,295  of  1,540.    Elapsed: 2:40:42.\n",
      "  Batch 1,300  of  1,540.    Elapsed: 2:41:00.\n",
      "  Batch 1,305  of  1,540.    Elapsed: 2:41:19.\n",
      "  Batch 1,310  of  1,540.    Elapsed: 2:41:38.\n",
      "  Batch 1,315  of  1,540.    Elapsed: 2:41:57.\n",
      "  Batch 1,320  of  1,540.    Elapsed: 2:42:15.\n",
      "  Batch 1,325  of  1,540.    Elapsed: 2:42:34.\n",
      "  Batch 1,330  of  1,540.    Elapsed: 2:42:59.\n",
      "  Batch 1,335  of  1,540.    Elapsed: 2:43:21.\n",
      "  Batch 1,340  of  1,540.    Elapsed: 2:43:39.\n",
      "  Batch 1,345  of  1,540.    Elapsed: 2:43:58.\n",
      "  Batch 1,350  of  1,540.    Elapsed: 2:44:16.\n",
      "  Batch 1,355  of  1,540.    Elapsed: 2:44:36.\n",
      "  Batch 1,360  of  1,540.    Elapsed: 2:44:54.\n",
      "  Batch 1,365  of  1,540.    Elapsed: 2:45:13.\n",
      "  Batch 1,370  of  1,540.    Elapsed: 2:45:32.\n",
      "  Batch 1,375  of  1,540.    Elapsed: 2:45:50.\n",
      "  Batch 1,380  of  1,540.    Elapsed: 2:46:08.\n",
      "  Batch 1,385  of  1,540.    Elapsed: 2:46:29.\n",
      "  Batch 1,390  of  1,540.    Elapsed: 2:46:47.\n",
      "  Batch 1,395  of  1,540.    Elapsed: 2:47:06.\n",
      "  Batch 1,400  of  1,540.    Elapsed: 2:47:29.\n",
      "  Batch 1,405  of  1,540.    Elapsed: 2:47:48.\n",
      "  Batch 1,410  of  1,540.    Elapsed: 2:48:06.\n",
      "  Batch 1,415  of  1,540.    Elapsed: 2:48:24.\n",
      "  Batch 1,420  of  1,540.    Elapsed: 2:48:43.\n",
      "  Batch 1,425  of  1,540.    Elapsed: 2:49:01.\n",
      "  Batch 1,430  of  1,540.    Elapsed: 2:49:19.\n",
      "  Batch 1,435  of  1,540.    Elapsed: 2:49:38.\n",
      "  Batch 1,440  of  1,540.    Elapsed: 2:49:56.\n",
      "  Batch 1,445  of  1,540.    Elapsed: 2:50:15.\n",
      "  Batch 1,450  of  1,540.    Elapsed: 2:50:37.\n",
      "  Batch 1,455  of  1,540.    Elapsed: 2:50:56.\n",
      "  Batch 1,460  of  1,540.    Elapsed: 2:51:15.\n",
      "  Batch 1,465  of  1,540.    Elapsed: 2:51:33.\n",
      "  Batch 1,470  of  1,540.    Elapsed: 2:51:53.\n",
      "  Batch 1,475  of  1,540.    Elapsed: 2:52:14.\n",
      "  Batch 1,480  of  1,540.    Elapsed: 2:52:33.\n",
      "  Batch 1,485  of  1,540.    Elapsed: 2:52:52.\n",
      "  Batch 1,490  of  1,540.    Elapsed: 2:53:12.\n",
      "  Batch 1,495  of  1,540.    Elapsed: 2:53:30.\n",
      "  Batch 1,500  of  1,540.    Elapsed: 2:53:48.\n",
      "  Batch 1,505  of  1,540.    Elapsed: 2:54:06.\n",
      "  Batch 1,510  of  1,540.    Elapsed: 2:54:24.\n",
      "  Batch 1,515  of  1,540.    Elapsed: 2:54:42.\n",
      "  Batch 1,520  of  1,540.    Elapsed: 2:55:02.\n",
      "  Batch 1,525  of  1,540.    Elapsed: 2:55:22.\n",
      "  Batch 1,530  of  1,540.    Elapsed: 2:55:40.\n",
      "  Batch 1,535  of  1,540.    Elapsed: 2:56:01.\n",
      "\n",
      "  Average training loss: 1.08\n",
      "  Training epcoh took: 2:56:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.70\n",
      "  Validation took: 0:02:57\n",
      "\n",
      "Training complete!\n",
      "--- 10754.772831916809 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing testing data\n",
    "sentences = df_test.Abstract.values\n",
    "labels = df_test.Math.values\n",
    "\n",
    "input_ids_test = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745,\n",
    "                   )\n",
    "\n",
    "    input_ids_test.append(encoded_sent)\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 5930 of 19678 (30.14%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df_test.Math.sum(), len(df_test.Math), (df_test.Math.sum() / len(df_test.Math) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.699\n"
     ]
    }
   ],
   "source": [
    "# Combining the predictions for every batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/BERT/Math/full/L16_B32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/BERT/Math/full/L16_B32/vocab.txt',\n",
       " './model_save/BERT/Math/full/L16_B32/special_tokens_map.json',\n",
       " './model_save/BERT/Math/full/L16_B32/added_tokens.json')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/BERT/Math/full/L16_B32'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "# model = BertForSequenceClassification.from_pretrained(\"./model_save/BERT/Math/full/L16_B32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13748     0]\n",
      " [ 5930     0]]\n",
      "Accuracy: 0.6986482366094116\n",
      "Macro Precision: 0.3493241183047058\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4112965954646084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT L16-B32 with InfoTheory first 1000 records using BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Abstracts_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Token IDs: [101, 9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012, 102]\n",
      "--- 4.0122129917144775 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Tokenizing all of the abstracts and map the tokenized words to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "for abstract in Abstracts_1000:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745, #we got the max len 743 and after 2 special tokens\n",
    "                   )\n",
    "    input_ids.append(encoded_abstract)\n",
    "\n",
    "print('Original: ', Abstracts_1000[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, InfoTheory_labels_1000, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, InfoTheory_labels_1000,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of     29.    Elapsed: 0:00:22.\n",
      "  Batch    10  of     29.    Elapsed: 0:00:40.\n",
      "  Batch    15  of     29.    Elapsed: 0:00:58.\n",
      "  Batch    20  of     29.    Elapsed: 0:01:20.\n",
      "  Batch    25  of     29.    Elapsed: 0:01:43.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:01:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "--- 117.72019028663635 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing testing data\n",
    "sentences = df_test.Abstract.values\n",
    "labels = df_test.InfoTheory.values\n",
    "\n",
    "input_ids_test = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745,\n",
    "                   )\n",
    "    \n",
    "    input_ids_test.append(encoded_sent)\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 3616 of 19678 (18.38%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df_test.InfoTheory.sum(), len(df_test.InfoTheory), (df_test.InfoTheory.sum() / len(df_test.InfoTheory) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.816\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16062     0]\n",
      " [ 3616     0]]\n",
      "Accuracy: 0.8162414879560931\n",
      "Macro Precision: 0.40812074397804654\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.44941242305540013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/BERT/InfoTheory/1000/L16_B32'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "# model = BertForSequenceClassification.from_pretrained('./model_save/BERT/InfoTheory/1000/L16_B32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CompVis1000 training with BERT_L16_B32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Token IDs: [101, 9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012, 102]\n",
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n",
      "--- 2.603178024291992 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Tokenizing all of the abstracts and map the tokenized words to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "for abstract in Abstracts_1000:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745, #we got the max len 743 and after 2 special tokens\n",
    "                   )\n",
    "    input_ids.append(encoded_abstract)\n",
    "\n",
    "print('Original: ', Abstracts_1000[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, CompVis_labels_1000, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, CompVis_labels_1000,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of     29.    Elapsed: 0:00:23.\n",
      "  Batch    10  of     29.    Elapsed: 0:00:41.\n",
      "  Batch    15  of     29.    Elapsed: 0:01:01.\n",
      "  Batch    20  of     29.    Elapsed: 0:01:24.\n",
      "  Batch    25  of     29.    Elapsed: 0:01:47.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:02:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "--- 127.69305300712585 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.CompVis.values\n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.891\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17526     0]\n",
      " [ 2152     0]]\n",
      "Accuracy: 0.8906392926110377\n",
      "Macro Precision: 0.44531964630551885\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4710783786689603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/BERT/CompVis/1000/L16_B32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/BERT/CompVis/1000/L16_B32/vocab.txt',\n",
       " './model_save/BERT/CompVis/1000/L16_B32/special_tokens_map.json',\n",
       " './model_save/BERT/CompVis/1000/L16_B32/added_tokens.json')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/BERT/CompVis/1000/L16_B32'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "# model = BertForSequenceClassification.from_pretrained('./model_save/BERT/CompVis/1000/L16_B32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math1000 training with BERT_L16_B32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. \n",
      "Token IDs: [101, 9089, 2098, 2938, 2483, 22749, 8553, 1037, 2569, 2553, 1997, 1996, 2938, 2483, 22749, 8553, 3291, 1010, 1999, 2029, 1996, 24059, 2031, 1037, 25835, 3252, 1010, 2003, 3491, 2000, 2022, 14017, 12423, 1999, 7399, 2051, 1010, 10262, 2008, 1996, 24059, 2031, 2042, 3421, 1999, 1037, 14057, 2126, 1012, 102]\n",
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n",
      "--- 2.7331020832061768 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Tokenizing all of the abstracts and map the tokenized words to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "for abstract in Abstracts_1000:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745, #we got the max len 743 and after 2 special tokens\n",
    "                   )\n",
    "    input_ids.append(encoded_abstract)\n",
    "\n",
    "print('Original: ', Abstracts_1000[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Math_labels_1000, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, Math_labels_1000,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of     29.    Elapsed: 0:00:21.\n",
      "  Batch    10  of     29.    Elapsed: 0:00:41.\n",
      "  Batch    15  of     29.    Elapsed: 0:01:03.\n",
      "  Batch    20  of     29.    Elapsed: 0:01:31.\n",
      "  Batch    25  of     29.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n",
      "--- 129.10327768325806 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.Math.values\n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.699\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13748     0]\n",
      " [ 5930     0]]\n",
      "Accuracy: 0.6986482366094116\n",
      "Macro Precision: 0.3493241183047058\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4112965954646084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/BERT/Math/1000/L16_B32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/BERT/Math/1000/L16_B32/vocab.txt',\n",
       " './model_save/BERT/Math/1000/L16_B32/special_tokens_map.json',\n",
       " './model_save/BERT/Math/1000/L16_B32/added_tokens.json')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/BERT/Math/1000/L16_B32'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "# model = BertForSequenceClassification.from_pretrained('./model_save/BERT/Math/1000/L16_B32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lemmatokenizer using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer()\n",
    "    def __call__(self,doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(analyzer='word',input='content',\n",
    "                           lowercase=True,\n",
    "                           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                           min_df=3,\n",
    "                           ngram_range=(1,2),\n",
    "                           tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocs = df_train.Abstract.tolist()[:1000]\n",
    "InfoTheoryLabels = df_train.InfoTheory.tolist() [:1000]\n",
    "CompVisLabels = df_train.CompVis.tolist()[:1000]\n",
    "MathLabels = df_train.Math.tolist()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDocs = df_test.Abstract.tolist()\n",
    "InfoTheory_testLabels = df_test.InfoTheory.tolist()\n",
    "CompVis_testLabels = df_test.CompVis.tolist()\n",
    "Math_testLabels = df_test.Math.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=vectorizer.fit_transform(trainDocs)\n",
    "x_test=vectorizer.transform(testDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.todense().tolist()\n",
    "x_test = x_test.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding encoded values of [CLS] and [SEP] as 101 and 102 in the first and last place respectively.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_train:\n",
    "    i.insert(0,101)\n",
    "    i.insert(10367,102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_test:\n",
    "    i.insert(0,101)\n",
    "    i.insert(10367,102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19678"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x_train\n",
    "input_ids_test = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 1000 InfoTheory labels using BERT model and LemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, InfoTheoryLabels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, InfoTheoryLabels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of     29.    Elapsed: 0:00:32.\n",
      "  Batch    10  of     29.    Elapsed: 0:00:55.\n",
      "  Batch    15  of     29.    Elapsed: 0:01:16.\n",
      "  Batch    20  of     29.    Elapsed: 0:01:37.\n",
      "  Batch    25  of     29.    Elapsed: 0:02:04.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:02:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n",
      "--- 141.072340965271 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# MAX_LEN = 16\n",
    "\n",
    "# print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "# input_ids = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", \n",
    "#                           value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# print('\\nDone.')\n",
    "\n",
    "# # Creating attention masks\n",
    "# attention_masks = []\n",
    "\n",
    "# for id in input_ids_test:\n",
    "    \n",
    "#     # Creating the attention mask.\n",
    "#     #   - If a token ID is 0, set the mask to 0.\n",
    "#     #   - If a token ID is > 0, set the mask to 1.\n",
    "#     att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "#     attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.InfoTheory.values\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.816\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16062     0]\n",
      " [ 3616     0]]\n",
      "Accuracy: 0.8162414879560931\n",
      "Macro Precision: 0.40812074397804654\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.44941242305540013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On first 1000 Compvis labels using BERT and lemma tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, CompVisLabels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, CompVisLabels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of     29.    Elapsed: 0:00:30.\n",
      "  Batch    10  of     29.    Elapsed: 0:00:56.\n",
      "  Batch    15  of     29.    Elapsed: 0:01:17.\n",
      "  Batch    20  of     29.    Elapsed: 0:01:40.\n",
      "  Batch    25  of     29.    Elapsed: 0:01:59.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:02:14\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n",
      "--- 138.37577319145203 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# MAX_LEN = 16\n",
    "\n",
    "# print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "# input_ids = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", \n",
    "#                           value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# print('\\nDone.')\n",
    "\n",
    "# # Creating attention masks\n",
    "# attention_masks = []\n",
    "\n",
    "# for id in input_ids_test:\n",
    "    \n",
    "#     # Creating the attention mask.\n",
    "#     #   - If a token ID is 0, set the mask to 0.\n",
    "#     #   - If a token ID is > 0, set the mask to 1.\n",
    "#     att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "#     attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.CompVis.values\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.891\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17526     0]\n",
      " [ 2152     0]]\n",
      "Accuracy: 0.8906392926110377\n",
      "Macro Precision: 0.44531964630551885\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4710783786689603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On first 1000 Math labels using BERT and lemma tokenizer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, MathLabels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, MathLabels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     5  of     29.    Elapsed: 0:00:26.\n",
      "  Batch    10  of     29.    Elapsed: 0:00:47.\n",
      "  Batch    15  of     29.    Elapsed: 0:01:08.\n",
      "  Batch    20  of     29.    Elapsed: 0:01:29.\n",
      "  Batch    25  of     29.    Elapsed: 0:01:51.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "--- 128.23313784599304 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# MAX_LEN = 16\n",
    "\n",
    "# print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "# input_ids = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", \n",
    "#                           value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# print('\\nDone.')\n",
    "\n",
    "# # Creating attention masks\n",
    "# attention_masks = []\n",
    "\n",
    "# for id in input_ids_test:\n",
    "    \n",
    "#     # Creating the attention mask.\n",
    "#     #   - If a token ID is 0, set the mask to 0.\n",
    "#     #   - If a token ID is > 0, set the mask to 1.\n",
    "#     att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "#     attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.Math.values\n",
    "MAX_LEN = 16\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 19,678 test sentences...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.699\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13748     0]\n",
      " [ 5930     0]]\n",
      "Accuracy: 0.6986482366094116\n",
      "Macro Precision: 0.3493241183047058\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4112965954646084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT and LemmaTokenizer for all the InfoTheory Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocs = df_train.Abstract.tolist()\n",
    "InfoTheoryLabels = df_train.InfoTheory.tolist()\n",
    "CompVisLabels = df_train.CompVis.tolist()\n",
    "MathLabels = df_train.Math.tolist()\n",
    "\n",
    "testDocs = df_test.Abstract.tolist()\n",
    "InfoTheory_testLabels = df_test.InfoTheory.tolist()\n",
    "CompVis_testLabels = df_test.CompVis.tolist()\n",
    "Math_testLabels = df_test.Math.tolist()\n",
    "\n",
    "x_train=vectorizer.fit_transform(trainDocs)\n",
    "x_test=vectorizer.transform(testDocs)\n",
    "\n",
    "x_train = x_train.todense().tolist()\n",
    "x_test = x_test.todense().tolist()\n",
    "\n",
    "# Adding encoded values of [CLS] and [SEP] as 101 and 102 in the first and last place respectively..\n",
    "\n",
    "for i in x_train:\n",
    "    i.insert(0,101)\n",
    "    i.insert(10367,102)\n",
    "\n",
    "len(x_train)\n",
    "\n",
    "for i in x_test:\n",
    "    i.insert(0,101)\n",
    "    i.insert(10367,102)\n",
    "\n",
    "len(x_test)\n",
    "\n",
    "# len(x_test[0])\n",
    "\n",
    "input_ids = x_train\n",
    "input_ids_test = x_test\n",
    "\n",
    "len(input_ids)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "len(input_ids)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, InfoTheoryLabels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, InfoTheoryLabels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "labels = df_test.InfoTheory.values\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')\n",
    "\n",
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT and lemma tokenizer for all the CompVis Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, CompVisLabels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, CompVisLabels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "labels = df_test.CompVis.values\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')\n",
    "\n",
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT and lemma tokenizer on all the Math Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "# Creating attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for id in input_ids:\n",
    "    \n",
    "    # Creating the attention mask.\n",
    "    #   - If a token ID is 0, set the mask to 0.\n",
    "    #   - If a token ID is > 0, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in id]\n",
    "    \n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, MathLabels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "#Doing the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, MathLabels,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#defining batch size\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "start_time = time.time()\n",
    "import random\n",
    "\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the avg loss after each epoch.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        #Progress update every 5 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            #Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        # batch contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(\"b_input_ids\",len(b_input_ids),type(b_input_ids))\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(\"b_input_mask\",len(b_input_mask),type(b_input_mask))\n",
    "#         print(b_input_mask.shape)\n",
    "#         print(\"b_labels\",len(b_labels),type(b_labels))\n",
    "#         print(b_labels.shape)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients..\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculating the accuracy for this batch\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Reporting the final accuracy for the validation.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "labels = df_test.Math.values\n",
    "MAX_LEN = 16\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks_test = []\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids_test)\n",
    "prediction_masks = torch.tensor(attention_masks_test)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('done.')\n",
    "\n",
    "# Combine the predictions for each batch into a list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuray: %.3f' % accuracy)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_predict = np.asarray(flat_predictions)\n",
    "y_test = np.asarray(flat_true_labels)\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "recall=recall_score(y_test,y_predict,average='macro')\n",
    "precision=precision_score(y_test,y_predict,average='macro')\n",
    "f1score=f1_score(y_test,y_predict,average='macro')\n",
    "accuracy=accuracy_score(y_test,y_predict)\n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(flat_true_labels,flat_predictions)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-greece",
   "metadata": {},
   "source": [
    "## Using Statistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "worst-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize    \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB,BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot\n",
    "#!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unsigned-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 54,731\n",
      "\n",
      "Number of testing sentences: 19,678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_train = pd.read_csv(\".//assignment1_data/axcs_train.csv\", delimiter=',')\n",
    "df_test = pd.read_csv(\".//assignment1_data/axcs_test.csv\", delimiter=',')\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df_train.shape[0]))\n",
    "print('Number of testing sentences: {:,}\\n'.format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gross-spyware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>InfoTheory</th>\n",
       "      <th>CompVis</th>\n",
       "      <th>Math</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42829</th>\n",
       "      <td>no-13125952</td>\n",
       "      <td>arxiv.org/abs/1312.5952</td>\n",
       "      <td>2013-12-20</td>\n",
       "      <td>Proceedings of the Fourth International Worksh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Proceedings of the Fourth International Works...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39914</th>\n",
       "      <td>no-13093201</td>\n",
       "      <td>arxiv.org/abs/1309.3201</td>\n",
       "      <td>2013-09-12</td>\n",
       "      <td>On topological and geometric $(19_4)$ configur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>On topological and geometric configurations A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29401</th>\n",
       "      <td>no-12082405</td>\n",
       "      <td>arxiv.org/abs/1208.2405</td>\n",
       "      <td>2012-08-12</td>\n",
       "      <td>Routing Load of Route Discovery and Route Main...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Routing Load of Route Discovery and Route Mai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26138</th>\n",
       "      <td>no-12022561</td>\n",
       "      <td>arxiv.org/abs/1202.2561</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>On the Diversity Gain Region of the Z-interfer...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>On the Diversity Gain Region of the Z-interfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51138</th>\n",
       "      <td>no-14093367</td>\n",
       "      <td>arxiv.org/abs/1409.3367</td>\n",
       "      <td>2014-09-11</td>\n",
       "      <td>HTML5 WebSocket protocol and its application t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HTML5 WebSocket protocol and its application ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                      URL        Date  \\\n",
       "42829  no-13125952  arxiv.org/abs/1312.5952  2013-12-20   \n",
       "39914  no-13093201  arxiv.org/abs/1309.3201  2013-09-12   \n",
       "29401  no-12082405  arxiv.org/abs/1208.2405  2012-08-12   \n",
       "26138  no-12022561  arxiv.org/abs/1202.2561  2012-04-03   \n",
       "51138  no-14093367  arxiv.org/abs/1409.3367  2014-09-11   \n",
       "\n",
       "                                                   Title  InfoTheory  CompVis  \\\n",
       "42829  Proceedings of the Fourth International Worksh...           0        0   \n",
       "39914  On topological and geometric $(19_4)$ configur...           0        0   \n",
       "29401  Routing Load of Route Discovery and Route Main...           0        0   \n",
       "26138  On the Diversity Gain Region of the Z-interfer...           1        0   \n",
       "51138  HTML5 WebSocket protocol and its application t...           0        0   \n",
       "\n",
       "       Math                                           Abstract  \n",
       "42829     0   Proceedings of the Fourth International Works...  \n",
       "39914     1   On topological and geometric configurations A...  \n",
       "29401     0   Routing Load of Route Discovery and Route Mai...  \n",
       "26138     1   On the Diversity Gain Region of the Z-interfe...  \n",
       "51138     0   HTML5 WebSocket protocol and its application ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-marathon",
   "metadata": {},
   "source": [
    "### Using LemmaTokenizer with all the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "expressed-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer()\n",
    "    def __call__(self,doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "present-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(analyzer='word',input='content',\n",
    "                           lowercase=True,\n",
    "                           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                           min_df=3,\n",
    "                           ngram_range=(1,2),\n",
    "                           tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "altered-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "trained-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocs = df_train.Abstract.tolist() \n",
    "InfoTheoryLabels = df_train.InfoTheory.tolist() \n",
    "CompVisLabels = df_train.CompVis.tolist()\n",
    "MathLabels = df_train.Math.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "large-lancaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54731\n"
     ]
    }
   ],
   "source": [
    "print(len(trainDocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "genetic-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=vectorizer.fit_transform(trainDocs)\n",
    "y_train=np.asarray(InfoTheoryLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "respiratory-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309348"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "mighty-praise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '! )',\n",
       " '! ,',\n",
       " '! -',\n",
       " '! :',\n",
       " '! =',\n",
       " '! =np',\n",
       " '! a',\n",
       " '! and',\n",
       " '! in',\n",
       " '! possible',\n",
       " '! the',\n",
       " '! this',\n",
       " '! we',\n",
       " '! }',\n",
       " '#',\n",
       " '# ,',\n",
       " '# .',\n",
       " '# 1',\n",
       " '# 64257',\n",
       " '# 8211',\n",
       " '# 8217',\n",
       " '# and',\n",
       " '# bi',\n",
       " '# csp',\n",
       " '# csps',\n",
       " '# p',\n",
       " '# p-complete',\n",
       " '# p-completeness',\n",
       " '# p-hard',\n",
       " '# p.',\n",
       " '# rhpi_1',\n",
       " '# sat',\n",
       " '$',\n",
       " '$ $',\n",
       " '$ (',\n",
       " '$ ,',\n",
       " '$ .',\n",
       " '$ \\\\varphi\\\\in\\\\alpha',\n",
       " '$ are',\n",
       " '$ is',\n",
       " '$ we',\n",
       " '$ {',\n",
       " '%',\n",
       " '% (',\n",
       " '% )',\n",
       " '% +/-',\n",
       " '% ,',\n",
       " '% -',\n",
       " '% -30',\n",
       " '% .',\n",
       " '% ;',\n",
       " '% a',\n",
       " '% accuracy',\n",
       " '% accurate',\n",
       " '% and',\n",
       " '% are',\n",
       " '% area',\n",
       " '% at',\n",
       " '% average',\n",
       " '% better',\n",
       " '% by',\n",
       " '% can',\n",
       " '% character',\n",
       " '% classification',\n",
       " '% compared',\n",
       " '% confidence',\n",
       " '% correct',\n",
       " '% coverage',\n",
       " '% decrease',\n",
       " '% depending',\n",
       " '% detection',\n",
       " '% drop',\n",
       " '% efficiency',\n",
       " '% energy',\n",
       " '% enhancement',\n",
       " '% error',\n",
       " '% even',\n",
       " '% false',\n",
       " '% faster',\n",
       " '% fewer',\n",
       " '% first',\n",
       " '% for',\n",
       " '% from',\n",
       " '% gain',\n",
       " '% have',\n",
       " '% higher',\n",
       " '% improvement',\n",
       " '% in',\n",
       " '% increase',\n",
       " '% is',\n",
       " '% larger',\n",
       " '% le',\n",
       " '% lesser',\n",
       " '% level',\n",
       " '% loss',\n",
       " '% lower',\n",
       " '% mean',\n",
       " '% more',\n",
       " '% most',\n",
       " '% of',\n",
       " '% on',\n",
       " '% only',\n",
       " '% or',\n",
       " '% over',\n",
       " '% overall',\n",
       " '% overhead',\n",
       " '% overlap',\n",
       " '% packet',\n",
       " '% parallel',\n",
       " '% per',\n",
       " '% performance',\n",
       " '% power',\n",
       " '% precision',\n",
       " '% probability',\n",
       " '% rate',\n",
       " '% recall',\n",
       " '% recognition',\n",
       " '% reduction',\n",
       " '% relative',\n",
       " '% respectively',\n",
       " '% sample',\n",
       " '% saving',\n",
       " '% sensitivity',\n",
       " '% success',\n",
       " '% than',\n",
       " '% the',\n",
       " '% throughput',\n",
       " '% time',\n",
       " '% to',\n",
       " '% true',\n",
       " '% using',\n",
       " '% wa',\n",
       " '% we',\n",
       " '% were',\n",
       " '% when',\n",
       " '% which',\n",
       " '% while',\n",
       " '% with',\n",
       " '% without',\n",
       " '&',\n",
       " '& #',\n",
       " '& a',\n",
       " '& application',\n",
       " '& average',\n",
       " '& c',\n",
       " '& cbr',\n",
       " '& connector',\n",
       " '& d',\n",
       " '& data',\n",
       " '& development',\n",
       " '& dsr',\n",
       " '& e',\n",
       " '& efficient',\n",
       " '& exploration',\n",
       " '& f',\n",
       " '& forward',\n",
       " '& gas',\n",
       " '& hci',\n",
       " '& high',\n",
       " '& humanity',\n",
       " '& is',\n",
       " '& m',\n",
       " '& p',\n",
       " '& s',\n",
       " '& security',\n",
       " '& software',\n",
       " '& system',\n",
       " '& t',\n",
       " '& technology',\n",
       " '& the',\n",
       " '& v',\n",
       " \"'\",\n",
       " \"' (\",\n",
       " \"' )\",\n",
       " \"' ,\",\n",
       " \"' -\",\n",
       " \"' --\",\n",
       " \"' .\",\n",
       " \"' :\",\n",
       " \"' ;\",\n",
       " \"' <\",\n",
       " \"' =\",\n",
       " \"' >\",\n",
       " \"' ?\",\n",
       " \"' [\",\n",
       " \"' a\",\n",
       " \"' a\\\\v\",\n",
       " \"' ability\",\n",
       " \"' access\",\n",
       " \"' accuracy\",\n",
       " \"' action\",\n",
       " \"' activity\",\n",
       " \"' aggregate\",\n",
       " \"' algorithm\",\n",
       " \"' amplitude\",\n",
       " \"' analysis\",\n",
       " \"' and\",\n",
       " \"' application\",\n",
       " \"' approach\",\n",
       " \"' are\",\n",
       " \"' argument\",\n",
       " \"' assignment\",\n",
       " \"' at\",\n",
       " \"' attention\",\n",
       " \"' attitude\",\n",
       " \"' average\",\n",
       " \"' backward\",\n",
       " \"' behavior\",\n",
       " \"' behavioral\",\n",
       " \"' behaviour\",\n",
       " \"' belief\",\n",
       " \"' best\",\n",
       " \"' between\",\n",
       " \"' bid\",\n",
       " \"' bound\",\n",
       " \"' by\",\n",
       " \"' c\",\n",
       " \"' can\",\n",
       " \"' capability\",\n",
       " \"' channel\",\n",
       " \"' characteristic\",\n",
       " \"' choice\",\n",
       " \"' clustering\",\n",
       " \"' codebooks\",\n",
       " \"' collaboration\",\n",
       " \"' communication\",\n",
       " \"' community\",\n",
       " \"' component\",\n",
       " \"' computation\",\n",
       " \"' computer\",\n",
       " \"' concept\",\n",
       " \"' condition\",\n",
       " \"' conjecture\",\n",
       " \"' connectivity\",\n",
       " \"' constraint\",\n",
       " \"' content\",\n",
       " \"' contribution\",\n",
       " \"' control\",\n",
       " \"' correlation\",\n",
       " \"' cost\",\n",
       " \"' csi\",\n",
       " \"' current\",\n",
       " \"' data\",\n",
       " \"' database\",\n",
       " \"' decision\",\n",
       " \"' decision-making\",\n",
       " \"' degree\",\n",
       " \"' delay\",\n",
       " \"' demand\",\n",
       " \"' dilemma\",\n",
       " \"' distance\",\n",
       " \"' distribution\",\n",
       " \"' domain\",\n",
       " \"' dynamic\",\n",
       " \"' e\",\n",
       " \"' effect\",\n",
       " \"' encoding\",\n",
       " \"' energy\",\n",
       " \"' evaluation\",\n",
       " \"' evolution\",\n",
       " \"' expectation\",\n",
       " \"' experience\",\n",
       " \"' factor\",\n",
       " \"' feature\",\n",
       " \"' feedback\",\n",
       " \"' for\",\n",
       " \"' frequency\",\n",
       " \"' from\",\n",
       " \"' function\",\n",
       " \"' g\",\n",
       " \"' g\\\\leq\",\n",
       " \"' graph\",\n",
       " \"' ha\",\n",
       " \"' habit\",\n",
       " \"' have\",\n",
       " \"' high\",\n",
       " \"' historical\",\n",
       " \"' idea\",\n",
       " \"' identity\",\n",
       " \"' if\",\n",
       " \"' image\",\n",
       " \"' implicit\",\n",
       " \"' in\",\n",
       " \"' individual\",\n",
       " \"' influence\",\n",
       " \"' information\",\n",
       " \"' initial\",\n",
       " \"' input\",\n",
       " \"' intention\",\n",
       " \"' interaction\",\n",
       " \"' interest\",\n",
       " \"' introduced\",\n",
       " \"' involvement\",\n",
       " \"' is\",\n",
       " \"' it\",\n",
       " \"' joint\",\n",
       " \"' judgment\",\n",
       " \"' knowledge\",\n",
       " \"' language\",\n",
       " \"' law\",\n",
       " \"' learning\",\n",
       " \"' lemma\",\n",
       " \"' level\",\n",
       " \"' linear\",\n",
       " \"' link\",\n",
       " \"' local\",\n",
       " \"' location\",\n",
       " \"' magnitude\",\n",
       " \"' mechanism\",\n",
       " \"' message\",\n",
       " \"' method\",\n",
       " \"' mobility\",\n",
       " \"' model\",\n",
       " \"' module\",\n",
       " \"' motion\",\n",
       " \"' move\",\n",
       " \"' movement\",\n",
       " \"' multiple\",\n",
       " \"' need\",\n",
       " \"' network\",\n",
       " \"' node\",\n",
       " \"' o\",\n",
       " \"' observation\",\n",
       " \"' of\",\n",
       " \"' offer\",\n",
       " \"' on\",\n",
       " \"' online\",\n",
       " \"' operation\",\n",
       " \"' opinion\",\n",
       " \"' optimization\",\n",
       " \"' or\",\n",
       " \"' original\",\n",
       " \"' output\",\n",
       " \"' overall\",\n",
       " \"' own\",\n",
       " \"' package\",\n",
       " \"' paper\",\n",
       " \"' paradox\",\n",
       " \"' parameter\",\n",
       " \"' part\",\n",
       " \"' payment\",\n",
       " \"' payoff\",\n",
       " \"' perception\",\n",
       " \"' performance\",\n",
       " \"' personal\",\n",
       " \"' perspective\",\n",
       " \"' phenomenon\",\n",
       " \"' point\",\n",
       " \"' policy\",\n",
       " \"' position\",\n",
       " \"' power\",\n",
       " \"' preference\",\n",
       " \"' previous\",\n",
       " \"' prior\",\n",
       " \"' privacy\",\n",
       " \"' private\",\n",
       " \"' probability\",\n",
       " \"' problem\",\n",
       " \"' process\",\n",
       " \"' profile\",\n",
       " \"' property\",\n",
       " \"' quality\",\n",
       " \"' quantum\",\n",
       " \"' query\",\n",
       " \"' random\",\n",
       " \"' rate\",\n",
       " \"' rating\",\n",
       " \"' reasoning\",\n",
       " \"' recommendation\",\n",
       " \"' relative\",\n",
       " \"' representation\",\n",
       " \"' reputation\",\n",
       " \"' request\",\n",
       " \"' requirement\",\n",
       " \"' research\",\n",
       " \"' resource\",\n",
       " \"' result\",\n",
       " \"' risk\",\n",
       " \"' rule\",\n",
       " \"' satisfaction\",\n",
       " \"' scheme\",\n",
       " \"' science\",\n",
       " \"' scientific\",\n",
       " \"' score\",\n",
       " \"' search\",\n",
       " \"' security\",\n",
       " \"' selection\",\n",
       " \"' semantics\",\n",
       " \"' sensing\",\n",
       " \"' sensitivity\",\n",
       " \"' service\",\n",
       " \"' set\",\n",
       " \"' share\",\n",
       " \"' side\",\n",
       " \"' signal\",\n",
       " \"' size\",\n",
       " \"' social\",\n",
       " \"' solution\",\n",
       " \"' source\",\n",
       " \"' specification\",\n",
       " \"' speech\",\n",
       " \"' state\",\n",
       " \"' statistical\",\n",
       " \"' strategic\",\n",
       " \"' strategy\",\n",
       " \"' strength\",\n",
       " \"' structure\",\n",
       " \"' success\",\n",
       " \"' such\",\n",
       " \"' system\",\n",
       " \"' technique\",\n",
       " \"' technology\",\n",
       " \"' text\",\n",
       " \"' that\",\n",
       " \"' the\",\n",
       " \"' theorem\",\n",
       " \"' theory\",\n",
       " \"' this\",\n",
       " \"' threshold\",\n",
       " \"' through\",\n",
       " \"' throughput\",\n",
       " \"' time\",\n",
       " \"' to\",\n",
       " \"' total\",\n",
       " \"' traffic\",\n",
       " \"' transition\",\n",
       " \"' transmission\",\n",
       " \"' transmit\",\n",
       " \"' trust\",\n",
       " \"' type\",\n",
       " \"' u\",\n",
       " \"' using\",\n",
       " \"' utility\",\n",
       " \"' valuation\",\n",
       " \"' value\",\n",
       " \"' vertex\",\n",
       " \"' visual\",\n",
       " \"' we\",\n",
       " \"' weight\",\n",
       " \"' weighted\",\n",
       " \"' where\",\n",
       " \"' which\",\n",
       " \"' willingness\",\n",
       " \"' window\",\n",
       " \"' with\",\n",
       " \"' work\",\n",
       " \"' y\",\n",
       " \"' {\",\n",
       " \"' }\",\n",
       " \"''\",\n",
       " \"'' (\",\n",
       " \"'' )\",\n",
       " \"'' ,\",\n",
       " \"'' .\",\n",
       " \"'' ;\",\n",
       " \"'' a\",\n",
       " \"'' algorithm\",\n",
       " \"'' and\",\n",
       " \"'' are\",\n",
       " \"'' between\",\n",
       " \"'' by\",\n",
       " \"'' can\",\n",
       " \"'' constraint\",\n",
       " \"'' for\",\n",
       " \"'' from\",\n",
       " \"'' function\",\n",
       " \"'' grammar\",\n",
       " \"'' in\",\n",
       " \"'' information\",\n",
       " \"'' is\",\n",
       " \"'' mean\",\n",
       " \"'' method\",\n",
       " \"'' model\",\n",
       " \"'' of\",\n",
       " \"'' or\",\n",
       " \"'' parameter\",\n",
       " \"'' problem\",\n",
       " \"'' quantum\",\n",
       " \"'' result\",\n",
       " \"'' strategy\",\n",
       " \"'' that\",\n",
       " \"'' the\",\n",
       " \"'' theorem\",\n",
       " \"'' this\",\n",
       " \"'' to\",\n",
       " \"'' version\",\n",
       " \"'' we\",\n",
       " \"'' which\",\n",
       " \"'' with\",\n",
       " \"'' word\",\n",
       " \"'05\",\n",
       " \"'06\",\n",
       " \"'06 ]\",\n",
       " \"'07\",\n",
       " \"'07 )\",\n",
       " \"'09\",\n",
       " \"'10\",\n",
       " \"'10 ]\",\n",
       " \"'11\",\n",
       " \"'13\",\n",
       " \"'81\",\n",
       " \"'89\",\n",
       " \"'96\",\n",
       " \"'99\",\n",
       " \"'and\",\n",
       " \"'and '\",\n",
       " \"'best\",\n",
       " \"'big\",\n",
       " \"'d\",\n",
       " \"'em\",\n",
       " \"'global\",\n",
       " \"'good\",\n",
       " \"'good '\",\n",
       " \"'in\",\n",
       " \"'information\",\n",
       " \"'ll\",\n",
       " \"'ll also\",\n",
       " \"'n\",\n",
       " \"'n '\",\n",
       " \"'on\",\n",
       " \"'or\",\n",
       " \"'or '\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'s '\",\n",
       " \"'s (\",\n",
       " \"'s )\",\n",
       " \"'s ,\",\n",
       " \"'s .\",\n",
       " \"'s 1970\",\n",
       " \"'s :\",\n",
       " \"'s [\",\n",
       " \"'s \\\\emph\",\n",
       " \"'s `\",\n",
       " \"'s ``\",\n",
       " \"'s a\",\n",
       " \"'s ability\",\n",
       " \"'s abstract\",\n",
       " \"'s abstraction\",\n",
       " \"'s accelerated\",\n",
       " \"'s acceleration\",\n",
       " \"'s access\",\n",
       " \"'s accuracy\",\n",
       " \"'s achievable\",\n",
       " \"'s action\",\n",
       " \"'s active\",\n",
       " \"'s activity\",\n",
       " \"'s actual\",\n",
       " \"'s adaptive\",\n",
       " \"'s adjacency\",\n",
       " \"'s advanced\",\n",
       " \"'s advantage\",\n",
       " \"'s aim\",\n",
       " \"'s algebraic\",\n",
       " \"'s algorithm\",\n",
       " \"'s an\",\n",
       " \"'s analysis\",\n",
       " \"'s and\",\n",
       " \"'s android\",\n",
       " \"'s anonymity\",\n",
       " \"'s answer\",\n",
       " \"'s antenna\",\n",
       " \"'s applicability\",\n",
       " \"'s application\",\n",
       " \"'s approach\",\n",
       " \"'s approximation\",\n",
       " \"'s architecture\",\n",
       " \"'s are\",\n",
       " \"'s argument\",\n",
       " \"'s arithmetical\",\n",
       " \"'s arrival\",\n",
       " \"'s asset\",\n",
       " \"'s asymptotic\",\n",
       " \"'s attack\",\n",
       " \"'s attempt\",\n",
       " \"'s attention\",\n",
       " \"'s attitude\",\n",
       " \"'s attribute\",\n",
       " \"'s authentication\",\n",
       " \"'s author\",\n",
       " \"'s available\",\n",
       " \"'s average\",\n",
       " \"'s axiom\",\n",
       " \"'s axiomatization\",\n",
       " \"'s background\",\n",
       " \"'s bandwidth\",\n",
       " \"'s base\",\n",
       " \"'s basic\",\n",
       " \"'s battery\",\n",
       " \"'s behavior\",\n",
       " \"'s behavioral\",\n",
       " \"'s behaviour\",\n",
       " \"'s belief\",\n",
       " \"'s benefit\",\n",
       " \"'s best\",\n",
       " \"'s bias\",\n",
       " \"'s binary\",\n",
       " \"'s bipartite\",\n",
       " \"'s bit\",\n",
       " \"'s block\",\n",
       " \"'s board\",\n",
       " \"'s body\",\n",
       " \"'s book\",\n",
       " \"'s bound\",\n",
       " \"'s boundary\",\n",
       " \"'s braid\",\n",
       " \"'s brain\",\n",
       " \"'s browser\",\n",
       " \"'s browsing\",\n",
       " \"'s buffer\",\n",
       " \"'s built-in\",\n",
       " \"'s business\",\n",
       " \"'s by\",\n",
       " \"'s cache\",\n",
       " \"'s cad\",\n",
       " \"'s calculus\",\n",
       " \"'s can\",\n",
       " \"'s canonical\",\n",
       " \"'s capability\",\n",
       " \"'s capacity\",\n",
       " \"'s case\",\n",
       " \"'s category\",\n",
       " \"'s causal\",\n",
       " \"'s celebrated\",\n",
       " \"'s cell\",\n",
       " \"'s central\",\n",
       " \"'s channel\",\n",
       " \"'s characteristic\",\n",
       " \"'s characterization\",\n",
       " \"'s choice\",\n",
       " \"'s citation\",\n",
       " \"'s claim\",\n",
       " \"'s classic\",\n",
       " \"'s classical\",\n",
       " \"'s classification\",\n",
       " \"'s coalgebraic\",\n",
       " \"'s code\",\n",
       " \"'s codebooks\",\n",
       " \"'s coding\",\n",
       " \"'s coefficient\",\n",
       " \"'s coherence\",\n",
       " \"'s combination\",\n",
       " \"'s combinatorial\",\n",
       " \"'s common\",\n",
       " \"'s communication\",\n",
       " \"'s community\",\n",
       " \"'s compact\",\n",
       " \"'s competitive\",\n",
       " \"'s complement\",\n",
       " \"'s completeness\",\n",
       " \"'s complex\",\n",
       " \"'s complexity\",\n",
       " \"'s component\",\n",
       " \"'s computational\",\n",
       " \"'s computer\",\n",
       " \"'s concavity\",\n",
       " \"'s concentration\",\n",
       " \"'s concept\",\n",
       " \"'s conclusion\",\n",
       " \"'s concurrent\",\n",
       " \"'s condition\",\n",
       " \"'s conditional\",\n",
       " \"'s configuration\",\n",
       " \"'s conjecture\",\n",
       " \"'s connectivity\",\n",
       " \"'s constant\",\n",
       " \"'s constraint\",\n",
       " \"'s construction\",\n",
       " \"'s constructive\",\n",
       " \"'s content\",\n",
       " \"'s context\",\n",
       " \"'s continuous\",\n",
       " \"'s contribution\",\n",
       " \"'s control\",\n",
       " \"'s convergence\",\n",
       " \"'s core\",\n",
       " \"'s correct\",\n",
       " \"'s correctness\",\n",
       " \"'s correlation\",\n",
       " \"'s cost\",\n",
       " \"'s coverage\",\n",
       " \"'s criterion\",\n",
       " \"'s csi\",\n",
       " \"'s current\",\n",
       " \"'s curvature\",\n",
       " \"'s customer\",\n",
       " \"'s data\",\n",
       " \"'s database\",\n",
       " \"'s decision\",\n",
       " \"'s decoding\",\n",
       " \"'s decomposition\",\n",
       " \"'s default\",\n",
       " \"'s definition\",\n",
       " \"'s degree\",\n",
       " \"'s delay\",\n",
       " \"'s demand\",\n",
       " \"'s demon\",\n",
       " \"'s density\",\n",
       " \"'s deployment\",\n",
       " \"'s derivative\",\n",
       " \"'s description\",\n",
       " \"'s design\",\n",
       " \"'s designed\",\n",
       " \"'s desire\",\n",
       " \"'s detection\",\n",
       " \"'s development\",\n",
       " \"'s dialectica\",\n",
       " \"'s dichotomy\",\n",
       " \"'s digital\",\n",
       " \"'s dilemma\",\n",
       " \"'s dimension\",\n",
       " \"'s direct\",\n",
       " \"'s directed\",\n",
       " \"'s dirty\",\n",
       " \"'s discovery\",\n",
       " \"'s discrete\",\n",
       " \"'s disease\",\n",
       " \"'s distance\",\n",
       " \"'s distribution\",\n",
       " \"'s divergence\",\n",
       " \"'s domain\",\n",
       " \"'s dual\",\n",
       " \"'s dynamic\",\n",
       " \"'s dynamical\",\n",
       " \"'s earlier\",\n",
       " \"'s early\",\n",
       " \"'s ec2\",\n",
       " \"'s economy\",\n",
       " \"'s edge\",\n",
       " \"'s effect\",\n",
       " \"'s effective\",\n",
       " \"'s effectiveness\",\n",
       " \"'s efficiency\",\n",
       " \"'s efficient\",\n",
       " \"'s effort\",\n",
       " \"'s electrical\",\n",
       " \"'s elegant\",\n",
       " \"'s element\",\n",
       " \"'s emotion\",\n",
       " \"'s empirical\",\n",
       " \"'s encoding\",\n",
       " \"'s endpoint\",\n",
       " \"'s energy\",\n",
       " \"'s entropy\",\n",
       " \"'s environment\",\n",
       " \"'s equation\",\n",
       " \"'s equilibrium\",\n",
       " \"'s equivalence\",\n",
       " \"'s era\",\n",
       " \"'s ergodic\",\n",
       " \"'s error\",\n",
       " \"'s estimate\",\n",
       " \"'s estimation\",\n",
       " \"'s estimator\",\n",
       " \"'s evaluation\",\n",
       " \"'s evidence\",\n",
       " \"'s evolution\",\n",
       " \"'s exact\",\n",
       " \"'s example\",\n",
       " \"'s execution\",\n",
       " \"'s existence\",\n",
       " \"'s expectation\",\n",
       " \"'s expected\",\n",
       " \"'s experience\",\n",
       " \"'s experimental\",\n",
       " \"'s exponent\",\n",
       " \"'s extension\",\n",
       " \"'s eye\",\n",
       " \"'s f5\",\n",
       " \"'s face\",\n",
       " \"'s factoring\",\n",
       " \"'s factorization\",\n",
       " \"'s family\",\n",
       " \"'s famous\",\n",
       " \"'s fast\",\n",
       " \"'s feasibility\",\n",
       " \"'s feature\",\n",
       " \"'s feedback\",\n",
       " \"'s find\",\n",
       " \"'s finding\",\n",
       " \"'s finite\",\n",
       " \"'s first\",\n",
       " \"'s first-order\",\n",
       " \"'s fixed\",\n",
       " \"'s flexibility\",\n",
       " \"'s footrule\",\n",
       " \"'s for\",\n",
       " \"'s form\",\n",
       " \"'s formal\",\n",
       " \"'s formula\",\n",
       " \"'s formulation\",\n",
       " \"'s framework\",\n",
       " \"'s friend\",\n",
       " \"'s full\",\n",
       " \"'s function\",\n",
       " \"'s functional\",\n",
       " \"'s functionality\",\n",
       " \"'s functioning\",\n",
       " \"'s fundamental\",\n",
       " \"'s fusion\",\n",
       " \"'s future\",\n",
       " \"'s fuzzy\",\n",
       " \"'s game\",\n",
       " \"'s gap\",\n",
       " \"'s general\",\n",
       " \"'s generality\",\n",
       " \"'s generalization\",\n",
       " \"'s generalized\",\n",
       " \"'s genome\",\n",
       " \"'s geometric\",\n",
       " \"'s geometry\",\n",
       " \"'s global\",\n",
       " \"'s goal\",\n",
       " \"'s graph\",\n",
       " \"'s group\",\n",
       " \"'s guide\",\n",
       " \"'s h-index\",\n",
       " \"'s head\",\n",
       " \"'s hierarchical\",\n",
       " \"'s hierarchy\",\n",
       " \"'s high\",\n",
       " \"'s higher-order\",\n",
       " \"'s highly\",\n",
       " \"'s history\",\n",
       " \"'s holonomic\",\n",
       " \"'s home\",\n",
       " \"'s hyperbolic\",\n",
       " \"'s hypothesis\",\n",
       " \"'s idea\",\n",
       " \"'s ideal\",\n",
       " \"'s identity\",\n",
       " \"'s impact\",\n",
       " \"'s implementation\",\n",
       " \"'s improved\",\n",
       " \"'s in\",\n",
       " \"'s incentive\",\n",
       " \"'s incompleteness\",\n",
       " \"'s index\",\n",
       " \"'s individual\",\n",
       " \"'s inductive\",\n",
       " \"'s inequality\",\n",
       " \"'s infinite\",\n",
       " \"'s influence\",\n",
       " \"'s influential\",\n",
       " \"'s information\",\n",
       " \"'s infrastructure\",\n",
       " \"'s initial\",\n",
       " \"'s inner\",\n",
       " \"'s input\",\n",
       " \"'s intellectual\",\n",
       " \"'s intent\",\n",
       " \"'s intention\",\n",
       " \"'s interaction\",\n",
       " \"'s interest\",\n",
       " \"'s internal\",\n",
       " \"'s international\",\n",
       " \"'s internet\",\n",
       " \"'s interpolation\",\n",
       " \"'s interpretation\",\n",
       " \"'s interval\",\n",
       " \"'s intrinsic\",\n",
       " \"'s intuition\",\n",
       " \"'s intuitionistic\",\n",
       " \"'s ip\",\n",
       " \"'s is\",\n",
       " \"'s iteration\",\n",
       " \"'s iterative\",\n",
       " \"'s java\",\n",
       " \"'s job\",\n",
       " \"'s joint\",\n",
       " \"'s journal\",\n",
       " \"'s judgment\",\n",
       " \"'s key\",\n",
       " \"'s knowledge\",\n",
       " \"'s language\",\n",
       " \"'s large\",\n",
       " \"'s large-scale\",\n",
       " \"'s largest\",\n",
       " \"'s last\",\n",
       " \"'s lattice\",\n",
       " \"'s law\",\n",
       " \"'s learning\",\n",
       " \"'s legacy\",\n",
       " \"'s lemma\",\n",
       " \"'s length\",\n",
       " \"'s level\",\n",
       " \"'s library\",\n",
       " \"'s life\",\n",
       " \"'s lifetime\",\n",
       " \"'s light\",\n",
       " \"'s limit\",\n",
       " \"'s linear\",\n",
       " \"'s link\",\n",
       " \"'s list\",\n",
       " \"'s load\",\n",
       " \"'s local\",\n",
       " \"'s location\",\n",
       " \"'s logic\",\n",
       " \"'s logical\",\n",
       " \"'s long\",\n",
       " \"'s long-term\",\n",
       " \"'s loss\",\n",
       " \"'s lower\",\n",
       " \"'s machine\",\n",
       " \"'s main\",\n",
       " \"'s major\",\n",
       " \"'s marginal\",\n",
       " \"'s market\",\n",
       " \"'s matched\",\n",
       " \"'s mathematical\",\n",
       " \"'s matrix\",\n",
       " \"'s maximum\",\n",
       " \"'s measure\",\n",
       " \"'s measurement\",\n",
       " \"'s mechanical\",\n",
       " \"'s median\",\n",
       " \"'s member\",\n",
       " \"'s memory\",\n",
       " \"'s message\",\n",
       " \"'s method\",\n",
       " \"'s metric\",\n",
       " \"'s millionaire\",\n",
       " \"'s minimax\",\n",
       " \"'s minimum\",\n",
       " \"'s mobile\",\n",
       " \"'s mobility\",\n",
       " \"'s model\",\n",
       " \"'s modified\",\n",
       " \"'s modular\",\n",
       " \"'s modularity\",\n",
       " \"'s module\",\n",
       " \"'s modulus\",\n",
       " \"'s more\",\n",
       " \"'s most\",\n",
       " \"'s motion\",\n",
       " \"'s motivation\",\n",
       " \"'s movement\",\n",
       " \"'s moving\",\n",
       " \"'s mutual\",\n",
       " \"'s name\",\n",
       " \"'s natural\",\n",
       " \"'s nearest\",\n",
       " \"'s necessary\",\n",
       " \"'s need\",\n",
       " \"'s neighbor\",\n",
       " \"'s network\",\n",
       " \"'s new\",\n",
       " \"'s next\",\n",
       " \"'s node\",\n",
       " \"'s non-fregean\",\n",
       " \"'s normal\",\n",
       " \"'s not\",\n",
       " \"'s notion\",\n",
       " \"'s novel\",\n",
       " \"'s number\",\n",
       " \"'s object\",\n",
       " \"'s objective\",\n",
       " \"'s observation\",\n",
       " \"'s observed\",\n",
       " \"'s of\",\n",
       " \"'s on\",\n",
       " \"'s one\",\n",
       " \"'s online\",\n",
       " \"'s operation\",\n",
       " \"'s operational\",\n",
       " \"'s operator\",\n",
       " \"'s opinion\",\n",
       " \"'s optimal\",\n",
       " \"'s or\",\n",
       " \"'s order\",\n",
       " \"'s organization\",\n",
       " \"'s original\",\n",
       " \"'s other\",\n",
       " \"'s outage\",\n",
       " \"'s outcome\",\n",
       " \"'s outer\",\n",
       " \"'s output\",\n",
       " \"'s over\",\n",
       " \"'s overall\",\n",
       " \"'s own\",\n",
       " \"'s packet\",\n",
       " \"'s pagerank\",\n",
       " \"'s paper\",\n",
       " \"'s paradigm\",\n",
       " \"'s paradox\",\n",
       " \"'s parallel\",\n",
       " \"'s parameter\",\n",
       " \"'s part\",\n",
       " \"'s partial\",\n",
       " \"'s past\",\n",
       " \"'s path\",\n",
       " \"'s payoff\",\n",
       " \"'s perception\",\n",
       " \"'s perfect\",\n",
       " \"'s performance\",\n",
       " \"'s personal\",\n",
       " \"'s personality\",\n",
       " \"'s perspective\",\n",
       " \"'s phd\",\n",
       " ...]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "certified-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDocs = df_test.Abstract.tolist() \n",
    "InfoTheory_testLabels = df_test.InfoTheory.tolist() \n",
    "CompVis_testLabels = df_test.CompVis.tolist()\n",
    "Math_testLabels = df_test.Math.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "caring-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=vectorizer.transform(testDocs)\n",
    "y_test=np.asarray(InfoTheory_testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "charged-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "     model_name = model.__class__.__name__\n",
    "     accuracies = cross_val_score(model, x_train, y_train, scoring='accuracy', cv=CV)\n",
    "     for fold_idx, accuracy in enumerate(accuracies):\n",
    "          entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-height",
   "metadata": {},
   "source": [
    "## Logistic Regression on all the records of InfoTheory using Lemma Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "essential-priority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[15906   156]\n",
      " [  872  2744]]\n",
      "Accuracy: 0.9477589185892875\n",
      "Macro Precision: 0.9471170374998459\n",
      "Macro Recall: 0.8745685964674497\n",
      "Macro F1 score:0.9054656055103563\n",
      "MCC:0.8184766366683934\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "    print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-morning",
   "metadata": {},
   "source": [
    "## Logistic Regression on all the CompVis records using Lemma Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "imported-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(CompVisLabels)\n",
    "y_test = np.asarray(CompVis_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "accompanied-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[17470    56]\n",
      " [  899  1253]]\n",
      "Accuracy: 0.9514686451875191\n",
      "Macro Precision: 0.9541390502424598\n",
      "Macro Recall: 0.7895269089323266\n",
      "Macro F1 score:0.8487314057957174\n",
      "MCC:0.7252185200121738\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "    print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-passport",
   "metadata": {},
   "source": [
    "## Logistic Regression on all the Math records using Lemma Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "rotary-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(MathLabels)\n",
    "y_test = np.asarray(Math_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "color-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[13039   709]\n",
      " [ 1747  4183]]\n",
      "Accuracy: 0.8751905681471694\n",
      "Macro Precision: 0.868458597495432\n",
      "Macro Recall: 0.8269125762152865\n",
      "Macro F1 score:0.8434910664406337\n",
      "MCC:0.6941289486425499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "    print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-independence",
   "metadata": {},
   "source": [
    "## Considering all the InfoTheory records using Bert Tokenizer and Statistical model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "stainless-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Loading the BERT tokenizer.\n",
    "print('Loading BERT tokenizer')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "enclosed-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infotheory labels\n",
    "y_train = np.asarray(InfoTheoryLabels)\n",
    "y_test = np.asarray(InfoTheory_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "practical-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "\n",
    "for abstract in trainDocs:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745,\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "satisfactory-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "different-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "medical-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "\n",
    "for abstract in testDocs:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 745,\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "affected-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bearing-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "magnetic-knock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[16062     0]\n",
      " [ 3616     0]]\n",
      "Accuracy: 0.8162414879560931\n",
      "Macro Precision: 0.40812074397804654\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.44941242305540013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-draft",
   "metadata": {},
   "source": [
    "## Considering all the CompVis records using Bert Tokenizer and Statistical model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "consistent-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For CompVisLabels\n",
    "y_train = np.asarray(CompVisLabels)\n",
    "y_test = np.asarray(CompVis_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "primary-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[17526     0]\n",
      " [ 2152     0]]\n",
      "Accuracy: 0.8906392926110377\n",
      "Macro Precision: 0.44531964630551885\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4710783786689603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-skiing",
   "metadata": {},
   "source": [
    "## Considering all the Math records using Bert Tokenizer and Statistical model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "corrected-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Math labels\n",
    "y_train = np.asarray(MathLabels)\n",
    "y_test = np.asarray(Math_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "polyphonic-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[13748     0]\n",
      " [ 5930     0]]\n",
      "Accuracy: 0.6986482366094116\n",
      "Macro Precision: 0.3493241183047058\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4112965954646084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dense-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocs = df_train.Abstract.tolist()[:1000]\n",
    "InfoTheoryLabels = df_train.InfoTheory.tolist()[:1000]\n",
    "CompVisLabels = df_train.CompVis.tolist()[:1000]\n",
    "MathLabels = df_train.Math.tolist()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-sequence",
   "metadata": {},
   "source": [
    "## Considering first 1000 InfoTheory records using LemmaTokenizer and Statistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "prospective-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocs = df_train.Abstract.tolist()[:1000]\n",
    "InfoTheoryLabels = df_train.InfoTheory.tolist() [:1000]\n",
    "CompVisLabels = df_train.CompVis.tolist()[:1000]\n",
    "MathLabels = df_train.Math.tolist()[:1000]\n",
    "# print(len(InfoTheoryLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "expressed-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testDocs = df_test.Abstract.tolist()[:1000]\n",
    "# InfoTheory_testLabels = df_test.InfoTheory.tolist()[:1000]\n",
    "# CompVis_testLabels = df_test.CompVis.tolist()[:1000]\n",
    "# Math_testLabels = df_test.Math.tolist()[:1000]\n",
    "\n",
    "# testDocs = df_test.Abstract.tolist()\n",
    "# InfoTheory_testLabels = df_test.InfoTheory.tolist()\n",
    "# CompVis_testLabels = df_test.CompVis.tolist()\n",
    "# Math_testLabels = df_test.Math.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "surprising-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=vectorizer.fit_transform(trainDocs)\n",
    "x_test=vectorizer.transform(testDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "rotary-suffering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. ',\n",
       " ' A note on digitized angles We study the configurations of pixels that occur when two digitized straight lines meet each other. ',\n",
       " \" Textbook examples of recursion We discuss properties of recursive schemas related to McCarthy's ``91 function'' and to Takeuchi's triple recursion. Several theorems are proposed as interesting candidates for machine verification, and some intriguing open questions are raised. \",\n",
       " ' Theory and practice The author argues to Silicon Valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical. He particularly considers the intersection of the theory of algorithms and practical software development. He combines examples from the development of the TeX typesetting system with clever jokes, criticisms, and encouragements. ',\n",
       " \" Context-free multilanguages This article is a sketch of ideas that were once intended to appear in the author's famous series, The Art of Computer Programming . He generalizes the notion of a context-free language from a set to a multiset of words over an alphabet. The idea is to keep track of the number of ways to parse a string. For example, fruit flies like a banana can famously be parsed in two ways; analogous examples in the setting of programming languages may yet be important in the future. The treatment is informal but essentially rigorous. \",\n",
       " ' The problem of compatible representatives The purpose of this note is to attach a name to a natural class of combinatorial problems and to point out that this class includes many important special cases. We also show that a simple problem of placing nonoverlapping labels on a rectangular map is NP-complete. ',\n",
       " ' A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms. ',\n",
       " ' Dynamic Backtracking Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches. ',\n",
       " \" An Empirical Analysis of Search in GSAT We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms. \",\n",
       " ' A linear construction for certain Kerdock and Preparata codes The Nordstrom-Robinson, Kerdock, and (slightly modified) Pre\\\\- parata codes are shown to be linear over , the integers . The Kerdock and Preparata codes are duals over , and the Nordstrom-Robinson code is self-dual. All these codes are just extended cyclic codes over . This provides a simple definition for these codes and explains why their Hamming weight distributions are dual to each other. First- and second-order Reed-Muller codes are also linear codes over , but Hamming codes in general are not, nor is the Golay code. ',\n",
       " \" Software Agents: Completing Patterns and Constructing User Interfaces To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime. \",\n",
       " ' The Difficulties of Learning Logic Programs with Cut As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages. ',\n",
       " ' Decidable Reasoning in Terminological Knowledge Representation Systems Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted. ',\n",
       " ' Mini-indexes for literate programs This paper describes how to implement a documentation technique that helps readers to understand large programs or collections of programs, by providing local indexes to all identifiers that are visible on every two-page spread. A detailed example is given for a program that finds all Hamiltonian circuits in an undirected graph. ',\n",
       " ' Teleo-Reactive Programs for Agent Control A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots. ',\n",
       " \" Bias-Driven Revision of Logical Domain Theories The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories. \",\n",
       " ' Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator SPA based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms. ',\n",
       " \" Substructure Discovery Using Minimum Description Length and Background Knowledge The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value. \",\n",
       " \" Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees. \",\n",
       " ' An Alternative Conception of Tree-Adjoining Derivation The precise formulation of derivation for tree-adjoining grammars has important ramifications for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and statistical language modeling. We argue that the definition of tree-adjoining derivation must be reformulated in order to manifest the proper linguistic dependencies in derivations. The particular proposal is both precisely characterizable through a definition of TAG derivations as equivalence classes of ordered derivation trees, and computationally operational, by virtue of a compilation to linear indexed grammars together with an efficient algorithm for recognition and parsing according to the compiled grammar. ',\n",
       " \" Lessons from a Restricted Turing Test We report on the recent Loebner prize competition inspired by Turing's test of intelligent behavior. The presentation covers the structure of the competition and the outcome of its first instantiation in an actual event, and an analysis of the purpose, design, and appropriateness of such a competition. We argue that the competition has no clear purpose, that its design prevents any useful outcome, and that such a competition is inappropriate given the current level of technology. We then speculate as to suitable alternatives to the Loebner prize. \",\n",
       " \" An Empirically Motivated Reinterpretation of Dependency Grammar Dependency grammar is usually interpreted as equivalent to a strict form of X--bar theory that forbids the stacking of nodes of the same bar level (e.g., N' immediately dominating N' with the same head). But adequate accounts of _one_--anaphora and of the semantics of multiple modifiers require such stacking and accordingly argue against dependency grammar. Dependency grammar can be salvaged by reinterpreting its claims about phrase structure, so that modifiers map onto binary--branching X--bar trees rather than ``flat'' ones. \",\n",
       " ' Memoization in Constraint Logic Programming This paper shows how to apply memoization (caching of subgoals and associated answer substitutions) in a constraint logic programming setting. The research is is motivated by the desire to apply constraint logic programming CLP to problems in natural language processing that involve (constraint) interleaving or coroutining, such as GB and HPSG parsing. ',\n",
       " \" SPANISH 1992 (S92): corpus-based analysis of present-day Spanish for medical purposes S92 research was begun in 1987 to analyze word frequencies in present-day Spanish for making speech pathology evaluation tools. 500 2,000-word samples of children, adolescents and adults' language were input between 1988-1991, calculations done in 1992; statistical and Lewandowski analyses were carried out in 1993. \",\n",
       " ' Constraint-Based Categorial Grammar We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints. In particular, the introduction of relational constraints allows one to capture the effects of (recursive) lexical rules in a computationally attractive manner. We illustrate the linguistic merits of the new approach by showing how it accounts for the syntax of Dutch cross-serial dependencies and the position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints. ',\n",
       " ' Principles and Implementation of Deductive Parsing We present a system for generating parsers based directly on the metaphor of parsing as deduction. Parsing algorithms can be represented directly as deduction systems, and a single deduction engine can interpret such deduction systems so as to implement the corresponding parser. The method generalizes easily to parsers for augmented phrase structure formalisms, such as definite-clause grammars and other logic grammar formalisms, and has been used for rapid prototyping of parsing algorithms for a variety of formalisms including variants of tree-adjoining grammars, categorial grammars, and lexicalized context-free grammars. ',\n",
       " \" Adjuncts and the Processing of Lexical Rules The standard HPSG analysis of Germanic verb clusters can not explain the observed narrow-scope readings of adjuncts in such verb clusters. We present an extension of the HPSG analysis that accounts for the systematic ambiguity of the scope of adjuncts in verb cluster constructions, by treating adjuncts as members of the subcat list. The extension uses powerful recursive lexical rules, implemented as complex constraints. We show how `delayed evaluation' techniques from constraint-logic programming can be used to process such lexical rules. \",\n",
       " \" Similarity-Based Estimation of Word Cooccurrence Probabilities In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error. \",\n",
       " ' Some Bibliographical References on Intonation and Intonational Meaning A by-no-means-complete collection of references for those interested in intonational meaning, with other miscellaneous references on intonation included. Additional references are welcome, and should be sent to julia@research.att.com. ',\n",
       " ' Temporal Relations: Reference or Discourse Coherence? The temporal relations that hold between events described by successive utterances are often left implicit or underspecified. We address the role of two phenomena with respect to the recovery of these relations: (1) the referential properties of tense, and (2) the role of temporal constraints imposed by coherence relations. We account for several facets of the identification of temporal relations through an integration of these. ',\n",
       " ' Common Topics and Coherent Situations: Interpreting Ellipsis in the Context of Discourse Inference It is claimed that a variety of facts concerning ellipsis, event reference, and interclausal coherence can be explained by two features of the linguistic form in question: (1) whether the form leaves behind an empty constituent in the syntax, and (2) whether the form is anaphoric in the semantics. It is proposed that these features interact with one of two types of discourse inference, namely {\\\\it Common Topic} inference and {\\\\it Coherent Situation} inference. The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account. ',\n",
       " ' Efficiency, Robustness, and Accuracy in Picky Chart Parsing This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called {\\\\em probabilistic prediction} to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser. ',\n",
       " ' Pearl: A Probabilistic Chart Parser This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the best parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar. ',\n",
       " \" Syntactic-Head-Driven Generation The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990. This is the case for the semantic analysis rules of certain constraint-based semantic representations, e.g. Underspecified Discourse Representation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven generation in general has its merits, we simply return to a syntactic definition of `head' and demonstrate the feasibility of syntactic-head-driven generation. In addition to its generality, a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of (syntactic) heads, for which only ad-hoc solutions existed, so far. \",\n",
       " ' Towards History-based Grammars: Using Richer Models for Probabilistic Parsing We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error. ',\n",
       " \" Natural Language Parsing as Statistical Pattern Recognition Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules. In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%. \",\n",
       " \" A Stochastic Finite-State Word-Segmentation Algorithm for Chinese We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and productively derived words, and providing pronunciations for these words; the method incorporates a class-based model in its treatment of personal names. We also evaluate the system's performance, taking into account the fact that people often do not agree on a single segmentation. \",\n",
       " ' A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues This paper presents a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system (consultant) and user (executing agent) disagree. Our work contributes to an overall system for collaborative problem-solving by providing a plan-based framework that captures the {\\\\em Propose-Evaluate-Modify} cycle of collaboration, and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims. In addition, our system handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. Furthermore, it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered. ',\n",
       " ' Integration Of Visual Inter-word Constraints And Linguistic Knowledge In Degraded Text Recognition Degraded text recognition is a difficult task. Given a noisy text image, a word recognizer can be applied to generate several candidates for each word image. High-level knowledge sources can then be used to select a decision from the candidate set for each word image. In this paper, we propose that visual inter-word constraints can be used to facilitate candidate selection. Visual inter-word constraints provide a way to link word images inside the text page, and to interpret them systematically. ',\n",
       " ' Classifying Cue Phrases in Text and Speech Using Machine Learning Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification rules from sets of pre-classified cue phrases and their features. Machine learning is shown to be an effective technique for not only automating the generation of classification rules, but also for improving upon previous results. ',\n",
       " ' Collaboration on reference to objects that are not mutually known In conversation, a person sometimes has to refer to an object that is not previously known to the other participant. We present a plan-based model of how agents collaborate on reference of this sort. In making a reference, an agent uses the most salient attributes of the referent. In understanding a reference, an agent determines his confidence in its adequacy as a means of identifying the referent. To collaborate, the agents use judgment, suggestion, and elaboration moves to refashion an inadequate referring expression. ',\n",
       " \" Intention-based Segmentation: Human Reliability and Correlation with Linguistic Cues Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics. \",\n",
       " \" Best-first Model Merging for Hidden Markov Model Induction This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models. \",\n",
       " ' Precise n-gram Probabilities from Stochastic Context-free Grammars We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. We discuss efficient implementation of the algorithm and report our practical experience with it. ',\n",
       " ' Memory-Based Lexical Acquisition and Processing Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described. ',\n",
       " ' Determination of referential property and number of nouns in Japanese sentences for machine translation into English When translating Japanese nouns into English, we face the problem of articles and numbers which the Japanese language does not have, but which are necessary for the English composition. To solve this difficult problem we classified the referential property and the number of nouns into three types respectively. This paper shows that the referential property and the number of nouns in a sentence can be estimated fairly reliably by the words in the sentence. Many rules for the estimation were written in forms similar to rewriting rules in expert systems. We obtained the correct recognition scores of 85.5\\\\% and 89.0\\\\% in the estimation of the referential property and the number respectively for the sentences which were used for the construction of our rules. We tested these rules for some other texts, and obtained the scores of 68.9\\\\% and 85.6\\\\% respectively. ',\n",
       " ' Capturing CFLs with Tree Adjoining Grammars We define a decidable class of TAGs that is strongly equivalent to CFGs and is cubic-time parsable. This class serves to lexicalize CFGs in the same manner as the LCFGs of Schabes and Waters but with considerably less restriction on the form of the grammars. The class provides a normal form for TAGs that generate local sets in much the same way that regular grammars provide a normal form for CFGs that generate regular sets. ',\n",
       " ' Generating Precondition Expressions in Instructional Text This study employs a knowledge intensive corpus analysis to identify the elements of the communicative context which can be used to determine the appropriate lexical and grammatical form of instructional texts. \\\\ig, an instructional text generation system based on this analysis, is presented, particularly with reference to its expression of precondition relations. ',\n",
       " \" An Integrated Heuristic Scheme for Partial Parse Evaluation GLR* is a recently developed robust version of the Generalized LR Parser, that can parse almost ANY input sentence by ignoring unrecognizable parts of the sentence. On a given input sentence, the parser returns a collection of parses that correspond to maximal, or close to maximal, parsable subsets of the original input. This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed ``best'' from such a collection. We describe the heuristic measures used and their combination scheme. Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported. \",\n",
       " ' Grammar Specialization through Entropy Thresholds Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values. ',\n",
       " \" Abductive Equivalential Translation and its application to Natural Language Database Interfacing The thesis describes a logical formalization of natural-language database interfacing. We assume the existence of a ``natural language engine'' capable of mediating between surface linguistic string and their representations as ``literal'' logical forms: the focus of interest will be the question of relating ``literal'' logical forms to representations in terms of primitives meaningful to the underlying database engine. We begin by describing the nature of the problem, and show how a variety of interface functionalities can be considered as instances of a type of formal inference task which we call ``Abductive Equivalential Translation'' AET; functionalities which can be reduced to this form include answering questions, responding to commands, reasoning about the completeness of answers, answering meta-questions of type ``Do you know...'', and generating assertions and questions. In each case, a ``linguistic domain theory'' LDT and an input formula are given, and the goal is to construct a formula with certain properties which is equivalent to , given and a set of permitted assumptions. If the LDT is of a certain specified type, whose formulas are either conditional equivalences or Horn-clauses, we show that the AET problem can be reduced to a goal-directed inference method. We present an abstract description of this method, and sketch its realization in Prolog. The relationship between AET and several problems previously discussed in the literature is discussed. In particular, we show how AET can provide a simple and elegant solution to the so-called ``Doctor on Board'' problem, and in effect allows a ``relativization'' of the Closed World Assumption. The ideas in the thesis have all been implemented concretely within the SRI CLARE project, using a real projects and payments database. The LDT for the example database is described in detail, and examples of the types of functionality that can be achieved within the example domain are presented. \",\n",
       " \" An Extended Theory of Head-Driven Parsing We show that more head-driven parsing algorithms can be formulated than those occurring in the existing literature. These algorithms are inspired by a family of left-to-right parsing algorithms from a recent publication. We further introduce a more advanced notion of ``head-driven parsing'' which allows more detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques. \",\n",
       " ' An Optimal Tabular Parsing Algorithm In this paper we relate a number of parsing algorithms which have been developed in very different areas of parsing theory, and which include deterministic algorithms, tabular algorithms, and a parallel algorithm. We show that these algorithms are based on the same underlying ideas. By relating existing ideas, we hope to provide an opportunity to improve some algorithms based on features of others. A second purpose of this paper is to answer a question which has come up in the area of tabular parsing, namely how to obtain a parsing algorithm with the property that the table will contain as little entries as possible, but without the possibility that two entries represent the same subderivation. ',\n",
       " ' Acquiring Receptive Morphology: A Connectionist Model This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffixation, prefixation, infixation, circumfixation, mutation, template, and deletion rules. Separate network modules responsible for syllables enable to the network to learn simple reduplication rules as well. The model also embodies constraints against association-line crossing. ',\n",
       " \" A Deductive Account of Quantification in LFG The relationship between Lexical-Functional Grammar LFG functional structures (f-structures) for sentences and their semantic interpretations can be expressed directly in a fragment of linear logic in a way that explains correctly the constrained interactions between quantifier scope ambiguity and bound anaphora. The use of a deductive framework to account for the compositional properties of quantifying expressions in natural language obviates the need for additional mechanisms, such as Cooper storage, to represent the different scopes that a quantifier might take. Instead, the semantic contribution of a quantifier is recorded as an ordinary logical formula, one whose use in a proof will establish the scope of the quantifier. The properties of linear logic ensure that each quantifier is scoped exactly once. Our analysis of quantifier scope can be seen as a recasting of Pereira's analysis (Pereira, 1991), which was expressed in higher-order intuitionistic logic. But our use of LFG and linear logic provides a much more direct and computationally more flexible interpretation mechanism for at least the same range of phenomena. We have developed a preliminary Prolog implementation of the linear deductions described in this work. \",\n",
       " \" Semantics of Complex Sentences in Japanese The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively. However if there can be relations between every pair of semantic roles, the amount of computation to identify the relations that hold in the given sentence is extremely large. In this paper, for semantics of Japanese complex sentence, we introduce new pragmatic roles called `observer' and `motivated' respectively to bridge semantic roles of subordinate and those of main clauses. By these new roles constraints on the relations among semantic/pragmatic roles are known to be almost local within subordinate or main clause. In other words, as for the semantics of the whole complex sentence, the only role we should deal with is a motivated. \",\n",
       " ' An Attributive Logic of Set Descriptions and Set Operations This paper provides a model theoretic semantics to feature terms augmented with set descriptions. We provide constraints to specify HPSG style set descriptions, fixed cardinality set descriptions, set-membership constraints, restricted universal role quantifications, set union, intersection, subset and disjointness. A sound, complete and terminating consistency checking procedure is provided to determine the consistency of any given term in the logic. It is shown that determining consistency of terms is a NP-complete problem. ',\n",
       " ' Modularity in a Connectionist Model of Morphology Acquisition This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. In its simplest version, the network consists of separate simple recurrent subnetworks for root and inflection identification; both networks take the phone sequence as inputs. It is shown that the performance of the two separate modular networks is superior to a single network responsible for both root and inflection identification. In a more elaborate version of the model, the network learns to use separate hidden-layer modules to solve the separate tasks of root and inflection identification. ',\n",
       " \" Priority Union and Generalization in Discourse Grammars We describe an implementation in Carpenter's typed feature formalism, ALE, of a discourse grammar of the kind proposed by Scha, Polanyi, et al. We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization. We describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of VP-ellipsis which exhibit a strict/sloppy ambiguity. \",\n",
       " ' Structural Tags, Annealing and Automatic Word Classification This paper describes an automatic word classification system which uses a locally optimal annealing algorithm and average class mutual information. A new word-class representation, the structural tag is introduced and its advantages for use in statistical language modelling are presented. A summary of some results with the one million word LOB corpus is given; the algorithm is also shown to discover the vowel-consonant distinction and displays an ability to cluster words syntactically in a Latin corpus. Finally, a comparison is made between the current classification system and several leading alternative systems, which shows that the current system performs respectably well. ',\n",
       " \" Applying GSAT to Non-Clausal Formulas In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular ``score'' function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far. \",\n",
       " ' A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest. ',\n",
       " ' Dual-Coding Theory and Connectionist Lexical Selection We introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation. This lexical selection approach has two advantages. First, it is learnable. Little human effort on knowledge engineering is required. Secondly, it is psycholinguistically well-founded. ',\n",
       " ' Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation To acquire noun phrases from running texts is useful for many applications, such as word grouping,terminology indexing, etc. The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem. In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism. The test texts are SUSANNE Corpus and the results are evaluated by comparing the parse field of SUSANNE Corpus automatically. The results of this preliminary experiment are encouraging. ',\n",
       " ' Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers. ',\n",
       " ' A Learning Approach to Natural Language Understanding In this paper we propose a learning paradigm for the problem of understanding spoken language. The basis of the work is in a formalization of the understanding problem as a communication problem. This results in the definition of a stochastic model of the production of speech or text starting from the meaning of a sentence. The resulting understanding algorithm consists in a Viterbi maximization procedure, analogous to that commonly used for recognizing speech. The algorithm was implemented for building ',\n",
       " ' Intentions and Information in Discourse This paper is about the flow of inference between communicative intentions, discourse structure and the domain during discourse processing. We augment a theory of discourse interpretation with a theory of distinct mental attitudes and reasoning about them, in order to provide an account of how the attitudes interact with reasoning about discourse structure. ',\n",
       " ' Speech Dialogue with Facial Displays: Multimodal Human-Computer Conversation Human face-to-face conversation is an ideal model for human-computer dialogue. One of the major features of face-to-face communication is its multiplicity of communication channels that act on multiple modalities. To realize a natural multimodal dialogue, it is necessary to study how humans perceive information and determine the information to which humans are sensitive. A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial expressions. We have developed an experimental system that integrates speech dialogue and facial animation, to investigate the effect of introducing communicative facial expressions as a new modality in human-computer conversation. Our experiments have shown that facial expressions are helpful, especially upon first contact with the system. We have also discovered that featuring facial expressions at an early stage improves subsequent interaction. ',\n",
       " ' Towards a Principled Representation of Discourse Plans We argue that discourse plans must capture the intended causal and decompositional relations between communicative actions. We present a planning algorithm, DPOCL, that builds plan structures that properly capture these relations, and show how these structures are used to solve the problems that plagued previous discourse planners, and allow a system to participate effectively and flexibly in an ongoing dialogue. ',\n",
       " ' Word-Sense Disambiguation Using Decomposable Models Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest . We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data. ',\n",
       " \" Aligning a Parallel English-Chinese Corpus Statistically with Lexical Criteria We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues. \",\n",
       " ' Detecting and Correcting Speech Repairs Interactive spoken dialog provides many new challenges for spoken language systems. One of the most critical is the prevalence of speech repairs. This paper presents an algorithm that detects and corrects speech repairs based on finding the repair pattern. The repair pattern is built by finding word matches and word replacements, and identifying fragments and editing terms. Rather than using a set of prebuilt templates, we build the pattern on the fly. In a fair test, our method, when combined with a statistical model to filter possible repairs, was successful at detecting and correcting 80\\\\% of the repairs, without using prosodic information or a parser. ',\n",
       " \" Multiset-Valued Linear Index Grammars: Imposing Dominance Constraints on Derivations This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links. The former models certain uses of multiset-valued feature structures in unification-based formalisms, while the latter is motivated by word order variation and by ``quasi-trees'', a generalization of trees. The two formalisms are weakly equivalent, and an important subset is at most context-sensitive and polynomially parsable. \",\n",
       " \" Parsing Turkish with the Lexical Functional Grammar Formalism This paper describes our work on parsing Turkish using the lexical-functional grammar formalism. This work represents the first significant effort for parsing Turkish. Our implementation is based on Tomita's parser developed at Carnegie-Mellon University Center for Machine Translation. The grammar covers a substantial subset of Turkish including simple and complex sentences, and deals with a reasonable amount of word order freeness. The complex agglutinative morphology of Turkish lexical structures is handled using a separate two-level morphological analyzer. After a discussion of key relevant issues regarding Turkish grammar, we discuss aspects of our system and present results from our implementation. Our initial results suggest that our system can parse about 82\\\\% of the sentences directly and almost all the remaining with very minor pre-editing. \",\n",
       " ' Some Advances in Transformation-Based Part of Speech Tagging Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In [Brill92], a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that are not captured by stochastic taggers. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty. ',\n",
       " \" Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging Eric Brill has recently proposed a simple and powerful corpus-based language modeling approach that can be applied to various tasks including part-of-speech tagging and building phrase structure trees. The method learns a series of symbolic transformational rules, which can then be applied in sequence to a test corpus to produce predictions. The learning process only requires counting matches for a given set of rule templates, allowing the method to survey a very large space of possible contextual factors. This paper analyses Brill's approach as an interesting variation on existing decision tree methods, based on experiments involving part-of-speech tagging for both English and ancient Greek corpora. In particular, the analysis throws light on why the new mechanism seems surprisingly resistant to overtraining. A fast, incremental implementation and a mechanism for recording the dependencies that underlie the resulting rule sequence are also described. \",\n",
       " ' Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions With the advent of faster computers, the notion of doing machine translation from a huge stored database of translation examples is no longer unreasonable. This paper describes an attempt to merge the Example-Based Machine Translation EBMT approach with psycholinguistic principles. A new formalism for context- free grammars, called *marker-normal form*, is demonstrated and used to describe language data in a way compatible with psycholinguistic theories. By embedding this formalism in a standard multivariate optimization framework, a system can be built that infers correct transfer functions for a set of bilingual sentence pairs and then uses those functions to translate novel sentences. The validity of this line of reasoning has been tested in the development of a system called METLA-1. This system has been used to infer English->French and English->Urdu transfer functions from small corpora. The results of those experiments are examined, both in engineering terms as well as in more linguistic terms. In general, the results of these experiments were psycho- logically and linguistically well-grounded while still achieving a respectable level of success when compared against a similar prototype using Hidden Markov Models. ',\n",
       " ' Graded Unification: A Framework for Interactive Processing An extension to classical unification, called {\\\\em graded unification} is presented. It is capable of combining contradictory information. An interactive processing paradigm and parser based on this new operator are also presented. ',\n",
       " \" A Hybrid Reasoning Model for Indirect Answers This paper presents our implemented computational model for interpreting and generating indirect answers to Yes-No questions. Its main features are 1) a discourse-plan-based approach to implicature, 2) a reversible architecture for generation and interpretation, 3) a hybrid reasoning model that employs both plan inference and logical inference, and 4) use of stimulus conditions to model a speaker's motivation for providing appropriate, unrequested information. The model handles a wider range of types of indirect answers than previous computational models and has several significant advantages. \",\n",
       " ' An Automatic Method of Finding Topic Boundaries This article outlines a new method of locating discourse boundaries based on lexical cohesion and a graphical technique called dotplotting. The application of dotplotting to discourse segmentation can be performed either manually, by examining a graph, or automatically, using an optimization algorithm. The results of two experiments involving automatically locating boundaries between a series of concatenated documents are presented. Areas of application and future directions for this work are also outlined. ',\n",
       " ' Corpus-Driven Knowledge Acquisition for Discourse Analysis The availability of large on-line text corpora provides a natural and promising bridge between the worlds of natural language processing NLP and machine learning ML. In recent years, the NLP community has been aggressively investigating statistical techniques to drive part-of-speech taggers, but application-specific text corpora can be used to drive knowledge acquisition at much higher levels as well. In this paper we will show how ML techniques can be used to support knowledge acquisition for information extraction systems. It is often very difficult to specify an explicit domain model for many information extraction applications, and it is always labor intensive to implement hand-coded heuristics for each new domain. We have discovered that it is nevertheless possible to use ML algorithms in order to capture knowledge that is only implicitly present in a representative text corpus. Our work addresses issues traditionally associated with discourse analysis and intersentential inference generation, and demonstrates the utility of ML algorithms at this higher level of language analysis. The benefits of our work address the portability and scalability of information extraction IE technologies. When hand-coded heuristics are used to manage discourse analysis in an information extraction system, months of programming effort are easily needed to port a successful IE system to a new domain. We will show how ML algorithms can reduce this ',\n",
       " ' Statistical Augmentation of a Chinese Machine-Readable Dictionary We describe a method of using statistically-collected Chinese character groups from a corpus to augment a Chinese dictionary. The method is particularly useful for extracting domain-specific and regional words not readily available in machine-readable dictionaries. Output was evaluated both using human evaluators and against a previously available dictionary. We also evaluated performance improvement in automatic Chinese tokenization. Results show that our method outputs legitimate words, acronymic constructions, idioms, names and titles, as well as technical compounds, many of which were lacking from the original dictionary. ',\n",
       " ' DPOCL: A Principled Approach to Discourse Planning Research in discourse processing has identified two representational requirements for discourse planning systems. First, discourse plans must adequately represent the intentional structure of the utterances they produce in order to enable a computational discourse agent to respond effectively to communicative failures \\\\cite{MooreParisCL}. Second, discourse plans must represent the informational structure of utterances. In addition to these representational requirements, we argue that discourse planners should be formally characterizable in terms of soundness and completeness. ',\n",
       " ' An implemented model of punning riddles In this paper, we discuss a model of simple question-answer punning, implemented in a program, JAPE, which generates riddles from humour-independent lexical entries. The model uses two main types of structure: schemata, which determine the relationships between key words in a joke, and templates, which produce the surface form of the joke. JAPE succeeds in generating pieces of text that are recognizably jokes, but some of them are not very good jokes. We mention some potential improvements and extensions, including post-production heuristics for ordering the jokes according to quality. ',\n",
       " \" A symbolic description of punning riddles and its computer implementation Riddles based on simple puns can be classified according to the patterns of word, syllable or phrase similarity they depend upon. We have devised a formal model of the semantic and syntactic regularities underlying some of the simpler types of punning riddle. We have also implemented this preliminary theory in a computer program which can generate riddles from a lexicon containing general data about words and phrases; that is, the lexicon content is not customised to produce jokes. Informal evaluation of the program's results by a set of human judges suggest that the riddles produced by this program are of comparable quality to those in general circulation among school children. \",\n",
       " ' A Spanish Tagset for the CRATER Project This working paper describes the Spanish tagset to be used in the context of CRATER, a CEC funded project aiming at the creation of a multilingual (English, French, Spanish) aligned corpus using the International Telecommunications Union corpus. In this respect, each version of the corpus will be (or is currently) tagged. Xerox PARC tagger will be adapted to Spanish in order to perform the tagging of the Spanish version. This tagset has been devised as the ideal one for Spanish, and has been posted to several lists in order to get feedback to it. ',\n",
       " ' TDL--- A Type Description Language for Constraint-Based Grammars This paper presents \\\\tdl, a typed feature-based representation language and inference system. Type definitions in \\\\tdl\\\\ consist of type and feature constraints over the boolean connectives. \\\\tdl\\\\ supports open- and closed-world reasoning over types and allows for partitions and incompatible types. Working with partially as well as with fully expanded types is possible. Efficient reasoning in \\\\tdl\\\\ is accomplished through specialized modules. ',\n",
       " ' Emergent Parsing and Generation with Generalized Chart A new, flexible inference method for Horn logic program is proposed, which is a drastic generalization of chart parsing, partial instantiations of clauses in a program roughly corresponding to arcs in a chart. Chart-like parsing and semantic-head-driven generation emerge from this method. With a parsimonious instantiation scheme for ambiguity packing, the parsing complexity reduces to that of standard chart-based algorithms. ',\n",
       " ' Learning Fault-tolerant Speech Parsing with SCREEN This paper describes a new approach and a system SCREEN for fault-tolerant speech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for Natural language. Speech parsing describes the syntactic and semantic analysis of spontaneous spoken language. The general approach is based on incremental immediate flat analysis, learning of syntactic and semantic speech parsing, parallel integration of current hypotheses, and the consideration of various forms of speech related errors. The goal for this approach is to explore the parallel interactions between various knowledge sources for learning incremental fault-tolerant speech parsing. This approach is examined in a system SCREEN using various hybrid connectionist techniques. Hybrid connectionist techniques are examined because of their promising properties of inherent fault tolerance, learning, gradedness and parallel constraint integration. The input for SCREEN is hypotheses about recognized words of a spoken utterance potentially analyzed by a speech system, the output is hypotheses about the flat syntactic and semantic analysis of the utterance. In this paper we focus on the general approach, the overall architecture, and examples for learning flat syntactic speech parsing. Different from most other speech language architectures SCREEN emphasizes an interactive rather than an autonomous position, learning rather than encoding, flat analysis rather than in-depth analysis, and fault-tolerant processing of phonetic, syntactic and semantic knowledge. ',\n",
       " ' The Very Idea of Dynamic Semantics Natural languages are programming languages for minds. Can we or should we take this slogan seriously? If so, how? Can answers be found by looking at the various dynamic treatments of natural language developed over the last decade or so, mostly in response to problems associated with donkey anaphora? In Dynamic Logic of Programs, the meaning of a program is a binary relation on the set of states of some abstract machine. This relation is meant to model aspects of the effects of the execution of the program, in particular its input-output behavior. What, if anything, are the dynamic aspects of various proposed dynamic semantics for natural languages supposed to model? Is there anything dynamic to be modeled? If not, what is all the full about? We shall try to answer some, at least, of these questions and provide materials for answers to others. ',\n",
       " ' A Complete and Recursive Feature Theory Various feature descriptions are being employed in logic programming languages and constrained-based grammar formalisms. The common notational primitive of these descriptions are functional attributes called features. The descriptions considered in this paper are the possibly quantified first-order formulae obtained from a signature of binary and unary predicates called features and sorts, respectively. We establish a first-order theory FT by means of three axiom schemes, show its completeness, and construct three elementarily equivalent models. One of the models consists of so-called feature graphs, a data structure common in computational linguistics. The other two models consist of so-called feature trees, a record-like data structure generalizing the trees corresponding to first-order terms. Our completeness proof exhibits a terminating simplification system deciding validity and satisfiability of possibly quantified feature descriptions. ',\n",
       " ' Analyzing and Improving Statistical Language Models for Speech Recognition In many current speech recognizers, a statistical language model is used to indicate how likely it is that a certain word will be spoken next, given the words recognized so far. How can statistical language models be improved so that more complex speech recognition tasks can be tackled? Since the knowledge of the weaknesses of any theory often makes improving the theory easier, the central idea of this thesis is to analyze the weaknesses of existing statistical language models in order to subsequently improve them. To that end, we formally define a weakness of a statistical language model in terms of the logarithm of the total probability, LTP, a term closely related to the standard perplexity measure used to evaluate statistical language models. We apply our definition of a weakness to a frequently used statistical language model, called a bi-pos model. This results, for example, in a new modeling of unknown words which improves the performance of the model by 14% to 21%. Moreover, one of the identified weaknesses has prompted the development of our generalized N-pos language model, which is also outlined in this thesis. It can incorporate linguistic knowledge even if it extends over many words and this is not feasible in a traditional N-pos model. This leads to a discussion of whatknowledge should be added to statistical language models in general and we give criteria for selecting potentially useful knowledge. These results show the usefulness of both our definition of a weakness and of performing an analysis of weaknesses of statistical language models in general. ',\n",
       " \" A Computational Model of Syntactic Processing: Ambiguity Resolution from Interpretation Syntactic ambiguity abounds in natural language, yet humans have no difficulty coping with it. In fact, the process of ambiguity resolution is almost always unconscious. But it is not infallible, however, as example 1 demonstrates. 1. The horse raced past the barn fell. This sentence is perfectly grammatical, as is evident when it appears in the following context: 2. Two horses were being shown off to a prospective buyer. One was raced past a meadow. and the other was raced past a barn. ... Grammatical yet unprocessable sentences such as 1 are called `garden-path sentences.' Their existence provides an opportunity to investigate the human sentence processing mechanism by studying how and when it fails. The aim of this thesis is to construct a computational model of language understanding which can predict processing difficulty. The data to be modeled are known examples of garden path and non-garden path sentences, and other results from psycholinguistics. It is widely believed that there are two distinct loci of computation in sentence processing: syntactic parsing and semantic interpretation. One longstanding controversy is which of these two modules bears responsibility for the immediate resolution of ambiguity. My claim is that it is the latter, and that the syntactic processing module is a very simple device which blindly and faithfully constructs all possible analyses for the sentence up to the current point of processing. The interpretive module serves as a filter, occasionally discarding certain of these analyses which it deems less appropriate for the ongoing discourse than their competitors. This document is divided into three parts. The first is introductory, and reviews a selection of proposals from the sentence processing literature. The second part explores a body of data which has been adduced in support of a theory of structural preferences --- one that is inconsistent with the present claim. I show how the current proposal can be specified to account for the available data, and moreover to predict where structural preference theories will go wrong. The third part is a theoretical investigation of how well the proposed architecture can be realized using current conceptions of linguistic competence. In it, I present a parsing algorithm and a meaning-based ambiguity resolution method. \",\n",
       " \" A Psycholinguistically Motivated Parser for CCG Considering the speed in which humans resolve syntactic ambiguity, and the overwhelming evidence that syntactic ambiguity is resolved through selection of the analysis whose interpretation is the most `sensible', one comes to the conclusion that interpretation, hence parsing take place incrementally, just about every word. Considerations of parsimony in the theory of the syntactic processor lead one to explore the simplest of parsers: one which represents only analyses as defined by the grammar and no other information. Toward this aim of a simple, incremental parser I explore the proposal that the competence grammar is a Combinatory Categorial Grammar CCG. I address the problem of the proliferating analyses that stem from CCG's associativity of derivation. My solution involves maintaining only the maximally incremental analysis and, when necessary, computing the maximally right-branching analysis. I use results from the study of rewrite systems to show that this computation is efficient. \",\n",
       " ' Resolution of Syntactic Ambiguity: the Case of New Subjects I review evidence for the claim that syntactic ambiguities are resolved on the basis of the meaning of the competing analyses, not their structure. I identify a collection of ambiguities that do not yet have a meaning-based account and propose one which is based on the interaction of discourse and grammatical function. I provide evidence for my proposal by examining statistical properties of the Penn Treebank of syntactically annotated text. ',\n",
       " ' The complexity of normal form rewrite sequences for Associativity The complexity of a particular term-rewrite system is considered: the rule of associativity (x*y)*z --> x*(y*z). Algorithms and exact calculations are given for the longest and shortest sequences of applications of --> that result in normal form NF. The shortest NF sequence for a term x is always n-drm(x), where n is the number of occurrences of * in x and drm(x) is the depth of the rightmost leaf of x. The longest NF sequence for any term is of length n(n-1)/2. ',\n",
       " \" Anytime Algorithms for Speech Parsing? This paper discusses to which extent the concept of ``anytime algorithms'' can be applied to parsing algorithms with feature unification. We first try to give a more precise definition of what an anytime algorithm is. We arque that parsing algorithms have to be classified as contract algorithms as opposed to (truly) interruptible algorithms. With the restriction that the transaction being active at the time an interrupt is issued has to be completed before the interrupt can be executed, it is possible to provide a parser with limited anytime behavior, which is in fact being realized in our research prototype. \",\n",
       " ' Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. ',\n",
       " ' Multi-Paragraph Segmentation of Expository Text This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts. ',\n",
       " ' Text Analysis Tools in Spoken Language Processing This submission contains the postscript of the final version of the slides used in our ACL-94 tutorial. ',\n",
       " ' Verb Semantics and Lexical Selection This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation MT. Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches KBMT, and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection. ',\n",
       " ' An Empirical Model of Acknowledgment for Spoken-Language Systems We refine and extend prior views of the description, purposes, and contexts-of-use of acknowledgment acts through empirical examination of the use of acknowledgments in task-based conversation. We distinguish three broad classes of acknowledgments (other-->ackn, self-->other-->ackn, and self+ackn) and present a catalogue of 13 patterns within these classes that account for the specific uses of acknowledgment in the corpus. ',\n",
       " ' Three studies of grammar-based surface-syntactic parsing of unrestricted English text. A summary and orientation The dissertation addresses the design of parsing grammars for automatic surface-syntactic analysis of unconstrained English text. It consists of a summary and three articles. {\\\\it Morphological disambiguation} documents a grammar for morphological (or part-of-speech) disambiguation of English, done within the Constraint Grammar framework proposed by Fred Karlsson. The disambiguator seeks to discard those of the alternative morphological analyses proposed by the lexical analyser that are contextually illegitimate. The 1,100 constraints express some 23 general, essentially syntactic statements as restrictions on the linear order of morphological tags. The error rate of the morphological disambiguator is about ten times smaller than that of another state-of-the-art probabilistic disambiguator, given that both are allowed to leave some of the hardest ambiguities unresolved. This accuracy suggests the viability of the grammar-based approach to natural language parsing, thus also contributing to the more general debate concerning the viability of probabilistic vs.\\\\ linguistic techniques. {\\\\it Experiments with heuristics} addresses the question of how to resolve those ambiguities that survive the morphological disambiguator. Two approaches are presented and empirically evaluated: (i) heuristic disambiguation constraints and (ii) techniques for learning from the fully disambiguated part of the corpus and then applying this information to resolving remaining ambiguities. ',\n",
       " ' Learning unification-based grammars using the Spoken English Corpus This paper describes a grammar learning system that combines model-based and data-driven learning within a single framework. Our results from learning grammars using the Spoken English Corpus SEC suggest that combined model-based and data-driven learning can produce a more plausible grammar than is the case when using either learning style isolation. ',\n",
       " ' DISCO---An HPSG-based NLP System and its Application for Appointment Scheduling (Project Note) The natural language system DISCO is described. It combines o a powerful and flexible grammar development system; o linguistic competence for German including morphology, syntax and semantics; o new methods for linguistic performance modelling on the basis of high-level competence grammars; o new methods for modelling multi-agent dialogue competence; o an interesting sample application for appointment scheduling and calendar management. ',\n",
       " ' Compact Representations by Finite-State Transducers Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation. ',\n",
       " ' Morphology with a Null-Interface We present an integrated architecture for word-level and sentence-level processing in a unification-based paradigm. The core of the system is a CLP implementation of a unification engine for feature structures supporting relational values. In this framework an HPSG-style grammar is implemented. Word-level processing uses X2MorF, a morphological component based on an extended version of two-level morphology. This component is tightly integrated with the grammar as a relation. The advantage of this approach is that morphology and syntax are kept logically autonomous while at the same time minimizing interface problems. ',\n",
       " ' Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm Local grammars can be represented in a very convenient way by automata. This paper describes and illustrates an efficient algorithm for the application of local grammars put in this form to lemmatized texts. ',\n",
       " ' GEMINI: A Natural Language System for Spoken-Language Understanding Gemini is a natural language understanding system developed for spoken language applications. The paper describes the architecture of Gemini, paying particular attention to resolving the tension between robustness and overgeneration. Gemini features a broad-coverage unification-based grammar of English, fully interleaved syntactic and semantic processing in an all-paths, bottom-up parser, and an utterance-level parser to find interpretations of sentences that might not be analyzable as complete sentences. Gemini also includes novel components for recognizing and correcting grammatical disfluencies, and for doing parse preferences. This paper presents a component-by-component view of Gemini, providing detailed relevant measurements of size, efficiency, and performance. ',\n",
       " ' Interleaving Syntax and Semantics in an Efficient Bottom-Up Parser We describe an efficient bottom-up parser that interleaves syntactic and semantic structure building. Two techniques are presented for reducing search by reducing local ambiguity: Limited left-context constraints are used to reduce local syntactic ambiguity, and deferred sortal-constraint application is used to reduce local semantic ambiguity. We experimentally evaluate these techniques, and show dramatic reductions in both number of chart-edges and total parsing time. The robust processing capabilities of the parser are demonstrated in its use in improving the accuracy of a speech recognizer. ',\n",
       " \" Japanese word sense disambiguation based on examples of synonyms (This is not the abstract): The language is Japanese. If your printer does not have fonts for Japases characters, the characters in figures will not be printed out correctly. Dissertation for Bachelor's degree at Kyoto University(Nagao lab.),March 1994. \",\n",
       " ' A Corrective Training Algorithm for Adaptive Learning in Bag Generation The sampling problem in training corpus is one of the major sources of errors in corpus-based applications. This paper proposes a corrective training algorithm to best-fit the run-time context domain in the application of bag generation. It shows which objects to be adjusted and how to adjust their probabilities. The resulting techniques are greatly simplified and the experimental results demonstrate the promising effects of the training algorithm from generic domain to specific domain. In general, these techniques can be easily extended to various language models and corpus-based applications. ',\n",
       " \" Tricolor DAGs for Machine Translation Machine translation MT has recently been formulated in terms of constraint-based knowledge representation and unification theories, but it is becoming more and more evident that it is not possible to design a practical MT system without an adequate method of handling mismatches between semantic representations in the source and target languages. In this paper, we introduce the idea of ``information-based'' MT, which is considerably more flexible than interlingual MT or the conventional transfer-based MT. \",\n",
       " ' Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists A simple and general method is described that can combine different knowledge sources to reorder N-best lists of hypotheses produced by a speech recognizer. The method is automatically trainable, acquiring information from both positive and negative examples. Experiments are described in which it was tested on a 1000-utterance sample of unseen ATIS data. ',\n",
       " ' Estimating Performance of Pipelined Spoken Language Translation Systems Most spoken language translation systems developed to date rely on a pipelined architecture, in which the main stages are speech recognition, linguistic analysis, transfer, generation and speech synthesis. When making projections of error rates for systems of this kind, it is natural to assume that the error rates for the individual components are independent, making the system accuracy the product of the component accuracies. The paper reports experiments carried out using the SRI-SICS-Telia Research Spoken Language Translator and a 1000-utterance sample of unseen data. The results suggest that the naive performance model leads to serious overestimates of system error rates, since there are in fact strong dependencies between the components. Predicting the system error rate on the independence assumption by simple multiplication resulted in a 16\\\\% proportional overestimate for all utterances, and a 19\\\\% overestimate when only utterances of length 1-10 words were considered. ',\n",
       " ' Discourse Obligations in Dialogue Processing We show that in modeling social interaction, particularly dialogue, the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief, goal, and intention and their mutual and shared counterparts. In particular, we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system. ',\n",
       " ' Phoneme Recognition Using Acoustic Events This paper presents a new approach to phoneme recognition using nonsequential sub--phoneme units. These units are called acoustic events and are phonologically meaningful as well as recognizable from speech signals. Acoustic events form a phonologically incomplete representation as compared to distinctive features. This problem may partly be overcome by incorporating phonological constraints. Currently, 24 binary events describing manner and place of articulation, vowel quality and voicing are used to recognize all German phonemes. Phoneme recognition in this paradigm consists of two steps: After the acoustic events have been determined from the speech signal, a phonological parser is used to generate syllable and phoneme hypotheses from the event lattice. Results obtained on a speaker--dependent corpus are presented. ',\n",
       " ' The Acquisition of a Lexicon from Paired Phoneme Sequences and Semantic Representations We present an algorithm that acquires words (pairings of phonological forms and semantic representations) from larger utterances of unsegmented phoneme sequences and semantic representations. The algorithm maintains from utterance to utterance only a single coherent dictionary, and learns in the presence of homonymy, synonymy, and noise. Test results over a corpus of utterances generated from the Childes database of mother-child interactions are presented. ',\n",
       " ' Abstract Machine for Typed Feature Structures This paper describes a first step towards the definition of an abstract machine for linguistic formalisms that are based on typed feature structures, such as HPSG. The core design of the abstract machine is given in detail, including the compilation process from a high-level specification language to the abstract machine language and the implementation of the abstract instructions. We thus apply methods that were proved useful in computer science to the study of natural languages: a grammar specified using the formalism is endowed with an operational semantics. Currently, our machine supports the unification of simple feature structures, unification of sequences of such structures, cyclic structures and disjunction. ',\n",
       " \" Specifying Intonation from Context for Speech Synthesis This paper presents a theory and a computational implementation for generating prosodically appropriate synthetic speech in response to database queries. Proper distinctions of contrast and emphasis are expressed in an intonation contour that is synthesized by rule under the control of a grammar, a discourse model, and a knowledge base. The theory is based on Combinatory Categorial Grammar, a formalism which easily integrates the notions of syntactic constituency, semantics, prosodic phrasing and information structure. Results from our current implementation demonstrate the system's ability to generate a variety of intonational possibilities for a given sentence depending on the discourse context. \",\n",
       " \" Generating Context-Appropriate Word Orders in Turkish Turkish has considerably freer word order than English. The interpretations of different word orders in Turkish rely on information that describes how a sentence relates to its discourse context. To capture the syntactic features of a free word order language, I present an adaptation of Combinatory Categorial Grammars called {}-CCGs (set-CCGs). In {}-CCGs, a verb's subcategorization requirements are relaxed so that it requires a set of arguments without specifying their linear order. I integrate a level of information structure, representing pragmatic functions such as topic and focus, with {}-CCGs to allow certain pragmatic distinctions in meaning to influence the word order of a sentence in a compositional way. Finally, I discuss how this strategy is used within an implemented generation system which produces Turkish sentences with context-appropriate word orders in a simple database query task. \",\n",
       " \" The Role of Cognitive Modeling in Achieving Communicative Intentions A discourse planner for (task-oriented) dialogue must be able to make choices about whether relevant, but optional information (for example, the satellites in an RST-based planner) should be communicated. We claim that effective text planners must explicitly model aspects of the Hearer's cognitive state, such as what the hearer is attending to and what inferences the hearer can draw, in order to make these choices. We argue that a mere representation of the Hearer's knowledge is inadequate. We support this claim by (1) an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). \",\n",
       " ' Generating Multilingual Documents from a Knowledge Base: The TECHDOC Project TECHDOC is an implemented system demonstrating the feasibility of generating multilingual technical documents on the basis of a language-independent knowledge base. Its application domain is user and maintenance instructions, which are produced from underlying plan structures representing the activities, the participating objects with their properties, relations, and so on. This paper gives a brief outline of the system architecture and discusses some recent developments in the project: the addition of actual event simulation in the KB, steps towards a document authoring tool, and a multimodal user interface. (slightly corrected version of a paper to appear in: COLING 94, Proceedings) ',\n",
       " \" Tracking Point of View in Narrative Third-person fictional narrative text is composed not only of passages that objectively narrate events, but also of passages that present characters' thoughts, perceptions, and inner states. Such passages take a character's ``psychological point of view''. A language understander must determine the current psychological point of view in order to distinguish the beliefs of the characters from the facts of the story, to correctly attribute beliefs and other attitudes to their sources, and to understand the discourse relations among sentences. Tracking the psychological point of view is not a trivial problem, because many sentences are not explicitly marked for point of view, and whether the point of view of a sentence is objective or that of a character (and if the latter, which character it is) often depends on the context in which the sentence appears. Tracking the psychological point of view is the problem addressed in this work. The approach is to seek, by extensive examinations of naturally-occurring narrative, regularities in the ways that authors manipulate point of view, and to develop an algorithm that tracks point of view on the basis of the regularities found. This paper presents this algorithm, gives demonstrations of an implemented system, and describes the results of some preliminary empirical studies, which lend support to the algorithm. \",\n",
       " ' A Sequential Algorithm for Training Text Classifiers The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness. ',\n",
       " ' K-vec: A New Approach for Aligning Parallel Texts Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates(Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French pe^ches by noting that the distribution of fisheries in the English text is similar to the distribution of pe^ches in the French. K-vec does not depend on sentence boundaries. ',\n",
       " ' Comparative Discourse Analysis of Parallel Texts A quantitative representation of discourse structure can be computed by measuring lexical cohesion relations among adjacent blocks of text. These representations have been proposed to deal with sub-topic text segmentation. In a parallel corpus, similar representations can be derived for versions of a text in various languages. These can be used for parallel segmentation and as an alternative measure of text-translation similarity. ',\n",
       " ' Multi-Tape Two-Level Morphology: A Case Study in Semitic Non-linear Morphology This paper presents an implemented multi-tape two-level model capable of describing Semitic non-linear morphology. The computational framework behind the current work is motivated by Kay (1987); the formalism presented here is an extension to the formalism reported by Pulman and Hepple (1993). The objectives of the current work are: to stay as close as possible, in spirit, to standard two-level morphology, to stay close to the linguistic description of Semitic stems, and to present a model which can be used with ease by the Semitist. The paper illustrates that if finite-state transducers (FSTs) in a standard two-level morphology model are replaced with multi-tape auxiliary versions (AFSTs), one can account for Semitic root-and-pattern morphology using high level notation. ',\n",
       " ' PRINCIPAR---An Efficient, Broad-coverage, Principle-based Parser We present an efficient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries. ',\n",
       " ' Recovering From Parser Failures: A Hybrid Statistical/Symbolic Approach We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system. We describe a module which takes as input a fragmented parse and returns a repaired meaning representation. It negotiates with the speaker about what the complete meaning of the utterance is by generating hypotheses about how to fit the fragments of the partial parse together into a coherent meaning representation. By drawing upon both statistical and symbolic information, it constrains its repair hypotheses to those which are both likely and meaningful. Because it updates its statistical model during use, it improves its performance over time. ',\n",
       " ' Automated Postediting of Documents Large amounts of low- to medium-quality English texts are now being produced by machine translation MT systems, optical character readers OCR, and non-native speakers of English. Most of this text must be postedited by hand before it sees the light of day. Improving text quality is tedious work, but its automation has not received much research attention. Anyone who has postedited a technical report or thesis written by a non-native speaker of English knows the potential of an automated postediting system. For the case of MT-generated text, we argue for the construction of postediting modules that are portable across MT systems, as an alternative to hardcoding improvements inside any one system. As an example, we have built a complete self-contained postediting module for the task of article selection (a, an, the) for English noun phrases. This is a notoriously difficult problem for Japanese-English MT. Our system contains over 200,000 rules derived automatically from online text resources. We report on learning algorithms, accuracy, and comparisons with human performance. ',\n",
       " ' Building a Large-Scale Knowledge Base for Machine Translation Knowledge-based machine translation KBMT systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PANGLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese. ',\n",
       " ' Parsing as Tree Traversal This paper presents a unified approach to parsing, in which top-down, bottom-up and left-corner parsers are related to preorder, postorder and inorder tree traversals. It is shown that the simplest bottom-up and left-corner parsers are left recursive and must be converted using an extended Greibach normal form. With further partial execution, the bottom-up and left-corner parsers collapse together as in the BUP parser of Matsumoto. ',\n",
       " ' Tagging and Morphological Disambiguation of Turkish Text Automatic text tagging is an important component in higher level analysis of text corpora, and its output can be used in many natural language processing applications. In languages like Turkish or Finnish, with agglutinative morphology, morphological disambiguation is a very crucial process in tagging, as the structures of many lexical forms are morphologically ambiguous. This paper describes a POS tagger for Turkish text based on a full-scale two-level specification of Turkish morphology that is based on a lexicon of about 24,000 root words. This is augmented with a multi-word and idiomatic construct recognizer, and most importantly morphological disambiguator based on local neighborhood constraints, heuristics and limited amount of statistical information. The tagger also has functionality for statistics compilation and fine tuning of the morphological analyzer, such as logging erroneous morphological parses, commonly used roots, etc. Preliminary results indicate that the tagger can tag about 98-99\\\\% of the texts accurately with very minimal user intervention. Furthermore for sentences morphologically disambiguated with the tagger, an LFG parser developed for Turkish, generates, on the average, 50\\\\% less ambiguous parses and parses almost 2.5 times faster. The tagging functionality is not specific to Turkish, and can be applied to any language with a proper morphological analysis interface. ',\n",
       " ' Computing FIRST and FOLLOW Functions for Feature-Theoretic Grammars This paper describes an algorithm for the computation of FIRST and FOLLOW sets for use with feature-theoretic grammars in which the value of the sets consists of pairs of feature-theoretic categories. The algorithm preserves as much information from the grammars as possible, using negative restriction to define equivalence classes. Addition of a simple data structure leads to an order of magnitude improvement in execution time over a naive implementation. ',\n",
       " \" A System for Induction of Oblique Decision Trees This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees. \",\n",
       " ' Pattern Matching and Discourse Processing in Information Extraction from Japanese Text Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance. ',\n",
       " ' Random Worlds and Maximum Entropy Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain {1,...,N} that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods. ',\n",
       " \" Computational Analyses of Arabic Morphology This paper demonstrates how a (multi-tape) two-level formalism can be used to write two-level grammars for Arabic non-linear morphology using a high level, but computationally tractable, notation. Three illustrative grammars are provided based on CV-, moraic- and affixational analyses. These are complemented by a proposal for handling the hitherto computationally untreated problem of the broken plural. It will be shown that the best grammars for describing Arabic non-linear morphology are moraic in the case of templatic stems, and affixational in the case of a-templatic stems. The paper will demonstrate how the broken plural can be derived under two-level theory via the `implicit' derivation of the singular. \",\n",
       " ' A Modular and Flexible Architecture for an Integrated Corpus Query System The paper describes the architecture of an integrated and extensible corpus query system developed at the University of Stuttgart and gives examples of some of the modules realized within this architecture. The modules form the core of a corpus workbench. Within the proposed architecture, information required for the evaluation of queries may be derived from different knowledge sources (the corpus text, databases, on-line thesauri) and by different means: either through direct lookup in a database or by calling external tools which may infer the necessary information at the time of query evaluation. The information available and the method of information access can be stated declaratively and individually for each corpus, leading to a flexible, extensible and modular corpus workbench. ',\n",
       " ' Parsing with Principles and Probabilities This paper is an attempt to bring together two approaches to language analysis. The possible use of probabilistic information in principle-based grammars and parsers is considered, including discussion on some theoretical and computational problems that arise. Finally a partial implementation of these ideas is presented, along with some preliminary results from testing on a small set of sentences. ',\n",
       " ' The Correct and Efficient Implementation of Appropriateness Specifications for Typed Feature Structures In this paper, we argue that type inferencing incorrectly implements appropriateness specifications for typed feature structures, promote a combination of type resolution and unfilling as a correct and efficient alternative, and consider the expressive limits of this alternative approach. Throughout, we use feature cooccurence restrictions as illustration and linguistic motivation. ',\n",
       " ' Typed Feature Structures as Descriptions A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable. ',\n",
       " ' LHIP: Extended DCGs for Configurable Robust Parsing We present LHIP, a system for incremental grammar development using an extended DCG formalism. The system uses a robust island-based parsing method controlled by user-defined performance thresholds. ',\n",
       " ' Emergent Linguistic Rules from Inducing Decision Trees: Disambiguating Discourse Clue Words We apply decision tree induction to the problem of discourse clue word sense disambiguation with a genetic algorithm. The automatic partitioning of the training set which is intrinsic to decision tree induction gives rise to linguistically viable rules. ',\n",
       " ' Statistical versus symbolic parsing for captioned-information retrieval We discuss implementation issues of MARIE-1, a mostly symbolic parser fully implemented, and MARIE-2, a more statistical parser partially implemented. They address a corpus of 100,000 picture captions. We argue that the mixed approach of MARIE-2 should be better for this corpus because its algorithms (not data) are simpler. ',\n",
       " \" Tagging accurately -- Don't guess if you know We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers. We use two mature taggers, ENGCG and Xerox Tagger, to independently tag the same text and combine the results to produce a fully disambiguated text. In a 27000 word test sample taken from a previously unseen corpus we achieve 98.5% accuracy. This paper presents the data in detail. We describe the problems we encountered in the course of combining the two taggers and discuss the problem of evaluating taggers. \",\n",
       " ' On Using Selectional Restriction in Language Models for Speech Recognition In this paper, we investigate the use of selectional restriction -- the constraints a predicate imposes on its arguments -- in a language model for speech recognition. We use an un-tagged corpus, followed by a public domain tagger and a very simple finite state machine to obtain verb-object pairs from unrestricted English text. We then measure the impact the knowledge of the verb has on the prediction of the direct object in terms of the perplexity of a cluster-based language model. The results show that even though a clustered bigram is more useful than a verb-object model, the combination of the two leads to an improvement over the clustered bigram model. ',\n",
       " \" Distributional Clustering of English Words We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ``soft'' clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. \",\n",
       " ' Intensional Verbs Without Type-Raising or Lexical Ambiguity We present an analysis of the semantic interpretation of intensional verbs such as seek that allows them to take direct objects of either individual or quantifier type, producing both de dicto and de re readings in the quantifier case, all without needing to stipulate type-raising or quantifying-in rules. This simple account follows directly from our use of logical deduction in linear logic to express the relationship between syntactic structures and meanings. While our analysis resembles current categorial approaches in important ways, it differs from them in allowing the greater type flexibility of categorial semantics while maintaining a precise connection to syntax. As a result, we are able to provide derivations for certain readings of sentences with intensional verbs and complex direct objects that are not derivable in current purely categorial accounts of the syntax-semantics interface. The analysis forms a part of our ongoing work on semantic interpretation within the framework of Lexical-Functional Grammar. ',\n",
       " ' Qualitative and Quantitative Models of Speech Translation This paper compares a qualitative reasoning model of translation with a quantitative statistical model. We consider these models within the context of two hypothetical speech translation systems, starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second, quantitative design. The quantitative language and translation models are based on relations between lexical heads of phrases. Statistical parameters for structural dependency, lexical transfer, and linear order are used to select a set of implicit relations between words in a source utterance, a corresponding set of relations between target language words, and the most likely translation of the original utterance. ',\n",
       " ' Training and Scaling Preference Functions for Disambiguation We present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least-squares minimization problem, and improvements are then made by hill-climbing. The method is applied to disambiguating sentences in the ATIS (Air Travel Information System) corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations. ',\n",
       " ' Approximate N-Gram Markov Model for Natural Language Generation This paper proposes an Approximate n-gram Markov Model for bag generation. Directed word association pairs with distances are used to approximate (n-1)-gram and n-gram training tables. This model has parameters of word association model, and merits of both word association model and Markov Model. The training knowledge for bag generation can be also applied to lexical selection in machine translation design. ',\n",
       " \" Experimentally Evaluating Communicative Strategies: The Effect of the Task Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents' resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents' resources and communicative strategies. \",\n",
       " ' Restricting the Weak-Generative Capacity of Synchronous Tree-Adjoining Grammars The formalism of synchronous tree-adjoining grammars, a variant of standard tree-adjoining grammars TAG, was intended to allow the use of TAGs for language transduction in addition to language specification. In previous work, the definition of the transduction relation defined by a synchronous TAG was given by appeal to an iterative rewriting process. The rewriting definition of derivation is problematic in that it greatly extends the expressivity of the formalism and makes the design of parsing algorithms difficult if not impossible. We introduce a simple, natural definition of synchronous tree-adjoining derivation, based on isomorphisms between standard tree-adjoining derivations, that avoids the expressivity and implementability problems of the original rewriting definition. The decrease in expressivity, which would otherwise make the method unusable, is offset by the incorporation of an alternative definition of standard tree-adjoining derivation, previously proposed for completely separate reasons, thereby making it practical to entertain using the natural definition of synchronous derivation. Nonetheless, some remaining problematic cases call for yet more flexibility in the definition; the isomorphism requirement may have to be relaxed. It remains for future research to tune the exact requirements on the allowable mappings. ',\n",
       " \" A Unified Process Model of Syntactic and Semantic Error Recovery in Sentence Understanding The development of models of human sentence processing has traditionally followed one of two paths. Either the model posited a sequence of processing modules, each with its own task-specific knowledge (e.g., syntax and semantics), or it posited a single processor utilizing different types of knowledge inextricably integrated into a monolithic knowledge base. Our previous work in modeling the sentence processor resulted in a model in which different processing modules used separate knowledge sources but operated in parallel to arrive at the interpretation of a sentence. One highlight of this model is that it offered an explanation of how the sentence processor might recover from an error in choosing the meaning of an ambiguous word. Recent experimental work by Laurie Stowe strongly suggests that the human sentence processor deals with syntactic error recovery using a mechanism very much like that proposed by our model of semantic error recovery. Another way to interpret Stowe's finding is this: the human sentence processor consists of a single unified processing module utilizing multiple independent knowledge sources in parallel. A sentence processor built upon this architecture should at times exhibit behavior associated with modular approaches, and at other times act like an integrated system. In this paper we explore some of these ideas via a prototype computational model of sentence processing called COMPERE, and propose a set of psychological experiments for testing our theories. \",\n",
       " ' Building a Parser That can Afford to Interact with Semantics Natural language understanding programs get bogged down by the multiplicity of possible syntactic structures while processing real world texts that human understanders do not have much difficulty with. In this work, I analyze the relationships between parsing strategies, the degree of local ambiguity encountered by them, and semantic feedback to syntax, and propose a parsing algorithm called {\\\\em Head-Signaled Left Corner Parsing} HSLC that minimizes local ambiguities while supporting interactive syntactic and semantic analysis. Such a parser has been implemented in a sentence understanding program called COMPERE. ',\n",
       " ' Having Your Cake and Eating It Too: Autonomy and Interaction in a Model of Sentence Processing Is the human language understander a collection of modular processes operating with relative autonomy, or is it a single integrated process? This ongoing debate has polarized the language processing community, with two fundamentally different types of model posited, and with each camp concluding that the other is wrong. One camp puts forth a model with separate processors and distinct knowledge sources to explain one body of data, and the other proposes a model with a single processor and a homogeneous, monolithic knowledge source to explain the other body of data. In this paper we argue that a hybrid approach which combines a unified processor with separate knowledge sources provides an explanation of both bodies of data, and we demonstrate the feasibility of this approach with the computational model called COMPERE. We believe that this approach brings the language processing community significantly closer to offering human-like language processing systems. ',\n",
       " ' On Implementing an HPSG theory -- Aspects of the logical architecture, the formalization, and the implementation of head-driven phrase structure grammars The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix. ',\n",
       " ' On Planning while Learning This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner. ',\n",
       " ' Reaping the Benefits of Interactive Syntax and Semantics Semantic feedback is an important source of information that a parser could use to deal with local ambiguities in syntax. However, it is difficult to devise a systematic communication mechanism for interactive syntax and semantics. In this article, I propose a variant of left-corner parsing to define the points at which syntax and semantics should interact, an account of grammatical relations and thematic roles to define the content of the communication, and a conflict resolution strategy based on independent preferences from syntax and semantics. The resulting interactive model has been implemented in a program called COMPERE and shown to account for a wide variety of psycholinguistic data on structural and lexical ambiguities. ',\n",
       " ' Uniform Representations for Syntax-Semantics Arbitration Psychological investigations have led to considerable insight into the working of the human language comprehension system. In this article, we look at a set of principles derived from psychological findings to argue for a particular organization of linguistic knowledge along with a particular processing strategy and present a computational model of sentence processing based on those principles. Many studies have shown that human sentence comprehension is an incremental and interactive process in which semantic and other higher-level information interacts with syntactic information to make informed commitments as early as possible at a local ambiguity. Early commitments may be made by using top-down guidance from knowledge of different types, each of which must be applicable independently of others. Further evidence from studies of error recovery and delayed decisions points toward an arbitration mechanism for combining syntactic and semantic information in resolving ambiguities. In order to account for all of the above, we propose that all types of linguistic knowledge must be represented in a common form but must be separable so that they can be applied independently of each other and integrated at processing time by the arbitrator. We present such a uniform representation and a computational model called COMPERE based on the representation and the processing strategy. ',\n",
       " ' Integrating Knowledge Bases and Statistics in MT We summarize recent machine translation MT research at the Information Sciences Institute of USC, and we describe its application to the development of a Japanese-English newspaper MT system. Our work aims at scaling up grammar-based, knowledge-based MT techniques. This scale-up involves the use of statistical methods, both in acquiring effective knowledge resources and in making reasonable linguistic choices in the face of knowledge gaps. ',\n",
       " ' A Probabilistic Model of Compound Nouns Compound nouns such as example noun compound are becoming more common in natural language and pose a number of difficult problems for NLP systems, notably increasing the complexity of parsing. In this paper we develop a probabilistic model for syntactically analysing such compounds. The model predicts compound noun structures based on knowledge of affinities between nouns, which can be acquired from a corpus. Problems inherent in this corpus-based approach are addressed: data sparseness is overcome by the use of semantically motivated word classes and sense ambiguity is explicitly handled in the model. An implementation based on this model is described in Lauer (1994) and correctly parses 77% of the test set. ',\n",
       " ' An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora. The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items. Some experimental results about the performance of the method are provided. ',\n",
       " ' Focusing for Pronoun Resolution in English Discourse: An Implementation Anaphora resolution is one of the most active research areas in natural language processing. This study examines focusing as a tool for the resolution of pronouns which are a kind of anaphora. Focusing is a discourse phenomenon like anaphora. Candy Sidner formalized focusing in her 1979 MIT PhD thesis and devised several algorithms to resolve definite anaphora including pronouns. She presented her theory in a computational framework but did not generally implement the algorithms. Her algorithms related to focusing and pronoun resolution are implemented in this thesis. This implementation provides a better comprehension of the theory both from a conceptual and a computational point of view. The resulting program is tested on different discourse segments, and evaluation and analysis of the experiments are presented together with the statistical results. ',\n",
       " \" Situated Modeling of Epistemic Puzzles Situation theory is a mathematical theory of meaning introduced by Jon Barwise and John Perry. It has evoked great theoretical and practical interest and motivated the framework of a few `computational' systems. PROSIT is the pioneering work in this direction. Unfortunately, there is a lack of real-life applications on these systems and this study is a preliminary attempt to remedy this deficiency. Here, we examine how much PROSIT reflects situation-theoretic concepts and solve a group of epistemic puzzles using the constructs provided by this programming language. \",\n",
       " \" Treating `Free Word Order' in Machine Translation In `free word order' languages, every sentence is embedded in its specific context. Among others, the order of constituents is determined by the categories `theme', `rheme' and `contrastive focus'. This paper shows how to recognise and to translate these categories automatically on a sentential basis, so that sentence embedding can be achieved without having to refer to the context. Modifier classes, which are traditionally neglected in linguistic description, are fully covered by the proposed method. (Coling 94, Kyoto, Vol. I, pages 69-75) \",\n",
       " ' Linguistics Computation, Automatic Model Generation, and Intensions Techniques are presented for defining models of computational linguistics theories. The methods of generalized diagrams that were developed by this author for modeling artificial intelligence planning and reasoning are shown to be applicable to models of computation of linguistics theories. It is shown that for extensional and intensional interpretations, models can be generated automatically which assign meaning to computations of linguistics theories for natural languages. Keywords: Computational Linguistics, Reasoning Models, G-diagrams For Models, Dynamic Model Implementation, Linguistics and Logics For Artificial Intelligence ',\n",
       " ' Parsing of Spoken Language under Time Constraints Spoken language applications in natural dialogue settings place serious requirements on the choice of processing architecture. Especially under adverse phonetic and acoustic conditions parsing procedures have to be developed which do not only analyse the incoming speech in a time-synchroneous and incremental manner, but which are able to schedule their resources according to the varying conditions of the recognition process. Depending on the actual degree of local ambiguity the parser has to select among the available constraints in order to narrow down the search space with as little effort as possible. A parsing approach based on constraint satisfaction techniques is discussed. It provides important characteristics of the desired real-time behaviour and attempts to mimic some of the attention focussing capabilities of the human speech comprehension mechanism. ',\n",
       " \" Inducing Probabilistic Grammars by Bayesian Model Merging We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are {\\\\em incorporated} by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are {\\\\em merged} to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (`Occam's Razor'). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based grams, and stochastic context-free grammars. \",\n",
       " ' Number of faults a system can withstand without repairs We consider the following scheduling problem. A system is composed of processors drawn from a pool of . The processors can become faulty while in operation and faulty processors never recover. A report is issued whenever a fault occurs. This report states only the existence of a fault, but does not indicate its location. Based on this report, the scheduler can reconfigure the system and choose another set of processors. The system operates satisfactorily as long as at most of the selected processors are faulty. We exhibit a scheduling strategy allowing the system to operate satisfactorily until approximately faults are reported in the worst case. Our precise bound is tight. ',\n",
       " \" Proving probabilistic correctness statements: the case of Rabin's algorithm for mutual exclusion The correctness of most randomized distributed algorithms is expressed by a statement of the form ``some predicate of the executions holds with high probability, regardless of the order in which actions are scheduled''. In this paper, we present a general methodology to prove correctness statements of such randomized algorithms. Specifically, we show how to prove such statements by a series of refinements, which terminate in a statement independent of the schedule. To demonstrate the subtlety of the issues involved in this type of analysis, we focus on Rabin's randomized distributed algorithm for mutual exclusion [Rabin 82]. Surprisingly, it turns out that the algorithm does not maintain one of the requirements of the problem under a certain schedule. In particular, we give a schedule under which a set of processes can suffer lockout for arbitrary long periods. \",\n",
       " \" Proving time bounds for randomized distributed algorithms A method of analyzing time bounds for randomized distributed algorithms is presented, in the context of a new and general framework for describing and reasoning about randomized algorithms. The method consists of proving auxiliary statements of the form U (t)->(p) U', which means that whenever the algorithm begins in a state in set U, with probability p, it will reach a state in set U' within time t. The power of the method is illustrated by its use in proving a constant upper bound on the expected time for some process to reach its critical region, in Lehmann and Rabin's Dining Philosophers algorithm. \",\n",
       " \" An on-line algorithm for improving performance in navigation Recent papers have shown optimally-competitive on-line strategies for a robot traveling from a point to a point in certain unknown geometric environments. We consider the question: Having gained some partial information about the scene on its first trip from to , can the robot improve its performance on subsequent trips it might make? This is a type of on-line problem where a strategy must exploit {\\\\em partial information \\\\/} about the future (e.g., about obstacles that lie ahead). For scenes with axis-parallel rectangular obstacles where the Euclidean distance between and is , we present a deterministic algorithm whose {\\\\em average\\\\/} trip length after trips, , is times the length of the shortest path in the scene. We also show that this is the best a deterministic strategy can do. This algorithm can be thought of as performing an optimal tradeoff between search effort and the goodness of the path found. We improve this algorithm so that for {\\\\em every\\\\/} , the robot's th trip length is times the shortest path length. A key idea of the paper is that a {\\\\em tree\\\\/} structure can be defined in the scene, where the nodes are portions of certain obstacles and the edges are ``short'' paths from a node to its children. The core of our algorithms is an on-line strategy for traversing this tree optimally. \",\n",
       " ' Geometry based heuristics for unit disk graphs Unit disk graphs are intersection graphs of circles of unit radius in the plane. We present simple and provably good heuristics for a number of classical NP-hard optimization problems on unit disk graphs. The problems considered include maximum independent set, minimum vertex cover, minimum coloring and minimum dominating set. We also present an on-line coloring heuristic which achieves a competitive ratio of 6 for unit disk graphs. Our heuristics do not need a geometric representation of unit disk graphs. Geometric representations are used only in establishing the performance guarantees of the heuristics. Several of our approximation algorithms can be extended to intersection graphs of circles of arbitrary radii in the plane, intersection graphs of regular polygons, and to intersection graphs of higher dimensional regular objects. ',\n",
       " ' On the minimum latency problem We are given a set of points and a symmetric distance matrix giving the distance between and . We wish to construct a tour that minimizes , where is the {\\\\em latency} of , defined to be the distance traveled before first visiting . This problem is also known in the literature as the {\\\\em deliveryman problem} or the {\\\\em traveling repairman problem}. It arises in a number of applications including disk-head scheduling, and turns out to be surprisingly different from the traveling salesman problem in character. We give exact and approximate solutions to a number of cases, including a constant-factor approximation algorithm whenever the distance matrix satisfies the triangle inequality. ',\n",
       " ' The complexity of approximating PSPACE-Complete problems for hierarchical specifications We extend the concept of polynomial time approximation algorithms to apply to problems for hierarchically specified graphs, many of which are PSPACE-complete. Assuming P != PSPACE, the existence or nonexistence of such efficient approximation algorithms is characterized, for several standard graph theoretic and combinatorial problems. We present polynomial time approximation algorithms for several standard PSPACE-hard problems considered in the literature. In contrast, we show that unless P = PSPACE, there is no polynomial time epsilon-approximation for any epsilon>0, for several other problems, when the instances are specified hierarchically. We present polynomial time approximation algorithms for the following problems when the graphs are specified hierarchically: {minimum vertex cover}, {maximum 3SAT}, {weighted max cut}, {minimum maximal matching}, {bounded degree maximum independent set} In contrast, we show that unless P = PSPACE, there is no polynomial time epsilon-approximation for any epsilon>0, for the following problems when the instances are specified hierarchically: {the number of true gates in a monotone acyclic circuit when all input values are specified} and {the optimal value of the objective function of a linear program} It is also shown that unless P = PSPACE, a performance guarantee of less than 2 cannot be obtained in polynomial time for the following problems when the instances are specified hierarchically: {high degree subgraph}, {k-vertex connected subgraph}, and {k-edge connected subgraph} ',\n",
       " ' Aligning Noisy Parallel Corpora Across Language Groups : Word Pair Feature Matching by Dynamic Time Warping We propose a new algorithm called DK-vec for aligning pairs of Asian/Indo-European noisy parallel texts without sentence boundaries. DK-vec improves on previous alignment algorithms in that it handles better the non-linear nature of noisy corpora. The algorithm uses frequency, position and recency information as features for pattern matching. Dynamic Time Warping is used as the matching technique between word pairs. This algorithm produces a small bilingual lexicon which provides anchor points for alignment. ',\n",
       " ' Towards an Automatic Dictation System for Translators: the TransTalk Project Professional translators often dictate their translations orally and have them typed afterwards. The TransTalk project aims at automating the second part of this process. Its originality as a dictation system lies in the fact that both the acoustic signal produced by the translator and the source text under translation are made available to the system. Probable translations of the source text can be predicted and these predictions used to help the speech recognition system in its lexical choices. We present the results of the first prototype, which show a marked improvement in the performance of the speech recognition task when translation predictions are taken into account. ',\n",
       " ' Improving Language Models by Clustering Training Sentences Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences. I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster. This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model. It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model. As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain. These results are consistent with other findings for such models, suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model. ',\n",
       " ' Lexikoneintraege fuer deutsche Adverbien (Dictionary Entries for German Adverbs) Modifiers in general, and adverbs in particular, are neglected categories in linguistics, and consequently, their treatment in Natural Language Processing poses problems. In this article, we present the dictionary information for German adverbs which is necessary to deal with word order, degree modifier scope and other problems in NLP. We also give evidence for the claim that a classification according to position classes differs from any semantic classification. ',\n",
       " ' Principle Based Semantics for HPSG The paper presents a constraint based semantic formalism for HPSG. The advantages of the formlism are shown with respect to a grammar for a fragment of German that deals with (i) quantifier scope ambiguities triggered by scrambling and/or movement and (ii) ambiguities that arise from the collective/distributive distinction of plural NPs. The syntax-semantics interface directly implements syntactic conditions on quantifier scoping and distributivity. The construction of semantic representations is guided by general principles governing the interaction between syntax and semantics. Each of these principles acts as a constraint to narrow down the set of possible interpretations of a sentence. Meanings of ambiguous sentences are represented by single partial representations (so-called U(nderspecified) D(iscourse) R(epresentation) S(tructure)s) to which further constraints can be added monotonically to gain more information about the content of a sentence. There is no need to build up a large number of alternative representations of the sentence which are then filtered by subsequent discourse and world knowledge. The advantage of UDRSs is not only that they allow for monotonic incremental interpretation but also that they are equipped with truth conditions and a proof theory that allows for inferences to be drawn directly on structures where quantifier scope is not resolved. ',\n",
       " ' Spelling Correction in Agglutinative Languages This paper presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic programming based search algorithm. Spelling correction in agglutinative languages is significantly different than in languages like English. The concept of a word in such languages is much wider that the entries found in a dictionary, owing to {}~productive word formation by derivational and inflectional affixations. After an overview of certain issues and relevant mathematical preliminaries, we formally present the problem and our solution. We then present results from our experiments with spelling correction in Turkish, a Ural--Altaic agglutinative language. Our results indicate that we can find the intended correct word in 95\\\\% of the cases and offer it as the first candidate in 74\\\\% of the cases, when the edit distance is 1. ',\n",
       " ' A Centering Approach to Pronouns In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in Grosz, Joshi and Weinstein (1986), the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting. We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application. ',\n",
       " ' Evaluating Discourse Processing Algorithms In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining. ',\n",
       " ' Simulation of simplicity: a technique to cope with degenerate cases in geometric algorithms This paper describes a general-purpose programming technique, called the Simulation of Simplicity, which can be used to cope with degenerate input data for geometric algorithms. It relieves the programmer from the task to provide a consistent treatment for every single special case that can occur. The programs that use the technique tend to be considerably smaller and more robust than those that do not use it. We believe that this technique will become a standard tool in writing geometric software. ',\n",
       " \" Three-dimensional alpha shapes Frequently, data in scientific computing is in its abstract form a finite point set in space, and it is sometimes useful or required to compute what one might call the ``shape'' of the set. For that purpose, this paper introduces the formal notion of the family of shapes of a finite point set in . Each shape is a well-defined polytope, derived from the Delaunay triangulation of the point set, with a parameter controlling the desired level of detail. An algorithm is presented that constructs the entire family of shapes for a given set of size in time , worst case. A robust implementation of the algorithm is discussed and several applications in the area of scientific computing are mentioned. \",\n",
       " ' A Formal Look at Dependency Grammars and Phrase-Structure Grammars, with Special Consideration of Word-Order Phenomena The central role of the lexicon in Meaning-Text Theory MTT and other dependency-based linguistic theories cannot be replicated in linguistic theories based on context-free grammars (CFGs). We describe Tree Adjoining Grammar TAG as a system that arises naturally in the process of lexicalizing CFGs. A TAG grammar can therefore be compared directly to an Meaning-Text Model MTM. We illustrate this point by discussing the computational complexity of certain non-projective constructions, and suggest a way of incorporating locality of word-order definitions into the Surface-Syntactic Component of MTT. ',\n",
       " ' Lexical Functions and Machine Translation This paper discusses the lexicographical concept of lexical functions and their potential exploitation in the development of a machine translation lexicon designed to handle collocations. ',\n",
       " ' Recognizing Text Genres with Simple Metrics Using Discriminant Analysis A simple method for categorizing texts into predetermined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus. Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream, and combine them into a small number of functions, with the parameters weighted on basis of how useful they are for discriminating text genres. An application to information retrieval is discussed. ',\n",
       " ' XTAG system - A Wide Coverage Grammar for English This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar TAG formalism that includes a wide-coverage syntactic grammar for English. The various components of the system are discussed and preliminary evaluation results from the parsing of various corpora are given. Results from the comparison of XTAG against the IBM statistical parser and the Alvey Natural Language Tool parser are also given. ',\n",
       " \" A Freely Available Syntactic Lexicon for English This paper presents a syntactic lexicon for English that was originally derived from the Oxford Advanced Learner's Dictionary and the Oxford Dictionary of Current Idiomatic English, and then modified and augmented by hand. There are more than 37,000 syntactic entries from all 8 parts of speech. An X-windows based tool is available for maintaining the lexicon and performing searches. C and Lisp hooks are also available so that the lexicon can be easily utilized by parsers and other programs. \",\n",
       " ' Automatic Error Detection in Part of Speech Tagging A technique for detecting errors made by Hidden Markov Model taggers is described, based on comparing observable values of the tagging process with a threshold. The resulting approach allows the accuracy of the tagger to be improved by accepting a lower efficiency, defined as the proportion of words which are tagged. Empirical observations are presented which demonstrate the validity of the technique and suggest how to choose an appropriate threshold. ',\n",
       " ' Dilemma - An Instant Lexicographer Dilemma is intended to enhance quality and increase productivity of expert human translators by presenting to the writer relevant lexical information mechanically extracted from comparable existing translations, thus replacing - or compensating for the absence of - a lexicographer and stand-by terminologist rather than the translator. Using statistics and crude surface analysis and a minimum of prior information, Dilemma identifies instances and suggests their counterparts in parallel source and target texts, on all levels down to individual words. Dilemma forms part of a tool kit for translation where focus is on text structure and over-all consistency in large text volumes rather than on framing sentences, on interaction between many actors in a large project rather than on retrieval of machine-stored data and on decision making rather than on application of given rules. In particular, the system has been tuned to the needs of the ongoing translation of European Community legislation into the languages of candidate member countries. The system has been demonstrated to and used by professional translators with promising results. ',\n",
       " ' Lexicalization and Grammar Development In this paper we present a fully lexicalized grammar formalism as a particularly attractive framework for the specification of natural language grammars. We discuss in detail Feature-based, Lexicalized Tree Adjoining Grammars (FB-LTAGs), a representative of the class of lexicalized grammars. We illustrate the advantages of lexicalized grammars in various contexts of natural language processing, ranging from wide-coverage grammar development to parsing and machine translation. We also present a method for compact and efficient representation of lexicalized trees. ',\n",
       " ' A Freely Available Wide Coverage Morphological Analyzer for English This paper presents a morphological lexicon for English that handles more than 317000 inflected forms derived from over 90000 stems. The lexicon is available in two formats. The first can be used by an implementation of a two-level processor for morphological analysis. The second, derived from the first one for efficiency reasons, consists of a disk-based database using a UNIX hash table facility. We also built an X Window tool to facilitate the maintenance and browsing of the lexicon. The package is ready to be integrated into an natural language application such as a parser through hooks written in Lisp and C. ',\n",
       " ' Concurrent Lexicalized Dependency Parsing: A Behavioral View on ParseTalk Events The behavioral specification of an object-oriented grammar model is considered. The model is based on full lexicalization, head-orientation via valency constraints and dependency relations, inheritance as a means for non-redundant lexicon specification, and concurrency of computation. The computation model relies upon the actor paradigm, with concurrency entering through asynchronous message passing between actors. In particular, we here elaborate on principles of how the global behavior of a lexically distributed grammar and its corresponding parser can be specified in terms of event type networks and event networks, resp. ',\n",
       " ' Concurrent Lexicalized Dependency Parsing: The ParseTalk Model A grammar model for concurrent, object-oriented natural language parsing is introduced. Complete lexical distribution of grammatical knowledge is achieved building upon the head-oriented notions of valency and dependency, while inheritance mechanisms are used to capture lexical generalizations. The underlying concurrent computation model relies upon the actor paradigm. We consider message passing protocols for establishing dependency relations and ambiguity handling. ',\n",
       " ' Construction of a Bilingual Dictionary Intermediated by a Third Language When using a third language to construct a bilingual dictionary, it is necessary to discriminate equivalencies from inappropriate words derived as a result of ambiguity in the third language. We propose a method to treat this by utilizing the structures of dictionaries to measure the nearness of the meanings of words. The resulting dictionary is a word-to-word bilingual dictionary of nouns and can be used to refine the entries and equivalencies in published bilingual dictionaries. ',\n",
       " ' Does Baum-Welch Re-estimation Help Taggers? In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. More recently, Cutting {\\\\it et al.} (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of {\\\\em a priori} information about probabilities, by using Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model. ',\n",
       " \" Dutch Cross Serial Dependencies in HPSG We present an analysis of Dutch cross serial dependencies in Head-driven Phrase Structure Grammar. Arguably, our analysis differs from other analyses in that we do not refer to `additional' mechanisms (e.g., sequence union, head wrapping): just standard structure sharing, an immediate dominance schema and a linear precedence rule. \",\n",
       " ' Korean to English Translation Using Synchronous TAGs It is often argued that accurate machine translation requires reference to contextual knowledge for the correct treatment of linguistic phenomena such as dropped arguments and accurate lexical selection. One of the historical arguments in favor of the interlingua approach has been that, since it revolves around a deep semantic representation, it is better able to handle the types of linguistic phenomena that are seen as requiring a knowledge-based approach. In this paper we present an alternative approach, exemplified by a prototype system for machine translation of English and Korean which is implemented in Synchronous TAGs. This approach is essentially transfer based, and uses semantic feature unification for accurate lexical selection of polysemous verbs. The same semantic features, when combined with a discourse model which stores previously mentioned entities, can also be used for the recovery of topicalized arguments. In this paper we concentrate on the translation of Korean to English. ',\n",
       " ' Part-of-Speech Tagging with Neural Networks Text corpora which are tagged with part-of-speech information are useful in many areas of linguistic research. In this paper, a new part-of-speech tagging method based on neural networks (Net- Tagger) is presented and its performance is compared to that of a HMM-tagger and a trigram-based tagger. It is shown that the Net- Tagger performs as well as the trigram-based tagger and better than the HMM-tagger. ',\n",
       " ' Reference Resolution Using Semantic Patterns in Japanese Newspaper Articles Reference resolution is one of the important tasks in natural language processing. In this paper, the author first determines the referents and their locations of dousha , literally meaning the same company , which appear in Japanese newspaper articles. Secondly, three heuristic methods, two of which use semantic information in text such as company names and their patterns, are proposed and tested on how accurately they identify the correct referents. The proposed methods based on semantic patterns show high accuracy for reference resolution of dousha (more than 90\\\\%). This suggests that semantic pattern-matching methods are effective for reference resolution in newspaper articles. ',\n",
       " ' A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem. ',\n",
       " ' Automated tone transcription In this paper I report on an investigation into the problem of assigning tones to pitch contours. The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages. Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang (Cameroon). Following recent work by Liberman and others, I provide a parametrised F_0 prediction function P which generates F_0 values from a tone sequence, and I explore the asymptotic behaviour of downstep. Next, I observe that transcribing a sequence X of pitch (i.e. F_0) values amounts to finding a tone sequence T such that P(T) {}~= X. This is a combinatorial optimisation problem, for which two non-deterministic search techniques are provided: a genetic algorithm and a simulated annealing algorithm. Finally, two implementations---one for each technique---are described and then compared using both artificial and real data for sequences of up to 20 tones. These programs can be adapted to other tone languages by adjusting the F_0 prediction function. ',\n",
       " \" Minimal Change and Bounded Incremental Parsing Ideally, the time that an incremental algorithm uses to process a change should be a function of the size of the change rather than, say, the size of the entire current input. Based on a formalization of ``the set of things changed'' by an incremental modification, this paper investigates how and to what extent it is possible to give such a guarantee for a chart-based parsing framework and discusses the general utility of a minimality notion in incremental processing. \",\n",
       " ' Probabilistic Tagging with Feature Structures The described tagger is based on a hidden Markov model and uses tags composed of features such as part-of-speech, gender, etc. The contextual probability of a tag (state transition probability) is deduced from the contextual probabilities of its feature-value-pairs. This approach is advantageous when the available training corpus is small and the tag set large, which can be the case with morphologically rich languages. ',\n",
       " \" Syntactic Analysis Of Natural Language Using Linguistic Rules And Corpus-based Patterns We are concerned with the syntactic annotation of unrestricted text. We combine a rule-based analysis with subsequent exploitation of empirical data. The rule-based surface syntactic analyser leaves some amount of ambiguity in the output that is resolved using empirical patterns. We have implemented a system for generating and applying corpus-based patterns. Some patterns describe the main constituents in the sentence and some the local context of the each syntactic function. There are several (partly) reduntant patterns, and the ``pattern'' parser selects analysis of the sentence that matches the strictest possible pattern(s). The system is applied to an experimental corpus. We present the results and discuss possible refinements of the method from a linguistic point of view. \",\n",
       " ' Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing In a lexicalized grammar formalism such as Lexicalized Tree-Adjoining Grammar LTAG, each lexical item is associated with at least one elementary structure (supertag) that localizes syntactic and semantic dependencies. Thus a parser for a lexicalized grammar must search a large set of supertags to choose the right ones to combine for the parse of the sentence. We present techniques for disambiguating supertags using local information such as lexical preference and local lexical dependencies. The similarity between LTAG and Dependency grammars is exploited in the dependency model of supertag disambiguation. The performance results for various models of supertag disambiguation such as unigram, trigram and dependency-based models are presented. ',\n",
       " ' Feature-Based TAG in place of multi-component adjunction: Computational Implications Using feature-based Tree Adjoining Grammar TAG, this paper presents linguistically motivated analyses of constructions claimed to require multi-component adjunction. These feature-based TAG analyses permit parsing of these constructions using an existing unification-based Earley-style TAG parser, thus obviating the need for a multi-component TAG parser without sacrificing linguistic coverage for English. ',\n",
       " ' Towards a More User-friendly Correction We first present our view of detection and correction of syntactic errors. We then introduce a new correction method, based on heuristic criteria used to decide which correction should be preferred. Weighting of these criteria leads to a flexible and parametrable system, which can adapt itself to the user. A partitioning of the trees based on linguistic criteria: agreement rules, rather than computational criteria is then necessary. We end by proposing extensions to lexical correction and to some syntactic errors. Our aim is an adaptable and user-friendly system capable of automatic correction for some applications. ',\n",
       " \" Planning Argumentative Texts This paper presents \\\\proverb\\\\, a text planner for argumentative texts. \\\\proverb\\\\'s main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way. The former splits the task of presenting a particular proof into subtasks of presenting subproofs. The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus. \",\n",
       " ' Default Handling in Incremental Generation Natural language generation must work with insufficient input. Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input. The paper aims to escape from such dead-end situations by making assumptions. We discuss global aspects of default handling. Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion. ',\n",
       " \" A Comparison of Two Smoothing Methods for Word Bigram Models A COMPARISON OF TWO SMOOTHING METHODS FOR WORD BIGRAM MODELS Linda Bauman Peto Department of Computer Science University of Toronto Abstract Word bigram models estimated from text corpora require smoothing methods to estimate the probabilities of unseen bigrams. The deleted estimation method uses the formula: Pr(i|j) = lambda f_i + (1-lambda)f_i|j, where f_i and f_i|j are the relative frequency of i and the conditional relative frequency of i given j, respectively, and lambda is an optimized parameter. MacKay (1994) proposes a Bayesian approach using Dirichlet priors, which yields a different formula: Pr(i|j) = (alpha/F_j + alpha) m_i + (1 - alpha/F_j + alpha) f_i|j where F_j is the count of j and alpha and m_i are optimized parameters. This thesis describes an experiment in which the two methods were trained on a two-million-word corpus taken from the Canadian _Hansard_ and compared on the basis of the experimental perplexity that they assigned to a shared test corpus. The methods proved to be about equally accurate, with MacKay's method using fewer resources. \",\n",
       " \" CLARE: A Contextual Reasoning and Cooperative Response Framework for the Core Language Engine This report describes the research, design and implementation work carried out in building the CLARE system at SRI International, Cambridge, England. CLARE was designed as a natural language processing system with facilities for reasoning and understanding in context and for generating cooperative responses. The project involved both further development of SRI's Core Language Engine (Alshawi, 1992, MIT Press) natural language processor and the design and implementation of new components for reasoning and response generation. The CLARE system has advanced the state of the art in a wide variety of areas, both through the use of novel techniques developed on the project, and by extending the coverage or scale of known techniques. The language components are application-independent and provide interfaces for the development of new types of application. \",\n",
       " ' Sublanguage Terms: Dictionaries, Usage, and Automatic Classification The use of terms from natural and social scientific titles and abstracts is studied from the perspective of sublanguages and their specialized dictionaries. Different notions of sublanguage distinctiveness are explored. Objective methods for separating hard and soft sciences are suggested based on measures of sublanguage use, dictionary characteristics, and sublanguage distinctiveness. Abstracts were automatically classified with a high degree of accuracy by using a formula that considers the degree of uniqueness of terms in each sublanguage. This may prove useful for text filtering or information retrieval systems. ',\n",
       " \" Lower bounds for identifying subset members with subset queries An instance of a group testing problem is a set of objects and an unknown subset of . The task is to determine by using queries of the type ``does intersect '', where is a subset of . This problem occurs in areas such as fault detection, multiaccess communications, optimal search, blood testing and chromosome mapping. Consider the two stage algorithm for solving a group testing problem. In the first stage a predetermined set of queries are asked in parallel and in the second stage, is determined by testing individual objects. Let . Suppose that is generated by independently adding each to with probability . Let () be the number of queries asked in the first (second) stage of this algorithm. We show that if , then , while there exist algorithms with and . The proof involves a relaxation technique which can be used with arbitrary distributions. The best previously known bound is . For general group testing algorithms, our results imply that if the average number of queries over the course of () independent experiments is , then with high probability non-singleton subsets are queried. This settles a conjecture of Bill Bruno and David Torney and has important consequences for the use of group testing in screening DNA libraries and other applications where it is more cost effective to use non-adaptive algorithms and/or too expensive to prepare a subset for its first test. \",\n",
       " \" Bootstrapping A Wide-Coverage CCG from FB-LTAG A number of researchers have noted the similarities between LTAGs and CCGs. Observing this resemblance, we felt that we could make use of the wide-coverage grammar developed in the XTAG project to build a wide-coverage CCG. To our knowledge there have been no attempts to construct a large-scale CCG parser with the lexicon to support it. In this paper, we describe such a system, built by adapting various XTAG components to CCG. We find that, despite the similarities between the formalisms, certain parts of the grammatical workload are distributed differently. In addition, the flexibility of CCG derivations allows the translated grammar to handle a number of ``non-constituent'' constructions which the XTAG grammar cannot. \",\n",
       " \" Constraining Lexical Selection Across Languages Using TAGs Lexical selection in Machine Translation consists of several related components. Two that have received a lot of attention are lexical mapping from an underlying concept or lexical item, and choosing the correct subcategorization frame based on argument structure. Because most MT applications are small or relatively domain specific, a third component of lexical selection is generally overlooked - distinguishing between lexical items that are closely related conceptually. While some MT systems have proposed using a 'world knowledge' module to decide which word is more appropriate based on various pragmatic or stylistic constraints, we are interested in seeing how much we can accomplish using a combination of syntax and lexical semantics. By using separate ontologies for each language implemented in FB-LTAGs, we are able to elegantly model the more specific and language dependent syntactic and semantic distinctions necessary to further filter the choice of the lexical item. \",\n",
       " ' Determining Determiner Sequencing: A Syntactic Analysis for English Previous work on English determiners has primarily concentrated on their semantics or scoping properties rather than their complex ordering behavior. The little work that has been done on determiner ordering generally splits determiners into three subcategories. However, this small number of categories does not capture the finer distinctions necessary to correctly order determiners. This paper presents a syntactic account of determiner sequencing based on eight independently identified semantic features. Complex determiners, such as genitives, partitives, and determiner modifying adverbials, are also presented. This work has been implemented as part of XTAG, a wide-coverage grammar for English based in the Feature-Based, Lexicalized Tree Adjoining Grammar (FB-LTAG) formalism. ',\n",
       " ' Parsing Free Word-Order Languages in Polynomial Time We present a parsing algorithm with polynomial time complexity for a large subset of V-TAG languages. V-TAG, a variant of multi-component TAG, can handle free-word order phenomena which are beyond the class LCFRS (which includes regular TAG). Our algorithm is based on a CYK-style parser for TAGs. ',\n",
       " \" Status of the XTAG System XTAG is an ongoing project to develop a wide-coverage grammar for English, based on the Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) formalism. The XTAG system integrates a morphological analyzer, an N-best part-of-speech tagger, an Early-style parser and an X-window interface, along with a wide-coverage grammar for English developed using the system. This system serves as a linguist's workbench for developing FB-LTAG specifications. This paper presents a description of and recent improvements to the various components of the XTAG system. It also presents the recent performance of the wide-coverage grammar on various corpora and compares it against the performance of other wide-coverage and domain-specific grammars. \",\n",
       " \" The Linguistic Relevance of Quasi-Trees We discuss two constructions (long scrambling and ECM verbs) which challenge most syntactic theories (including traditional TAG approaches) since they seem to require exceptional mechanisms and postulates. We argue that these constructions should in fact be analyzed in a similar manner, namely as involving a verb which selects for a ``defective'' complement. These complements are defective in that they lack certain Case-assigning abilities (represented as functional heads). The constructions differ in how many such abilities are lacking. Following the previous analysis of scrambling of Rambow (1994), we propose a TAG analysis based on quasi-trees. \",\n",
       " ' Acquiring Knowledge from Encyclopedic Texts A computational model for the acquisition of knowledge from encyclopedic texts is described. The model has been implemented in a program, called SNOWY, that reads unedited texts from {\\\\em The World Book Encyclopedia}, and acquires new concepts and conceptual relations about topics dealing with the dietary habits of animals, their classifications and habitats. The program is also able to answer an ample set of questions about the knowledge that it has acquired. This paper describes the essential components of this model, namely semantic interpretation, inferences and representation, and ends with an evaluation of the performance of the program, a sample of the questions that it is able to answer, and its relation to other programs of similar nature. ',\n",
       " ' From Regular to Context Free to Mildly Context Sensitive Tree Rewriting Systems: The Path of Child Language Acquisition Current syntactic theory limits the range of grammatical variation so severely that the logical problem of grammar learning is trivial. Yet, children exhibit characteristic stages in syntactic development at least through their sixth year. Rather than positing maturational delays, I suggest that acquisition difficulties are the result of limitations in manipulating grammatical representations. I argue that the genesis of complex sentences reflects increasing generative capacity in the systems generating structural descriptions: conjoined clauses demand only a regular tree rewriting system; sentential embedding uses a context-free tree substitution grammar; modification requires TAG, a mildly context-sensitive system. ',\n",
       " ' The Whiteboard Architecture: a way to integrate heterogeneous components of NLP systems We present a new software architecture for NLP systems made of heterogeneous components, and demonstrate an architectural prototype we have built at ATR in the context of Speech Translation. ',\n",
       " \" Adnominal adjectives, code-switching and lexicalized TAG In codeswitching contexts, the language of a syntactic head determines the distribution of its complements. Mahootian 1993 derives this generalization by representing heads as the anchors of elementary trees in a lexicalized TAG. However, not all codeswitching sequences are amenable to a head-complement analysis. For instance, adnominal adjectives can occupy positions not available to them in their own language, and the TAG derivation of such sequences must use unanchored auxiliary trees. palabras heavy-duty `heavy-duty words' (Spanish-English; Poplack 1980:584) taste lousy sana `very lousy taste' (English-Swahili; Myers-Scotton 1993:29, (10)) Given the null hypothesis that codeswitching and monolingual sequences are derived in an identical manner, sequences like those above provide evidence that pure lexicalized TAGs are inadequate for the description of natural language. \",\n",
       " ' Phoneme-level speech and natural language intergration for agglutinative languages A new tightly coupled speech and natural language integration model is presented for a TDNN-based large vocabulary continuous speech recognition system. Unlike the popular n-best techniques developed for integrating mainly HMM-based speech and natural language systems in word level, which is obviously inadequate for the morphologically complex agglutinative languages, our model constructs a spoken language system based on the phoneme-level integration. The TDNN-CYK spoken language architecture is designed and implemented using the TDNN-based diphone recognition module integrated with the table-driven phonological/morphological co-analysis. Our integration model provides a seamless integration of speech and natural language for connectionist speech recognition systems especially for morphologically complex languages such as Korean. Our experiment results show that the speaker-dependent continuous Eojeol (word) recognition can be integrated with the morphological analysis with over 80\\\\% morphological analysis success rate directly from the speech input for the middle-level vocabularies. ',\n",
       " ' Parsing Using Linearly Ordered Phonological Rules A generate and test algorithm is described which parses a surface form into one or more lexical entries using linearly ordered phonological rules. This algorithm avoids the exponential expansion of search space which a naive parsing algorithm would face by encoding into the form being parsed the ambiguities which arise during parsing. The algorithm has been implemented and tested on real language data, and its speed compares favorably with that of a KIMMO-type parser. ',\n",
       " ' Extending DRT with a Focusing Mechanism for Pronominal Anaphora and Ellipsis Resolution Cormack (1992) proposed a framework for pronominal anaphora resolution. Her proposal integrates focusing theory (Sidner et al.) and DRT (Kamp and Reyle). We analyzed this methodology and adjusted it to the processing of Portuguese texts. The scope of the framework was widened to cover sentences containing restrictive relative clauses and subject ellipsis. Tests were conceived and applied to probe the adequacy of proposed modifications when dealing with processing of current texts. ',\n",
       " ' Comlex Syntax: Building a Computational Lexicon We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled. ',\n",
       " ' Interlanguage Signs and Lexical Transfer Errors A theory of interlanguage IL lexicons is outlined, with emphasis on IL lexical entries, based on the HPSG notion of lexical sign. This theory accounts for idiosyncratic or lexical transfer of syntactic subcategorisation and idioms from the first language to the IL. It also accounts for developmental stages in IL lexical grammar, and grammatical variation in the use of the same lexical item. The theory offers a tool for robust parsing of lexical transfer errors and diagnosis of such errors. ',\n",
       " \" Extraction in Dutch with Lexical Rules Unbounded dependencies are often modelled by ``traces'' (and ``gap threading'') in unification-based grammars. Pollard and Sag, however, suggest an analysis of extraction based on lexical rules, which excludes the notion of traces (P&S 1994, Chapter 9). In parsing, it suggests a trade of indeterminism for lexical ambiguity. This paper provides a short introduction to this approach to extraction with lexical rules, and illustrates the linguistic power of the approach by applying it to particularly idiosyncratic Dutch extraction data. \",\n",
       " ' Focus on ``only and ``Not Krifka [1993] has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on, rather than as a specific semantic or pragmatic function itself. The current paper describes an implementation of this general idea, and applies it to the interpretation of {\\\\em only} and {\\\\em not}. ',\n",
       " \" McColm conjecture Gregory McColm conjectured that positive elementary inductions are bounded in a class K of finite structures if every (FO + LFP) formula is equivalent to a first-order formula in K. Here (FO + LFP) is the extension of first-order logic with the least fixed point operator. We disprove the conjecture. Our main results are two model-theoretic constructions, one deterministic and the other randomized, each of which refutes McColm's conjecture. \",\n",
       " ' On finite rigid structures The main result of this paper is a probabilistic construction of finite rigid structures. It yields a finitely axiomatizable class of finite rigid structures where no L^omega_{infty, omega} formula with counting quantifiers defines a linear order. ',\n",
       " ' Free-ordered CUG on Chemical Abstract Machine We propose a paradigm for concurrent natural language generation. In order to represent grammar rules distributively, we adopt categorial unification grammar CUG where each category owns its functional type. We augment typed lambda calculus with several new combinators, to make the order of lambda-conversions free for partial / local processing. The concurrent calculus is modeled with Chemical Abstract Machine. We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination. ',\n",
       " ' Abstract Generation based on Rhetorical Structure Extraction We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction. The system first extracts the rhetorical structure, the compound of the rhetorical relations between sentences, and then cuts out less important parts in the extracted structure to generate an abstract of the desired length. Evaluation of the generated abstract showed that it contains at maximum 74\\\\% of the most important sentences of the original text. The system is now utilized as a text browser for a prototypical interactive document retrieval system. ',\n",
       " ' Multi-Dimensional Inheritance In this paper, we present an alternative approach to multiple inheritance for typed feature structures. In our approach, a feature structure can be associated with several types coming from different hierarchies (dimensions). In case of multiple inheritance, a type has supertypes from different hierarchies. We contrast this approach with approaches based on a single type hierarchy where a feature structure has only one unique most general type, and multiple inheritance involves computation of greatest lower bounds in the hierarchy. The proposed approach supports current linguistic analyses in constraint-based formalisms like HPSG, inheritance in the lexicon, and knowledge representation for NLP systems. Finally, we show that multi-dimensional inheritance hierarchies can be compiled into a Prolog term representation, which allows to compute the conjunction of two types efficiently by Prolog term unification. ',\n",
       " ' Reverse Queries in DATR DATR is a declarative representation language for lexical information and as such, in principle, neutral with respect to particular processing strategies. Previous DATR compiler/interpreter systems support only one access strategy that closely resembles the set of inference rules of the procedural semantics of DATR (Evans & Gazdar 1989a). In this paper we present an alternative access strategy (reverse query strategy) for a non-trivial subset of DATR. ',\n",
       " ' The complexity of broadcasting in bounded-degree networks Broadcasting concerns the dissemination of a message originating at one node of a network to all other nodes. This task is accomplished by placing a series of calls over the communication lines of the network between neighboring nodes, where each call requires a unit of time and a call can involve only two nodes. We show that for bounded-degree networks determining the minimum broadcast time from an originating node remains NP-complete. ',\n",
       " ' Automatically Identifying Morphological Relations in = Machine-Readable Dictionaries We describe an automated method for identifying classes of morphologically related words in an on-line dictionary, and for linking individual senses in the derived form to one or more senses in the base form by means of morphological relation attributes. We also present an algorithm for computing a score reflecting the system=92s certainty in these derivational links; this computation relies on the content of semantic relations associated with each sense, which are extracted automatically by parsing each sense definition and subjecting the parse structure to automated semantic analysis. By processing the entire set of headwords in the dictionary in this fashion we create a large set of directed derivational graphs, which can then be accessed by other components in our broad-coverage NLP system. Spurious or unlikely derivations are not discarded, but are rather added to the dictionary and assigned a negative score; this allows the system to handle non-standard uses of these forms. ',\n",
       " ' Adaptive Sentence Boundary Disambiguation Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. After training for less than one minute, the method correctly labels over 98.5\\\\% of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts. ',\n",
       " ' Manipulating Human-oriented Dictionaries with very simple tools This paper presents a methodology for building and manipulating human-oriented dictionaries. This methodology has been applied in the construction of a French-English-Malay dictionary which has been obtained by crossing semi-automatically two bilingual dictionaries. We use only Microsoft Word, a specialized language for writing transcriptors and a small but powerful dictionary tool. ',\n",
       " ' The Speech-Language Interface in the Spoken Language Translator The Spoken Language Translator is a prototype for practically useful systems capable of translating continuous spoken language within restricted domains. The prototype system translates air travel ATIS queries from spoken English to spoken Swedish and to French. It is constructed, with as few modifications as possible, from existing pieces of speech and language processing software. The speech recognizer and language understander are connected by a fairly conventional pipelined N-best interface. This paper focuses on the ways in which the language processor makes intelligent use of the sentence hypotheses delivered by the recognizer. These ways include (1) producing modified hypotheses to reflect the possible presence of repairs in the uttered word sequence; (2) fast parsing with a version of the grammar automatically specialized to the more frequent constructions in the training corpus; and (3) allowing syntactic and semantic factors to interact with acoustic ones in the choice of a meaning structure for translation, so that the acoustically preferred hypothesis is not always selected even if it is within linguistic coverage. ',\n",
       " \" An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. \",\n",
       " ' Automatic Generation of Technical Documentation Natural-language generation NLG techniques can be used to automatically produce technical documentation from a domain knowledge base and linguistic and contextual models. We discuss this application of NLG technology from both a technical and a usefulness (costs and benefits) perspective. This discussion is based largely on our experiences with the IDAS documentation-generation project, and the reactions various interested people from industry have had to IDAS. We hope that this summary of our experiences with IDAS and the lessons we have learned from it will be beneficial for other researchers who wish to build technical-documentation generation systems. ',\n",
       " ' Complexity of Scrambling: A New Twist to the Competence - Performance Distinction In this paper we discuss the following issue: How do we decide whether a certain property of language is a competence property or a performance property? Our claim is that the answer to this question is not given a-priori. The answer depends on the formal devices (formal grammars and machines) available to us for describing language. We discuss this issue in the context of the complexity of processing of center embedding (of relative clauses in English) and scrambling (in German, for example) from arbitrary depths of embedding. ',\n",
       " \" Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible? I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this `consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems. \",\n",
       " ' Operations for Learning with Graphical Models This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms. ',\n",
       " ' Total-Order and Partial-Order Planning: A Comparative Analysis For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners. ',\n",
       " ' Wrap-Up: a Trainable Discourse Module for Information Extraction The vast amounts of on-line text now available have led to renewed interest in information extraction IE systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain. ',\n",
       " ' Dependency Grammar and the Parsing of Chinese Sentences Dependency Grammar has been used by linguists as the basis of the syntactic components of their grammar formalisms. It has also been used in natural language parsing. In China, attempts have been made to use this grammar formalism to parse Chinese sentences using corpus-based techniques. This paper reviews the properties of Dependency Grammar as embodied in four axioms for the well-formedness conditions for dependency structures. It is shown that allowing multiple governors as done by some followers of this formalism is unnecessary. The practice of augmenting Dependency Grammar with functional labels is also discussed in the light of building functional structures when the sentence is parsed. This will also facilitate semantic interpretation. ',\n",
       " ' N-Gram Cluster Identification During Empirical Knowledge Representation Generation This paper presents an overview of current research concerning knowledge extraction from technical texts. In particular, the use of empirical techniques during the identification and generation of a semantic representation is considered. A key step is the discovery of useful n-grams and correlations between clusters of these n-grams. ',\n",
       " \" An Extended Clustering Algorithm for Statistical Language Models Statistical language models frequently suffer from a lack of training data. This problem can be alleviated by clustering, because it reduces the number of free parameters that need to be trained. However, clustered models have the following drawback: if there is ``enough'' data to train an unclustered model, then the clustered variant may perform worse. On currently used language modeling corpora, e.g. the Wall Street Journal corpus, how do the performances of a clustered and an unclustered model compare? While trying to address this question, we develop the following two ideas. First, to get a clustering algorithm with potentially high performance, an existing algorithm is extended to deal with higher order N-grams. Second, to make it possible to cluster large amounts of training data more efficiently, a heuristic to speed up the algorithm is presented. The resulting clustering algorithm can be used to cluster trigrams on the Wall Street Journal corpus and the language models it produces can compete with existing back-off models. Especially when there is only little training data available, the clustered models clearly outperform the back-off models. \",\n",
       " ' Knowledge Representation for Lexical Semantics: Is Standard First Order Logic Enough? Natural language understanding applications such as interactive planning and face-to-face translation require extensive inferencing. Many of these inferences are based on the meaning of particular open class words. Providing a representation that can support such lexically-based inferences is a primary concern of lexical semantics. The representation language of first order logic has well-understood semantics and a multitude of inferencing systems have been implemented for it. Thus it is a prime candidate to serve as a lexical semantics representation. However, we argue that FOL, although a good starting point, needs to be extended before it can efficiently and concisely support all the lexically-based inferences needed. ',\n",
       " ' Segmenting speech without a lexicon: The roles of phonotactics and speech source Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon. Several sources of information in speech might help infants solve this problem, including prosody, semantic correlations and phonotactics. Research to date has focused on determining to which of these sources infants might be sensitive, but little work has been done to determine the potential usefulness of each source. The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences. The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle. Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules, the combination of both sources is most useful. ',\n",
       " \" Robust stochastic parsing using the inside-outside algorithm The paper describes a parser of sequences of (English) part-of-speech labels which utilises a probabilistic grammar trained using the inside-outside algorithm. The initial (meta)grammar is defined by a linguist and further rules compatible with metagrammatical constraints are automatically generated. During training, rules with very low probability are rejected yielding a wide-coverage parser capable of ranking alternative analyses. A series of corpus-based experiments describe the parser's performance. \",\n",
       " ' Coupling Phonology and Phonetics in a Constraint-Based Gestural Model An implemented approach which couples a constraint-based phonology component with an articulatory speech synthesizer is proposed. Articulatory gestures ensure a tight connection between both components, as they comprise both physical-phonetic and phonological aspects. The phonological modelling of e.g. syllabification and phonological processes such as German final devoicing is expressed in the constraint logic programming language CUF. Extending CUF by arithmetic constraints allows the simultaneous description of both phonology and phonetics. Thus declarative lexicalist theories of grammar such as HPSG may be enriched up to the level of detailed phonetic realisation. Initial acoustic demonstrations show that our approach is in principle capable of synthesizing full utterances in a linguistically motivated fashion. ',\n",
       " ' Analysis of Japanese Compound Nouns using Collocational Information Analyzing compound nouns is one of the crucial issues for natural language processing systems, in particular for those systems that aim at a wide coverage of domains. In this paper, we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus. An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters. The accuracy of this method is about 80%. ',\n",
       " \" A Domain-Independent Algorithm for Plan Adaptation The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph. \",\n",
       " \" Solving Multiclass Learning Problems via Error-Correcting Output Codes Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k ``classes''). The definition is acquired by studying collections of training examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems. \",\n",
       " ' Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning Temporal difference TD methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda &gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning. ',\n",
       " \" Using default inheritance to describe LTAG We present the results of an investigation into how the set of elementary trees of a Lexicalized Tree Adjoining Grammar can be represented in the lexical knowledge representation language DATR (Evans & Gazdar 1989a,b). The LTAG under consideration is based on the one described in Abeille et al. (1990). Our approach is similar to that of Vijay-Shanker & Schabes (1992) in that we formulate an inheritance hierarchy that efficiently encodes the elementary trees. However, rather than creating a new representation formalism for this task, we employ techniques of established utility in other lexically-oriented frameworks. In particular, we show how DATR's default mechanism can be used to eliminate the need for a non-immediate dominance relation in the descriptions of the surface LTAG entries. This allows us to embed the tree structures in the feature theory in a manner reminiscent of HPSG subcategorisation frames, and hence express lexical rules as relations over feature structures. \",\n",
       " ' NL Understanding with a Grammar of Constructions We present an approach to natural language understanding based on a computable grammar of constructions. A construction consists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key element of Mincal, an implemented natural language, speech-enabled interface to an on-line calendar system. The system consists of a NL grammar, a parser, an on-line calendar, a domain knowledge base (about dates, times and meetings), an application knowledge base (about the calendar), a speech recognizer, a speech generator, and the interfaces between those modules. We claim that this architecture should work in general for spoken interfaces in small domains. In this paper we present two novel aspects of the architecture: (a) the use of constructions, integrating descriptions of form, meaning and context into one whole; and (b) the separation of domain knowledge from application knowledge. We describe the data structures for encoding constructions, the structure of the knowledge bases, and the interactions of the key modules of the system. ',\n",
       " ' An HPSG Parser Based on Description Logics In this paper I present a parser based on Description Logics DL for a German HPSG -style fragment. The specified parser relies mainly on the inferential capabilities of the underlying DL system. Given a preferential default extension for DL disambiguation is achieved by choosing the parse containing a qualitatively minimal number of exceptions. ',\n",
       " ' Lexical Knowledge Representation in an Intelligent Dictionary Help System The frame-based knowledge representation model adopted in IDHS (Intelligent Dictionary Help System) is described in this paper. It is used to represent the lexical knowledge acquired automatically from a conventional dictionary. Moreover, the enrichment processes that have been performed on the Dictionary Knowledge Base and the dynamic exploitation of this knowledge - both based on the exploitation of the properties of lexical semantic relations - are also described. ',\n",
       " ' A Tool for Collecting Domain Dependent Sortal Constraints From Corpora In this paper, we describe a tool designed to generate semi-automatically the sortal constraints specific to a domain to be used in a natural language NL understanding system. This tool is evaluated using the SRI Gemini NL understanding system in the ATIS domain. ',\n",
       " ' Interlingual Lexical Organisation for Multilingual Lexical Databases in NADIA We propose a lexical organisation for multilingual lexical databases MLDB. This organisation is based on acceptions (word-senses). We detail this lexical organisation and show a mock-up built to experiment with it. We also present our current work in defining and prototyping a specialised system for the management of acception-based MLDB. Keywords: multilingual lexical database, acception, linguistic structure. ',\n",
       " \" Learning Unification-Based Natural Language Grammars When parsing unrestricted language, wide-covering grammars often undergenerate. Undergeneration can be tackled either by sentence correction, or by grammar correction. This thesis concentrates upon automatic grammar correction (or machine learning of grammar) as a solution to the problem of undergeneration. Broadly speaking, grammar correction approaches can be classified as being either {\\\\it data-driven}, or {\\\\it model-based}. Data-driven learners use data-intensive methods to acquire grammar. They typically use grammar formalisms unsuited to the needs of practical text processing and cannot guarantee that the resulting grammar is adequate for subsequent semantic interpretation. That is, data-driven learners acquire grammars that generate strings that humans would judge to be grammatically ill-formed (they {\\\\it overgenerate}) and fail to assign linguistically plausible parses. Model-based learners are knowledge-intensive and are reliant for success upon the completeness of a {\\\\it model of grammaticality}. But in practice, the model will be incomplete. Given that in this thesis we deal with undergeneration by learning, we hypothesise that the combined use of data-driven and model-based learning would allow data-driven learning to compensate for model-based learning's incompleteness, whilst model-based learning would compensate for data-driven learning's unsoundness. We describe a system that we have used to test the hypothesis empirically. The system combines data-driven and model-based learning to acquire unification-based grammars that are more suitable for practical text parsing. Using the Spoken English Corpus as data, and by quantitatively measuring undergeneration, overgeneration and parse plausibility, we show that this hypothesis is correct. \",\n",
       " ' Bottom-Up Earley Deduction We propose a bottom-up variant of Earley deduction. Bottom-up deduction is preferable to top-down deduction because it allows incremental processing (even for head-driven grammars), it is data-driven, no subsumption check is needed, and preference values attached to lexical items can be used to guide best-first search. We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps. ',\n",
       " ' ProFIT: Prolog with Features, Inheritance and Templates ProFIT is an extension of Standard Prolog with Features, Inheritance and Templates. ProFIT allows the programmer or grammar developer to declare an inheritance hierarchy, features and templates. Sorted feature terms can be used in ProFIT programs together with Prolog terms to provide a clearer description language for linguistic structures. ProFIT compiles all sorted feature terms into a Prolog term representation, so that the built-in Prolog term unification can be used for the unification of sorted feature structures, and no special unification algorithm is needed. ProFIT programs are compiled into Prolog programs, so that no meta-interpreter is needed for their execution. ProFIT thus provides a direct step from grammars developed with sorted feature terms to Prolog programs usable for practical NLP systems. ',\n",
       " ' Off-line Optimization for Earley-style HPSG Processing A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and inputs the primed grammar to an advanced Earley-style processor. This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation. Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar. ',\n",
       " ' Rapid Development of Morphological Descriptions for Full Language Processing Systems I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system. The compiler is optimized for a class of languages including many or most European ones, and for rapid development and debugging of descriptions of new languages. The key design decision is to compose morphophonological and morphosyntactic information, but not the lexicon, when compiling the description. This results in typical compilation times of about a minute, and has allowed a reasonably full, feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system. ',\n",
       " ' On Learning More Appropriate Selectional Restrictions We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora. It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes. Evaluation measures for the Selectional Restrictions learning task are discussed. Finally, an experimental evaluation of these variations is reported. ',\n",
       " ' Utilization of a Lexicon for Spelling Correction in Modern Greek In this paper we present an interactive spelling correction system for Modern Greek. The entire system is based on a morphological lexicon. Emphasis is given to the development of the lexicon, especially as far as storage economy, speed efficiency and dictionary coverage is concerned. Extensive research was conducted from both the computer engineering and linguisting fields, in order to describe inflectional morphology as economically as possible. ',\n",
       " ' A Robust and Efficient Three-Layered Dialogue Component for a Speech-to-Speech Translation System We present the dialogue component of the speech-to-speech translation system VERBMOBIL. In contrast to conventional dialogue systems it mediates the dialogue while processing maximally 50% of the dialogue in depth. Special requirements like robustness and efficiency lead to a 3-layered hybrid architecture for the dialogue module, using statistics, an automaton and a planner. A dialogue memory is constructed incrementally. ',\n",
       " ' Ambiguity resolution in a reductionistic parser We are concerned with dependency-oriented morphosyntactic parsing of running text. While a parsing grammar should avoid introducing structurally unresolvable distinctions in order to optimise on the accuracy of the parser, it also is beneficial for the grammarian to have as expressive a structural representation available as possible. In a reductionistic parsing system this policy may result in considerable ambiguity in the input; however, even massive ambiguity can be tackled efficiently with an accurate parsing description and effective parsing technology. ',\n",
       " ' Ellipsis and Quantification: a substitutional approach The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple, Shieber and Pereira (1991), but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution. It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition, instead of the more common view of interpretation as actually performing the composition ',\n",
       " ' NPtool, a detector of English noun phrases NPtool is a fast and accurate system for extracting noun phrases from English texts for the purposes of e.g. information retrieval, translation unit discovery, and corpus studies. After a general introduction, the system architecture is presented in outline. Then follows an examination of a recently written Constraint Syntax. An evaluation report concludes the paper. ',\n",
       " ' Specifying a shallow grammatical representation for parsing purposes Is it possible to specify a grammatical representation (descriptors and their application guidelines) to such a degree that it can be consistently applied by different grammarians e.g. for producing a benchmark corpus for parser evaluation? Arguments for and against have been given, but very little empirical evidence. In this article we report on a double-blind experiment with a surface-oriented morphosyntactic grammatical representation used in a large-scale English parser. We argue that a consistently applicable representation for morphology and also shallow syntax can be specified. A grammatical representation with a near-100% coverage of running text can be specified with a reasonable effort, especially if the representation is based on structural distinctions (i.e. it is structurally resolvable). ',\n",
       " ' The Semantics of Resource Sharing in Lexical-Functional Grammar We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures. We provide an extension to the previous account of LFG semantics (Dalrymple et al., 1993b) according to which dependencies between f-structures are viewed as resources; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained. The resulting system is sufficiently restricted in cases where other approaches overgenerate; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation. ',\n",
       " ' A syntax-based part-of-speech analyser There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven. Recent state-of-the-art part-of-speech taggers are based on the data-driven approach. Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the data-driven approach in part-of-speech analysis may appear surprising. In this paper, a case is made for the syntactic nature of part-of-speech tagging. A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated. Tested against a benchmark corpus of 38,000 words of previously unseen text, this syntax-based system reaches an accuracy of above 99%. Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis. ',\n",
       " \" Formalization and Parsing of Typed Unification-Based ID/LP Grammars This paper defines unification based ID/LP grammars based on typed feature structures as nonterminals and proposes a variant of Earley's algorithm to decide whether a given input sentence is a member of the language generated by a particular typed unification ID/LP grammar. A solution to the problem of the nonlocal flow of information in unification ID/LP grammars as discussed in Seiffert (1991) is incorporated into the algorithm. At the same time, it tries to connect this technical work with linguistics by presenting an example of the problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa 1994, Richter and Sailer 1995) and with computational linguistics by drawing connections from this approach to systems implementing HPSG, especially the TROLL system, Gerdemann et al. (forthcoming). \",\n",
       " \" Integrating Free Word Order Syntax and Information Structure This paper describes a combinatory categorial formalism called Multiset-CCG that can capture the syntax and interpretation of ``free'' word order in languages such as Turkish. The formalism compositionally derives the predicate-argument structure and the information structure (e.g. topic, focus) of a sentence in parallel, and uniformly handles word order variation among the arguments and adjuncts within a clause, as well as in complex clauses and across clause boundaries. \",\n",
       " ' Deterministic Consistency Checking of LP Constraints We provide a constraint based computational model of linear precedence as employed in the HPSG grammar formalism. An extended feature logic which adds a wide range of constraints involving precedence is described. A sound, complete and terminating deterministic constraint solving procedure is given. Deterministic computational model is achieved by weakening the logic such that it is sufficient for linguistic applications involving word-order. ',\n",
       " ' A Tractable Extension of Linear Indexed Grammars It has been shown that Linear Indexed Grammars can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing. This paper describes a formalism that is more powerful than Linear Indexed Grammar, but which can also be processed in polynomial time using similar techniques. The formalism, which we refer to as Partially Linear PATR manipulates feature structures rather than stacks. ',\n",
       " ' Higher-order Linear Logic Programming of Categorial Deduction We show how categorial deduction can be implemented in higher-order (linear) logic programming, thereby realising parsing as deduction for the associative and non-associative Lambek calculi. This provides a method of solution to the parsing problem of Lambek categorial grammar applicable to a variety of its extensions. ',\n",
       " ' Stochastic HPSG In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag. We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures, then provide an extended interpretation which allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora. ',\n",
       " ' Splitting the Reference Time: Temporal Anaphora and Quantification in DRT This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory. The analysis in (Partee 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after . This problem has been previously analyzed in (de Swart 1991) as an instance of the proportion problem, and given a solution from a Generalized Quantifier approach. By using a careful distinction between the different notions of reference time, based on (Kamp and Reyle 1993), we propose a solution to this problem, within the framework of DRT. We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences. ',\n",
       " \" Algorithms for Analysing the Temporal Structure of Discourse We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense, aspect, temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure. It is part of a discourse grammar implemented in Carpenter's ALE formalism. The method for building up the temporal structure of the discourse combines constraints and preferences: we use constraints to reduce the number of possible structures, exploiting the HPSG type hierarchy and unification for this purpose; and we apply preferences to choose between the remaining options using a temporal centering mechanism. We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal/rhetorical structure until higher-level information can be used to disambiguate. \",\n",
       " ' A Robust Parser Based on Syntactic Information In this paper, we propose a robust parser which can parse extragrammatical sentences. This parser can recover them using only syntactic information. It can be easily modified and extended because it utilize only syntactic information. ',\n",
       " ' Principle Based Semantics for HPSG The paper presents a constraint based semantic formalism for HPSG. The syntax-semantics interface directly implements syntactic conditions on quantifier scoping and distributivity. The construction of semantic representations is guided by general principles governing the interaction between syntax and semantics. Each of these principles acts as a constraint to narrow down the set of possible interpretations of a sentence. Meanings of ambiguous sentences are represented by single partial representations (so-called U(nderspecified) D(iscourse) R(epresentation) S(tructure)s) to which further constraints can be added monotonically to gain more information about the content of a sentence. There is no need to build up a large number of alternative representations of the sentence which are then filtered by subsequent discourse and world knowledge. The advantage of UDRSs is not only that they allow for monotonic incremental interpretation but also that they are equipped with truth conditions and a proof theory that allows for inferences to be drawn directly on structures where quantifier scope is not resolved. ',\n",
       " ' Towards an Account of Extraposition in HPSG This paper investigates the syntax of extraposition in the HPSG framework. We present English and German data (partly taken from corpora), and provide an analysis using lexical rules and a nonlocal dependency. The condition for binding this dependency is formulated relative to the antecedent of the extraposed phrase, which entails that no fixed site for extraposition exists. Our analysis accounts for the interaction of extraposition with fronting and coordination, and predicts constraints on multiple extraposition. ',\n",
       " ' Lexical Acquisition via Constraint Solving This paper describes a method to automatically acquire the syntactic and semantic classifications of unknown words. Our method reduces the search space of the lexical acquisition problem by utilizing both the left and the right context of the unknown word. Link Grammar provides a convenient framework in which to implement our method. ',\n",
       " ' On Reasoning with Ambiguities The paper adresses the problem of reasoning with ambiguities. Semantic representations are presented that leave scope relations between quantifiers and/or other operators unspecified. Truth conditions are provided for these representations and different consequence relations are judged on the basis of intuitive correctness. Finally inference patterns are presented that operate directly on these underspecified structures, i.e. do not rely on any translation into the set of their disambiguations. ',\n",
       " \" An NLP Approach to a Specific Type of Texts: Car Accident Reports The work reported here is the result of a study done within a larger project on the ``Semantics of Natural Languages'' viewed from the field of Artificial Intelligence and Computational Linguistics. In this project, we have chosen a corpus of insurance claim reports. These texts deal with a relatively circumscribed domain, that of road traffic, thereby limiting the extra-linguistic knowledge necessary to understand them. Moreover, these texts present a number of very specific characteristics, insofar as they are written in a quasi-institutional setting which imposes many constraints on their production. We first determine what these constraints are in order to then show how they provide the writer with the means to create as succint a text as possible, and in a symmetric way, how they provide the reader with the means to interpret the text and to distinguish between its factual and argumentative aspects. \",\n",
       " ' Bi-directional memory-based dialog translation: The KEMDT approach A bi-directional Korean/English dialog translation system is designed and implemented using the memory-based translation technique. The system KEMDT (Korean/English Memory-based Dialog Translation system) can perform Korean to English, and English to Korean translation using unified memory network and extended marker passing algorithm. We resolve the word order variation and frequent word omission problems in Korean by classifying the concept sequence element in four different types and extending the marker- passing-based-translation algorithm. Unlike the previous memory-based translation systems, the KEMDT system develops the bilingual memory network and the unified bi-directional marker passing translation algorithm. For efficient language specific processing, we separate the morphological processors from the memory-based translator. The KEMDT technology provides a hierarchical memory network and an efficient marker-based control for the recent example-based MT paradigm. ',\n",
       " ' Cooperative Error Handling and Shallow Processing This paper is concerned with the detection and correction of sub-sentential English text errors. Previous spelling programs, unless restricted to a very small set of words, have operated as post-processors. And to date, grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse, assuming a complete sentence. Work described below is aimed at evaluating the effectiveness of shallow (sub-sentential) processing and the feasibility of cooperative error checking, through building and testing appropriately an error-processing system. A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parsing. Intended testing is discussed. ',\n",
       " ' Topic Identification in Discourse This paper proposes a corpus-based language model for topic identification. We analyze the association of noun-noun and noun-verb pairs in LOB Corpus. The word association norms are based on three factors: 1) word importance, 2) pair co-occurrence, and 3) distance. They are trained on the paragraph and sentence levels for noun-noun and noun-verb pairs, respectively. Under the topic coherence postulation, the nouns that have the strongest connectivities with the other nouns and verbs in the discourse form the preferred topic set. The collocational semantics then is used to identify the topics from paragraphs and to discuss the topic shift phenomenon among paragraphs. ',\n",
       " ' An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation Process This paper concerns both anaphora resolution and prepositional phrase PP attachment that are the most frequent ambiguities in natural language processing. Several methods have been proposed to deal with each phenomenon separately, however none of proposed systems has considered the way of dealing both phenomena. We tackle this issue, proposing an algorithm to co-ordinate the treatment of these two problems efficiently, i.e., the aim is also to exploit at each step all the results that each component can provide. ',\n",
       " ' Grouping Words Using Statistical Context This paper (cmp-lg/yymmnnn) has been accepted for publication in the student session of EACL-95. It outlines ongoing work using statistical and unsupervised neural network methods for clustering words in untagged corpora. Such approaches are of interest when attempting to understand the development of human intuitive categorization of language as well as for trying to improve computational methods in natural language understanding. Some preliminary results using a simple statistical approach are described, along with work using an unsupervised neural network to distinguish between the sense classes into which words fall. ',\n",
       " ' Incorporating Unconscious Reanalysis into an Incremental, Monotonic Parser This paper describes an implementation based on a recent model in the psycholinguistic literature. We define a parsing operation which allows the reanalysis of dependencies within an incremental and monotonic processing architecture, and discuss search strategies for its application in a head-initial language (English) and a head-final language (Japanese). ',\n",
       " ' A State-Transition Grammar for Data-Oriented Parsing This paper presents a grammar formalism designed for use in data-oriented approaches to language processing. The formalism is best described as a right-linear indexed grammar extended in linguistically interesting ways. The paper goes on to investigate how a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts. ',\n",
       " \" Implementation and evaluation of a German HMM for POS disambiguation A German language model for the Xerox HMM tagger is presented. This model's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora. The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French. Finally, the model's error types are described. I argue that although the overall performance of these models for German is comparable to results for English and French, a more exact analysis demonstrates important differences in the types of disambiguation involved for German. \",\n",
       " ' Literal Movement Grammars Literal movement grammars (LMGs) provide a general account of extraposition phenomena through an attribute mechanism allowing top-down displacement of syntactical information. LMGs provide a simple and efficient treatment of complex linguistic phenomena such as cross-serial dependencies in German and Dutch---separating the treatment of natural language into a parsing phase closely resembling traditional context-free treatment, and a disambiguation phase which can be carried out using matching, as opposed to full unification employed in most current grammar formalisms of linguistical relevance. ',\n",
       " \" Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICET's search in bias space and discovers a way to improve the search. \",\n",
       " ' On the Informativeness of the DNA Promoter Sequences Domain Theory The DNA promoter sequences domain theory and database have become popular for testing systems that integrate empirical and analytical learning. This note reports a simple change and reinterpretation of the domain theory in terms of M-of-N concepts, involving no learning, that results in an accuracy of 93.4% on the 106 items of the database. Moreover, an exhaustive search of the space of M-of-N domain theory interpretations indicates that the expected accuracy of a randomly chosen interpretation is 76.5%, and that a maximum accuracy of 97.2% is achieved in 12 cases. This demonstrates the informativeness of the domain theory, without the complications of understanding the interactions between various learning algorithms and the theory. In addition, our results help characterize the difficulty of learning using the DNA promoters theory. ',\n",
       " ' Computational dialectology in Irish Gaelic Dialect groupings can be discovered objectively and automatically by cluster analysis of phonetic transcriptions such as those found in a linguistic atlas. The first step in the analysis, the computation of linguistic distance between each pair of sites, can be computed as Levenshtein distance between phonetic strings. This correlates closely with the much more laborious technique of determining and counting isoglosses, and is more accurate than the more familiar metric of computing Hamming distance based on whether vocabulary entries match. In the actual clustering step, traditional agglomerative clustering works better than the top-down technique of partitioning around medoids. When agglomerative clustering of phonetic string comparison distances is applied to Gaelic, reasonable dialect boundaries are obtained, corresponding to national and (within Ireland) provincial boundaries. ',\n",
       " ' Using a Corpus for Teaching Turkish Morphology This paper reports on the preliminary phase of our ongoing research towards developing an intelligent tutoring environment for Turkish grammar. One of the components of this environment is a corpus search tool which, among other aspects of the language, will be used to present the learner sample sentences along with their morphological analyses. Following a brief introduction to the Turkish language and its morphology, the paper describes the morphological analysis and ambiguity resolution used to construct the corpus used in the search tool. Finally, implementation issues and details involving the user interface of the tool are discussed. ',\n",
       " ' Tagging French -- comparing a statistical and a constraint-based method In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model. We describe the two systems and compare the results. The accuracy of the statistical method is reasonably good, comparable to taggers for English. But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development. ',\n",
       " \" A specification language for Lexical Functional Grammars This paper defines a language L for specifying LFG grammars. This enables constraints on LFG's composite ontology (c-structures synchronised with f-structures) to be stated directly; no appeal to the LFG construction algorithm is needed. We use L to specify schemata annotated rules and the LFG uniqueness, completeness and coherence principles. Broader issues raised by this work are noted and discussed. \",\n",
       " \" ParseTalk about Sentence- and Text-Level Anaphora We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model. Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model. \",\n",
       " ' The Semantics of Motion In this paper we present a semantic study of motion complexes (ie. of a motion verb followed by a spatial preposition). We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs, on the one hand, and of the spatial prepositions, on the other hand. Then, we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components. ',\n",
       " ' Distributional Part-of-Speech Tagging This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus. ',\n",
       " ' Ellipsis and Higher-Order Unification We present a new method for characterizing the interpretive possibilities generated by elliptical constructions in natural language. Unlike previous analyses, which postulate ambiguity of interpretation or derivation in the full clause source of the ellipsis, our analysis requires no such hidden ambiguity. Further, the analysis follows relatively directly from an abstract statement of the ellipsis interpretation problem. It predicts correctly a wide range of interactions between ellipsis and other semantic phenomena such as quantifier scope and bound anaphora. Finally, although the analysis itself is stated nonprocedurally, it admits of a direct computational method for generating interpretations. ',\n",
       " \" A Note on Zipf's Law, Natural Languages, and Noncoding DNA regions In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the basis of Zipf rank frequency data that noncoding DNA sequence regions are more like natural languages than coding regions. We argue on the contrary that an empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to natural languages. Although DNA is a presumably an ``organized system of signs'' in Mandelbrot's (1961) sense, an observation of statistical features of the sort presented in the Mantegna et al. paper does not shed light on the similarity between DNA's ``grammar'' and natural language grammars, just as the observation of exact Zipf-like behavior cannot distinguish between the underlying processes of tossing an sided die or a finite-state branching process. \",\n",
       " ' Corpus-based Method for Automatic Identification of Support Verbs for Nominalizations Nominalization is a highly productive phenomena in most languages. The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal ). The choice of a support verb for a given nominalization is unpredictable, causing a problem for language learners as well as for natural language processing systems. We present here a method of discovering support verbs from an untagged corpus via low-level syntactic processing and comparison of arguments attached to verbal forms and potential nominalized forms. The result of the process is a list of potential support verbs for the nominalized form of a given predicate. ',\n",
       " ' Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies An automatic word classification system has been designed which processes word unigram and bigram frequency statistics extracted from a corpus of natural language utterances. The system implements a binary top-down form of word clustering which employs an average class mutual information metric. Resulting classifications are hierarchical, allowing variable class granularity. Words are represented as structural tags --- unique bit numbers the most significant bit-patterns of which incorporate class information. Access to a structural tag immediately provides access to all classification levels for the corresponding word. The classification system has successfully revealed some of the structure of English, from the phonemic to the semantic level. The system has been compared --- directly and indirectly --- with other recent word classification systems. Class based interpolated language models have been constructed to exploit the extra information supplied by the classifications and some experiments have shown that the new models improve model performance. ',\n",
       " \" Multilingual Sentence Categorization according to Language In this paper, we describe an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency. The implementation is fast, small, robust and textual errors tolerant. Tested for french, english, spanish and german discrimination, the system gives very interesting results, achieving in one test 99.4% correct assignments on real sentences. The resolution power is based on grammatical words (not the most common words) and alphabet. Having the grammatical words and the alphabet of each language at its disposal, the system computes for each of them its likelihood to be selected. The name of the language having the optimum likelihood will tag the sentence --- but non resolved ambiguities will be maintained. We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system's classification performance. Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions. \",\n",
       " ' Incremental Interpretation: Applications, Theory, and Relationship to Dynamic Semantics Why should computers interpret language incrementally? In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling, suggesting that humans perform semantic interpretation before constituent boundaries, possibly word by word. However, possible computational applications have received less attention. In this paper we consider various potential applications, in particular graphical interaction and dialogue. We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations. Finally, we tease apart the relationship between dynamic semantics and incremental interpretation. ',\n",
       " ' Incremental Interpretation of Categorial Grammar The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation. The parser does not require fragments of sentences to form constituents, and thereby avoids problems of spurious ambiguity. The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus. It also includes a discussion of some of the issues which arise when parsing lexicalised grammars, and the possibilities for using statistical techniques for tuning to particular languages. ',\n",
       " ' Non-Constituent Coordination: Theory and Practice Despite the large amount of theoretical work done on non-constituent coordination during the last two decades, many computational systems still treat coordination using adapted parsing strategies, in a similar fashion to the SYSCONJ system developed for ATNs. This paper reviews the theoretical literature, and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing. Finally, it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars. ',\n",
       " ' Discourse and Deliberation: Testing a Collaborative Strategy A discourse strategy is a strategy for communicating with another agent. Designing effective dialogue systems requires designing agents that can choose among discourse strategies. We claim that the design of effective strategies must take cognitive factors into account, propose a new method for testing the hypothesized factors, and present experimental results on an effective strategy for supporting deliberation. The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics. ',\n",
       " ' Natural Language Interfaces to Databases - An Introduction This paper is an introduction to natural language interfaces to databases (NLIDBs). A brief overview of the history of NLIDBs is first given. Some advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBs have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBs. The paper ends with reflections on the current state of the art. ',\n",
       " \" Redundancy in Collaborative Dialogue In dialogues in which both agents are autonomous, each agent deliberates whether to accept or reject the contributions of the current speaker. A speaker cannot simply assume that a proposal or an assertion will be accepted. However, an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection. Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer's next dialogue contribution. In this paper, I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance. The model (1) requires a theory of mutual belief that supports mutual beliefs of various strengths; (2) explains the function of a class of informationally redundant utterances that cannot be explained by other accounts; and (3) contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption. \",\n",
       " ' Different Issues in the Design of a Lemmatizer/Tagger for Basque This paper presents relevant issues that have been considered in the design of a general purpose lemmatizer/tagger for Basque EUSLEM. The lemmatizer/tagger is conceived as a basic tool necessary for other linguistic applications. It uses the lexical data base and the morphological analyzer previously developed and implemented. Due to the characteristics of the language, the tagset here proposed in structured in for levels, so that each level is a refinement of the previous one in the sense that it adds more detailed information. We will focus on the problems found in designing this tagset and on the strategies for morphological disambiguation that will be used. ',\n",
       " ' SATZ - An Adaptive Sentence Segmentation System This paper provides a detailed description of the sentence segmentation system first introduced in cmp-lg/9411022. It provides results of systematic experiments involving sentence boundary determination, including context size, lexicon size, and single-case texts. Also included are the results of successfully adapting the system to German and French. The source code for the system is available as a compressed tar file at ftp://cs-tr.CS.Berkeley.EDU/pub/cstr/satz.tar.Z . ',\n",
       " \" A Note on the Complexity of Restricted Attribute-Value Grammars The recognition problem for attribute-value grammars (AVGs) was shown to be undecidable by Johnson in 1988. Therefore, the general form of AVGs is of no practical use. In this paper we study a very restricted form of AVG, for which the recognition problem is decidable (though still NP-complete), the R-AVG. We show that the R-AVG formalism captures all of the context free languages and more, and introduce a variation on the so-called `off-line parsability constraint', the `honest parsability constraint', which lets different types of R-AVG coincide precisely with well-known time complexity classes. \",\n",
       " ' Assessing Complexity Results in Feature Theories In this paper, we assess the complexity results of formalisms that describe the feature theories used in computational linguistics. We show that from these complexity results no immediate conclusions can be drawn about the complexity of the recognition problem of unification grammars using these feature theories. On the one hand, the complexity of feature theories does not provide an upper bound for the complexity of such unification grammars. On the other hand, the complexity of feature theories need not provide a lower bound. Therefore, we argue for formalisms that describe actual unification grammars instead of feature theories. Thus the complexity results of these formalisms judge upon the hardness of unification grammars in computational linguistics. ',\n",
       " ' A fast partial parse of natural language sentences using a connectionist method The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language. This paper describes a fully automated hybrid system, using neural nets operating within a grammatic framework. It addresses the representation of language for connectionist processing, and describes methods of constraining the problem size. The function of the network is briefly explained, and results are given. ',\n",
       " ' From compositional to systematic semantics We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous. We then show that when compositional semantics is required to be systematic (that is, the meaning function cannot be arbitrary, but must belong to some class), it is possible to distinguish between compositional and non-compositional semantics. As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic formal comparisons of different systems of grammars. ',\n",
       " ' Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction. ',\n",
       " ' Co-occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors. ',\n",
       " \" A Computational Treatment of HPSG Lexical Rules as Covariation in Lexical Entries We describe a compiler which translates a set of HPSG lexical rules and their interaction into definite relations used to constrain lexical entries. The compiler ensures automatic transfer of properties unchanged by a lexical rule. Thus an operational semantics for the full lexical rule mechanism as used in HPSG linguistics is provided. Program transformation techniques are used to advance the resulting encoding. The final output constitutes a computational counterpart of the linguistic generalizations captured by lexical rules and allows ``on the fly'' application. \",\n",
       " ' Collaborating on Referring Expressions This paper presents a computational model of how conversational participants collaborate in order to make a referring action successful. The model is based on the view of language as goal-directed behavior. We propose that the content of a referring expression can be accounted for by the planning paradigm. Not only does this approach allow the processes of building referring expressions and identifying their referents to be captured by plan construction and plan inference, it also allows us to account for how participants clarify a referring expression by using meta-actions that reason about and manipulate the plan derivation that corresponds to the referring expression. To account for how clarification goals arise and how inferred clarification plans affect the agent, we propose that the agents are in a certain state of mind, and that this state includes an intention to achieve the goal of referring and a plan that the agents are currently considering. It is this mental state that sanctions the adoption of goals and the acceptance of inferred plans, and so acts as a link between understanding and generation. ',\n",
       " ' Tagset Design and Inflected Languages An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described, using corpora in English, French and Swedish. In particular, the question of internal versus external criteria for tagset design is considered, with the general conclusion that external (linguistic) criteria should be followed. Some problems associated with tagging unknown words in inflected languages are briefly considered. ',\n",
       " ' Automatic processing proper names in texts This paper shows first the problems raised by proper names in natural language processing. Second, it introduces the knowledge representation structure we use based on conceptual graphs. Then it explains the techniques which are used to process known and unknown proper names. At last, it gives the performance of the system and the further works we intend to deal with. ',\n",
       " ' Constraint Logic Programming for Natural Language Processing This paper proposes an evaluation of the adequacy of the constraint logic programming paradigm for natural language processing. Theoretical aspects of this question have been discussed in several works. We adopt here a pragmatic point of view and our argumentation relies on concrete solutions. Using actual contraints (in the CLP sense) is neither easy nor direct. However, CLP can improve parsing techniques in several aspects such as concision, control, efficiency or direct representation of linguistic formalism. This discussion is illustrated by several examples and the presentation of an HPSG parser. ',\n",
       " ' Creating a tagset, lexicon and guesser for a French tagger We earlier described two taggers for French, a statistical one and a constraint-based one. The two taggers have the same tokeniser and morphological analyser. In this paper, we describe aspects of this work concerned with the definition of the tagset, the building of the lexicon, derived from an existing two-level morphological analyser, and the definition of a lexical transducer for guessing unknown words. ',\n",
       " ' Cues and control in Expert-Client Dialogues We conducted an empirical analysis into the relation between control and discourse structure. We applied control criteria to four dialogues and identified 3 levels of discourse structure. We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not. ',\n",
       " ' Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation Conversation between two people is usually of mixed-initiative, with control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles. ',\n",
       " ' Path optimization and near-greedy analysis for graph partitioning: an empirical study This paper presents the results of an experimental study of graph partitioning. We describe a new heuristic technique, path optimization, and its application to two variations of graph partitioning: the max_cut problem and the min_quotient_cut problem. We present the results of computational comparisons between this technique and the Kernighan-Lin algorithm, the simulated annealing algorithm, the FLOW-lagorithm the multilevel algorithm, and teh recent 0.878-approximation algorithm. The experiments were conducted on two classes of graphs that have become standard for such tests: random and random geometric. They show that for both classes of inputs and both variations of the problem, the new heuristic is competitive with the other algorithms and holds an advantage for min_quotient_cut when applied to very large, sparse geometric graphs. In the last part of the paper, we describe an approach to analyzing graph partitioning algorithms from the statistical point of view. Every partitioning of a graph is viewed as a result achieved by a near gready partitioning algorithm. The experiments show that for good partitionings, the number of non-greedy steps needed to obtain them is quite small; moreover, it is statistically smaller for better partitionings. This led us to conjecture that there exists an optimal distribution of the non-greedy steps that characterize the classes of graphs that we studied. ',\n",
       " \" Abstract Machine for Typed Feature Structures This paper describes an abstract machine for linguistic formalisms that are based on typed feature structures, such as HPSG. The core design of the abstract machine is given in detail, including the compilation process from a high-level language to the abstract machine language and the implementation of the abstract instructions. The machine's engine supports the unification of typed, possibly cyclic, feature structures. A separate module deals with control structures and instructions to accommodate parsing for phrase structure grammars. We treat the linguistic formalism as a high-level declarative programming language, applying methods that were proved useful in computer science to the study of natural languages: a grammar specified using the formalism is endowed with an operational semantics. \",\n",
       " \" MAXIMUM LIKELIHOOD AND MINIMUM ENTROPY IDENTIFICATION OF GRAMMARS Using the Thermodynamic Formalism, we introduce a Gibbsian model for the identification of regular grammars based only on positive evidence. This model mimics the natural language acquisition procedure driven by prosody which is here represented by the thermodynamical potential. The statistical question we face is how to estimate the incidenc e matrix of a subshift of finite type from a sample produced by a Gibbs state whose potential is known. The model acquaints for both the robustness of t he language acquisition procedure and language changes. The probabilistic appr oach we use avoids invoking ad-hoc restrictions as Berwick's Subset Principle. \",\n",
       " \" A Processing Model for Free Word Order Languages Like many verb-final languages, Germn displays considerable word-order freedom: there is no syntactic constraint on the ordering of the nominal arguments of a verb, as long as the verb remains in final position. This effect is referred to as ``scrambling'', and is interpreted in transformational frameworks as leftward movement of the arguments. Furthermore, arguments from an embedded clause may move out of their clause; this effect is referred to as ``long-distance scrambling''. While scrambling has recently received considerable attention in the syntactic literature, the status of long-distance scrambling has only rarely been addressed. The reason for this is the problematic status of the data: not only is long-distance scrambling highly dependent on pragmatic context, it also is strongly subject to degradation due to processing constraints. As in the case of center-embedding, it is not immediately clear whether to assume that observed unacceptability of highly complex sentences is due to grammatical restrictions, or whether we should assume that the competence grammar does not place any restrictions on scrambling (and that, therefore, all such sentences are in fact grammatical), and the unacceptability of some (or most) of the grammatically possible word orders is due to processing limitations. In this paper, we will argue for the second view by presenting a processing model for German. \",\n",
       " \" Linear Logic for Meaning Assembly Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts. Meanings are often assumed to combine via function application, which works well when constituent structure trees are used to guide semantic composition. However, we believe that the functional structure of Lexical-Functional Grammar is best used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format. It has been difficult, however, to reconcile this approach with the combination of meanings by function application. In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure. Our use of linear logic as a `glue' for assembling meanings allows for a coherent treatment of the LFG requirements of completeness and coherence as well as of modification and quantification. \",\n",
       " \" NLG vs. Templates One of the most important questions in applied NLG is what benefits (or `value-added', in business-speak) NLG technology offers over template-based approaches. Despite the importance of this question to the applied NLG community, however, it has not been discussed much in the research NLG community, which I think is a pity. In this paper, I try to summarize the issues involved and recap current thinking on this topic. My goal is not to answer this question (I don't think we know enough to be able to do so), but rather to increase the visibility of this issue in the research community, in the hope of getting some input and ideas on this very important question. I conclude with a list of specific research areas I would like to see more work in, because I think they would increase the `value-added' of NLG over templates. \",\n",
       " ' Estimating Lexical Priors for Low-Frequency Syncretic Forms Given a previously unseen form that is morphologically n-ways ambiguous, what is the best estimator for the lexical prior probabilities for the various functions of the form? We argue that the best estimator is provided by computing the relative frequencies of the various functions among the hapax legomena --- the forms that occur exactly once in a corpus. This result has important implications for the development of stochastic morphological taggers, especially when some initial hand-tagging of a corpus is required: For predicting lexical priors for very low-frequency morphologically ambiguous types (most of which would not occur in any given corpus) one should concentrate on tagging a good representative sample of the hapax legomena, rather than extensively tagging words of all frequency ranges. ',\n",
       " ' LexGram - a practical categorial grammar formalism - We present the LexGram system, an amalgam of (Lambek) categorial grammar and Head Driven Phrase Structure Grammar HPSG, and show that the grammar formalism it implements is a well-structured and useful tool for actual grammar development. ',\n",
       " ' SKOPE: A connectionist/symbolic architecture of spoken Korean processing Spoken language processing requires speech and natural language integration. Moreover, spoken Korean calls for unique processing methodology due to its linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic spoken Korean processing engine, which emphasizes that: 1) connectionist and symbolic techniques must be selectively applied according to their relative strength and weakness, and 2) the linguistic characteristics of Korean must be fully considered for phoneme recognition, speech and language integration, and morphological/syntactic processing. The design and implementation of SKOPE demonstrates how connectionist/symbolic hybrid architectures can be constructed for spoken agglutinative language processing. Also SKOPE presents many novel ideas for speech and language processing. The phoneme recognition, morphological analysis, and syntactic analysis experiments show that SKOPE is a viable approach for the spoken Korean processing. ',\n",
       " \" An Implemented Formalism for Computing Linguistic Presuppositions and Existential Commitments We rely on the strength of linguistic and philosophical perspectives in constructing a framework that offers a unified explanation for presuppositions and existential commitment. We use a rich ontology and a set of methodological principles that embed the essence of Meinong's philosophy and Grice's conversational principles into a stratified logic, under an unrestricted interpretation of the quantifiers. The result is a logical formalism that yields a tractable computational method that uniformly calculates all the presuppositions of a given utterance, including the existential ones. \",\n",
       " \" A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called ``stratified logic'', that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances. \",\n",
       " ' Memoization of Top Down Parsing This paper discusses the relationship between memoized top-down recognizers and chart parsers. It presents a version of memoization suitable for continuation-passing style programs. When applied to a simple formalization of a top-down recognizer it yields a terminating parser. ',\n",
       " \" A Formalism and an Algorithm for Computing Pragmatic Inferences and Detecting Infelicities Since Austin introduced the term ``infelicity'', the linguistic literature has been flooded with its use, but no formal or computational explanation has been given for it. This thesis provides one for those infelicities that occur when a pragmatic inference is cancelled. Our contribution assumes the existence of a finer grained taxonomy with respect to pragmatic inferences. It is shown that if one wants to account for the natural language expressiveness, one should distinguish between pragmatic inferences that are felicitous to defeat and pragmatic inferences that are infelicitously defeasible. Thus, it is shown that one should consider at least three types of information: indefeasible, felicitously defeasible, and infelicitously defeasible. The cancellation of the last of these determines the pragmatic infelicities. A new formalism has been devised to accommodate the three levels of information, called ``stratified logic''. Within it, we are able to express formally notions such as ``utterance U presupposes P'' or ``utterance U is infelicitous''. Special attention is paid to the implications that our work has in solving some well-known existential philosophical puzzles. The formalism yields an algorithm for computing interpretations for utterances, for determining their associated presuppositions, and for signalling infelicitous utterances that has been implemented in Common Lisp. The algorithm applies equally to simple and complex utterances and sequences of utterances. \",\n",
       " ' Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions We examine the problem of generating definite noun phrases that are appropriate referring expressions; i.e, noun phrases that (1) successfully identify the intended referent to the hearer whilst (2) not conveying to her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system. This paper will appear in the the April--June 1995 issue of Cognitive Science, and is made available on cmp-lg with the permission of Ablex, the publishers of that journal. ',\n",
       " ' Constraints, Exceptions and Representations This paper shows that default-based phonologies have the potential to capture morphophonological generalisations which cannot be captured by non-defaul theories. In achieving this result, I offer a characterisation of Underspecification Theory and Optimality Theory in terms of their methods for ordering defaults. The result means that machine learning techniques for building non-defualt analyses may not provide a suitable basis for morphophonological analysis. ',\n",
       " ' Phonological Derivation in Optimality Theory Optimality Theory is a constraint-based theory of phonology which allows constraints to be violated. Consequently, implementing the theory presents problems for declarative constraint-based processing frameworks. On the basis of two regularity assumptions, that candidate sets are regular and that constraints can be modelled by transducers, this paper presents and proves correct algorithms for computing the action of constraints, and hence deriving surface forms. ',\n",
       " ' Discourse Processing of Dialogues with Multiple Threads In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-to-speech translation system. We will demonstrate that theories of discourse which postulate a strict tree structure of discourse on either the intentional or attentional level are not totally adequate for handling spontaneous dialogues. We will present our extension to this approach along with its implementation in our plan-based discourse processor. We will demonstrate that the implementation of our approach outperforms an implementation based on the strict tree structure approach. ',\n",
       " ' A Morphographemic Model for Error Correction in Nonconcatenative Strings This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism. Handling of various Semitic error problems is illustrated, with reference to Arabic and Syriac examples. The model handles errors vocalisation, diacritics, phonetic syncopation and morphographemic idiosyncrasies, in addition to Damerau errors. A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined. ',\n",
       " \" Quantifiers, Anaphora, and Intensionality The relationship between Lexical-Functional Grammar LFG {\\\\em functional structures} (f-structures) for sentences and their semantic interpretations can be expressed directly in a fragment of linear logic in a way that correctly explains the constrained interactions between quantifier scope ambiguity, bound anaphora and intensionality. This deductive approach to semantic interpretaion obviates the need for additional mechanisms, such as Cooper storage, to represent the possible scopes of a quantified NP, and explains the interactions between quantified NPs, anaphora and intensional verbs such as `seek'. A single specification in linear logic of the argument requirements of intensional verbs is sufficient to derive the correct reading predictions for intensional-verb clauses both with nonquantified and with quantified direct objects. In particular, both de dicto and de re readings are derived for quantified objects. The effects of type-raising or quantifying-in rules in other frameworks here just follow as linear-logic theorems. While our approach resembles current categorial approaches in important ways, it differs from them in allowing the greater type flexibility of categorial semantics while maintaining a precise connection to syntax. As a result, we are able to provide derivations for certain readings of sentences with intensional verbs and complex direct objects that are not derivable in current purely categorial accounts of the syntax-semantics interface. \",\n",
       " \" Statistical Decision-Tree Models for Parsing Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing {}-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86\\\\% precision, 86\\\\% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91\\\\% precision, 90\\\\% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length. \",\n",
       " ' The intersection of Finite State Automata and Definite Clause Grammars Bernard Lang defines parsing as the calculation of the intersection of a FSA (the input) and a CFG. Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems, in which parsing takes a word lattice as input (rather than a word string). Furthermore, certain techniques for robust parsing can be modelled as finite state transducers. In this paper we investigate how we can generalize this approach for unification grammars. In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG. It is shown that existing parsing algorithms can be easily extended for FSA inputs. However, we also show that the termination properties change drastically: we show that it is undecidable whether the intersection of a FSA and a DCG is empty (even if the DCG is off-line parsable). Furthermore we discuss approaches to cope with the problem. ',\n",
       " ' Adaptive Load Balancing: A Study in Multi-Agent Learning We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency. ',\n",
       " ' Bayesian Grammar Induction for Language Modeling We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm. ',\n",
       " \" Pac-Learning Recursive Logic Programs: Efficient Algorithms We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional ``basecase'' oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs. \",\n",
       " \" Pac-learning Recursive Logic Programs: Negative Results In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses. \",\n",
       " ' Provably Bounded-Optimal Agents Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality ABO, that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory. ',\n",
       " ' Using Pivot Consistency to Decompose and Solve Functional CSPs Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n^2d^2) complexity (instead of O(n^3d^3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs. ',\n",
       " ' An Efficient Generation Algorithm for Lexicalist MT The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions. However, the Shake-and-Bake generation algorithm of (Whitelock, COLING-92) is NP-complete. We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism. ',\n",
       " ' New Techniques for Context Modeling We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic. Together these techniques result in language models that have few states, even fewer parameters, and low message entropies. For example, our techniques achieve a message entropy of 1.97 bits/char on the Brown corpus using only 89,325 parameters. In contrast, the character 4-gram model requires more than 250 times as many parameters in order to achieve a message entropy of only 2.47 bits/char. The fact that our model performs significantly better while using vastly fewer parameters indicates that it is a better probability model of natural language text. ',\n",
       " ' Response Generation in Collaborative Negotiation In collaborative planning activities, since the agents are autonomous and heterogeneous, it is inevitable that conflicts arise in their beliefs during the planning process. In cases where such conflicts are relevant to the task at hand, the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs. This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution. Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise, and of selecting appropriate evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions, our model can successfully handle embedded negotiation subdialogues. ',\n",
       " ' Compiling HPSG type constraints into definite clause programs We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs. This provides a clear and computationally useful correspondence between linguistic theories and their implementation. The compiler performs off-line constraint inheritance and code optimization. As a result, we are able to efficiently process with HPSG grammars without having to hand-translate them into definite clause or phrase structure based systems. ',\n",
       " \" DATR Theories and DATR Models Evans and Gazdar introduced DATR as a simple, non-monotonic language for representing natural language lexicons. Although a number of implementations of DATR exist, the full language has until now lacked an explicit, declarative semantics. This paper rectifies the situation by providing a mathematical semantics for DATR. We present a view of DATR as a language for defining certain kinds of partial functions by cases. The formal model provides a transparent treatment of DATR's notion of global context. It is shown that DATR's default mechanism can be accounted for by interpreting value descriptors as families of values indexed by paths. \",\n",
       " ' Treating Coordination with Datalog Grammars In previous work we studied a new type of DCGs, Datalog grammars, which are inspired on database theory. Their efficiency was shown to be better than that of their DCG counterparts under (terminating) OLDT-resolution. In this article we motivate a variant of Datalog grammars which allows us a meta-grammatical treatment of coordination. This treatment improves in some respects over previous work on coordination in logic grammars, although more research is needed for testing it in other respects. ',\n",
       " ' Compilation of HPSG to TAG We present an implemented compilation algorithm that translates HPSG into lexicalized feature-based TAG, relating concepts of the two theories. While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly. Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon, and identify maximal projections, auxiliary trees and foot nodes. ',\n",
       " ' Conciseness through Aggregation in Text Generation Aggregating different pieces of similar information is necessary to generate concise and easy to understand reports in technical domains. This paper presents a general algorithm that combines similar messages in order to generate one or more coherent sentences for them. The process is not as trivial as might be expected. Problems encountered are briefly described. ',\n",
       " ' Parsing a Flexible Word Order Language A logic formalism is presented which increases the expressive power of the ID/LP format of GPSG by enlarging the inventory of ordering relations and extending the domain of their application to non-siblings. This allows a concise, modular and declarative statement of intricate word order regularities. ',\n",
       " ' Tagset Reduction Without Information Loss A technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment. The technique ensures that all information that is provided by the original tagset can be restored from the reduced one. This is crucial, since we are interested in the linguistically motivated tags for part-of-speech disambiguation. The reduced tagset needs fewer parameters for its statistical model and allows more accurate parameter estimation. Additionally, there is a slight but not significant improvement of tagging accuracy. ',\n",
       " ' A Symbolic and Surgical Acquisition of Terms through Variation Terminological acquisition is an important issue in learning for NLP due to the constant terminological renewal through technological changes. Terms play a key role in several NLP-activities such as machine translation, automatic indexing or text understanding. In opposition to classical once-and-for-all approaches, we propose an incremental process for terminological enrichment which operates on existing reference lists and large corpora. Candidate terms are acquired by extracting variants of reference terms through {\\\\em FASTR}, a unification-based partial parser. As acquisition is performed within specific morpho-syntactic contexts (coordinations, insertions or permutations of compounds), rich conceptual links are learned together with candidate terms. A clustering of terms related through coordination yields classes of conceptually close terms while graphs resulting from insertions denote generic/specific relations. A graceful degradation of the volume of acquisition on partial initial lists confirms the robustness of the method to incomplete data. ',\n",
       " ' Evaluation of Semantic Clusters Semantic clusters of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation. Several attempts have been made to extract the semantic clusters of a domain by probabilistic or taxonomic techniques. However, not much progress has been made in evaluating the obtained semantic clusters. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts. ',\n",
       " ' Learning Syntactic Rules and Tags with Genetic Algorithms for Information Retrieval and Filtering: An Empirical Basis for Grammatical Rules The grammars of natural languages may be learned by using genetic algorithms that reproduce and mutate grammatical rules and part-of-speech tags, improving the quality of later generations of grammatical components. Syntactic rules are randomly generated and then evolve; those rules resulting in improved parsing and occasionally improved retrieval and filtering performance are allowed to further propagate. The LUST system learns the characteristics of the language or sublanguage used in document abstracts by learning from the document rankings obtained from the parsed abstracts. Unlike the application of traditional linguistic rules to retrieval and filtering applications, LUST develops grammatical structures and tags without the prior imposition of some common grammatical assumptions (e.g., part-of-speech assumptions), producing grammars that are empirically based and are optimized for this particular application. ',\n",
       " ' Compositionality for Presuppositions over Tableaux Tableaux originate as a decision method for a logical language. They can also be extended to obtain a structure that spells out all the information in a set of sentences in terms of truth value assignments to atomic formulas that appear in them. This approach is pursued here. Over such a structure, compositional rules are provided for obtaining the presuppositions of a logical statement from its atomic subformulas and their presuppositions. The rules are based on classical logic semantics and they are shown to model the behaviour of presuppositions observed in natural language sentences built with {\\\\em if \\\\ldots then}, {\\\\em and} and {\\\\em or}. The advantages of this method over existing frameworks for presuppositions are discussed. ',\n",
       " ' Efficient Analysis of Complex Diagrams using Constraint-Based Parsing This paper describes substantial advances in the analysis (parsing) of diagrams using constraint grammars. The addition of set types to the grammar and spatial indexing of the data make it possible to efficiently parse real diagrams of substantial complexity. The system is probably the first to demonstrate efficient diagram parsing using grammars that easily be retargeted to other domains. The work assumes that the diagrams are available as a flat collection of graphics primitives: lines, polygons, circles, Bezier curves and text. This is appropriate for future electronic documents or for vectorized diagrams converted from scanned images. The classes of diagrams that we have analyzed include x,y data graphs and genetic diagrams drawn from the biological literature, as well as finite state automata diagrams (states and arcs). As an example, parsing a four-part data graph composed of 133 primitives required 35 sec using Macintosh Common Lisp on a Macintosh Quadra 700. ',\n",
       " ' Utilizing Statistical Dialogue Act Processing in Verbmobil In this paper, we present a statistical approach for dialogue act processing in the dialogue component of the speech-to-speech translation system \\\\vm. Statistics in dialogue processing is used to predict follow-up dialogue acts. As an application example we show how it supports repair when unexpected dialogue states occur. ',\n",
       " ' A Pattern Matching method for finding Noun and Proper Noun Translations from Noisy Parallel Corpora We present a pattern matching method for compiling a bilingual lexicon of nouns and proper nouns from unaligned, noisy parallel texts of Asian/Indo-European language pairs. Tagging information of one language is used. Word frequency and position information for high and low frequency words are represented in two different vector forms for pattern matching. New anchor point finding and noise elimination techniques are introduced. We obtained a 73.1\\\\% precision. We also show how the results can be used in the compilation of domain-specific noun phrases. ',\n",
       " ' Acquiring a Lexicon from Unsegmented Speech We present work-in-progress on the machine acquisition of a lexicon from sentences that are each an unsegmented phone sequence paired with a primitive representation of meaning. A simple exploratory algorithm is described, along with the direction of current work and a discussion of the relevance of the problem for child language acquisition and computer speech recognition. ',\n",
       " ' Measuring semantic complexity We define {\\\\em semantic complexity} using a new concept of {\\\\em meaning automata}. We measure the semantic complexity of understanding of prepositional phrases, of an in depth understanding system , and of a natural language interface to an on-line calendar. We argue that it is possible to measure some semantic complexities of natural language processing systems before building them, and that systems that exhibit relatively complex behavior can be built from semantically simple components. ',\n",
       " ' Robust Parsing of Spoken Dialogue Using Contextual Knowledge and Recognition Probabilities In this paper we describe the linguistic processor of a spoken dialogue system. The parser receives a word graph from the recognition module as its input. Its task is to find the best path through the graph. If no complete solution can be found, a robust mechanism for selecting multiple partial results is applied. We show how the information content rate of the results can be improved if the selection is based on an integrated quality score combining word recognition scores and context-dependent semantic predictions. Results of parsing word graphs with and without predictions are reported. ',\n",
       " ' The Replace Operator This paper introduces to the calculus of regular expressions a replace operator, ->, and defines a set of replacement expressions that concisely encode several alternate variations of the operation. The basic case is unconditional obligatory replacement: UPPER -> LOWER Conditional versions of replacement, such as, UPPER -> LOWER || LEFT _ RIGHT constrain the operation by left and right contexts. UPPER, LOWER, LEFT, and RIGHT may be regular expressions of any complexity. Replace expressions denote regular relations. The replace operator is defined in terms of other regular expression operators using techniques introduced by Ronald M. Kaplan and Martin Kay in Regular Models of Phonological Rule Systems (Computational Linguistics 20:3 331-378. 1994). ',\n",
       " ' CRYSTAL: Inducing a Conceptual Dictionary One of the central knowledge sources of an information extraction system is a dictionary of linguistic patterns that can be used to identify the conceptual content of a text. This paper describes CRYSTAL, a system which automatically induces a dictionary of concept-node definitions sufficient to identify relevant information from a training corpus. Each of these concept-node definitions is generalized as far as possible without producing errors, so that a minimum number of dictionary entries cover the positive training instances. Because it tests the accuracy of each proposed definition, CRYSTAL can often surpass human intuitions in creating reliable extraction rules. ',\n",
       " ' Generating One-Anaphoric Expressions: Where Does the Decision Lie? Most natural language generation systems embody mechanisms for choosing whether to subsequently refer to an already-introduced entity by means of a pronoun or a definite noun phrase. Relatively few systems, however, consider referring to entites by means of one-anaphoric expressions such as \\\\lingform{the small green one}. This paper looks at what is involved in generating referring expressions of this type. Consideration of how to fit this capability into a standard algorithm for referring expression generation leads us to suggest a revision of some of the assumptions that underlie existing approaches. We demonstrate the usefulness of our approach to one-anaphora generation in the context of a simple database interface application, and make some observations about the impact of this approach on referring expression generation more generally. ',\n",
       " ' Improving the Efficiency of a Generation Algorithm for Shake and Bake Machine Translation Using Head-Driven Phrase Structure Grammar A Shake and Bake machine translation algorithm for Head-Driven Phrase Structure Grammar is introduced based on the algorithm proposed by Whitelock for unification categorial grammar. The translation process is then analysed to determine where the potential sources of inefficiency reside, and some proposals are introduced which greatly improve the efficiency of the generation algorithm. Preliminary empirical results from tests involving a small grammar are presented, and suggestions for greater improvement to the algorithm are provided. ',\n",
       " ' Combining Multiple Knowledge Sources for Discourse Segmentation We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data. We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). ',\n",
       " ' Exploring the role of Punctuation in Parsing Natural Text Few, if any, current NLP systems make any significant use of punctuation. Intuitively, a treatment of punctuation seems necessary to the analysis and production of text. Whilst this has been suggested in the fields of discourse structure, it is still unclear whether punctuation can help in the syntactic field. This investigation attempts to answer this question by parsing some corpus-based material with two similar grammars --- one including rules for punctuation, the other ignoring it. The punctuated grammar significantly out-performs the unpunctuated one, and so the conclusion is that punctuation can play a useful role in syntactic processing. ',\n",
       " \" Some Novel Applications of Explanation-Based Learning to Parsing Lexicalized Tree-Adjoining Grammars In this paper we present some novel applications of Explanation-Based Learning EBL technique to parsing Lexicalized Tree-Adjoining grammars. The novel aspects are (a) immediate generalization of parses in the training set, (b) generalization over recursive structures and (c) representation of generalized parses as Finite State Transducers. A highly impoverished parser called a ``stapler'' has also been introduced. We present experimental results using EBL for different corpora and architectures to show the effectiveness of our approach. \",\n",
       " ' Quantifier Scope and Constituency Traditional approaches to quantifier scope typically need stipulation to exclude readings that are unavailable to human understanders. This paper shows that quantifier scope phenomena can be precisely characterized by a semantic representation constrained by surface constituency, if the distinction between referential and quantificational NPs is properly observed. A CCG implementation is described and compared to other approaches. ',\n",
       " ' Tagging the Teleman Corpus Experiments were carried out comparing the Swedish Teleman and the English Susanne corpora using an HMM-based and a novel reductionistic statistical part-of-speech tagger. They indicate that tagging the Teleman corpus is the more difficult task, and that the performance of the two different taggers is comparable. ',\n",
       " ' D-Tree Grammars DTG are designed to share some of the advantages of TAG while overcoming some of its limitations. DTG involve two composition operations called subsertion and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a uniform analysis for em wh-movement in English and Kashmiri, despite the fact that the em wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English. ',\n",
       " ' Mapping Scrambled Korean Sentences into English Using Synchronous TAGs Synchronous Tree Adjoining Grammars can be used for Machine Translation. However, translating a free order language such as Korean to English is complicated. I present a mechanism to translate scrambled Korean sentences into English by combining the concepts of Multi-Component TAGs (MC-TAGs) and Synchronous TAGs (STAGs). ',\n",
       " ' Context and ontology in understanding of dialogs We present a model of NLP in which ontology and context are directly included in a grammar. The model is based on the concept of {\\\\em construction}, consisting of a set of features of form, a set of semantic and pragmatic conditions describing its application context, and a description of its meaning. In this model ontology is embedded into the grammar; e.g. the hierarchy of {\\\\it np} constructions is based on the corresponding ontology. Ontology is also used in defining contextual parameters; e.g. . A parser based on this model allowed us to build a set of dialog understanding systems that include an on-line calendar, a banking machine, and an insurance quote system. The proposed approach is an alternative to the standard pipeline design of morphology-syntax-semantics-pragmatics; the account of meaning conforms to our intuitions about compositionality, but there is no homomorphism from syntax to semantics. ',\n",
       " ' Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic Inheritance Hierarchy This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules. A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures. Such an approach eliminates the considerable redundancy otherwise associated with an LTAG lexicon. ',\n",
       " \" Memoization of Coroutined Constraints Some linguistic constraints cannot be effectively resolved during parsing at the location in which they are most naturally introduced. This paper shows how constraints can be propagated in a memoizing parser (such as a chart parser) in much the same way that variable bindings are, providing a general treatment of constraint coroutining in memoization. Prolog code for a simple application of our technique to Bouma and van Noord's (1994) categorial grammar analysis of Dutch is provided. \",\n",
       " ' The Compactness of Construction Grammars We present an argument for {\\\\em construction grammars} based on the minimum description length MDL principle (a formal version of the Ockham Razor). The argument consists in using linguistic and computational evidence in setting up a formal model, and then applying the MDL principle to prove its superiority with respect to alternative models. We show that construction-based representations are at least an order of magnitude more compact that the corresponding lexicalized representations of the same linguistic data. The result is significant for our understanding of the relationship between syntax and semantics, and consequently for choosing NLP architectures. For instance, whether the processing should proceed in a pipeline from syntax to semantics to pragmatics, and whether all linguistic information should be combined in a set of constraints. From a broader perspective, this paper does not only argue for a certain model of processing, but also provides a methodology for determining advantages of different approaches to NLP. ',\n",
       " \" Semantic Ambiguity and Perceived Ambiguity I explore some of the issues that arise when trying to establish a connection between the underspecification hypothesis pursued in the NLP literature and work on ambiguity in semantics and in the psychological literature. A theory of underspecification is developed `from the first principles', i.e., starting from a definition of what it means for a sentence to be semantically ambiguous and from what we know about the way humans deal with ambiguity. An underspecified language is specified as the translation language of a grammar covering sentences that display three classes of semantic ambiguity: lexical ambiguity, scopal ambiguity, and referential ambiguity. The expressions of this language denote sets of senses. A formalization of defeasible reasoning with underspecified representations is presented, based on Default Logic. Some issues to be confronted by such a formalization are discussed. \",\n",
       " ' User-Defined Nonmonotonicity in Unification-Based Formalisms A common feature of recent unification-based grammar formalisms is that they give the user the ability to define his own structures. However, this possibility is mostly limited and does not include nonmonotonic operations. In this paper we show how nonmonotonic operations can also be user-defined by applying default logic (Reiter 1980) and generalizing previous results on nonmonotonic sorts (Young & Rounds 1993). ',\n",
       " ' Development of a Spanish Version of the Xerox Tagger This paper describes work performed withing the CRATER ({\\\\em C}orpus {\\\\em R}esources {\\\\em A}nd {\\\\em T}erminology {\\\\em E}xt{\\\\em R}action, MLAP-93/20) project, funded by the Commission of the European Communities. In particular, it addresses the issue of adapting the Xerox Tagger to Spanish in order to tag the Spanish version of the ITU (International Telecommunications Union) corpus. The model implemented by this tagger is briefly presented along with some modifications performed on it in order to use some parameters not probabilistically estimated. Initial decisions, like the tagset, the lexicon and the training corpus are also discussed. Finally, results are presented and the benefits of the {\\\\em mixed model} justified. ',\n",
       " \" Integrating Gricean and Attentional Constraints This paper concerns how to generate and understand discourse anaphoric noun phrases. I present the results of an analysis of all discourse anaphoric noun phrases (N=1,233) in a corpus of ten narrative monologues, where the choice between a definite pronoun or phrasal NP conforms largely to Gricean constraints on informativeness. I discuss Dale and Reiter's [To appear] recent model and show how it can be augmented for understanding as well as generating the range of data presented here. I argue that integrating centering [Grosz et al., 1983] [Kameyama, 1985] with this model can be applied uniformly to discourse anaphoric pronouns and phrasal NPs. I conclude with a hypothesis for addressing the interaction between local and global discourse processing. \",\n",
       " ' Identifying Word Translations in Non-Parallel Texts Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages. ',\n",
       " ' On Descriptive Complexity, Language Complexity, and GB We introduce , a monadic second-order language for reasoning about trees which characterizes the strongly Context-Free Languages in the sense that a set of finite trees is definable in iff it is (modulo a projection) a Local Set---the set of derivation trees generated by a CFG. This provides a flexible approach to establishing language-theoretic complexity results for formalisms that are based on systems of well-formedness constraints on trees. We demonstrate this technique by sketching two such results for Government and Binding Theory. First, we show that {\\\\em free-indexation\\\\/}, the mechanism assumed to mediate a variety of agreement and binding relationships in GB, is not definable in and therefore not enforcible by CFGs. Second, we show how, in spite of this limitation, a reasonably complete GB account of English can be defined in . Consequently, the language licensed by that account is strongly context-free. We illustrate some of the issues involved in establishing this result by looking at the definition, in , of chains. The limitations of this definition provide some insight into the types of natural linguistic principles that correspond to higher levels of language complexity. We close with some speculation on the possible significance of these results for generative linguistics. ',\n",
       " \" Text Chunking using Transformation-Based Learning Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application. \",\n",
       " ' Ubiquitous Talker: Spoken Language Interaction with Real World Objects Augmented reality is a research area that tries to embody an electronic information space within the real world, through computational devices. A crucial issue within this area, is the recognition of real world objects or situations. In natural language processing, it is much easier to determine interpretations of utterances, even if they are ill-formed, when the context or situation is fixed. We therefore introduce robust, natural language processing into a system of augmented reality with situation awareness. Based on this idea, we have developed a portable system, called the Ubiquitous Talker. This consists of an LCD display that reflects the scene at which a user is looking as if it is a transparent glass, a CCD camera for recognizing real world objects with color-bar ID codes, a microphone for recognizing a human voice and a speaker which outputs a synthesized voice. The Ubiquitous Talker provides its user with some information related to a recognized object, by using the display and voice. It also accepts requests or questions as voice inputs. The user feels as if he/she is talking with the object itself through the system. ',\n",
       " ' Robust Parsing Based on Discourse Information: Completing partial parses of ill-formed sentences on the basis of discourse information In a consistent text, many words and phrases are repeatedly used in more than one sentence. When an identical phrase (a set of consecutive words) is repeated in different sentences, the constituent words of those sentences tend to be associated in identical modification patterns with identical parts of speech and identical modifiee-modifier relationships. Thus, when a syntactic parser cannot parse a sentence as a unified structure, parts of speech and modifiee-modifier relationships among morphologically identical words in complete parses of other sentences within the same text provide useful information for obtaining partial parses of the sentence. In this paper, we describe a method for completing partial parses by maintaining consistency among morphologically identical words within the same text as regards their part of speech and their modifiee-modifier relationship. The experimental results obtained by using this method with technical documents offer good prospects for improving the accuracy of sentence analysis in a broad-coverage natural language processing system such as a machine translation system. ',\n",
       " ' Using Decision Trees for Coreference Resolution This paper describes RESOLVE, a system that uses decision trees to learn how to classify coreferent phrases in the domain of business joint ventures. An experiment is presented in which the performance of RESOLVE is compared to the performance of a manually engineered set of rules for the same task. The results show that decision trees achieve higher performance than the rules in two of three evaluation metrics developed for the coreference task. In addition to achieving better performance than the rules, RESOLVE provides a framework that facilitates the exploration of the types of knowledge that are useful for solving the coreference problem. ',\n",
       " ' Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best Translation Lexicons This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora. ',\n",
       " ' Hybrid Transfer in an English-French Spoken Language Translator The paper argues the importance of high-quality translation for spoken language translation systems. It describes an architecture suitable for rapid development of high-quality limited-domain translation systems, which has been implemented within an advanced prototype English to French spoken language translator. The focus of the paper is the hybrid transfer model which combines unification-based rules and a set of trainable statistical preferences; roughly, rules encode domain-independent grammatical information and preferences encode domain-dependent distributional information. The preferences are trained from sets of examples produced by the system, which have been annotated by human judges as correct or incorrect. An experiment is described in which the model was tested on a 2000 utterance sample of previously unseen data. ',\n",
       " ' TAKTAG: Two-phase learning method for hybrid statistical/rule-based part-of-speech disambiguation Both statistical and rule-based approaches to part-of-speech POS disambiguation have their own advantages and limitations. Especially for Korean, the narrow windows provided by hidden markov model HMM cannot cover the necessary lexical and long-distance dependencies for POS disambiguation. On the other hand, the rule-based approaches are not accurate and flexible to new tag-sets and languages. In this regard, the statistical/rule-based hybrid method that can take advantages of both approaches is called for the robust and flexible POS disambiguation. We present one of such method, that is, a two-phase learning architecture for the hybrid statistical/rule-based POS disambiguation, especially for Korean. In this method, the statistical learning of morphological tagging is error-corrected by the rule-based learning of Brill [1992] style tagger. We also design the hierarchical and flexible Korean tag-set to cope with the multiple tagging applications, each of which requires different tag-set. Our experiments show that the two-phase learning method can overcome the undesirable features of solely HMM-based or solely rule-based tagging, especially for morphologically complex Korean. ',\n",
       " ' FLECS: Planning with a Flexible Commitment Strategy There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the best possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies. ',\n",
       " ' Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic). ',\n",
       " ' Ma(r)king concessions in English and German In order to generate cohesive discourse, many of the relations holding between text segments need to be signalled to the reader by means of cue words, or {\\\\em discourse markers}. Programs usually do this in a simplistic way, e.g., by using one marker per relation. In reality, however, language offers a very wide range of markers from which informed choices should be made. In order to account for the variety and to identify the parameters governing the choices, detailled linguistic analyses are necessary. We worked with one area of discourse relations, the Concession family, identified its underlying pragmatics and semantics, and undertook extensive corpus studies to examine the range of markers used in both English and German. On the basis of an initial classification of these markers, we propose a generation model for producing bilingual text that can incorporate marker choice into its overall decision framework. ',\n",
       " ' Syllable parsing in English and French In this paper I argue that Optimality Theory provides for an explanatory model of syllabic parsing in English and French. The argument is based on psycholinguistic facts that have been mysterious up to now. This argument is further buttressed by the computational implementation developed here. This model is important for several reasons. First, it provides a demonstration of how OT can be used in a performance domain. Second, it suggests a new relationship between phonological theory and psycholinguistics. (Code in Perl is included and a WWW-interface is running at http://mayo.douglass.arizona.edu.) ',\n",
       " \" Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs Many theories of semantic interpretation use lambda-term manipulation to compositionally compute the meaning of a sentence. These theories are usually implemented in a language such as Prolog that can simulate lambda-term operations with first-order unification. However, for some interesting cases, such as a Combinatory Categorial Grammar account of coordination constructs, this can only be done by obscuring the underlying linguistic theory with the ``tricks'' needed for implementation. This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar, including its handling of coordination constructs. \",\n",
       " ' Automatic Extraction of Tagset Mappings from Parallel-Annotated Corpora This paper describes some of the recent work of project AMALGAM (automatic mapping among lexico-grammatical annotation models). We are investigating ways to map between the leading corpus annotation schemes in order to improve their resuability. Collation of all the included corpora into a single large annotated corpus will provide a more detailed language model to be developed for tasks such as speech and handwriting recognition. In particular, we focus here on a method of extracting mappings from corpora that have been annotated according to more than one annotation scheme. ',\n",
       " \" Features and Agreement This paper compares the consistency-based account of agreement phenomena in `unification-based' grammars with an implication-based account based on a simple feature extension to Lambek Categorial Grammar LCG. We show that the LCG treatment accounts for constructions that have been recognized as problematic for `unification-based' treatments. \",\n",
       " ' CLiFF Notes: Research in the Language, Information and Computation Laboratory of the University of Pennsylvania Short abstracts by computational linguistics researchers at the University of Pennsylvania describing ongoing individual and joint projects. ',\n",
       " ' Filling Knowledge Gaps in a Broad-Coverage Machine Translation System Knowledge-based machine translation KBMT techniques yield high quality in domains with detailed semantic models, limited vocabulary, and controlled input grammar. Scaling up along these dimensions means acquiring large knowledge resources. It also means behaving reasonably when definitive knowledge is not yet available. This paper describes how we can fill various KBMT knowledge gaps, often using robust statistical techniques. We describe quantitative and qualitative results from JAPANGLOSS, a broad-coverage Japanese-English MT system. ',\n",
       " ' Two-level, Many-Paths Generation Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable. ',\n",
       " ' Unification-Based Glossing We present an approach to syntax-based machine translation that combines unification-style interpretation with statistical processing. This approach enables us to translate any Japanese newspaper article into English, with quality far better than a word-for-word translation. Novel ideas include the use of feature structures to encode word lattices and the use of unification to compose and manipulate lattices. Unification also allows us to specify abstract features that delay target-language synthesis until enough source-language information is assembled. Our statistical component enables us to search efficiently among competing translations and locate those with high English fluency. ',\n",
       " \" Presenting Punctuation Until recently, punctuation has received very little attention in the linguistics and computational linguistics literature. Since the publication of Nunberg's (1990) monograph on the topic, however, punctuation has seen its stock begin to rise: spurred in part by Nunberg's ground-breaking work, a number of valuable inquiries have been subsequently undertaken, including Hovy and Arens (1991), Dale (1991), Pascual (1993), Jones (1994), and Briscoe (1994). Continuing this line of research, I investigate in this paper how Nunberg's approach to presenting punctuation (and other formatting devices) might be incorporated into NLG systems. Insofar as the present paper focuses on the proper syntactic treatment of punctuation, it differs from these other subsequent works in that it is the first to examine this issue from the generation perspective. \",\n",
       " ' A Study of the Context(s) in a Specific Type of Texts: Car Accident Reports This paper addresses the issue of defining context, and more specifically the different contexts needed for understanding a particular type of texts. The corpus chosen is homogeneous and allows us to determine characteristic properties of the texts from which certain inferences can be drawn by the reader. These characteristic properties come from the real world domain (K-context), the type of events the texts describe (F-context) and the genre of the texts (E-context). Together, these three contexts provide elements for the resolution of anaphoric expressions and for several types of disambiguation. We show in particular that the argumentation aspect of these texts is an essential part of the context and explains some of the inferences that can be drawn. ',\n",
       " ' Inducing Features of Random Fields We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The statistical modeling techniques introduced in this paper differ from those common to much of the natural language processing literature since there is no probabilistic finite state or push-down automaton on which the model is built. Our approach also differs from the techniques common to the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches including decision trees and Boltzmann machines are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing. Key words: random field, Kullback-Leibler divergence, iterative scaling, divergence geometry, maximum entropy, EM algorithm, statistical learning, clustering, word morphology, natural language processing ',\n",
       " ' Indefeasible Semantics and Defeasible Pragmatics An account of utterance interpretation in discourse needs to face the issue of how the discourse context controls the space of interacting preferences. Assuming a discourse processing architecture that distinguishes the grammar and pragmatics subsystems in terms of monotonic and nonmonotonic inferences, I will discuss how independently motivated default preferences interact in the interpretation of intersentential pronominal anaphora. In the framework of a general discourse processing model that integrates both the grammar and pragmatics subsystems, I will propose a fine structure of the preferential interpretation in pragmatics in terms of defeasible rule interactions. The pronoun interpretation preferences that serve as the empirical ground draw from the survey data specifically obtained for the present purpose. ',\n",
       " \" The Effect of Pitch Accenting on Pronoun Referent Resolution By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features. \",\n",
       " ' Ambiguity in the Acquisition of Lexical Information This paper describes an approach to the automatic identification of lexical information in on-line dictionaries. This approach uses bootstrapping techniques, specifically so that ambiguity in the dictionary text can be treated properly. This approach consists of processing an on-line dictionary multiple times, each time refining the lexical information previously acquired and identifying new lexical information. The strength of this approach is that lexical information can be acquired from definitions which are syntactically ambiguous, given that information acquired during the first pass can be used to improve the syntactic analysis of definitions in subsequent passes. In the context of a lexical knowledge base, the types of lexical information that need to be represented cannot be viewed as a fixed set, but rather as a set that will change given the resources of the lexical knowledge base and the requirements of analysis systems which access it. ',\n",
       " \" Intelligent Voice Prosthesis: Converting Icons into Natural Language Sentences The Intelligent Voice Prosthesis is a communication tool which reconstructs the meaning of an ill-structured sequence of icons or symbols, and expresses this meaning into sentences of a Natural Language (French). It has been developed for the use of people who cannot express themselves orally in natural language, and further, who are not able to comply to grammatical rules such as those of natural language. We describe how available corpora of iconic communication by children with Cerebral Palsy has led us to implement a simple and relevant semantic description of the symbol lexicon. We then show how a unification-based, bottom-up semantic analysis allows the system to uncover the meaning of the user's utterances by computing proper dependencies between the symbols. The result of the analysis is then passed to a lexicalization module which chooses the right words of natural language to use, and builds a linguistic semantic network. This semantic network is then generated into French sentences via hierarchization into trees, using a lexicalized Tree Adjoining Grammar. Finally we describe the modular, customizable interface which has been developed for this system. \",\n",
       " \" Review of Charniak's Statistical Language Learning This article is an in-depth review of Eugene Charniak's book, Statistical Language Learning . The review evaluates the appropriateness of the book as an introductory text for statistical language learning for a variety of audiences. It also includes an extensive bibliography of articles and papers which might be used as a supplement to this book for learning or teaching statistical language modeling. \",\n",
       " \" GLR-Parsing of Word Lattices Using a Beam Search Method This paper presents an approach that allows the efficient integration of speech recognition and language understanding using Tomita's generalized LR-parsing algorithm. For this purpose the GLRP-algorithm is revised so that an agenda mechanism can be used to control the flow of computation of the parsing process. This new approach is used to integrate speech recognition and speech understanding incrementally with a beam search method. These considerations have been implemented and tested on ten word lattices. \",\n",
       " ' Prepositional Phrase Attachment through a Backed-Off Model Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form {v np1 p np2} are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events - ignoring events which occur less than 5 times in training data reduces performance to 81.6%. ',\n",
       " ' Deriving Procedural and Warning Instructions from Device and Environment Models This study is centred on the generation of instructions for household appliances. We show how knowledge about a device, together with knowledge about the environment, can be used for reasoning about instructions. The information communicated by the instructions can be planned from a version of the knowledge of the artifact and environment. We present the latter, which we call the {\\\\it planning knowledge}, in the form of axioms in the {\\\\it situation calculus}. This planning knowledge formally characterizes the behaviour of the artifact, and it is used to produce a basic plan of actions that the device and user take to accomplish a given goal. We explain how both procedural and warning instructions can be generated from this basic plan. In order to partially justify that instruction generation can be automated from a formal device design specification, we assume that the planning knowledge is {\\\\it derivable\\\\/} from the device and world knowledge. ',\n",
       " \" Empirical Discovery in Linguistics A discovery system for detecting correspondences in data is described, based on the familiar induction methods of J. S. Mill. Given a set of observations, the system induces the ``causally'' related facts in these observations. Its application to empirical linguistic discovery is described. \",\n",
       " ' A Categorial Framework for Composition in Multiple Linguistic Domains This paper describes a computational framework for a grammar architecture in which different linguistic domains such as morphology, syntax, and semantics are treated not as separate components but compositional domains. Word and phrase formation are modeled as uniform processes contributing to the derivation of the semantic form. The morpheme, as well as the lexeme, has lexical representation in the form of semantic content, tactical constraints, and phonological realization. The motivation for this work is to handle morphology-syntax interaction (e.g., valency change in causatives, subcategorization imposed by case-marking affixes) in an incremental way. The model is based on Combinatory Categorial Grammars. ',\n",
       " ' An Approach to Proper Name Tagging for German This paper presents an incremental method for the tagging of proper names in German newspaper texts. The tagging is performed by the analysis of the syntactic and textual contexts of proper names together with a morphological analysis. The proper names selected by this process supply new contexts which can be used for finding new proper names, and so on. This procedure was applied to a small German corpus (50,000 words) and correctly disambiguated 65% of the capitalized words, which should improve when it is applied to a very large corpus. ',\n",
       " ' Building and Refining Abstract Planning Cases by Change of Representation Language ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning. ',\n",
       " \" A Computational Approach to Aspectual Composition In this paper, I argue, contrary to the prevailing opinion in the linguistics and philosophy literature, that a sortal approach to aspectual composition can indeed be explanatory. In support of this view, I develop a synthesis of competing proposals by Hinrichs, Krifka and Jackendoff which takes Jackendoff's cross-cutting sortal distinctions as its point of departure. To show that the account is well-suited for computational purposes, I also sketch an implemented calculus of eventualities which yields many of the desired inferences. Further details on both the model-theoretic semantics and the implementation can be found in (White, 1994). \",\n",
       " ' Constraint Categorial Grammars Although unification can be used to implement a weak form of reduction, several linguistic phenomena are better handled by using some form of calculus. In this paper we present a higher order feature description calculus based on a typed calculus. We show how the techniques used in \\\\CLG for resolving complex feature constraints can be efficiently extended. \\\\CCLG is a simple formalism, based on categorial grammars, designed to test the practical feasibility of such a calculus. ',\n",
       " ' A framework for lexical representation In this paper we present a unification-based lexical platform designed for highly inflected languages (like Roman ones). A formalism is proposed for encoding a lemma-based lexical source, well suited for linguistic generalizations. From this source, we automatically generate an allomorph indexed dictionary, adequate for efficient processing. A set of software tools have been implemented around this formalism: access libraries, morphological processors, etc. ',\n",
       " \" Robust Processing of Natural Language Previous approaches to robustness in natural language processing usually treat deviant input by relaxing grammatical constraints whenever a successful analysis cannot be provided by ``normal'' means. This schema implies, that error detection always comes prior to error handling, a behaviour which hardly can compete with its human model, where many erroneous situations are treated without even noticing them. The paper analyses the necessary preconditions for achieving a higher degree of robustness in natural language processing and suggests a quite different approach based on a procedure for structural disambiguation. It not only offers the possibility to cope with robustness issues in a more natural way but eventually might be suited to accommodate quite different aspects of robust behaviour within a single framework. \",\n",
       " ' GRAMPAL: A Morphological Processor for Spanish implemented in Prolog A model for the full treatment of Spanish inflection for verbs, nouns and adjectives is presented. This model is based on feature unification and it relies upon a lexicon of allomorphs both for stems and morphemes. Word forms are built by the concatenation of allomorphs by means of special contextual features. We make use of standard Definite Clause Grammars DCG included in most Prolog implementations, instead of the typical finite-state approach. This allows us to take advantage of the declarativity and bidirectionality of Logic Programming for NLP. The most salient feature of this approach is simplicity: A really straightforward rule and lexical components. We have developed a very simple model for complex phenomena. Declarativity, bidirectionality, consistency and completeness of the model are discussed: all and only correct word forms are analysed or generated, even alternative ones and gaps in paradigms are preserved. A Prolog implementation has been developed for both analysis and generation of Spanish word forms. It consists of only six DCG rules, because our {\\\\em lexicalist\\\\/} approach --i.e. most information is in the dictionary. Although it is quite efficient, the current implementation could be improved for analysis by using the non logical features of Prolog, especially in word segmentation and dictionary access. ',\n",
       " ' Comparative Ellipsis and Variable Binding In this paper, we discuss the question whether phrasal comparatives should be given a direct interpretation, or require an analysis as elliptic constructions, and answer it with Yes and No. The most adequate analysis of wide reading attributive WRA comparatives seems to be as cases of ellipsis, while a direct (but asymmetric) analysis fits the data for narrow scope attributive comparatives. The question whether it is a syntactic or a semantic process which provides the missing linguistic material in the complement of WRA comparatives is also given a complex answer: Linguistic context is accessed by combining a reconstruction operation and a mechanism of anaphoric reference. The analysis makes only few and straightforward syntactic assumptions. In part, this is made possible because the use of Generalized Functional Application as a semantic operation allows us to model semantic composition in a flexible way. ',\n",
       " ' Transfer in a Connectionist Model of the Acquisition of Morphology The morphological systems of natural languages are replete with examples of the same devices used for multiple purposes: (1) the same type of morphological process (for example, suffixation for both noun case and verb tense) and (2) identical morphemes (for example, the same suffix for English noun plural and possessive). These sorts of similarity would be expected to convey advantages on language learners in the form of transfer from one morphological category to another. Connectionist models of morphology acquisition have been faulted for their supposed inability to represent phonological similarity across morphological categories and hence to facilitate transfer. This paper describes a connectionist model of the acquisition of morphology which is shown to exhibit transfer of this type. The model treats the morphology acquisition problem as one of learning to map forms onto meanings and vice versa. As the network learns these mappings, it makes phonological generalizations which are embedded in connection weights. Since these weights are shared by different morphological categories, transfer is enabled. In a set of experiments with artificial stimuli, networks were trained first on one morphological task (e.g., tense) and then on a second (e.g., number). It is shown that in the context of suffixation, prefixation, and template rules, the second task is facilitated when the second category either makes use of the same forms or the same general process type (e.g., prefixation) as the first. ',\n",
       " ' A Constraint-based Case Frame Lexicon Architecture In Turkish, (and possibly in many other languages) verbs often convey several meanings (some totally unrelated) when they are used with subjects, objects, oblique objects, adverbial adjuncts, with certain lexical, morphological, and semantic features, and co-occurrence restrictions. In addition to the usual sense variations due to selectional restrictions on verbal arguments, in most cases, the meaning conveyed by a case frame is idiomatic and not compositional, with subtle constraints. In this paper, we present an approach to building a constraint-based case frame lexicon for use in natural language processing in Turkish, whose prototype we have implemented under the TFS system developed at Univ. of Stuttgart. A number of observations that we have made on Turkish have indicated that we need something beyond the traditional transitive and intransitive distinction, and utilize a framework where verb valence is considered as the obligatory co-existence of an arbitrary subset of possible arguments along with the obligatory exclusion of certain others, relative to a verb sense. Additional morphological lexical and semantic constraints on the syntactic constituents organized as a 5-tier constraint hierarchy, are utilized to map a given syntactic structure case-fraame to a specific verb sense. ',\n",
       " ' An Efficient Algorithm for Surface Generation A method is given that inverts a logic grammar and displays it from the point of view of the logical form, rather than from that of the word string. LR-compiling techniques are used to allow a recursive-descent generation algorithm to perform functor merging much in the same way as an LR parser performs prefix merging. This is an improvement on the semantic-head-driven generator that results in a much smaller search space. The amount of semantic lookahead can be varied, and appropriate tradeoff points between table size and resulting nondeterminism can be found automatically. ',\n",
       " ' Error-tolerant Finite State Recognition with Applications to Morphological Analysis and Spelling Correction Error-tolerant recognition enables the recognition of strings that deviate mildly from any string in the regular set recognized by the underlying finite state recognizer. Such recognition has applications in error-tolerant morphological processing, spelling correction, and approximate string matching in information retrieval. After a description of the concepts and algorithms involved, we give examples from two applications: In the context of morphological analysis, error-tolerant recognition allows misspelled input word forms to be corrected, and morphologically analyzed concurrently. We present an application of this to error-tolerant analysis of agglutinative morphology of Turkish words. The algorithm can be applied to morphological analysis of any language whose morphology is fully captured by a single (and possibly very large) finite state transducer, regardless of the word formation processes and morphographemic phenomena involved. In the context of spelling correction, error-tolerant recognition can be used to enumerate correct candidate forms from a given misspelled string within a certain edit distance. Again, it can be applied to any language with a word list comprising all inflected forms, or whose morphology is fully described by a finite state transducer. We present experimental results for spelling correction for a number of languages. These results indicate that such recognition works very efficiently for candidate generation in spelling correction for many European languages such as English, Dutch, French, German, Italian (and others) with very large word lists of root and inflected forms (some containing well over 200,000 forms), generating all candidate solutions within 10 to 45 milliseconds (with edit distance 1) on a SparcStation 10/41. For spelling correction in Turkish, error-tolerant ',\n",
       " ' Specifying Logic Programs in Controlled Natural Language Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-specialists. Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable. The translation uses a definite clause grammar DCG enhanced by feature structures. Inter-text references of the specification, e.g. anaphora, are resolved with the help of discourse representation theory DRT. The generated Prolog clauses are added to a knowledge base. We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine. ',\n",
       " ' A Grammar Formalism and Cross-Serial Dependencies First we define a unification grammar formalism called the Tree Homomorphic Feature Structure Grammar. It is based on Lexical Functional Grammar LFG, but has a strong restriction on the syntax of the equations. We then show that this grammar formalism defines a full abstract family of languages, and that it is capable of describing cross-serial dependencies of the type found in Swiss German. ',\n",
       " ' Indexed Languages and Unification Grammars Indexed languages are interesting in computational linguistics because they are the least class of languages in the Chomsky hierarchy that has not been shown not to be adequate to describe the string set of natural language sentences. We here define a class of unification grammars that exactly describe the class of indexed languages. ',\n",
       " \" On-line Learning of Binary Lexical Relations Using Two-dimensional Weighted Majority Algorithms We consider the problem of learning a certain type of lexical semantic knowledge that can be expressed as a binary relation between words, such as the so-called sub-categorization of verbs (a verb-noun relation) and the compound noun phrase relation (a noun-noun relation). Specifically, we view this problem as an on-line learning problem in the sense of Littlestone's learning model in which the learner's goal is to minimize the total number of prediction mistakes. In the computational learning theory literature, Goldman, Rivest and Schapire and subsequently Goldman and Warmuth have considered the on-line learning problem for binary relations R : X * Y -> {0, 1} in which one of the domain sets X can be partitioned into a relatively small number of types, namely clusters consisting of behaviorally indistinguishable members of X. In this paper, we extend this model and suppose that both of the sets X, Y can be partitioned into a small number of types, and propose a host of prediction algorithms which are two-dimensional extensions of Goldman and Warmuth's weighted majority type algorithm proposed for the original model. We apply these algorithms to the learning problem for the `compound noun phrase' relation, in which a noun is related to another just in case they can form a noun phrase together. Our experimental results show that all of our algorithms out-perform Goldman and Warmuth's algorithm. We also theoretically analyze the performance of one of our algorithms, in the form of an upper bound on the worst case number of prediction mistakes it makes. \",\n",
       " ' Co-Indexing Labelled DRSs to Represent and Reason with Ambiguities The paper addresses the problem of representing ambiguities in a way that allows for monotonic disambiguation and for direct deductive computation. The paper focuses on an extension of the formalism of underspecified DRSs to ambiguities introduced by plural NPs. It deals with the collective/distributive distinction, and also with generic and cumulative readings. In addition it provides a systematic account for an underspecified treatment of plural pronoun resolution. ',\n",
       " ' An Integrated Framework for Learning and Reasoning Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems. ',\n",
       " ' Using Qualitative Hypotheses to Identify Inaccurate Data Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function SCF. SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems. ',\n",
       " ' A Robust Parsing Algorithm For Link Grammars In this paper we present a robust parsing algorithm based on the link grammar formalism for parsing natural languages. Our algorithm is a natural extension of the original dynamic programming recognition algorithm which recursively counts the number of linkages between two words in the input sentence. The modified algorithm uses the notion of a null link in order to allow a connection between any pair of adjacent words, regardless of their dictionary definitions. The algorithm proceeds by making three dynamic programming passes. In the first pass, the input is parsed using the original algorithm which enforces the constraints on links to ensure grammaticality. In the second pass, the total cost of each substring of words is computed, where cost is determined by the number of null links necessary to parse the substring. The final pass counts the total number of parses with minimal cost. All of the original pruning techniques have natural counterparts in the robust algorithm. When used together with memoization, these techniques enable the algorithm to run efficiently with cubic worst-case complexity. We have implemented these ideas and tested them by parsing the Switchboard corpus of conversational English. This corpus is comprised of approximately three million words of text, corresponding to more than 150 hours of transcribed speech collected from telephone conversations restricted to 70 different topics. Although only a small fraction of the sentences in this corpus are grammatical by standard criteria, the robust link grammar parser is able to extract relevant structure for a large portion of the sentences. We present the results of our experiments using this system, including the analyses of selected and random sentences from the corpus. ',\n",
       " ' Bridging as Coercive Accommodation In this paper we discuss the notion of bridging in Discourse Representation Theory as a tool to account for discourse referents that have only been established implicitly, through the lexical semantics of other referents. In doing so, we use ideas from Generative Lexicon theory, to introduce antecedents for anaphoric expressions that cannot be linked to a proper antecedent, but that do not need to be accommodated because they have some connection to the network of discourse referents that is already established. ',\n",
       " ' Parsing English with a Link Grammar We develop a formal grammatical system called a link grammar, show how English grammar can be encoded in such a system, and give algorithms for efficiently parsing with a link grammar. Although the expressive power of link grammars is equivalent to that of context free grammars, encoding natural language grammars appears to be much easier with the new system. We have written a program for general link parsing and written a link grammar for the English language. The performance of this preliminary system -- both in the breadth of English phenomena that it captures and in the computational resources used -- indicates that the approach may have practical uses as well as linguistic significance. Our program is written in C and may be obtained through the internet. ',\n",
       " \" An analysis of Bennett's pebble game Bennett's pebble game was introduced to obtain better time/space tradeoffs in the simulation of standard Turing machines by reversible ones. So far only upper bounds for the tradeoff based on the pebble game have been published. Here we give a recursion for the time optimal solution of the pebble game given a space bound. We analyze the recursion to obtain an explicit asymptotic expression for the best time-space product. \",\n",
       " \" Generating efficient belief models for task-oriented dialogues We have shown that belief modelling for dialogue can be simplified if the assumption is made that the participants are cooperating, i.e., they are not committed to any goals requiring deception. In such domains, there is no need to maintain individual representations of deeply nested beliefs; instead, three specific types of belief can be used to summarize all the states of nested belief that can exist about a domain entity. Here, we set out to design a ``compiler'' for belief models. This system will accept as input a description of agents' interactions with a task domain expressed in a fully-expressive belief logic with non-monotonic and temporal extensions. It generates an operational belief model for use in that domain, sufficient for the requirements of cooperative dialogue, including the negotiation of complex domain plans. The compiled model incorporates the belief simplification mentioned above, and also uses a simplified temporal logic of belief based on the restricted circumstances under which beliefs can change. We shall review the motivation for creating such a system, and introduce a general procedure for taking a logical specification for a domain and procesing it into an operational model. We shall then discuss the specific changes that are made during this procedure for limiting the level of abstraction at which the concepts of belief nesting, default reasoning and time are expressed. Finally we shall go through a worked example relating to the Map Task, a simple cooperative problem-solving exercise. \",\n",
       " ' A Matching Technique in Example-Based Machine Translation This paper addresses an important problem in Example-Based Machine Translation EBMT, namely how to measure similarity between a sentence fragment and a set of stored examples. A new method is proposed that measures similarity according to both surface structure and content. A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient. Results on a large number of test cases from the CELEX database are presented. ',\n",
       " \" Bi-Lexical Rules for Multi-Lexeme Translation in Lexicalist MT The paper presents a prototype lexicalist Machine Translation system (based on the so-called `Shake-and-Bake' approach of Whitelock (1992) consisting of an analysis component, a dynamic bilingual lexicon, and a generation component, and shows how it is applied to a range of MT problems. Multi-Lexeme translations are handled through bi-lexical rules which map bilingual lexical signs into new bilingual lexical signs. It is argued that much translation can be handled by equating translationally equivalent lists of lexical signs, either directly in the bilingual lexicon, or by deriving them through bi-lexical rules. Lexical semantic information organized as Qualia structures (Pustejovsky 1991) is used as a mechanism for restricting the domain of the rules. \",\n",
       " \" A Dynamic Approach to Rhythm in Language: Toward a Temporal Phonology It is proposed that the theory of dynamical systems offers appropriate tools to model many phonological aspects of both speech production and perception. A dynamic account of speech rhythm is shown to be useful for description of both Japanese mora timing and English timing in a phrase repetition task. This orientation contrasts fundamentally with the more familiar symbolic approach to phonology, in which time is modeled only with sequentially arrayed symbols. It is proposed that an adaptive oscillator offers a useful model for perceptual entrainment (or `locking in') to the temporal patterns of speech production. This helps to explain why speech is often perceived to be more regular than experimental measurements seem to justify. Because dynamic models deal with real time, they also help us understand how languages can differ in their temporal detail---contributing to foreign accents, for example. The fact that languages differ greatly in their temporal detail suggests that these effects are not mere motor universals, but that dynamical models are intrinsic components of the phonological characterization of language. \",\n",
       " ' New results on binary linear codes This research announcement describes in very rough terms methods and a computer language under development, which can be used to prove the nonexistence of binary linear codes. Over a hundred new results have been obtained by the author. For example, there is no [29,11,10] code. The proof of this is roughly outlined. ',\n",
       " ' A Labelled Analytic Theorem Proving Environment for Categorial Grammar We present a system for the investigation of computational properties of categorial grammar parsing based on a labelled analytic tableaux theorem prover. This proof method allows us to take a modular approach, in which the basic grammar can be kept constant, while a range of categorial calculi can be captured by assigning different properties to the labelling algebra. The theorem proving strategy is particularly well suited to the treatment of categorial grammar, because it allows us to distribute the computational cost between the algorithm which deals with the grammatical types and the algebraic checker which constrains the derivation. ',\n",
       " \" On Constraint-Based Lambek Calculi We explore the consequences of layering a Lambek proof system over an arbitrary (constraint) logic. A simple model-theoretic semantics for our hybrid language is provided for which a particularly simple combination of Lambek's and the proof system of the base logic is complete. Furthermore the proof system for the underlying base logic can be assumed to be a black box. The essential reasoning needed to be performed by the black box is that of {\\\\em entailment checking}. Assuming feature logic as the base logic entailment checking amounts to a {\\\\em subsumption} test which is a well-known quasi-linear time decidable problem. \",\n",
       " ' Heuristics and Parse Ranking There are currently two philosophies for building grammars and parsers -- Statistically induced grammars and Wide-coverage grammars. One way to combine the strengths of both approaches is to have a wide-coverage grammar with a heuristic component which is domain independent but whose contribution is tuned to particular domains. In this paper, we discuss a three-stage approach to disambiguation in the context of a lexicalized grammar, using a variety of domain independent heuristic techniques. We present a training algorithm which uses hand-bracketed treebank parses to set the weights of these heuristics. We compare the performance of our grammar against the performance of the IBM statistical grammar, using both untrained and trained weights for the heuristics. ',\n",
       " \" The Use of Knowledge Preconditions in Language Processing If an agent does not possess the knowledge needed to perform an action, it may privately plan to obtain the required information on its own, or it may involve another agent in the planning process by engaging it in a dialogue. In this paper, we show how the requirements of knowledge preconditions can be used to account for information-seeking subdialogues in discourse. We first present an axiomatization of knowledge preconditions for the SharedPlan model of collaborative activity (Grosz & Kraus, 1993), and then provide an analysis of information-seeking subdialogues within a general framework for discourse processing. In this framework, SharedPlans and relationships among them are used to model the intentional component of Grosz and Sidner's (1986) theory of discourse structure. \",\n",
       " ' A Natural Law of Succession Consider the problem of multinomial estimation. You are given an alphabet of k distinct symbols and are told that the i-th symbol occurred exactly n_i times in the past. On the basis of this information alone, you must now estimate the conditional probability that the next symbol will be i. In this report, we present a new solution to this fundamental problem in statistics and demonstrate that our solution outperforms standard approaches, both in theory and in practice. ',\n",
       " \" A Support Tool for Tagset Mapping Many different tagsets are used in existing corpora; these tagsets vary according to the objectives of specific projects (which may be as far apart as robust parsing vs. spelling correction). In many situations, however, one would like to have uniform access to the linguistic information encoded in corpus annotations without having to know the classification schemes in detail. This paper describes a tool which maps unstructured morphosyntactic tags to a constraint-based, typed, configurable specification language, a ``standard tagset''. The mapping relies on a manually written set of mapping rules, which is automatically checked for consistency. In certain cases, unsharp mappings are unavoidable, and noise, i.e. groups of word forms {\\\\sl not} conforming to the specification, will appear in the output of the mapping. The system automatically detects such noise and informs the user about it. The tool has been tested with rules for the UPenn tagset \\\\cite{up} and the SUSANNE tagset \\\\cite{garside}, in the framework of the EAGLES\\\\footnote{LRE project EAGLES, cf. \\\\cite{eagles}.} validation phase for standardised tagsets for European languages. \",\n",
       " ' Conserving Fuel in Statistical Language Learning: Predicting Data Requirements In this paper I address the practical concern of predicting how much training data is sufficient for a statistical language learning system. First, I briefly review earlier results and show how these can be combined to bound the expected accuracy of a mode-based learner as a function of the volume of training data. I then develop a more accurate estimate of the expected accuracy function under the assumption that inputs are uniformly distributed. Since this estimate is expensive to compute, I also give a close but cheaply computable approximation to it. Finally, I report on a series of simulations exploring the effects of inputs that are not uniformly distributed. Although these results are based on simplistic assumptions, they are a tentative step toward a useful theory of data requirements for SLL systems. ',\n",
       " ' How much is enough?: Data requirements for statistical NLP In this paper I explore a number of issues in the analysis of data requirements for statistical NLP systems. A preliminary framework for viewing such systems is proposed and a sample of existing works are compared within this framework. The first steps toward a theory of data requirements are made by establishing some results relevant to bounding the expected error rate of a class of simplified statistical language learners as a function of the volume of training data. ',\n",
       " ' Cluster Expansions and Iterative Scaling for Maximum Entropy Language Models The maximum entropy method has recently been successfully introduced to a variety of natural language applications. In each of these applications, however, the power of the maximum entropy method is achieved at the cost of a considerable increase in computational requirements. In this paper we present a technique, closely related to the classical cluster expansion from statistical mechanics, for reducing the computational demands necessary to calculate conditional maximum entropy language models. ',\n",
       " ' A Compositional Treatment of Polysemous Arguments in Categorial Grammar We discuss an extension of the standard logical rules (functional application and abstraction) in Categorial Grammar CG, in order to deal with some specific cases of polysemy. We borrow from Generative Lexicon theory which proposes the mechanism of {\\\\em coercion}, next to a rich nominal lexical semantic structure called {\\\\em qualia structure}. In a previous paper we introduced coercion into the framework of {\\\\em sign-based} Categorial Grammar and investigated its impact on traditional Fregean compositionality. In this paper we will elaborate on this idea, mostly working towards the introduction of a new semantic dimension. Where in current versions of sign-based Categorial Grammar only two representations are derived: a prosodic one (form) and a logical one (modelling), here we introduce also a more detaled representation of the lexical semantics. This extra knowledge will serve to account for linguistic phenomena like {\\\\em metonymy\\\\/}. ',\n",
       " ' The Development and Migration of Concepts from Donor to Borrower Disciplines: Sublanguage Term Use in Hard & Soft Sciences Academic disciplines, often divided into hard and soft sciences, may be understood as donor disciplines if they produce more concepts than they borrow from other disciplines, or borrower disciplines if they import more than they originate. Terms used to describe these concepts can be used to distinguish between hard and soft, donor and borrower, as well as individual discipline-specific sublanguages. Using term frequencies, the birth, growth, death, and migration of concepts and their associated terms are examined. ',\n",
       " ' Using Chinese Text Processing Technique for the Processing of Sanskrit Based Indian Languages: Maximum Resource Utilization and Maximum Compatibility Chinese text processing systems are using Double Byte Coding , while almost all existing Sanskrit Based Indian Languages have been using Single Byte coding for text processing. Through observation, Chinese Information Processing Technique has already achieved great technical development both in east and west. In contrast,Indian Languages are being processed by computer, more or less, for word processing purpose. This paper mainly emphasizes the method of processing Indian languages from a Computational Linguistic point of view. An overall design method is illustrated in this paper.This method concentrated on maximum resource utilization and compatibility: the ultimate goal is to have a Multiplatform Multilingual System. Keywords Text Procrssing, Multilingual Text Processing, Chinese Language Processing, Indian Language Processing, Character Coding. ',\n",
       " ' ParseTalk about Textual Ellipsis A hybrid methodology for the resolution of text-level ellipsis is presented in this paper. It incorporates conceptual proximity criteria applied to ontologically well-engineered domain knowledge bases and an approach to centering based on functional topic/comment patterns. We state text grammatical predicates for ellipsis and then turn to the procedural aspects of their evaluation within the framework of an actor-based implementation of a lexically distributed parser. ',\n",
       " ' Diffusion of Context and Credit Information in Markovian Models This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm. ',\n",
       " \" Improving Connectionist Energy Minimization Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the network's cycle-cutset. \",\n",
       " \" Learning Membership Functions in a Function-Based Object Recognition System Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a ``measure of goodness'' or ``membership value'' with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the object's shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value. \",\n",
       " ' POS Tagging Using Relaxation Labelling Relaxation labelling is an optimization technique used in many fields to solve constraint satisfaction problems. The algorithm finds a combination of values for a set of variables such that satisfies -to the maximum possible degree- a set of given constraints. This paper describes some experiments performed applying it to POS tagging, and the results obtained. It also ponders the possibility of applying it to word sense disambiguation. ',\n",
       " ' A Proposal for Word Sense Disambiguation using Conceptual Distance This paper presents a method for the resolution of lexical ambiguity and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiment have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus. ',\n",
       " ' Disambiguating bilingual nominal entries against WordNet This paper explores the acquisition of conceptual knowledge from bilingual dictionaries (French/English, Spanish/English and English/Spanish) using a pre-existing broad coverage Lexical Knowledge Base LKB WordNet. Bilingual nominal entries are disambiguated agains WordNet, therefore linking the bilingual dictionaries to WordNet yielding a multilingual LKB MLKB. The resulting MLKB has the same structure as WordNet, but some nodes are attached additionally to disambiguated vocabulary of other languages. Two different, complementary approaches are explored. In one of the approaches each entry of the dictionary is taken in turn, exploiting the information in the entry itself. The inferential capability for disambiguating the translation is given by Semantic Density over WordNet. In the other approach, the bilingual dictionary was merged with WordNet, exploiting mainly synonymy relations. Each of the approaches was used in a different dictionary. Both approaches attain high levels of precision on their own, showing that disambiguating bilingual nominal entries, and therefore linking bilingual dictionaries to WordNet is a feasible task. ',\n",
       " ' Classifier Assignment by Corpus-based Approach This paper presents an algorithm for selecting an appropriate classifier word for a noun. In Thai language, it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun, both from the point of view of the whole spe ech community and individual speakers. Basically, there is no exect rule for classifier selection. As far as we can do in the rule-based approach is to give a default rule to pick up a corresponding classifier of each noun. Registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation. We propose a corpus-based method (Biber, 1993; Nagao, 1993; Smadja, 1993) which generates Noun Classifier Associations NCA to overcome the problems in classifier assignment and semantic construction of noun phrase. The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences. ',\n",
       " ' Developing and Evaluating a Probabilistic LR Parser of Part-of-Speech and Punctuation Labels We describe an approach to robust domain-independent syntactic parsing of unrestricted naturally-occurring (English) input. The technique involves parsing sequences of part-of-speech and punctuation labels using a unification-based grammar coupled with a probabilistic LR parser. We describe the coverage of several corpora using this grammar and report the results of a parsing experiment using probabilities derived from bracketed training data. We report the first substantial experiments to assess the contribution of punctuation to deriving an accurate syntactic analysis, by parsing identical texts both with and without naturally-occurring punctuation marks. ',\n",
       " ' Incorporating Discourse Aspects in English -- Polish MT: Towards Robust Implementation The main aim of translation is an accurate transfer of meaning so that the result is not only grammatically and lexically correct but also communicatively adequate. This paper stresses the need for discourse analysis the aim of which is to preserve the communicative meaning in English--Polish machine translation. Unlike English, which is a positional language with word order grammatically determined, Polish displays a strong tendency to order constituents according to their degree of salience, so that the most informationally salient elements are placed towards the end of the clause regardless of their grammatical function. The Centering Theory developed for tracking down given information units in English and the Theory of Functional Sentence Perspective predicting informativeness of subsequent constituents provide theoretical background for this work. The notion of {\\\\em center} is extended to accommodate not only for pronominalisation and exact reiteration but also for definiteness and other center pointing constructs. Center information is additionally graded and applicable to all primary constituents in a given utterance. This information is used to order the post-transfer constituents correctly, relying on statistical regularities and some syntactic clues. ',\n",
       " ' Automatic Identification of Support Verbs: A Step Towards a Definition of Semantic Weight Current definitions of notions of lexical density and semantic weight are based on the division of words into closed and open classes, and on intuition. This paper develops a computationally tractable definition of semantic weight, concentrating on what it means for a word to be semantically light; the definition involves looking at the frequency of a word in particular syntactic constructions which are indicative of lightness. Verbs such as make and take , when they function as support verbs, are often considered to be semantically light. To test our definition, we carried out an experiment based on that of Grefenstette and Teufel (1995), where we automatically identify light instances of these words in a corpus; this was done by incorporating our frequency-related definition of semantic weight into a statistical approach similar to that of Grefenstette and Teufel. The results show that this is a plausible definition of semantic lightness for verbs, which can possibly be extended to defining semantic lightness for other classes of words. ',\n",
       " ' Flexibly Instructable Agents This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks. ',\n",
       " ' Toward an MT System without Pre-Editing --- Effects of New Methods in ALT-J/E --- Recently, several types of Japanese-to-English machine translation systems have been developed, but all of them require an initial process of rewriting the original text into easily translatable Japanese. Therefore these systems are unsuitable for translating information that needs to be speedily disseminated. To overcome this limitation, a Multi-Level Translation Method based on the Constructive Process Theory has been proposed. This paper describes the benefits of using this method in the Japanese-to-English machine translation system ALT-J/E. In comparison with conventional compositional methods, the Multi-Level Translation Method emphasizes the importance of the meaning contained in expression structures as a whole. It is shown to be capable of translating typical written Japanese based on the meaning of the text in its context, with comparative ease. We are now hopeful of carrying out useful machine translation with no manual pre-editing. ',\n",
       " ' Countability and Number in Japanese-to-English Machine Translation This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases. Incorporating this method into the machine translation system ALT-J/E, helped to raise the percentage of noun phrases generated with correct use of articles and number from 65% to 73%. ',\n",
       " \" Letting the Cat out of the Bag: Generation for Shake-and-Bake MT Describes an algorithm for the generation phase of a Shake-and-Bake Machine Translation system. Since the problem is NP-complete, it is unlikely that the algorithm will be efficient in all cases, but for the cases tested it offers an improvement over Whitelock's previously published algorithm. The work was carried out while the author was employed at Sharp Laboratories of Europe Ltd. \",\n",
       " \" The Effect of Resource Limits and Task Complexity on Collaborative Planning in Dialogue This paper shows how agents' choice in communicative action can be designed to mitigate the effect of their resource limits in the context of particular features of a collaborative planning task. I first motivate a number of hypotheses about effective language behavior based on a statistical analysis of a corpus of natural collaborative planning dialogues. These hypotheses are then tested in a dialogue testbed whose design is motivated by the corpus analysis. Experiments in the testbed examine the interaction between (1) agents' resource limits in attentional capacity and inferential capacity; (2) agents' choice in communication; and (3) features of communicative tasks that affect task difficulty such as inferential complexity, degree of belief coordination required, and tolerance for errors. The results show that good algorithms for communication must be defined relative to the agents' resource limits and the features of the task. Algorithms that are inefficient for inferentially simple, low coordination or fault-tolerant tasks are effective when tasks require coordination or complex inferences, or are fault-intolerant. The results provide an explanation for the occurrence of utterances in human dialogues that, prima facie, appear inefficient, and provide the basis for the design of effective algorithms for communicative choice for resource limited agents. \",\n",
       " \" Weak subsumption Constraints for Type Diagnosis: An Incremental Algorithm We introduce constraints necessary for type checking a higher-order concurrent constraint language, and solve them with an incremental algorithm. Our constraint system extends rational unification by constraints x y saying that `` has at least the structure of '', modelled by a weak instance relation between trees. This notion of instance has been carefully chosen to be weaker than the usual one which renders semi-unification undecidable. Semi-unification has more than once served to link unification problems arising from type inference and those considered in computational linguistics. Just as polymorphic recursion corresponds to subsumption through the semi-unification problem, our type constraint problem corresponds to weak subsumption of feature graphs in linguistics. The decidability problem for \\\\WhatsIt for feature graphs has been settled by D\\\\ orre~\\\\cite{Doerre:WeakSubsumption:94}. \\\\nocite{RuppRosnerJohnson:94} In contrast to D\\\\ orre's, our algorithm is fully incremental and does not refer to finite state automata. Our algorithm also is a lot more flexible. It allows a number of extensions (records, sorts, disjunctive types, type declarations, and others) which make it suitable for type inference of a full-fledged programming language. \",\n",
       " \" An investigation into the correlation of cue phrases, unfilled pauses and the structuring of spoken discourse Expectations about the correlation of cue phrases, the duration of unfilled pauses and the structuring of spoken discourse are framed in light of Grosz and Sidner's theory of discourse and are tested for a directions-giving dialogue. The results suggest that cue phrase and discourse structuring tasks may align, and show a correlation for pause length and some of the modifications that speakers can make to discourse structure. \",\n",
       " ' Chart-driven Connectionist Categorial Parsing of Spoken Korean While most of the speech and natural language systems which were developed for English and other Indo-European languages neglect the morphological processing and integrate speech and natural language at the word level, for the agglutinative languages such as Korean and Japanese, the morphological processing plays a major role in the language processing since these languages have very complex morphological phenomena and relatively simple syntactic functionality. Obviously degenerated morphological processing limits the usable vocabulary size for the system and word-level dictionary results in exponential explosion in the number of dictionary entries. For the agglutinative languages, we need sub-word level integration which leaves rooms for general morphological processing. In this paper, we developed a phoneme-level integration model of speech and linguistic processings through general morphological analysis for agglutinative languages and a efficient parsing scheme for that integration. Korean is modeled lexically based on the categorial grammar formalism with unordered argument and suppressed category extensions, and chart-driven connectionist parsing method is introduced. ',\n",
       " ' Disambiguating Noun Groupings with Respect to WordNet Senses Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is interested in relationships among word {\\\\em senses}, not words. This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns --- the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented. ',\n",
       " ' Using Information Content to Evaluate Semantic Similarity in a Taxonomy This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66). ',\n",
       " \" Decision-Theoretic Foundations for Causal Reasoning We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearl's representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning. \",\n",
       " ' Generalization of Clauses under Implication In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion. ',\n",
       " \" OPUS: An Efficient Admissible Algorithm for Unordered Search OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance. \",\n",
       " ' Rule-based Machine Learning Methods for Functional Prediction We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form DNF decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance. ',\n",
       " \" Statistical Feature Combination for the Evaluation of Game Positions This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches. \",\n",
       " ' Translating between Horn Representations and their Characteristic Models Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it. ',\n",
       " ' Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output. ',\n",
       " ' Analysis of the Arabic Broken Plural and Diminutive This paper demonstrates how the challenging problem of the Arabic broken plural and diminutive can be handled under a multi-tape two-level model, an extension to two-level morphology. ',\n",
       " ' The Unsupervised Acquisition of a Lexicon from Continuous Speech We present an unsupervised learning algorithm that acquires a natural-language lexicon from raw speech. The algorithm is based on the optimal encoding of symbol sequences in an MDL framework, and uses a hierarchical representation of language that overcomes many of the problems that have stymied previous grammar-induction procedures. The forward mapping from symbol sequences to the speech stream is modeled using features based on articulatory gestures. We present results on the acquisition of lexicons and language models from raw speech, text, and phonetic transcripts, and demonstrate that our algorithm compares very favorably to other reported results with respect to segmentation performance and statistical efficiency. ',\n",
       " ' Natural language processing: she needs something old and something new (maybe something borrowed and something blue, too) Given the present state of work in natural language processing, this address argues first, that advance in both science and applications requires a revival of concern about what language is about, broadly speaking the world; and second, that an attack on the summarising task, which is made ever more important by the growth of electronic text resources and requires an understanding of the role of large-scale discourse structure in marking important text content, is a good way forward. ',\n",
       " \" Term Encoding of Typed Feature Structures This paper presents an approach to Prolog-style term encoding of typed feature structures. The type feature structures to be encoded are constrained by appropriateness conditions as in Carpenter's ALE system. But unlike ALE, we impose a further independently motivated closed-world assumption. This assumption allows us to apply term encoding in cases that were problematic for previous approaches. In particular, previous approaches have ruled out multiple inheritance and further specification of feature-value declarations on subtypes. In the present approach, these spececial cases can be handled as well, though with some increase in complexity. For grammars without multiple inheritance and specification of feature values, the encoding presented here reduces to that of previous approaches. \",\n",
       " \" The Design and Experimental Analysis of Algorithms for Temporal Reasoning Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allen's influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size. \",\n",
       " ' Automatic Inference of DATR Theories This paper presents an approach for the automatic acquisition of linguistic knowledge from unstructured data. The acquired knowledge is represented in the lexical knowledge representation language DATR. A set of transformation rules that establish inheritance relationships and a default-inference algorithm make up the basis components of the system. Since the overall approach is not restricted to a special domain, the heuristic inference strategy uses criteria to evaluate the quality of a DATR theory, where different domains may require different criteria. The system is applied to the linguistic learning task of German noun inflection. ',\n",
       " ' Generic rules and non-constituent coordination We present a metagrammatical formalism, {\\\\em generic rules}, to give a default interpretation to grammar rules. Our formalism introduces a process of {\\\\em dynamic binding} interfacing the level of pure grammatical knowledge representation and the parsing level. We present an approach to non-constituent coordination within categorial grammars, and reformulate it as a generic rule. This reformulation is context-free parsable and reduces drastically the search space associated to the parsing task for such phenomena. ',\n",
       " ' Report of the Study Group on Assessment and Evaluation This is an interim report discussing possible guidelines for the assessment and evaluation of projects developing speech and language systems. It was prepared at the request of the European Commission DG XIII by an ad hoc study group, and is now being made available in the form in which it was submitted to the Commission. However, the report is not an official European Commission document, and does not reflect European Commission policy, official or otherwise. After a discussion of terminology, the report focusses on combining user-centred and technology-centred assessment, and on how meaningful comparisons can be made of a variety of systems performing different tasks for different domains. The report outlines the kind of infra-structure that might be required to support comparative assessment and evaluation of heterogenous projects, and also the results of a questionnaire concerning different approaches to evaluation. ',\n",
       " ' Noun Phrase Reference in Japanese-to-English Machine Translation This paper shows the necessity of distinguishing different referential uses of noun phrases in machine translation. We argue that differentiating between the generic, referential and ascriptive uses of noun phrases is the minimum necessary to generate articles and number correctly when translating from Japanese to English. Heuristics for determining these differences are proposed for a Japanese-to-English machine translation system. Finally the results of using the proposed heuristics are shown to have raised the percentage of noun phrases generated with correct use of articles and number in the Japanese-to-English machine translation system ALT-J/E from 65% to 77%. ',\n",
       " \" Possessive Pronouns as Determiners in Japanese-to-English Machine Translation Possessive pronouns are used as determiners in English when no equivalent would be used in a Japanese sentence with the same meaning. This paper proposes a heuristic method of generating such possessive pronouns even when there is no equivalent in the Japanese. The method uses information about the use of possessive pronouns in English treated as a lexical property of nouns, in addition to contextual information about noun phrase referentiality and the subject and main verb of the sentence that the noun phrase appears in. The proposed method has been implemented in NTT Communication Science Laboratories' Japanese-to-English machine translation system ALT-J/E. In a test set of 6,200 sentences, the proposed method increased the number of noun phrases with appropriate possessive pronouns generated, by 263 to 609, at the cost of generating 83 noun phrases with inappropriate possessive pronouns. \",\n",
       " ' Similarity between Words Computed by Spreading Activation on an English Dictionary This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis. The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary, LDOCE (Longman Dictionary of Contemporary English). Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary, and indirectly the similarity of all the other words in LDOCE. The similarity represents the strength of lexical cohesion or semantic relation, and also provides valuable information about similarity and coherence of texts. ',\n",
       " ' Text Segmentation Based on Similarity between Words This paper proposes a new indicator of text structure, called the lexical cohesion profile LCP, which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis. ',\n",
       " ' A General Architecture for Language Engineering GATE - a new approach to Language Engineering R&D This report argues for the provision of a common software infrastructure for NLP systems. Current trends in Language Engineering research are reviewed as motivation for this infrastructure, and relevant recent work discussed. A freely-available system called GATE is described which builds on this work. ',\n",
       " ' Logarithmic-Time Updates and Queries in Probabilistic Networks Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology. ',\n",
       " ' Parsing with Typed Feature Structures In this paper we provide for parsing with respect to grammars expressed in a general TFS-based formalism, a restriction of ALE. Our motivation being the design of an abstract (WAM-like) machine for the formalism, we consider parsing as a computational process and use it as an operational semantics to guide the design of the control structures for the abstract machine. We emphasize the notion of abstract typed feature structures (AFSs) that encode the essential information of TFSs and define unification over AFSs rather than over TFSs. We then introduce an explicit construct of multi-rooted feature structures (MRSs) that naturally extend TFSs and use them to represent phrasal signs as well as grammar rules. We also employ abstractions of MRSs and give the mathematical foundations needed for manipulating them. We formally define grammars and the languages they generate, and then describe a model for computation that corresponds to bottom-up chart parsing: grammars written in the TFS-based formalism are executed by the parser. We show that the computation is correct with respect to the independent definition. Finally, we discuss the class of grammars for which computations terminate and prove that termination can be guaranteed for off-line parsable grammars. ',\n",
       " ' Parsing with Typed Feature Structures In this paper we provide for parsing with respect to grammars expressed in a general TFS-based formalism, a restriction of ALE. Our motivation being the design of an abstract (WAM-like) machine for the formalism, we consider parsing as a computational process and use it as an operational semantics to guide the design of the control structures for the abstract machine. We emphasize the notion of abstract typed feature structures (AFSs) that encode the essential information of TFSs and define unification over AFSs rather than over TFSs. We then introduce an explicit construct of multi-rooted feature structures (MRSs) that naturally extend TFSs and use them to represent phrasal signs as well as grammar rules. We also employ abstractions of MRSs and give the mathematical foundations needed for manipulating them. We then present a simple bottom-up chart parser as a model for computation: grammars written in the TFS-based formalism are executed by the parser. Finally, we show that the parser is correct. ',\n",
       " ' Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach. ',\n",
       " ' How Part-of-Speech Tags Affect Text Retrieval and Filtering Performance Natural language processing NLP applied to information retrieval IR and filtering problems may assign part-of-speech tags to terms and, more generally, modify queries and documents. Analytic models can predict the performance of a text filtering system as it incorporates changes suggested by NLP, allowing us to make precise statements about the average effect of NLP operations on IR. Here we provide a model of retrieval and tagging that allows us to both compute the performance change due to syntactic parsing and to allow us to understand what factors affect performance and how. In addition to a prediction of performance with tags, upper and lower bounds for retrieval performance are derived, giving the best and worst effects of including part-of-speech tags. Empirical grounds for selecting sets of tags are considered. ',\n",
       " ' Situations and Computation: An Overview of Recent Research Serious thinking about the computational aspects of situation theory is just starting. There have been some recent proposals in this direction (viz. PROSIT and ASTL), with varying degrees of divergence from the ontology of the theory. We believe that a programming environment incorporating bona fide situation-theoretic constructs is needed and describe our very recent BABY-SIT implementation. A detailed critical account of PROSIT and ASTL is also offered in order to compare our system with these pioneering and influential frameworks. ',\n",
       " ' Text Windows and Phrases Differing by Discipline, Location in Document, and Syntactic Structure Knowledge of window style, content, location and grammatical structure may be used to classify documents as originating within a particular discipline or may be used to place a document on a theory versus practice spectrum. This distinction is also studied here using the type-token ratio to differentiate between sublanguages. The statistical significance of windows is computed, based on the the presence of terms in titles, abstracts, citations, and section headers, as well as binary independent BI and inverse document frequency IDF weightings. The characteristics of windows are studied by examining their within window density WWD and the S concentration SC, the concentration of terms from various document fields (e.g. title, abstract) in the fulltext. The rate of window occurrences from the beginning to the end of document fulltext differs between academic fields. Different syntactic structures in sublanguages are examined, and their use is considered for discriminating between specific academic disciplines and, more generally, between theory versus practice or knowledge versus applications oriented documents. ',\n",
       " ' Assessing agreement on classification tasks: the kappa statistic Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis. ',\n",
       " \" Active Learning with Statistical Models For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance. \",\n",
       " ' Improved Use of Continuous Attributes in C4.5 A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits. ',\n",
       " ' Mean Field Theory for Sigmoid Belief Networks We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits. ',\n",
       " ' Quantum Computing and Phase Transitions in Combinatorial Search We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems. ',\n",
       " ' Speech Recognition by Composition of Weighted Finite Automata We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition. ',\n",
       " ' Finite-State Approximation of Phrase-Structure Grammars Phrase-structure grammars are effective models for important syntactic and semantic aspects of natural languages, but can be computationally too demanding for use as language models in real-time speech recognition. Therefore, finite-state models are used instead, even though they lack expressive power. To reconcile those two alternatives, we designed an algorithm to compute finite-state approximations of context-free grammars and context-free-equivalent augmented phrase-structure grammars. The approximation is exact for certain context-free grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm has been used to build finite-state language models for limited-domain speech recognition tasks. ',\n",
       " ' Attempto - From Specifications in Controlled Natural Language towards Executable Specifications Deriving formal specifications from informal requirements is difficult since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge the conceptual gap we propose controlled natural language as a textual view on formal specifications in logic. The specification language Attempto Controlled English ACE is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage. The Attempto system translates specifications in ACE into discourse representation structures and into Prolog. The resulting knowledge base can be queried in ACE for verification, and it can be executed for simulation, prototyping and validation of the specification. ',\n",
       " ' Attempto Controlled English ACE Attempto Controlled English ACE allows domain specialists to interactively formulate requirements specifications in domain concepts. ACE can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage. The Attempto system translates specification texts in ACE into discourse representation structures and optionally into Prolog. Translated specification texts are incrementally added to a knowledge base. This knowledge base can be queried in ACE for verification, and it can be executed for simulation, prototyping and validation of the specification. ',\n",
       " \" Generalizing Case Frames Using a Thesaurus and the MDL Principle We address the problem of automatically acquiring case-frame patterns from large corpus data. In particular, we view this problem as the problem of estimating a (conditional) distribution over a partition of words, and propose a new generalization method based on the MDL (Minimum Description Length) principle. In order to assist with the efficiency, our method makes use of an existing thesaurus and restricts its attention on those partitions that are present as `cuts' in the thesaurus tree, thus reducing the generalization problem to that of estimating the `tree cut models' of the thesaurus. We then give an efficient algorithm which provably obtains the optimal tree cut model for the given frequency data, in the sense of MDL. We have used the case-frame patterns obtained using our method to resolve pp-attachment ambiguity.Our experimental results indicate that our method improves upon or is at least as effective as existing methods. \",\n",
       " ' Extraction of V-N-Collocations from Text Corpora: A Feasibility Study for German The usefulness of a statistical approach suggested by Church et al. (1991) is evaluated for the extraction of verb-noun (V-N) collocations from German text corpora. Some problematic issues of that method arising from properties of the German language are discussed and various modifications of the method are considered that might improve extraction results for German. The precision and recall of all variant methods is evaluated for V-N collocations containing support verbs, and the consequences for further work on the extraction of collocations from German corpora are discussed. With a sufficiently large corpus (>= 6 mio. word-tokens), the average error rate of wrong extractions can be reduced to 2.2% (97.8% precision) with the most restrictive method, however with a loss in data of almost 50% compared to a less restrictive method with still 87.6% precision. Depending on the goal to be achieved, emphasis can be put on a high recall for lexicographic purposes or on high precision for automatic lexical acquisition, in each case unfortunately leading to a decrease of the corresponding other variable. Low recall can still be acceptable if very large corpora (i.e. 50 - 100 million words) are available or if corpora for special domains are used in addition to the data found in machine readable (collocation) dictionaries. ',\n",
       " ' Integrated speech and morphological processing in a connectionist continuous speech understanding for Korean A new tightly coupled speech and natural language integration model is presented for a TDNN-based continuous possibly large vocabulary speech recognition system for Korean. Unlike popular n-best techniques developed for integrating mainly HMM-based speech recognition and natural language processing in a {\\\\em word level}, which is obviously inadequate for morphologically complex agglutinative languages, our model constructs a spoken language system based on a {\\\\em morpheme-level} speech and language integration. With this integration scheme, the spoken Korean processing engine SKOPE is designed and implemented using a TDNN-based diphone recognition module integrated with a Viterbi-based lexical decoding and symbolic phonological/morphological co-analysis. Our experiment results show that the speaker-dependent continuous {\\\\em eojeol} (Korean word) recognition and integrated morphological analysis can be achieved with over 80.6% success rate directly from speech inputs for the middle-level vocabularies. ',\n",
       " \" A Divergence Critic for Inductive Proof Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a ``difference matching'' procedure. The critic then proposes lemmas and generalizations which ``ripple'' these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone. \",\n",
       " \" Iterative Optimization and Simplification of Hierarchical Clusterings Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions. \",\n",
       " \" Practical Methods for Proving Termination of General Logic Programs Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs. \",\n",
       " ' A Constraint-based Case Frame Lexicon We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame. The lexicon uses a semantic sense as the basic unit and employs a multi-tiered constraint structure for the resolution of syntactic information into the appropriate senses and/or idiomatic usage. Valency changing transformations such as morphologically marked passivized or causativized forms are handled via lexical rules that manipulate case frames templates. The system has been implemented in a typed-feature system and applied to Turkish. ',\n",
       " \" Apportioning Development Effort in a Probabilistic LR Parsing System through Evaluation We describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-of-speech and punctuation labels coupled with a probabilistic LR parser. We present evaluations of the system's performance along several different dimensions; these enable us to assess the contribution that each individual part is making to the success of the system as a whole, and thus prioritise the effort to be devoted to its further enhancement. Currently, the system is able to parse around 80% of sentences in a substantial corpus of general text containing a number of distinct genres. On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83% and 84% respectively when evaluated against manually-disambiguated analyses. \",\n",
       " \" Combining Hand-crafted Rules and Unsupervised Learning in Constraint-based Morphological Disambiguation This paper presents a constraint-based morphological disambiguation approach that is applicable languages with complex morphology--specifically agglutinative languages with productive inflectional and derivational morphological phenomena. In certain respects, our approach has been motivated by Brill's recent work, but with the observation that his transformational approach is not directly applicable to languages like Turkish. Our system combines corpus independent hand-crafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statistical information from the corpus to be morphologically disambiguated. The hand-crafted rules are linguistically motivated and tuned to improve precision without sacrificing recall. The unsupervised learning process produces two sets of rules: (i) choose rules which choose morphological parses of a lexical item satisfying constraint effectively discarding other parses, and (ii) delete rules, which delete parses satisfying a constraint. Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexical item whose root is unknown. With this approach, well below 1 percent of the tokens remains as unknown in the texts we have experimented with. Our results indicate that by combining these hand-crafted,statistical and learned information sources, we can attain a recall of 96 to 97 percent with a corresponding precision of 93 to 94 percent, and ambiguity of 1.02 to 1.03 parses per token. \",\n",
       " ' Better Language Models with Model Merging This paper investigates model merging, a technique for deriving Markov models from text or speech corpora. Models are derived by starting with a large and specific model and by successively combining states to build smaller and more general models. We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models for a speech recognition task. The experiments show the advantage of model merging over the standard bigram approach. The merged model assigns a lower perplexity to the test set and uses considerably fewer states. ',\n",
       " ' Error-tolerant Tree Matching This paper presents an efficient algorithm for retrieving from a database of trees, all trees that match a given query tree approximately, that is, within a certain error tolerance. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. The algorithm has been implemented on SparcStations, and for large randomly generated synthetic tree databases (some having tens of thousands of trees) it can associatively search for trees with a small error, in a matter of tenths of a second to few seconds. ',\n",
       " \" The Role of the Gricean Maxims in the Generation of Referring Expressions Grice's maxims of conversation [Grice 1975] are framed as directives to be followed by a speaker of the language. This paper argues that, when considered from the point of view of natural language generation, such a characterisation is rather misleading, and that the desired behaviour falls out quite naturally if we view language generation as a goal-oriented process. We argue this position with particular regard to the generation of referring expressions. \",\n",
       " ' Collocational Grammar A perspective of statistical language models which emphasizes their collocational aspect is advocated. It is suggested that strings be generalized in terms of classes of relationships instead of classes of objects. The single most important characteristic of such a model is a mechanism for comparing patterns. When patterns are fully generalized a natural definition of syntactic class emerges as a subset of relational class. These collocational syntactic classes should be an unambiguous partition of traditional syntactic classes. ',\n",
       " \" Efficient Algorithms for Parsing the DOP Model Excellent results have been reported for Data-Oriented Parsing DOP of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers. \",\n",
       " ' Another Facet of LIG Parsing In this paper we present a new parsing algorithm for linear indexed grammars (LIGs) in the same spirit as the one described in (Vijay-Shanker and Weir, 1993) for tree adjoining grammars. For a LIG and an input string of length , we build a non ambiguous context-free grammar whose sentences are all (and exclusively) valid derivation sequences in which lead to . We show that this grammar can be built in time and that individual parses can be extracted in linear time with the size of the extracted parse tree. Though this upper bound does not improve over previous results, the average case behaves much better. Moreover, practical parsing times can be decreased by some statically performed computations. ',\n",
       " ' Off-line Constraint Propagation for Efficient HPSG Processing We investigate the use of a technique developed in the constraint programming community called constraint propagation to automatically make a HPSG theory more specific at those places where linguistically motivated underspecification would lead to inefficient processing. We discuss two concrete HPSG examples showing how off-line constraint propagation helps improve processing efficiency. ',\n",
       " ' Multi-level post-processing for Korean character recognition using morphological analysis and linguistic evaluation Most of the post-processing methods for character recognition rely on contextual information of character and word-fragment levels. However, due to linguistic characteristics of Korean, such low-level information alone is not sufficient for high-quality character-recognition applications, and we need much higher-level contextual information to improve the recognition results. This paper presents a domain independent post-processing technique that utilizes multi-level morphological, syntactic, and semantic information as well as character-level information. The proposed post-processing system performs three-level processing: candidate character-set selection, candidate eojeol (Korean word) generation through morphological analysis, and final single eojeol-sequence selection by linguistic evaluation. All the required linguistic information and probabilities are automatically acquired from a statistical corpus analysis. Experimental results demonstrate the effectiveness of our method, yielding error correction rate of 80.46%, and improved recognition rate of 95.53% from before-post-processing rate 71.2% for single best-solution selection. ',\n",
       " ' SemHe: A Generalised Two-Level System This paper presents a generalised two-level implementation which can handle linear and non-linear morphological operations. An algorithm for the interpretation of multi-tape two-level rules is described. In addition, a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac. ',\n",
       " ' Syntactic Analyses for Parallel Grammars: Auxiliaries and Genitive NPs This paper focuses on two disparate aspects of German syntax from the perspective of parallel grammar development. As part of a cooperative project, we present an innovative approach to auxiliaries and multiple genitive NPs in German. The LFG-based implementation presented here avoids unnessary structural complexity in the representation of auxiliaries by challenging the traditional analysis of auxiliaries as raising verbs. The approach developed for multiple genitive NPs provides a more abstract, language independent representation of genitives associated with nominalized verbs. Taken together, the two approaches represent a step towards providing uniformly applicable treatments for differing languages, thus lightening the burden for machine translation. ',\n",
       " ' Computing Prosodic Morphology This paper establishes a framework under which various aspects of prosodic morphology, such as templatic morphology and infixation, can be handled under two-level theory using an implemented multi-tape two-level model. The paper provides a new computational analysis of root-and-pattern morphology based on prosody. ',\n",
       " ' The importance of being lazy -- using lazy evaluation to process queries to HPSG grammars Linguistic theories formulated in the architecture of {\\\\sc hpsg} can be very precise and explicit since {\\\\sc hpsg} provides a formally well-defined setup. However, when querying a faithful implementation of such an explicit theory, the large data structures specified can make it hard to see the relevant aspects of the reply given by the system. Furthermore, the system spends much time applying constraints which can never fail just to be able to enumerate specific answers. In this paper we want to describe lazy evaluation as the result of an off-line compilation technique. This method of evaluation can be used to answer queries to an {\\\\sc hpsg} system so that only the relevant aspects are checked and output. ',\n",
       " ' Fast Parsing using Pruning and Grammar Specialization We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning. These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work. Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application. ',\n",
       " ' Processing Metonymy: a Domain-Model Heuristic Graph Traversal Approach We address here the treatment of metonymic expressions from a knowledge representation perspective, that is, in the context of a text understanding system which aims to build a conceptual representation from texts according to a domain model expressed in a knowledge representation formalism. We focus in this paper on the part of the semantic analyser which deals with semantic composition. We explain how we use the domain model to handle metonymy dynamically, and more generally, to underlie semantic composition, using the knowledge descriptions attached to each concept of our ontology as a kind of concept-level, multiple-role qualia structure. We rely for this on a heuristic path search algorithm that exploits the graphic aspects of the conceptual graphs formalism. The methods described have been implemented and applied on French texts in the medical domain. ',\n",
       " ' The Measure of a Model This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set. These determinants are the appropriateness, for the test set, of the results of (1) feature selection, (2) formulation of the parametric form of the model, and (3) parameter estimation. These are part of any model formulation procedure, even if not broken out as separate steps, so the tradeoffs explored in this paper are relevant to a wide variety of methods. The measures are demonstrated in a large experiment, in which they are used to analyze the results of roughly 300 classifiers that perform word-sense disambiguation. ',\n",
       " ' Magic for Filter Optimization in Dynamic Bottom-up Processing Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar. The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness. Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest. ',\n",
       " \" Translating into Free Word Order Languages In this paper, I discuss machine translation of English text into Turkish, a relatively ``free'' word order language. I present algorithms that determine the topic and the focus of each target sentence (using salience (Centering Theory), old vs. new information, and contrastiveness in the discourse model) in order to generate the contextually appropriate word orders in the target language. \",\n",
       " ' 2Planning for Contingencies: A Decision-based Approach A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures. ',\n",
       " ' Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations. ',\n",
       " ' A Formal Framework for Speedup Learning from Problems and Solutions Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning EBL, and Probably Approximately Correct PAC Learning. ',\n",
       " ' A Model-Theoretic Framework for Theories of Syntax A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism---to express syntactic theories purely in terms of the properties of the class of structures they license. By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties, this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization, allowing disparate theories to be compared on the basis of those properties. We discuss , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive---supporting direct statement of most linguistically significant syntactic properties---but having well-defined strong generative capacity---languages are definable in iff they are strongly context-free. We draw examples from the realms of GPSG and GB. ',\n",
       " ' Connectivity in Bag Generation This paper presents a pruning technique which can be used to reduce the number of paths searched in rule-based bag generators of the type proposed by \\\\cite{poznanskietal95} and \\\\cite{popowich95}. Pruning the search space in these generators is important given the computational cost of bag generation. The technique relies on a connectivity constraint between the semantic indices associated with each lexical sign in a bag. Testing the algorithm on a range of sentences shows reductions in the generation time and the number of edges constructed. ',\n",
       " \" Further Experimental Evidence against the Utility of Occam's Razor This paper presents new experimental evidence against the utility of Occam's razor. A~systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning. \",\n",
       " ' Learning Part-of-Speech Guessing Rules from Lexicon: Extension to Non-Concatenative Operations One of the problems in part-of-speech tagging of real-word texts is that of unknown to the lexicon words. In Mikheev (ACL-96 cmp-lg/9604022), a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words was proposed. One of the over-simplification assumed by this learning technique was the acquisition of morphological rules which obey only simple concatenative regularities of the main word with an affix. In this paper we extend this technique to the non-concatenative cases of suffixation and assess the gain in the performance. ',\n",
       " ' Least Generalizations and Greatest Specializations of Sets of Clauses The main operations in Inductive Logic Programming ILP are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages. ',\n",
       " \" Reinforcement Learning: A Survey This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning. \",\n",
       " ' Towards a Workbench for Acquisition of Domain Knowledge from Natural Language In this paper we describe an architecture and functionality of main components of a workbench for an acquisition of domain knowledge from large text corpora. The workbench supports an incremental process of corpus analysis starting from a rough automatic extraction and organization of lexico-semantic regularities and ending with a computer supported analysis of extracted data and a semi-automatic refinement of obtained hypotheses. For doing this the workbench employs methods from computational linguistics, information retrieval and knowledge engineering. Although the workbench is currently under implementation some of its components are already implemented and their performance is illustrated with samples from engineering for a medical domain. ',\n",
       " ' Unsupervised Learning of Word-Category Guessing Rules Words unknown to the lexicon present a substantial problem to part-of-speech tagging. In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words. Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules. The learning was performed on the Brown Corpus data and rule-sets, with a highly competitive performance, were produced and compared with the state-of-the-art. ',\n",
       " ' Building Natural-Language Generation Systems This is a very short paper that briefly discusses some of the tasks that NLG systems perform. It is of no research interest, but I have occasionally found it useful as a way of introducing NLG to potential project collaborators who know nothing about the field. ',\n",
       " \" Compiling a Partition-Based Two-Level Formalism This paper describes an algorithm for the compilation of a two (or more) level orthographic or phonological rule notation into finite state transducers. The notation is an alternative to the standard one deriving from Koskenniemi's work: it is believed to have some practical descriptive advantages, and is quite widely used, but has a different interpretation. Efficient interpreters exist for the notation, but until now it has not been clear how to compile to equivalent automata in a transparent way. The present paper shows how to do this, using some of the conceptual tools provided by Kaplan and Kay's regular relations calculus. \",\n",
       " \" Focus and Higher-Order Unification Pulman has shown that Higher--Order Unification HOU can be used to model the interpretation of focus. In this paper, we extend the unification--based approach to cases which are often seen as a test--bed for focus theory: utterances with multiple focus operators and second occurrence expressions. We then show that the resulting analysis favourably compares with two prominent theories of focus (namely, Rooth's Alternative Semantics and Krifka's Structured Meanings theory) in that it correctly generates interpretations which these alternative theories cannot yield. Finally, we discuss the formal properties of the approach and argue that even though HOU need not terminate, for the class of unification--problems dealt with in this paper, HOU avoids this shortcoming and is in fact computationally tractable. \",\n",
       " \" Higher-Order Coloured Unification and Natural Language Semantics In this paper, we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple, Shieber and Pereira, 1991)'s analysis called for. \",\n",
       " ' Yet Another Paper about Partial Verb Phrase Fronting in German I describe a very simple HPSG analysis for partial verb phrase fronting. I will argue that the presented account is more adequate than others made during the past years because it allows the description of constituents in fronted positions with their modifier remaining in the non-fronted part of the sentence. A problem with ill-formed signs that are admitted by all HPSG accounts for partial verb phrase fronting known so far will be explained and a solution will be suggested that uses the difference between combinatoric relations of signs and their representation in word order domains. ',\n",
       " ' Active Constraints for a Direct Interpretation of HPSG In this paper, we characterize the properties of a direct interpretation of HPSG and present the advantages of this approach. High-level programming languages constitute in this perspective an efficient solution: we show how a multi-paradigm approach, containing in particular constraint logic programming, offers mechanims close to that of the theory and preserves its fundamental properties. We take the example of LIFE and describe the implementation of the main HPSG mechanisms. ',\n",
       " \" Resolving Anaphors in Embedded Sentences We propose an algorithm to resolve anaphors, tackling mainly the problem of intrasentential antecedents. We base our methodology on the fact that such antecedents are likely to occur in embedded sentences. Sidner's focusing mechanism is used as the basic algorithm in a more complete approach. The proposed algorithm has been tested and implemented as a part of a conceptual analyser, mainly to process pronouns. Details of an evaluation are given. \",\n",
       " ' Tactical Generation in a Free Constituent Order Language This paper describes tactical generation in Turkish, a free constituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University--Center for Machine Translation. Morphological realization has been implemented using an external morphological analysis/generation component which performs concrete morpheme selection and handles morphographemic processes. ',\n",
       " ' A New Statistical Parser Based on Bigram Lexical Dependencies This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95, Jelinek et al 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy. ',\n",
       " ' Best-First Surface Realization Current work in surface realization concentrates on the use of general, abstract algorithms that interpret large, reversible grammars. Only little attention has been paid so far to the many small and simple applications that require coverage of a small sublanguage at different degrees of sophistication. The system TG/2 described in this paper can be smoothly integrated with deep generation processes, it integrates canned text, templates, and context-free rules into a single formalism, it allows for both textual and tabular output, and it can be parameterized according to linguistic preferences. These features are based on suitably restricted production system techniques and on a generic backtracking regime. ',\n",
       " ' Counting Coordination Categorially This paper presents a way of reducing the complexity of parsing free coordination. It lives on the Coordinative Count Invariant, a property of derivable sequences in occurrence-sensitive categorial grammar. This invariant can be exploited to cut down deterministically the search space for coordinated sentences to minimal fractions. The invariant is based on inequalities, which is shown to be the best one can get in the presence of coordination without proper parsing. It is implemented in a categorial parser for Dutch. Some results of applying the invariant to the parsing of coordination in this parser are presented. ',\n",
       " \" Extended Dependency Structures and their Formal Interpretation We describe two ``semantically-oriented'' dependency-structure formalisms, U-forms and S-forms. U-forms have been previously used in machine translation as interlingual representations, but without being provided with a formal interpretation. S-forms, which we introduce in this paper, are a scoped version of U-forms, and we define a compositional semantics mechanism for them. Two types of semantic composition are basic: complement incorporation and modifier incorporation. Binding of variables is done at the time of incorporation, permitting much flexibility in composition order and a simple account of the semantic effects of permuting several incorporations. \",\n",
       " ' A Chart Generator for Shake and Bake Machine Translation A generation algorithm based on an active chart parsing algorithm is introduced which can be used in conjunction with a Shake and Bake machine translation system. A concise Prolog implementation of the algorithm is provided, and some performance comparisons with a shift-reduce based algorithm are given which show the chart generator is much more efficient for generating all possible sentences from an input specification. ',\n",
       " ' Adapting the Core Language Engine to French and Spanish We describe how substantial domain-independent language-processing systems for French and Spanish were quickly developed by manually adapting an existing English-language system, the SRI Core Language Engine. We explain the adaptation process in detail, and argue that it provides a fairly general recipe for converting a grammar-based system for English into a corresponding one for a Romance language. ',\n",
       " \" Parsing for Semidirectional Lambek Grammar is NP-Complete We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call {\\\\em semidirectional}. In semidirectional Lambek calculus there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent's left-hand side, thus permitting non-peripheral extraction. grammars are able to generate each context-free language and more than that. We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem. \",\n",
       " \" Efficient Tabular LR Parsing We give a new treatment of tabular LR parsing, which is an alternative to Tomita's generalized LR algorithm. The advantage is twofold. Firstly, our treatment is conceptually more attractive because it uses simpler concepts, such as grammar transformations and standard tabulation techniques also know as chart parsing. Secondly, the static and dynamic complexity of parsing, both in space and time, is significantly reduced. \",\n",
       " ' Noun-Phrase Analysis in Unrestricted Text for Information Retrieval Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text. This paper reports on the application of a few simple, yet robust and efficient noun-phrase analysis techniques to create better indexing phrases for information retrieval. In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics. Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system. The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction. ',\n",
       " \" Where Defaults Don't Help: the Case of the German Plural System The German plural system has become a focal point for conflicting theories of language, both linguistic and cognitive. We present simulation results with three simple classifiers - an ordinary nearest neighbour algorithm, Nosofsky's `Generalized Context Model' GCM and a standard, three-layer backprop network - predicting the plural class from a phonological representation of the singular in German. Though these are absolutely `minimal' models, in terms of architecture and input information, they nevertheless do remarkably well. The nearest neighbour predicts the correct plural class with an accuracy of 72% for a set of 24,640 nouns from the CELEX database. With a subset of 8,598 (non-compound) nouns, the nearest neighbour, the GCM and the network score 71.0%, 75.0% and 83.5%, respectively, on novel items. Furthermore, they outperform a hybrid, `pattern-associator + default rule', model, as proposed by Marcus et al. (1995), on this data set. \",\n",
       " ' A Simple Transformation for Offline-Parsable Grammars and its Termination Properties We present, in easily reproducible terms, a simple transformation for offline-parsable grammars which results in a provably terminating parsing program directly top-down interpretable in Prolog. The transformation consists in two steps: (1) removal of empty-productions, followed by: (2) left-recursion elimination. It is related both to left-corner parsing (where the grammar is compiled, rather than interpreted through a parsing program, and with the advantage of guaranteed termination in the presence of empty productions) and to the Generalized Greibach Normal Form for DCGs (with the advantage of implementation simplicity). ',\n",
       " ' Functional Centering Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering. ',\n",
       " ' Processing Complex Sentences in the Centering Framework We extend the centering model for the resolution of intra-sentential anaphora and specify how to handle complex sentences. An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence. ',\n",
       " \" Using Terminological Knowledge Representation Languages to Manage Linguistic Resources I examine how terminological languages can be used to manage linguistic data during NL research and development. In particular, I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions, identify the need for new verb classes, and identify appropriate linguistic hypotheses for a new verb's behavior. \",\n",
       " \" A Conceptual Reasoning Approach to Textual Ellipsis We present a hybrid text understanding methodology for the resolution of textual ellipsis. It integrates conceptual criteria (based on the well-formedness and conceptual strength of role chains in a terminological knowledge base) and functional constraints reflecting the utterances' information structure (based on the distinction between context-bound and unbound discourse elements). The methodological framework for text ellipsis resolution is the centering model that has been adapted to these constraints. \",\n",
       " \" Learning Word Association Norms Using Tree Cut Pair Models We consider the problem of learning co-occurrence information between two word categories, or more in general between two discrete random variables taking values in a hierarchically classified domain. In particular, we consider the problem of learning the `association norm' defined by A(x,y)=p(x, y)/(p(x)*p(y)), where p(x, y) is the joint distribution for x and y and p(x) and p(y) are marginal distributions induced by p(x, y). We formulate this problem as a sub-task of learning the conditional distribution p(x|y), by exploiting the identity p(x|y) = A(x,y)*p(x). We propose a two-step estimation method based on the MDL principle, which works as follows: It first estimates p(x) as p1 using MDL, and then estimates p(x|y) for a fixed y by applying MDL on the hypothesis class of {A * p1 | A \\\\in B} for some given class B of representations for association norm. The estimation of A is therefore obtained as a side-effect of a near optimal estimation of p(x|y). We then apply this general framework to the problem of acquiring case-frame patterns. We assume that both p(x) and A(x, y) for given y are representable by a model based on a classification that exists within an existing thesaurus tree as a `cut,' and hence p(x|y) is represented as the product of a pair of `tree cut models.' We then devise an efficient algorithm that implements our general strategy. We tested our method by using it to actually acquire case-frame patterns and conducted disambiguation experiments using the acquired knowledge. The experimental results show that our method improves upon existing methods. \",\n",
       " ' Restricted Parallelism in Object-Oriented Lexical Parsing We present an approach to parallel natural language parsing which is based on a concurrent, object-oriented model of computation. A depth-first, yet incomplete parsing algorithm for a dependency grammar is specified and several restrictions on the degree of its parallelization are discussed. ',\n",
       " \" Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept Accuracy In this paper we describe an approach to automatic evaluation of both the speech recognition and understanding capabilities of a spoken dialogue system for train time table information. We use word accuracy for recognition and concept accuracy for understanding performance judgement. Both measures are calculated by comparing these modules' output with a correct reference answer. We report evaluation results for a spontaneous speech corpus with about 10000 utterances. We observed a nearly linear relationship between word accuracy and concept accuracy. \",\n",
       " ' Trading off Completeness for Efficiency --- The \\\\textsc{ParseTalk} Performance Grammar Approach to Real-World Text Parsing We argue for a performance-based design of natural language grammars and their associated parsers in order to meet the constraints posed by real-world natural language understanding. This approach incorporates declarative and procedural knowledge about language and language use within an object-oriented specification framework. We discuss several message passing protocols for real-world text parsing and provide reasons for sacrificing completeness of the parse in favor of efficiency. ',\n",
       " ' Incremental Centering and Center Ambiguity In this paper, we present a model of anaphor resolution within the framework of the centering model. The consideration of an incremental processing mode introduces the need to manage structural ambiguity at the center level. Hence, the centering framework is further refined to account for local and global parsing ambiguities which propagate up to the level of center representations, yielding moderately adapted data structures for the centering algorithm. ',\n",
       " \" Efficient Algorithms for Parsing the DOP Model? A Reply to Joshua Goodman This note is a reply to Joshua Goodman's paper Efficient Algorithms for Parsing the DOP Model (Goodman, 1996; cmp-lg/9604008). In his paper, Goodman makes a number of claims about (my work on) the Data-Oriented Parsing model (Bod, 1992-1996). This note shows that some of these claims must be mistaken. \",\n",
       " ' Synchronous Models of Language In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system. ',\n",
       " ' Example-Based Optimization of Surface-Generation Tables A method is given that inverts a logic grammar and displays it from the point of view of the logical form, rather than from that of the word string. LR-compiling techniques are used to allow a recursive-descent generation algorithm to perform functor merging much in the same way as an LR parser performs prefix merging. This is an improvement on the semantic-head-driven generator that results in a much smaller search space. The amount of semantic lookahead can be varied, and appropriate tradeoff points between table size and resulting nondeterminism can be found automatically. This can be done by removing all spurious nondeterminism for input sufficiently close to the examples of a training corpus, and large portions of it for other input, while preserving completeness. ',\n",
       " ' Handling Sparse Data by Successive Abstraction A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and outperformed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori. ',\n",
       " ' Notes on LR Parser Design The design of an LR parser based on interleaving the atomic symbol processing of a context-free backbone grammar with the full constraints of the underlying unification grammar is described. The parser employs a set of reduced constraints derived from the unification grammar in the LR parsing step. Gap threading is simulated to reduce the applicability of empty productions. ',\n",
       " \" Parsing Algorithms and Metrics Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the ``Labelled Recall Algorithm,'' which maximizes the expected Labelled Recall Rate, and the ``Bracketed Recall Algorithm,'' which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize. \",\n",
       " ' A Principled Approach Towards Symbolic Geometric Constraint Satisfaction An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry. This approach, called degrees of freedom analysis, employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. A potential drawback, which limits the scalability of this approach, is concerned with the difficulty of writing plan fragments. In this paper we address this limitation by showing how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology. ',\n",
       " \" On Partially Controlled Multi-Agent Systems Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship. \",\n",
       " ' Efficient Normal-Form Parsing for Combinatory Categorial Grammar Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses. Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique. The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated. ',\n",
       " \" A Bayesian hybrid method for context-sensitive spelling correction Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated. \",\n",
       " \" Combining Trigram-based and Feature-based Methods for Context-Sensitive Spelling Correction This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as ``peace'' and ``piece'', or ``quiet'' and ``quite'') and also the problem of correcting particular word usage errors (such as ``amount'' and ``number'', or ``among'' and ``between''). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix `spell'. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance. \",\n",
       " ' Classification in Feature-based Default Inheritance Hierarchies Increasingly, inheritance hierarchies are being used to reduce redundancy in natural language processing lexicons. Systems that utilize inheritance hierarchies need to be able to insert words under the optimal set of classes in these hierarchies. In this paper, we formalize this problem for feature-based default inheritance hierarchies. Since the problem turns out to be NP-complete, we present an approximation algorithm for it. We show that this algorithm is efficient and that it performs well with respect to a number of standard problems for default inheritance. A prototype implementation has been tested on lexical hierarchies and it has produced encouraging results. The work presented here is also relevant to other types of default hierarchies. ',\n",
       " ' Clustered Language Models with Context-Equivalent States In this paper, a hierarchical context definition is added to an existing clustering algorithm in order to increase its robustness. The resulting algorithm, which clusters contexts and events separately, is used to experiment with different ways of defining the context a language model takes into account. The contexts range from standard bigram and trigram contexts to part of speech five-grams. Although none of the models can compete directly with a backoff trigram, they give up to 9\\\\% improvement in perplexity when interpolated with a trigram. Moreover, the modified version of the algorithm leads to a performance increase over the original version of up to 12\\\\%. ',\n",
       " ' Morphological Cues for Lexical Semantics Most natural language processing tasks require lexical semantic information. Automated acquisition of this information would thus increase the robustness and portability of NLP systems. This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information. One advantage of this method, and of other methods that rely only on surface characteristics of language, is that the necessary input is currently available. ',\n",
       " ' Part-of-Speech-Tagging using morphological information This paper presents the results of an experiment to decide the question of authenticity of the supposedly spurious Rhesus - a attic tragedy sometimes credited to Euripides. The experiment involves use of statistics in order to test whether significant deviations in the distribution of word categories between Rhesus and the other works of Euripides can or cannot be found. To count frequencies of word categories in the corpus, a part-of-speech-tagger for Greek has been implemented. Some special techniques for reducing the problem of sparse data are used resulting in an accuracy of ca. 96.6%. ',\n",
       " ' Coordination in Tree Adjoining Grammars: Formalization and Implementation In this paper we show that an account for coordination can be constructed using the derivation structures in a lexicalized Tree Adjoining Grammar LTAG. We present a notion of derivation in LTAGs that preserves the notion of fixed constituency in the LTAG lexicon while providing the flexibility needed for coordination phenomena. We also discuss the construction of a practical parser for LTAGs that can handle coordination including cases of non-constituent coordination. ',\n",
       " ' Coordination as a Direct Process We propose a treatment of coordination based on the concepts of functor, argument and subcategorization. Its formalization comprises two parts which are conceptually independent. On one hand, we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements. On the other hand, we have considered the conjunction {\\\\em et (and)} as the head of the coordinate structure, so that coordinate structures stem simply from the subcategorization specifications of {\\\\em et} and the general schemata of a head saturation. Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended. ',\n",
       " ' Word Sense Disambiguation using Conceptual Density This paper presents a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus. ',\n",
       " ' Modularizing Contexted Constraints This paper describes a method for compiling a constraint-based grammar into a potentially more efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possible by determining their independence. When a group of dependent disjunctions is split into smaller groups, an exponential amount of redundant information is reduced. At runtime, this means that an exponential amount of processing can be saved as well. Since the performance of an algorithm for processing constraints with dependent disjunctions is highly determined by its input, the transformation presented in this paper should prove beneficial for all such algorithms. ',\n",
       " ' An Empirical Study of Smoothing Techniques for Language Modeling We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. ',\n",
       " ' An Efficient Inductive Unsupervised Semantic Tagger We report our development of a simple but fast and efficient inductive unsupervised semantic tagger for Chinese words. A POS hand-tagged corpus of 348,000 words is used. The corpus is being tagged in two steps. First, possible semantic tags are selected from a semantic dictionary(Tong Yi Ci Ci Lin), the POS and the conditional probability of semantic from POS, i.e., P(S|P). The final semantic tag is then assigned by considering the semantic tags before and after the current word and the semantic-word conditional probability P(S|W) derived from the first step. Semantic bigram probabilities P(S|S) are used in the second step. Final manual checking shows that this simple but efficient algorithm has a hit rate of 91%. The tagger tags 142 words per second, using a 120 MHz Pentium running FOXPRO. It runs about 2.3 times faster than a Viterbi tagger. ',\n",
       " ' Building Probabilistic Models for Natural Language In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure. ',\n",
       " ' Stabilizing the Richardson Algorithm by Controlling Chaos By viewing the operations of the Richardson purification algorithm as a discrete time dynamical process, we propose a method to overcome the instability of the algorithm by controlling chaos. We present theoretical analysis and numerical results on the behavior and performance of the stabilized algorithm. ',\n",
       " ' Compilation of Weighted Finite-State Transducers from Decision Trees We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). ',\n",
       " ' With raised eyebrows or the eyebrows raised ? A Neural Network Approach to Grammar Checking for Definiteness In this paper, we use a feature model of the semantics of plural determiners to present an approach to grammar checking for definiteness. Using neural network techniques, a semantics -- morphological category mapping was learned. We then applied a textual encoding technique to the 125 occurences of the relevant category in a 10 000 word narrative text and learned a surface -- semantics mapping. By applying the learned generation function to the newly generated representations, we achieved a correct category assignment in many cases (87 %). These results are considerably better than a direct surface categorization approach (54 %), with a baseline (always guessing the dominant category) of 60 %. It is discussed, how these results could be used in multilingual NLP applications. ',\n",
       " ' An Iterative Algorithm to Build Chinese Language Models We present an iterative procedure to build a Chinese language model LM. We segment Chinese text into words based on a word-based Chinese language model. However, the construction of a Chinese LM itself requires word boundaries. To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM. Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus. ',\n",
       " ' Computational Complexity of Probabilistic Disambiguation by means of Tree-Grammars This paper studies the computational complexity of disambiguation under probabilistic tree-grammars and context-free grammars. It presents a proof that the following problems are NP-hard: computing the Most Probable Parse MPP from a sentence or from a word-graph, and computing the Most Probable Sentence MPS from a word-graph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars. Consequently, the existence of deterministic polynomial-time algorithms for solving these disambiguation problems is a highly improbable event. ',\n",
       " \" Computing Optimal Descriptions for Optimality Theory Grammars with Context-Free Position Structures This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures. This algorithm extends Tesar's dynamic programming approach [Tesar 1994][Tesar 1995] to computing optimal structural descriptions from regular to context-free structures. The generalization to context-free structures creates several complications, all of which are overcome without compromising the core dynamic programming approach. The resulting algorithm has a time complexity cubic in the length of the input, and is applicable to grammars with universal constraints that exhibit context-free locality. \",\n",
       " ' Two Questions about Data-Oriented Parsing In this paper I present ongoing work on the data-oriented parsing DOP model. In previous work, DOP was tested on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank, achieving excellent test results. This left, however, two important questions unanswered: (1) how does DOP perform if tested on unedited data, and (2) how can DOP be used for parsing word strings that contain unknown words? This paper addresses these questions. We show that parse results on unedited data are worse than on cleaned-up data, although very competitive if compared to other models. As to the parsing of word strings, we show that the hardness of the problem does not so much depend on unknown words, but on previously unseen lexical categories of known words. We give a novel method for parsing these words by estimating the probabilities of unknown subtrees. The method is of general interest since it shows that good performance can be obtained without the use of a part-of-speech tagger. To the best of our knowledge, our method outperforms other statistical parsers tested on Penn Treebank word strings. ',\n",
       " ' A Data-Oriented Approach to Semantic Interpretation In Data-Oriented Parsing DOP, an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new input sentence is constructed by combining sub-analyses from the corpus in the most probable way. This approach has been succesfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Treebank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method, and summarizes the results of a preliminary experiment. Semantic annotations were added to the syntactic annotations of most of the sentences of the ATIS corpus. A data-oriented semantic interpretation algorithm was succesfully tested on this semantically enriched corpus. ',\n",
       " ' A Robust System for Natural Spoken Dialogue This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training. ',\n",
       " ' An Efficient Compiler for Weighted Rewrite Rules Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this. ',\n",
       " \" Two Sources of Control over the Generation of Software Instructions This paper presents an analysis conducted on a corpus of software instructions in French in order to establish whether task structure elements (the procedural representation of the users' tasks) are alone sufficient to control the grammatical resources of a text generator. We show that the construct of genre provides a useful additional source of control enabling us to resolve undetermined cases. \",\n",
       " ' Linguistic Structure as Composition and Perturbation This paper discusses the problem of learning language from unprocessed text and speech signals, concentrating on the problem of learning a lexicon. In particular, it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters. The power of this representation is demonstrated by several examples in text segmentation and compression, acquisition of a lexicon from raw speech, and the acquisition of mappings between text and artificial representations of meaning. ',\n",
       " ' Maximizing Top-down Constraints for Unification-based Systems A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems. However, due to the nontermination of parsing with left-recursive grammars, top-down constraints must be weakened. In this paper, a general method of maximizing top-down constraints is proposed. The method provides a procedure to dynamically compute *restrictor*, a minimum set of features involved in an infinite loop for every propagation path; thus top-down constraints are maximally propagated. ',\n",
       " \" Relating Turing's Formula and Zipf's Law An asymptote is derived from Turing's local reestimation formula for population frequencies, and a local reestimation formula is derived from Zipf's law for the asymptotic behavior of population frequencies. The two are shown to be qualitatively different asymptotically, but nevertheless to be instances of a common class of reestimation-formula-asymptote pairs, in which they constitute the upper and lower bounds of the convergence region of the cumulative of the frequency function, as rank tends to infinity. The results demonstrate that Turing's formula is qualitatively different from the various extensions to Zipf's law, and suggest that it smooths the frequency estimates towards a geometric distribution. \",\n",
       " \" Directed Replacement This paper introduces to the finite-state calculus a family of directed replace operators. In contrast to the simple replace expression, UPPER -> LOWER, defined in Karttunen (ACL-95), the new directed version, UPPER @-> LOWER, yields an unambiguous transducer if the lower language consists of a single string. It transduces the input string from left to right, making only the longest possible replacement at each point. A new type of replacement expression, UPPER @-> PREFIX ... SUFFIX, yields a transducer that inserts text around strings that are instances of UPPER. The symbol ... denotes the matching part of the input which itself remains unchanged. PREFIX and SUFFIX are regular expressions describing the insertions. Expressions of the type UPPER @-> PREFIX ... SUFFIX may be used to compose a deterministic parser for a ``local grammar'' in the sense of Gross (1989). Other useful applications of directed replacement include tokenization and filtering of text streams. \",\n",
       " ' Minimizing Manual Annotation Cost In Supervised Training From Corpora Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotation cost by {\\\\it sample selection}. In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous work on {\\\\it committee-based sample selection} for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. ',\n",
       " ' Context-Sensitive Measurement of Word Distance by Adaptive Scaling of a Semantic Space The paper proposes a computationally feasible method for measuring context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multi-dimensional vector which is obtained from an English dictionary through a principal component analysis. Given a word set C which specifies a context for measuring word distance, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, distance between words in V becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. ',\n",
       " \" Research on Architectures for Integrated Speech/Language Systems in Verbmobil The German joint research project Verbmobil VM aims at the development of a speech to speech translation system. This paper reports on research done in our group which belongs to Verbmobil's subproject on system architectures (TP15). Our specific research areas are the construction of parsers for spontaneous speech, investigations in the parallelization of parsing and to contribute to the development of a flexible communication architecture with distributed control. \",\n",
       " ' Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach In this paper, we present a new approach for word sense disambiguation WSD using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named {\\\\sc Lexas}, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. {\\\\sc Lexas} achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of {\\\\sc WordNet}. ',\n",
       " ' GramCheck: A Grammar and Style Checker This paper presents a grammar and style checker demonstrator for Spanish and Greek native writers developed within the project GramCheck. Besides a brief grammar error typology for Spanish, a linguistically motivated approach to detection and diagnosis is presented, based on the generalized use of PROLOG extensions to highly typed unification-based grammars. The demonstrator, currently including full coverage for agreement errors and certain head-argument relation issues, also provides correction by means of an analysis-transfer-synthesis cycle. Finally, future extensions to the current system are discussed. ',\n",
       " ' Inducing Constraint Grammars Constraint Grammar rules are induced from corpora. A simple scheme based on local information, i.e., on lexical biases and next-neighbour contexts, extended through the use of barriers, reached 87.3 percent precision (1.12 tags/word) at 98.2 percent recall. The results compare favourably with other methods that are used for similar tasks although they are by no means as good as the results achieved using the original hand-written rules developed over several years time. ',\n",
       " ' Domain and Language Independent Feature Extraction for Statistical Text Categorization A generic system for text categorization is presented which uses a representative text corpus to adapt the processing steps: feature extraction, dimension reduction, and classification. Feature extraction automatically learns features from the corpus by reducing actual word forms using statistical information of the corpus and general linguistic knowledge. The dimension of feature vector is then reduced by linear transformation keeping the essential information. The classification principle is a minimum least square approach based on polynomials. The described system can be readily adapted to new domains or new languages. In application, the system is reliable, fast, and processes completely automatically. It is shown that the text categorizer works successfully both on text generated by document image analysis - DIA and on ground truth data. ',\n",
       " ' Integrating Syntactic and Prosodic Information for the Efficient Detection of Empty Categories We describe a number of experiments that demonstrate the usefulness of prosodic information for a processing module which parses spoken utterances with a feature-based grammar employing empty categories. We show that by requiring certain prosodic properties from those positions in the input where the presence of an empty category has to be hypothesized, a derivation can be accomplished more efficiently. The approach has been implemented in the machine translation project VERBMOBIL and results in a significant reduction of the work-load for the parser. ',\n",
       " ' An Information Structural Approach to Spoken Language Generation This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation. A two-tiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient, coherent speech that makes certain discourse relationships, such as explicit contrasts, appropriately salient. The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions. ',\n",
       " \" Head Automata and Bilingual Tiling: Translation with Minimal Representations We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases. The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations. We also present a model and algorithm for machine translation involving optimal ``tiling'' of a dependency tree with entries of a costed bilingual lexicon. Experimental results are reported comparing methods for assigning cost functions to these models. We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation. \",\n",
       " \" Head Automata for Speech Translation This paper presents statistical language and translation models based on collections of small finite state machines we call ``head automata''. The models are intended to capture the lexical sensitivity of N-gram models and direct statistical translation models, while at the same time taking account of the hierarchical phrasal structure of language. Two types of head automata are defined: relational head automata suitable for translation by transfer of dependency trees, and head transducers suitable for direct recursive lexical translation. \",\n",
       " ' From Submit to Submitted via Submission: On Lexical Rules in Large-Scale Lexicon Acquisition This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition. The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora. We show that, though the use of LRs is justified, they do not come cost-free. Semi-automatic output checking is required, even with blocking and preemtion procedures built in. Nevertheless, large-scope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition. We also argue that the place of LRs in the computational process is a complex issue. ',\n",
       " ' Efficient Implementation of a Semantic-based Transfer Approach This article gives an overview of a new semantic-based transfer approach developed and applied within the Verbmobil Machine Translation project. We present the declarative transfer formalism and discuss its implementation. ',\n",
       " ' Semantic-based Transfer This article presents a new semantic-based transfer approach developed and applied within the Verbmobil Machine Translation project. We give an overview of the declarative transfer formalism together with its procedural realization. Our approach is discussed and compared with several other approaches from the MT literature. ',\n",
       " ' Learning similarity-based word sense disambiguation from sparse data We describe a method for automatic word sense disambiguation using a text corpus and a machine-readable dictionary MRD. The method is based on word similarity and context similarity measures. Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words. The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD. A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context. Experiments show that this method performs well, and can learn even from very sparse training data. ',\n",
       " ' MBT: A Memory-Based Part of Speech Tagger-Generator We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using {\\\\em IGTree}, a tree-based formalism for indexing and searching huge case bases.} The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed. ',\n",
       " \" Pattern-Based Context-Free Grammars for Machine Translation This paper proposes the use of ``pattern-based'' context-free grammars as a basis for building machine translation MT systems, which are now being adopted as personal tools by a broad range of users in the cyberspace society. We discuss major requirements for such tools, including easy customization for diverse domains, the efficiency of the translation algorithm, and scalability (incremental improvement in translation quality through user interaction), and describe how our approach meets these requirements. \",\n",
       " ' Unsupervised Discovery of Phonological Categories through Supervised Learning of Morphological Rules We describe a case study in the application of {\\\\em symbolic machine learning} techniques for the discovery of linguistic rules and categories. A supervised rule induction algorithm is used to learn to predict the correct diminutive suffix given the phonological representation of Dutch nouns. The system produces rules which are comparable to rules proposed by linguists. Furthermore, in the process of learning this morphological task, the phonemes used are grouped into phonologically relevant categories. We discuss the relevance of our method for linguistics and language technology. ',\n",
       " ' A Corpus Study of Negative Imperatives in Natural Language Instructions In this paper, we define the notion of a preventative expression and discuss a corpus study of such expressions in instructional text. We discuss our coding schema, which takes into account both form and function features, and present measures of inter-coder reliability for those features. We then discuss the correlations that exist between the function and the form features. ',\n",
       " ' Learning Micro-Planning Rules for Preventative Expressions Building text planning resources by hand is time-consuming and difficult. Certainly, a number of planning architectures and their accompanying plan libraries have been implemented, but while the architectures themselves may be reused in a new domain, the library of plans typically cannot. One way to address this problem is to use machine learning techniques to automate the derivation of planning resources for new domains. In this paper, we apply this technique to build micro-planning rules for preventative expressions in instructional text. ',\n",
       " ' Beyond Word N-Grams We describe, analyze, and evaluate experimentally a new probabilistic model for word-sequence prediction in natural language based on prediction suffix trees (PSTs). By using efficient data structures, we extend the notion of PST to unbounded vocabularies. We also show how to use a Bayesian approach based on recursive priors over all possible PSTs to efficiently maintain tree mixtures. These mixtures have provably and practically better performance than almost any single model. We evaluate the model on several corpora. The low perplexity achieved by relatively small PST mixture models suggests that they may be an advantageous alternative, both theoretically and practically, to the widely used n-gram models. ',\n",
       " ' Natural Language Processing: Structure and Complexity We introduce a method for analyzing the complexity of natural language processing tasks, and for predicting the difficulty new NLP tasks. Our complexity measures are derived from the Kolmogorov complexity of a class of automata --- {\\\\it meaning automata}, whose purpose is to extract relevant pieces of information from sentences. Natural language semantics is defined only relative to the set of questions an automaton can answer. The paper shows examples of complexity estimates for various NLP programs and tasks, and some recipes for complexity management. It positions natural language processing as a subdomain of software engineering, and lays down its formal foundation. ',\n",
       " \" A Divide-and-Conquer Strategy for Parsing In this paper, we propose a novel strategy which is designed to enhance the accuracy of the parser by simplifying complex sentences before parsing. This approach involves the separate parsing of the constituent sub-sentences within a complex sentence. To achieve that, the divide-and-conquer strategy first disambiguates the roles of the link words in the sentence and segments the sentence based on these roles. The separate parse trees of the segmented sub-sentences and the noun phrases within them are then synthesized to form the final parse. To evaluate the effects of this strategy on parsing, we compare the original performance of a dependency parser with the performance when it is enhanced with the divide-and-conquer strategy. When tested on 600 sentences of the IPSM'95 data sets, the enhanced parser saw a considerable error reduction of 21.2% in its accuracy. \",\n",
       " ' Mental State Adjectives: the Perspective of Generative Lexicon This paper focusses on mental state adjectives and offers a unified analysis in the theory of Generative Lexicon (Pustejovsky, 1991, 1995). We show that, instead of enumerating the various syntactic constructions they enter into, with the different senses which arise, it is possible to give them a rich typed semantic representation which will explain both their semantic and syntactic polymorphism. ',\n",
       " ' TSNLP - Test Suites for Natural Language Processing The TSNLP project has investigated various aspects of the construction, maintenance and application of systematic test suites as diagnostic and evaluation tools for NLP applications. The paper summarizes the motivation and main results of the project: besides the solid methodological foundation, TSNLP has produced substantial multi-purpose and multi-user test suites for three European languages together with a set of specialized tools that facilitate the construction, extension, maintenance, retrieval, and customization of the test data. As TSNLP results, including the data and technology, are made publicly available, the project presents a valuable linguistic resourc e that has the potential of providing a wide-spread pre-standard diagnostic and evaluation tool for both developers and users of NLP applications. ',\n",
       " ' A Machine Learning Approach to the Classification of Dialogue Utterances The purpose of this paper is to present a method for automatic classification of dialogue utterances and the results of applying that method to a corpus. Superficial features of a set of training utterances (which we will call cues) are taken as the basis for finding relevant utterance classes and for extracting rules for assigning these classes to new utterances. Each cue is assumed to partially contribute to the communicative function of an utterance. Instead of relying on subjective judgments for the tasks of finding classes and rules, we opt for using machine learning techniques to guarantee objectivity. ',\n",
       " ' Morphological Analysis as Classification: an Inductive-Learning Approach Morphological analysis is an important subtask in text-to-speech conversion, hyphenation, and other language engineering tasks. The traditional approach to performing morphological analysis is to combine a morpheme lexicon, sets of (linguistic) rules, and heuristics to find a most probable analysis. In contrast we present an inductive learning approach in which morphological analysis is reformulated as a segmentation task. We report on a number of experiments in which five inductive learning algorithms are applied to three variations of the task of morphological analysis. Results show (i) that the generalisation performance of the algorithms is good, and (ii) that the lazy learning algorithm IB1-IG performs best on all three tasks. We conclude that lazy learning of morphological analysis as a classification task is indeed a viable approach; moreover, it has the strong advantages over the traditional approach of avoiding the knowledge-acquisition bottleneck, being fast and deterministic in learning and processing, and being language-independent. ',\n",
       " ' Phonological modeling for continuous speech recognition in Korean A new scheme to represent phonological changes during continuous speech recognition is suggested. A phonological tag coupled with its morphological tag is designed to represent the conditions of Korean phonological changes. A pairwise language model of these morphological and phonological tags is implemented in Korean speech recognition system. Performance of the model is verified through the TDNN-based speech recognition experiments. ',\n",
       " ' Applying Winnow to Context-Sensitive Spelling Correction Multiplicative weight-updating algorithms such as Winnow have been studied extensively in the COLT literature, but only recently have people started to use them in applications. In this paper, we apply a Winnow-based algorithm to a task in natural language: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting {\\\\it to\\\\/} for {\\\\it too}, {\\\\it casual\\\\/} for {\\\\it causal}, and so on. Previous approaches to this problem have been statistics-based; we compare Winnow to one of the more successful such approaches, which uses Bayesian classifiers. We find that: (1)~When the standard (heavily-pruned) set of features is used to describe problem instances, Winnow performs comparably to the Bayesian method; (2)~When the full (unpruned) set of features is used, Winnow is able to exploit the new features and convincingly outperform Bayes; and (3)~When a test set is encountered that is dissimilar to the training set, Winnow is better than Bayes at adapting to the unfamiliar test set, using a strategy we will present for combining learning on the training set with unsupervised learning on the (noisy) test set. ',\n",
       " \" New Methods, Current Trends and Software Infrastructure for NLP The increasing use of `new methods' in NLP, which the NeMLaP conference series exemplifies, occurs in the context of a wider shift in the nature and concerns of the discipline. This paper begins with a short review of this context and significant trends in the field. The review motivates and leads to a set of requirements for support software of general utility for NLP research and development workers. A freely-available system designed to meet these requirements is described (called GATE - a General Architecture for Text Engineering). Information Extraction IE, in the sense defined by the Message Understanding Conferences (ARPA \\\\cite{Arp95}), is an NLP application in which many of the new methods have found a home (Hobbs \\\\cite{Hob93}; Jacobs ed. \\\\cite{Jac92}). An IE system based on GATE is also available for research purposes, and this is described. Lastly we review related work. \",\n",
       " ' Building Knowledge Bases for the Generation of Software Documentation Automated text generation requires a underlying knowledge base from which to generate, which is often difficult to produce. Software documentation is one domain in which parts of this knowledge base may be derived automatically. In this paper, we describe \\\\drafter, an authoring support tool for generating user-centred software documentation, and in particular, we describe how parts of its required knowledge base can be obtained automatically. ',\n",
       " ' Learning Translation Rules From A Bilingual Corpus This paper proposes a mechanism for learning pattern correspondences between two languages from a corpus of translated sentence pairs. The proposed mechanism uses analogical reasoning between two translations. Given a pair of translations, the similar parts of the sentences in the source language must correspond the similar parts of the sentences in the target language. Similarly, the different parts should correspond to the respective parts in the translated sentences. The correspondences between the similarities, and also differences are learned in the form of translation rules. The system is tested on a small training dataset and produced promising results for further investigation. ',\n",
       " ' The Grammar of Sense: Is word-sense tagging much more than part-of-speech tagging? This squib claims that Large-scale Automatic Sense Tagging of text LAST can be done at a high-level of accuracy and with far less complexity and computational effort than has been believed until now. Moreover, it can be done for all open class words, and not just carefully selected opposed pairs as in some recent work. We describe two experiments: one exploring the amount of information relevant to sense disambiguation which is contained in the part-of-speech field of entries in Longman Dictionary of Contemporary English LDOCE. Another, more practical, experiment attempts sense disambiguation of all open class words in a text assigning LDOCE homographs as sense tags using only part-of-speech information. We report that 92% of open class words can be successfully tagged in this way. We plan to extend this work and to implement an improved large-scale tagger, a description of which is included here. ',\n",
       " ' A Lexical Semantic Database for Verbmobil This paper describes the development and use of a lexical semantic database for the Verbmobil speech-to-speech machine translation system. The motivation is to provide a common information source for the distributed development of the semantics, transfer and semantic evaluation modules and to store lexical semantic information application-independently. The database is organized around a set of abstract semantic classes and has been used to define the semantic contributions of the lemmata in the vocabulary of the system, to automatically create semantic lexica and to check the correctness of the semantic representations built up. The semantic classes are modelled using an inheritance hierarchy. The database is implemented using the lexicon formalism LeX4 developed during the project. ',\n",
       " \" Compositional Semantics in Verbmobil The paper discusses how compositional semantics is implemented in the Verbmobil speech-to-speech translation system using LUD, a description language for underspecified discourse representation structures. The description language and its formal interpretation in DRT are described as well as its implementation together with the architecture of the system's entire syntactic-semantic processing module. We show that a linguistically sound theory and formalism can be properly implemented in a system with (near) real-time requirements. \",\n",
       " ' Design and Implementation of a Tactical Generator for Turkish, a Free Constituent Order Language This thesis describes a tactical generator for Turkish, a free constituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University--Center for Machine Translation. Morphological realization has been implemented using an external morphological analysis/generation component which performs concrete morpheme selection and handles morphographemic processes. ',\n",
       " ' Multiple Discourse Relations on the Sentential Level in Japanese In the German government BMBF funded project Verbmobil, a semantic formalism Language for Underspecified Discourse Representation Structures LUD is used which describes several DRSs and allows for underspecification. Dealing with Japanese poses challenging problems. In this paper, a treatment of multiple discourse relation constructions on the sentential level is shown, which are common in Japanese but cause a problem for the formalism,. The problem is to distinguish discourse relations which take the widest scope compared with other scope-taking elements on the one hand and to have them underspecified among each other on the other hand. We also state a semantic constraint on the resolution of multiple discourse relations which seems to prevail over the syntactic c-command constraint. ',\n",
       " \" Using Multiple Sources of Information for Constraint-Based Morphological Disambiguation This thesis presents a constraint-based morphological disambiguation approach that is applicable to languages with complex morphology--specifically agglutinative languages with productive inflectional and derivational morphological phenomena. For morphologically complex languages like Turkish, automatic morphological disambiguation involves selecting for each token morphological parse(s), with the right set of inflectional and derivational markers. Our system combines corpus independent hand-crafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statistical information obtained from the corpus to be morphologically disambiguated. The hand-crafted rules are linguistically motivated and tuned to improve precision without sacrificing recall. In certain respects, our approach has been motivated by Brill's recent work, but with the observation that his transformational approach is not directly applicable to languages like Turkish. Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexical item whose root is unknown. With this approach, well below 1% of the tokens remains as unknown in the texts we have experimented with. Our results indicate that by combining these hand-crafted, statistical and learned information sources, we can attain a recall of 96 to 97% with a corresponding precision of 93 to 94%, and ambiguity of 1.02 to 1.03 parses per token. \",\n",
       " ' Using textual clues to improve metaphor processing In this paper, we propose a textual clue approach to help metaphor detection, in order to improve the semantic processing of this figure. The previous works in the domain studied the semantic regularities only, overlooking an obvious set of regularities. A corpus-based analysis shows the existence of surface regularities related to metaphors. These clues can be characterized by syntactic structures and lexical markers. We present an object oriented model for representing the textual clues that were found. This representation is designed to help the choice of a semantic processing, in terms of possible non-literal meanings. A prototype implementing this model is currently under development, within an incremental approach allowing step-by-step evaluations. \\\\footnote{This work takes part in a research project sponsored by the AUPELF-UREF (Francophone Agency For Education and Research)} ',\n",
       " ' A Hierarchy of Tractable Subsets for Computing Stable Models Finding the stable models of a knowledge base is a significant computational problem in artificial intelligence. This task is at the computational heart of truth maintenance systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes of knowledge bases, Omega_1,Omega_2,..., with the following properties: first, Omega_1 is the class of all stratified knowledge bases; second, if a knowledge base Pi is in Omega_k, then Pi has at most k stable models, and all of them may be found in time O(lnk), where l is the length of the knowledge base and n the number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find the minimum k such that Pi belongs to Omega_k in time polynomial in the size of Pi; and, last, where K is the class of all knowledge bases, it is the case that union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some class in the hierarchy. ',\n",
       " ' Automatic Construction of Clean Broad-Coverage Translation Lexicons Word-level translational equivalences can be extracted from parallel texts by surprisingly simple statistical techniques. However, these techniques are easily fooled by {\\\\em indirect associations} --- pairs of unrelated words whose statistical properties resemble those of mutual translations. Indirect associations pollute the resulting translation lexicons, drastically reducing their precision. This paper presents an iterative lexicon cleaning method. On each iteration, most of the remaining incorrect lexicon entries are filtered out, without significant degradation in recall. This lexicon cleaning technique can produce translation lexicons with recall and precision both exceeding 90\\\\%, as well as dictionary-sized translation lexicons that are over 99\\\\% correct. ',\n",
       " ' Completeness of Compositional Translation for Context-Free Grammars A machine translation system is said to be *complete* if all expressions that are correct according to the source-language grammar can be translated into the target language. This paper addresses the completeness issue for compositional machine translation in general, and for compositional machine translation of context-free grammars in particular. Conditions that guarantee translation completeness of context-free grammars are presented. ',\n",
       " ' Connected Text Recognition Using Layered HMMs and Token Passing We present a novel approach to lexical error recovery on textual input. An advanced robust tokenizer has been implemented that can not only correct spelling mistakes, but also recover from segmentation errors. Apart from the orthographic considerations taken, the tokenizer also makes use of linguistic expectations extracted from a training corpus. The idea is to arrange Hidden Markov Models HMM in multiple layers where the HMMs in each layer are responsible for different aspects of the processing of the input. We report on experimental evaluations with alternative probabilistic language models to guide the lexical error recovery process. ',\n",
       " ' Spatial Aggregation: Theory and Applications Visual thinking plays an important role in scientific reasoning. Based on the research in automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and fluid motion, we have identified a style of visual thinking, imagistic reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer structure and behavior. Programs incorporating imagistic reasoning have been shown to perform at an expert level in domains that defy current analytic or numerical methods. We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the following properties. It takes a continuous field and optional objective functions as input, and produces high-level descriptions of structure, behavior, or control actions. It computes a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such as aggregation, classification, and localization to perform bidirectional mapping between the information-rich field and successively more abstract spatial aggregates. It uses a data structure, the neighborhood graph, as a common interface to modularize computations. To illustrate our theory, we describe the computational structure of three implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the spatial aggregation generic operators by mixing and matching a library of commonly used routines. ',\n",
       " \" Storage of Natural Language Sentences in a Hopfield Network This paper look at how the Hopfield neural network can be used to store and recall patterns constructed from natural language sentences. As a pattern recognition and storage tool, the Hopfield neural network has received much attention. This attention however has been mainly in the field of statistical physics due to the model's simple abstraction of spin glass systems. A discussion is made of the differences, shown as bias and correlation, between natural language sentence patterns and the randomly generated ones used in previous experiments. Results are given for numerical simulations which show the auto-associative competence of the network when trained with natural language patterns. \",\n",
       " ' Controlling Functional Uncertainty There have been two different methods for checking the satisfiability of feature descriptions that use the functional uncertainty device, namely~\\\\cite{Kaplan:88CO} and \\\\cite{Backofen:94JSC}. Although only the one in \\\\cite{Backofen:94JSC} solves the satisfiability problem completely, both methods have their merits. But it may happen that in one single description, there are parts where the first method is more appropriate, and other parts where the second should be applied. In this paper, we present a common framework that allows one to combine both methods. This is done by presenting a set of rules for simplifying feature descriptions. The different methods are described as different controls on this rule set, where a control specifies in which order the different rules must be applied. ',\n",
       " ' Stylistic Variation in an Information Retrieval Experiment Texts exhibit considerable stylistic variation. This paper reports an experiment where a corpus of documents (N= 75 000) is analyzed using various simple stylistic metrics. A subset (n = 1000) of the corpus has been previously assessed to be relevant for answering given information retrieval queries. The experiment shows that this subset differs significantly from the rest of the corpus in terms of the stylistic metrics studied. ',\n",
       " ' Patterns of Language - A Population Model for Language Structure A key problem in the description of language structure is to explain its contradictory properties of specificity and generality, the contrasting poles of formulaic prescription and generative productivity. I argue that this is possible if we accept analogy and similarity as the basic mechanisms of structural definition. As a specific example I discuss how it would be possible to use analogy to define a generative model of syntactic structure. ',\n",
       " ' Parallel Replacement in Finite State Calculus This paper extends the calculus of regular expressions with new types of replacement expressions that enhance the expressiveness of the simple replace operator defined in Karttunen (1995). Parallel replacement allows multiple replacements to apply simultaneously to the same input without interfering with each other. We also allow a replacement to be constrained by any number of alternative contexts. With these enhancements, the general replacement expressions are more versatile than two-level rules for the description of complex morphological alternations. ',\n",
       " ' CLEARS - An Education and Research Tool for Computational Semantics The CLEARS (Computational Linguistics Education and Research for Semantics) tool provides a graphical interface allowing interactive construction of semantic representations in a variety of different formalisms, and using several construction methods. CLEARS was developed as part of the FraCaS project which was designed to encourage convergence between different semantic formalisms, such as Montague-Grammar, DRT, and Situation Semantics. The CLEARS system is freely available on the WWW from http://coli.uni-sb.de/~clears/clears.html ',\n",
       " ' Centering in Italian This paper explores the correlation between centering and different forms of pronominal reference in Italian, in particular zeros and overt pronouns in subject position. Such correlations, that I had proposed in earlier work (COLING 90), are verified through the analysis of a corpus of naturally occurring texts. In the process, I extend my previous analysis in several ways, for example by taking possessives and subordinates into account. I also provide a more detailed analysis of the continue transition: more specifically, I show that pronouns are used in a markedly different way in a continue preceded by another continue or by a shift , and in a continue preceded by a retain . ',\n",
       " ' Centering theory and the Italian pronominal system In this paper, I give an account of some phenomena of pronominalization in Italian in terms of centering theory. After a general introduction to the Italian pronominal system, I will review centering, and then show how the original rules have to be extended or modified. Finally, I will show that centering does not account for two phenomena: first, the functional role of an utterance may override the predictions of centering; second, a null subject can be used to refer to a whole discourse segment. ',\n",
       " \" Grapheme-to-Phoneme Conversion using Multiple Unbounded Overlapping Chunks We present in this paper an original extension of two data-driven algorithms for the transcription of a sequence of graphemes into the corresponding sequence of phonemes. In particular, our approach generalizes the algorithm originally proposed by Dedina and Nusbaum (D&N) (1991), which had originally been promoted as a model of the human ability to pronounce unknown words by analogy to familiar lexical items. We will show that DN's algorithm performs comparatively poorly when evaluated on a realistic test set, and that our extension allows us to improve substantially the performance of the analogy-based model. We will also suggest that both algorithms can be reformulated in a much more general framework, which allows us to anticipate other useful extensions. However, considering the inability to define in these models important notions like lexical neighborhood, we conclude that both approaches fail to offer a proper model of the analogical processes involved in reading aloud. \",\n",
       " \" Limited Attention and Discourse Structure This squib examines the role of limited attention in a theory of discourse structure and proposes a model of attentional state that relates current hierarchical theories of discourse structure to empirical evidence about human discourse processing capabilities. First, I present examples that are not predicted by Grosz and Sidner's stack model of attentional state. Then I consider an alternative model of attentional state, the cache model, which accounts for the examples, and which makes particular processing predictions. Finally I suggest a number of ways that future research could distinguish the predictions of the cache model and the stack model. \",\n",
       " ' The discourse functions of Italian subjects: a centering approach This paper examines the discourse functions that different types of subjects perform in Italian within the centering framework. I build on my previous work (COLING90) that accounted for the alternation of null and strong pronouns in subject position. I extend my previous analysis in several ways: for example, I refine the notion of {\\\\sc continue} and discuss the centering functions of full NPs. ',\n",
       " \" Fishing for Exactness Statistical methods for automatically identifying dependent word pairs (i.e. dependent bigrams) in a corpus of natural language text have traditionally been performed using asymptotic tests of significance. This paper suggests that Fisher's exact test is a more appropriate test due to the skewed and sparse data samples typical of this problem. Both theoretical and experimental comparisons between Fisher's exact test and a variety of asymptotic tests (the t-test, Pearson's chi-square test, and Likelihood-ratio chi-square test) are presented. These comparisons show that Fisher's exact test is more reliable in identifying dependent word pairs. The usefulness of Fisher's exact test extends to other problems in statistical natural language processing as skewed and sparse data appears to be the rule in natural language. The experiment presented in this paper was performed using PROC FREQ of the SAS System. \",\n",
       " ' Punctuation in Quoted Speech Quoted speech is often set off by punctuation marks, in particular quotation marks. Thus, it might seem that the quotation marks would be extremely useful in identifying these structures in texts. Unfortunately, the situation is not quite so clear. In this work, I will argue that quotation marks are not adequate for either identifying or constraining the syntax of quoted speech. More useful information comes from the presence of a quoting verb, which is either a verb of saying or a punctual verb, and the presence of other punctuation marks, usually commas. Using a lexicalized grammar, we can license most quoting clauses as text adjuncts. A distinction will be made not between direct and indirect quoted speech, but rather between adjunct and non-adjunct quoting clauses. ',\n",
       " ' Multilingual Text Analysis for Text-to-Speech Synthesis We present a model of text analysis for text-to-speech TTS synthesis based on (weighted) finite-state transducers, which serves as the text-analysis module of the multilingual Bell Labs TTS system. The transducers are constructed using a lexical toolkit that allows declarative descriptions of lexicons, morphological rules, numeral-expansion rules, and phonological rules, inter alia. To date, the model has been applied to eight languages: Spanish, Italian, Romanian, French, German, Russian, Mandarin and Japanese. ',\n",
       " ' A Word Grammar of Turkish with Morphophonemic Rules In this thesis, morphological description of Turkish is encoded using the two-level model. This description is made up of the phonological component that contains the two-level morphophonemic rules, and the lexicon component which lists the lexical items and encodes the morphotactic constraints. The word grammar is expressed in tabular form. It includes the verbal and the nominal paradigm. Vowel and consonant harmony, epenthesis, reduplication, etc. are described in detail and coded in two-level notation. Loan-word phonology is modelled separately. The implementation makes use of Lexc/Twolc from Xerox. Mechanisms to integrate the morphological analyzer with the lexical and syntactic components are discussed, and a simple graphical user interface is provided. Work is underway to use this model in a classroom setting for teaching Turkish morphology to non-native speakers. ',\n",
       " ' Classifiers in Japanese-to-English Machine Translation This paper proposes an analysis of classifiers into four major types: UNIT, METRIC, GROUP and SPECIES, based on properties of both Japanese and English. The analysis makes possible a uniform and straightforward treatment of noun phrases headed by classifiers in Japanese-to-English machine translation, and has been implemented in the MT system ALT-J/E. Although the analysis is based on the characteristics of, and differences between, Japanese and English, it is shown to be also applicable to the unrelated language Thai. ',\n",
       " \" Shellsort with three increments A perturbation technique can be used to simplify and sharpen A. C. Yao's theorems about the behavior of shellsort with increments . In particular, when and , the average running time is . The proof involves interesting properties of the inversions in random permutations that have been sorted and sorted. \",\n",
       " ' Morphological Productivity in the Lexicon In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections, derivations, and lexical category changes to control the proliferation of lexical entries. Lexical rules handle changes in grammatical roles, enforce type constraints, and control the mapping of subcategorization frames in valency-changing operations. A lexical inheritance hierarchy facilitates the enforcement of type constraints. Semantic compositions in inflections and derivations are constrained by the properties of the terms and predicates. The design has been tested as part of a HPSG grammar for Turkish. In terms of performance, run-time execution of the rules seems to be a far better alternative than pre-compilation. The latter causes exponential growth in the lexicon due to intensive use of inflections and derivations in Turkish. ',\n",
       " ' Automatic Alignment of English-Chinese Bilingual Texts of CNS News In this paper we address a method to align English-Chinese bilingual news reports from China News Service, combining both lexical and satistical approaches. Because of the sentential structure differences between English and Chinese, matching at the sentence level as in many other works may result in frequent matching of several sentences en masse. In view of this, the current work also attempts to create shorter alignment pairs by permitting finer matching between clauses from both texts if possible. The current method is based on statiscal correlation between sentence or clause length of both texts and at the same time uses obvious anchors such as numbers and place names appearing frequently in the news reports as lexcial cues. ',\n",
       " ' A Sign-Based Phrase Structure Grammar for Turkish This study analyses Turkish syntax from an informational point of view. Sign based linguistic representation and principles of HPSG (Head-driven Phrase Structure Grammar) theory are adapted to Turkish. The basic informational elements are nested and inherently sorted feature structures called signs. In the implementation, logic programming tool ALE (Attribute Logic Engine) which is primarily designed for implementing HPSG grammars is used. A type and structure hierarchy of Turkish language is designed. Syntactic phenomena such a s subcategorization, relative clauses, constituent order variation, adjuncts, nomina l predicates and complement-modifier relations in Turkish are analyzed. A parser is designed and implemented in ALE. ',\n",
       " ' Isolated-Word Confusion Metrics and the PGPfone Alphabet Although the confusion of individual phonemes and features have been studied and analyzed since (Miller and Nicely, 1955), there has been little work done on extending this to a predictive theory of word-level confusions. The PGPfone alphabet is a good touchstone problem for developing such word-level confusion metrics. This paper presents some difficulties incurred, along with their proposed solutions, in the extension of phonetic confusion results to a theoretical whole-word phonetic distance metric. The proposed solutions have been used, in conjunction with a set of selection filters, in a genetic algorithm to automatically generate appropriate word lists for a radio alphabet. This work illustrates some principles and pitfalls that should be addressed in any numeric theory of isolated word perception. ',\n",
       " \" Phonetic Ambiguity : Approaches, Touchstones, Pitfalls and New Approaches Phonetic ambiguity and confusibility are bugbears for any form of bottom-up or data-driven approach to language processing. The question of when an input is ``close enough'' to a target word pervades the entire problem spaces of speech recognition, synthesis, language acquisition, speech compression, and language representation, but the variety of representations that have been applied are demonstrably inadequate to at least some aspects of the problem. This paper reviews this inadequacy by examining several touchstone models in phonetic ambiguity and relating them to the problems they were designed to solve. An good solution would be, among other things, efficient, accurate, precise, and universally applicable to representation of words, ideally usable as a ``phonetic distance'' metric for direct measurement of the ``distance'' between word or utterance pairs. None of the proposed models can provide a complete solution to the problem; in general, there is no algorithmic theory of phonetic distance. It is unclear whether this is a weakness of our representational technology or a more fundamental difficulty with the problem statement. \",\n",
       " \" Using sentence connectors for evaluating MT output This paper elaborates on the design of a machine translation evaluation method that aims to determine to what degree the meaning of an original text is preserved in translation, without looking into the grammatical correctness of its constituent sentences. The basic idea is to have a human evaluator take the sentences of the translated text and, for each of these sentences, determine the semantic relationship that exists between it and the sentence immediately preceding it. In order to minimise evaluator dependence, relations between sentences are expressed in terms of the conjuncts that can connect them, rather than through explicit categories. For an n-sentence text this results in a list of n-1 sentence-to-sentence relationships, which we call the text's connectivity profile. This can then be compared to the connectivity profile of the original text, and the degree of correspondence between the two would be a measure for the quality of the translation. A set of essential conjuncts was extracted for English and Japanese, and a computer interface was designed to support the task of inserting the most fitting conjuncts between sentence pairs. With these in place, several sets of experiments were performed. \",\n",
       " \" Accelerating Partial-Order Planners: Some Techniques for Effective Search Control and Pruning We propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality. The first two techniques are aimed at improving search control while keeping overhead costs low. One is based on a simple adjustment to the default A* heuristic used by UCPOP to select plans for refinement. The other is based on preferring ``zero commitment'' (forced) plan refinements whenever possible, and using LIFO prioritization otherwise. A more radical technique is the use of operator parameter domains to prune search. These domains are initially computed from the definitions of the operators and the initial and goal conditions, using a polynomial-time algorithm that propagates sets of constants through the operator graph, starting in the initial conditions. During planning, parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats. In experiments based on modifications of UCPOP, our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems gave the greatest improvements. The pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems, both with the default UCPOP search strategy and with our improved strategy. The Lisp code for our techniques and for the test problems is provided in on-line appendices. \",\n",
       " ' Cue Phrase Classification Using Machine Learning Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods. ',\n",
       " ' Corrections and Higher-Order Unification We propose an analysis of corrections which models some of the requirements corrections place on context. We then show that this analysis naturally extends to the interaction of corrections with pronominal anaphora on the one hand, and (in)definiteness on the other. The analysis builds on previous unification--based approaches to NL semantics and relies on Higher--Order Unification with Equivalences, a form of unification which takes into account not only syntactic beta-eta-identity but also denotational equivalence. ',\n",
       " ' Inferring Acceptance and Rejection in Dialogue by Default Rules of Inference This paper discusses the processes by which conversants in a dialogue can infer whether their assertions and proposals have been accepted or rejected by their conversational partners. It expands on previous work by showing that logical consistency is a necessary indicator of acceptance, but that it is not sufficient, and that logical inconsistency is sufficient as an indicator of rejection, but it is not necessary. I show how conversants can use information structure and prosody as well as logical reasoning in distinguishing between acceptances and logically consistent rejections, and relate this work to previous work on implicature and default reasoning by introducing three new classes of rejection: {\\\\sc implicature rejections}, {\\\\sc epistemic rejections} and {\\\\sc deliberation rejections}. I show how these rejections are inferred as a result of default inferences, which, by other analyses, would have been blocked by the context. In order to account for these facts, I propose a model of the common ground that allows these default inferences to go through, and show how the model, originally proposed to account for the various forms of acceptance, can also model all types of rejection. ',\n",
       " ' Cue Phrase Classification Using Machine Learning Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods. ',\n",
       " ' Conceptual Association for Compound Noun Analysis This paper describes research toward the automatic interpretation of compound nouns using corpus statistics. An initial study aimed at syntactic disambiguation is presented. The approach presented bases associations upon thesaurus categories. Association data is gathered from unambiguous cases extracted from a corpus and is then applied to the analysis of ambiguous compound nouns. While the work presented is still in progress, a first attempt to syntactically analyse a test set of 244 examples shows 75% correctness. Future work is aimed at improving this accuracy and extending the technique to assign semantic role information, thus producing a complete interpretation. ',\n",
       " ' Corpus Statistics Meet the Noun Compound: Some Empirical Results A variety of statistical methods for noun compound analysis are implemented and compared. The results support two main conclusions. First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy. Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature. ',\n",
       " ' Algorithms for Speech Recognition and Language Processing Speech processing requires very efficient methods and algorithms. Finite-state transducers have been shown recently both to constitute a very useful abstract model and to lead to highly efficient time and space algorithms in this field. We present these methods and algorithms and illustrate them in the case of speech recognition. In addition to classical techniques, we describe many new algorithms such as minimization, global and local on-the-fly determinization of weighted automata, and efficient composition of transducers. These methods are currently used in large vocabulary speech recognition systems. We then show how the same formalism and algorithms can be used in text-to-speech applications and related areas of language processing such as morphology, syntax, and local grammars, in a very efficient way. The tutorial is self-contained and requires no specific computational or linguistic knowledge other than classical results. ',\n",
       " \" A Principled Framework for Constructing Natural Language Interfaces To Temporal Databases Most existing natural language interfaces to databases (NLIDBs) were designed to be used with ``snapshot'' database systems, that provide very limited facilities for manipulating time-dependent data. Consequently, most NLIDBs also provide very limited support for the notion of time. The database community is becoming increasingly interested in _temporal_ database systems. These are intended to store and manipulate in a principled manner information not only about the present, but also about the past and future. This thesis develops a principled framework for constructing English NLIDBs for _temporal_ databases (NLITDBs), drawing on research in tense and aspect theories, temporal logics, and temporal databases. I first explore temporal linguistic phenomena that are likely to appear in English questions to NLITDBs. Drawing on existing linguistic theories of time, I formulate an account for a large number of these phenomena that is simple enough to be embodied in practical NLITDBs. Exploiting ideas from temporal logics, I then define a temporal meaning representation language, TOP, and I show how the HPSG grammar theory can be modified to incorporate the tense and aspect account of this thesis, and to map a wide range of English questions involving time to appropriate TOP expressions. Finally, I present and prove the correctness of a method to translate from TOP to TSQL2, TSQL2 being a temporal extension of the SQL-92 database language. This way, I establish a sound route from English questions involving time to a general-purpose temporal database language, that can act as a principled framework for building NLITDBs. To demonstrate that this framework is workable, I employ it to develop a prototype NLITDB, implemented using ALE and Prolog. \",\n",
       " ' Centering in Japanese Discourse In this paper we propose a computational treatment of the resolution of zero pronouns in Japanese discourse, using an adaptation of the centering algorithm. We are able to factor language-specific dependencies into one parameter of the centering algorithm. Previous analyses have stipulated that a zero pronoun and its cospecifier must share a grammatical function property such as {\\\\sc Subject} or {\\\\sc NonSubject}. We show that this property-sharing stipulation is unneeded. In addition we propose the notion of {\\\\sc topic ambiguity} within the centering framework, which predicts some ambiguities that occur in Japanese discourse. This analysis has implications for the design of language-independent discourse modules for Natural Language systems. The centering algorithm has been implemented in an HPSG Natural Language system with both English and Japanese grammars. ',\n",
       " ' Discourse Coherence and Shifting Centers in Japanese Texts In languages such as Japanese, the use of {\\\\it zeros}, unexpressed arguments of the verb, in utterances that shift the topic involves a risk that the meaning intended by the speaker may not be transparent to the hearer. However, this potentially undesirable conversational strategy often occurs in the course of naturally-occurring discourse. In this chapter, I report on an empirical study of 250 utterances with {\\\\it zeros} in 20 Japanese newspaper articles. Each utterance is analyzed in terms of centering transitions and the form in which centers are realized by referring expressions. I also examine lexical subcategorization information, and tense and aspect in order to test the hypothesis that the speaker expects the hearer to use this information in determining global discourse structure. I explain the occurrence of {\\\\it zeros} in {\\\\sc retain} and {\\\\sc rough-shift} centering transitions, by claiming that a {\\\\it zero} can only be used in these cases when the shift of centers is supported by contextual information such as lexical semantics, tense and aspect, and agreement features. I then propose an algorithm by which centering can incorporate these observations to integrate centering with global discourse structure, and thus enhance its ability for non-local pronoun resolution. ',\n",
       " \" Japanese Discourse and the Process of Centering This paper has three aims: (1) to generalize a computational account of the discourse process called {\\\\sc centering}, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process, and demonstrate this argument with respect to the interpretation of {\\\\sc zeros}, unexpressed arguments of the verb, in Japanese. The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical {\\\\sc topic}, the postposition {\\\\it wa}, as well as those for grammatical functions such as {\\\\sc subject}, {\\\\em ga}, {\\\\sc object}, {\\\\em o} and {\\\\sc object2}, {\\\\em ni}. In addition, we investigate the role of speaker's {\\\\sc empathy}, which is the viewpoint from which an event is described. This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as {\\\\it kureta, kita}. Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of {\\\\sc zeros}, but that having previously been the {\\\\sc topic} and being realized as a {\\\\sc zero} also contributes to the salience of a discourse entity. We propose a discourse rule of {\\\\sc zero topic assignment}, and show that {\\\\sc centering} provides constraints on when a {\\\\sc zero} can be interpreted as the {\\\\sc zero topic}. \",\n",
       " \" A Probabilistic Disambiguation Method Based on Psycholinguistic Principles We address the problem of structural disambiguation in syntactic parsing. In psycholinguistics, a number of principles of disambiguation have been proposed, notably the Lexical Preference Rule LPR, the Right Association Principle RAP, and the Attach Low and Parallel Principle ALPP (an extension of RAP). We argue that in order to improve disambiguation results it is necessary to implement these principles on the basis of a probabilistic methodology. We define a `three-word probability' for implementing LPR, and a `length probability' for implementing RAP and ALPP. Furthermore, we adopt the `back-off' method to combine these two types of probabilities. Our experimental results indicate our method to be effective, attaining an accuracy of 89.2%. \",\n",
       " ' Clustering Words with the MDL Principle We address the problem of automatically constructing a thesaurus (hierarchically clustering words) based on corpus data. We view the problem of clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs, and propose an estimation algorithm using simulated annealing with an energy function based on the Minimum Description Length MDL Principle. We empirically compared the performance of our method based on the MDL Principle against that of one based on the Maximum Likelihood Estimator, and found that the former outperforms the latter. We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus. Our experimental results indicate that we can improve accuracy in disambiguation by using such a thesaurus. ',\n",
       " ' Designing Statistical Language Learners: Experiments on Noun Compounds The goal of this thesis is to advance the exploration of the statistical language learning design space. In pursuit of that goal, the thesis makes two main theoretical contributions: (i) it identifies a new class of designs by specifying an architecture for natural language analysis in which probabilities are given to semantic forms rather than to more superficial linguistic elements; and (ii) it explores the development of a mathematical theory to predict the expected accuracy of statistical language learning systems in terms of the volume of data used to train them. The theoretical work is illustrated by applying statistical language learning designs to the analysis of noun compounds. Both syntactic and semantic analysis of noun compounds are attempted using the proposed architecture. Empirical comparisons demonstrate that the proposed syntactic model is significantly better than those previously suggested, approaching the performance of human judges on the same task, and that the proposed semantic model, the first statistical approach to this problem, exhibits significantly better accuracy than the baseline strategy. These results suggest that the new class of designs identified is a promising one. The experiments also serve to highlight the need for a widely applicable theory of data requirements. ',\n",
       " ' Learning Dependencies between Case Frame Slots We address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. In particular, we propose a method of learning dependencies between case frame slots. We view the problem of learning case frame patterns as that of learning multi-dimensional discrete joint distributions, where random variables represent case slots. We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables. Since the number of parameters in a multi-dimensional joint distribution is exponential, it is infeasible to accurately estimate them in practice. To overcome this difficulty, we settle with approximating the target joint distribution by the product of low order component distributions, based on corpus data. In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task. Our experimental results indicate that for certain classes of verbs, the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies. ',\n",
       " \" A Geometric Approach to Mapping Bitext Correspondence The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation. Several automatic methods for this task have been proposed in recent years. Yet even the best of these methods can err by several typeset pages. The Smooth Injective Map Recognizer SIMR is a new bitext mapping algorithm. SIMR's errors are smaller than those of the previous front-runner by more than a factor of 4. Its robustness has enabled new commercial-quality applications. The greedy nature of the algorithm makes it independent of memory resources. Unlike other bitext mapping algorithms, SIMR allows crossing correspondences to account for word order differences. Its output can be converted quickly and easily into a sentence alignment. SIMR's output has been used to align over 200 megabytes of the Canadian Hansards for publication by the Linguistic Data Consortium. \",\n",
       " \" Automatic Detection of Omissions in Translations ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations. The algorithm relies solely on geometric analysis of bitext maps and uses no linguistic information. This property allows it to deal equally well with omissions that do not correspond to linguistic units, such as might result from word-processing mishaps. ADOMIT has proven itself by discovering many errors in a hand-constructed gold standard for evaluating bitext mapping algorithms. Quantitative evaluation on simulated omissions showed that, even with today's poor bitext mapping technology, ADOMIT is a valuable quality control tool for translators and translation bureaus. \",\n",
       " ' Learning First-Order Definitions of Functions First-order learning involves finding a clause-form definition of a relation from examples of the relation and relevant background information. In this paper, a particular first-order learning system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other first-order learning systems might benefit from similar specialization. ',\n",
       " \" Mechanisms for Automated Negotiation in State Oriented Domains This paper lays part of the groundwork for a domain theory of negotiation, that is, a way of classifying interactions so that it is clear, given a domain, which negotiation mechanisms and strategies are appropriate. We define State Oriented Domains, a general category of interaction. Necessary and sufficient conditions for cooperation are outlined. We use the notion of worth in an altered definition of utility, thus enabling agreements in a wider class of joint-goal reachable situations. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point). A Unified Negotiation Protocol UNP is developed that can be used in all types of encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals. Finally, we analyze cases where agents have incomplete information on the goals and worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase their utility. Then, we consider the situation where the agents' goals (and therefore stand-alone costs) are common knowledge, but the worth they attach to their goals is private information. We introduce two mechanisms, one 'strict', the other 'tolerant', and analyze their affects on the stability and efficiency of negotiation outcomes. \",\n",
       " \" Death and Lightness: Using a Demographic Model to Find Support Verbs Some verbs have a particular kind of binary ambiguity: they can carry their normal, full meaning, or they can be merely acting as a prop for the nominal object. It has been suggested that there is a detectable pattern in the relationship between a verb acting as a prop (a \\\\term{support verb}) and the noun it supports. The task this paper undertakes is to develop a model which identifies the support verb for a particular noun, and by extension, when nouns are enumerated, a model which disambiguates a verb with respect to its support status. The paper sets up a basic model as a standard for comparison; it then proposes a more complex model, and gives some results to support the model's validity, comparing it with other similar approaches. \",\n",
       " ' Gathering Statistics to Aspectually Classify Sentences with a Genetic Algorithm This paper presents a method for large corpus analysis to semantically classify an entire clause. In particular, we use cooccurrence statistics among similar clauses to determine the aspectual class of an input clause. The process examines linguistic features of clauses that are relevant to aspectual classification. A genetic algorithm determines what combinations of linguistic features to use for this task. ',\n",
       " ' Stochastic Attribute-Value Grammars Probabilistic analogues of regular and context-free grammars are well-known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define a correct parameter-estimation algorithm. In the present paper, I define stochastic attribute-value grammars and give a correct algorithm for estimating their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show how a variant of Gibbs sampling, the Metropolis-Hastings algorithm, can be used instead. ',\n",
       " \" A Faster Structured-Tag Word-Classification Method Several methods have been proposed for processing a corpus to induce a tagset for the sub-language represented by the corpus. This paper examines a structured-tag word classification method introduced by McMahon (1994) and discussed further by McMahon & Smith (1995) in cmp-lg/9503011 . Two major variations, (1) non-random initial assignment of words to classes and (2) moving multiple words in parallel, together provide robust non-random results with a speed increase of 200% to 450%, at the cost of slightly lower quality than McMahon's method's average quality. Two further variations, (3) retaining information from less- frequent words and (4) avoiding reclustering closed classes, are proposed for further study. Note: The speed increases quoted above are relative to my implementation of my understanding of McMahon's algorithm; this takes time measured in hours and days on a home PC. A revised version of the McMahon & Smith (1995) paper has appeared (June 1996) in Computational Linguistics 22(2):217- 247; this refers to a time of several weeks to cluster 569 words on a Sparc-IPC. \",\n",
       " ' A Morphology-System and Part-of-Speech Tagger for German This paper presents an integrated tool for German morphology and statistical part-of-speech tagging which aims at making some well established methods widely available. The software is very user friendly, runs on any PC and can be downloaded as a complete package (including lexicon and documentation) from the World Wide Web. Compared with the performance of other tagging systems the tagger produces similar results. ',\n",
       " \" Extensions and Corrections for: ``A Convex Geometric Approach to Counting the Roots of a Polynomial System'' This brief note corrects some errors in the paper quoted in the title, highlights a combinatorial result which may have been overlooked, and points to further improvements in recent literature. \",\n",
       " ' MUSE CSP: An Extension to the Constraint Satisfaction Problem This paper describes an extension to the constraint satisfaction problem CSP called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to compactly represent several similar instances of the constraint satisfaction problem. If multiple instances of a CSP have some common variables which have the same domains and constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to apply the constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a set of CSPs which are labeled to indicate when the same variable is shared by more than a single CSP. ',\n",
       " \" OT SIMPLE - a construction-kit approach to Optimality Theory implementation This paper details a simple approach to the implementation of Optimality Theory (OT, Prince and Smolensky 1993) on a computer, in part reusing standard system software. In a nutshell, OT's GENerating source is implemented as a BinProlog program interpreting a context-free specification of a GEN structural grammar according to a user-supplied input form. The resulting set of textually flattened candidate tree representations is passed to the CONstraint stage. Constraints are implemented by finite-state transducers specified as `sed' stream editor scripts that typically map ill-formed portions of the candidate to violation marks. EVALuation of candidates reduces to simple sorting: the violation-mark-annotated output leaving CON is fed into `sort', which orders candidates on the basis of the violation vector column of each line, thereby bringing the optimal candidate to the top. This approach gave rise to OT SIMPLE, the first freely available software tool for the OT framework to provide generic facilities for both GEN and CONstraint definition. Its practical applicability is demonstrated by modelling the OT analysis of apparent subtractive pluralization in Upper Hessian presented in Golston and Wiese (1996). \",\n",
       " \" Unsupervised Language Acquisition This thesis presents a computational theory of unsupervised language acquisition, precisely defining procedures for learning language from ordinary spoken or written utterances, with no explicit help from a teacher. The theory is based heavily on concepts borrowed from machine learning and statistical estimation. In particular, learning takes place by fitting a stochastic, generative model of language to the evidence. Much of the thesis is devoted to explaining conditions that must hold for this general learning strategy to arrive at linguistically desirable grammars. The thesis introduces a variety of technical innovations, among them a common representation for evidence and grammars, and a learning strategy that separates the ``content'' of linguistic parameters from their representation. Algorithms based on it suffer from few of the search problems that have plagued other computational approaches to language acquisition. The theory has been tested on problems of learning vocabularies and grammars from unsegmented text and continuous speech, and mappings between sound and representations of meaning. It performs extremely well on various objective criteria, acquiring knowledge that causes it to assign almost exactly the same structure to utterances as humans do. This work has application to data compression, language modeling, speech recognition, machine translation, information retrieval, and other tasks that rely on either structural or stochastic descriptions of language. \",\n",
       " \" Data-Oriented Language Processing. An Overview During the last few years, a new approach to language processing has started to emerge, which has become known under various labels such as data-oriented parsing , corpus-based interpretation , and tree-bank grammar (cf. van den Berg et al. 1994; Bod 1992-96; Bod et al. 1996a/b; Bonnema 1996; Charniak 1996a/b; Goodman 1996; Kaplan 1996; Rajman 1995a/b; Scha 1990-92; Sekine & Grishman 1995; Sima'an et al. 1994; Sima'an 1995-96; Tugwell 1995). This approach, which we will call data-oriented processing or DOP , embodies the assumption that human language perception and production works with representations of concrete past language experiences, rather than with abstract linguistic rules. The models that instantiate this approach therefore maintain large corpora of linguistic representations of previously occurring utterances. When processing a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. In this paper we give an in-depth discussion of a data-oriented processing model which employs a corpus of labelled phrase-structure trees. Then we review some other models that instantiate the DOP approach. Many of these models also employ labelled phrase-structure trees, but use different criteria for extracting fragments from the corpus or employ different disambiguation strategies (Bod 1996b; Charniak 1996a/b; Goodman 1996; Rajman 1995a/b; Sekine & Grishman 1995; Sima'an 1995-96); other models use richer formalisms for their corpus annotations (van den Berg et al. 1994; Bod et al., 1996a/b; Bonnema 1996; Kaplan 1996; Tugwell 1995). \",\n",
       " ' Nonuniform Markov models A statistical language model assigns probability to strings of arbitrary length. Unfortunately, it is not possible to gather reliable statistics on strings of arbitrary length from a finite corpus. Therefore, a statistical language model must decide that each symbol in a string depends on at most a small, finite number of other symbols in the string. In this report we propose a new way to model conditional independence in Markov models. The central feature of our nonuniform Markov model is that it makes predictions of varying lengths using contexts of varying lengths. Experiments on the Wall Street Journal reveal that the nonuniform model performs slightly better than the classic interpolated Markov model. This result is somewhat remarkable because both models contain identical numbers of parameters whose values are estimated in a similar manner. The only difference between the two models is how they combine the statistics of longer and shorter strings. Keywords: nonuniform Markov model, interpolated Markov model, conditional independence, statistical language model, discrete time series. ',\n",
       " ' Integrating HMM-Based Speech Recognition With Direct Manipulation In A Multimodal Korean Natural Language Interface This paper presents a HMM-based speech recognition engine and its integration into direct manipulation interfaces for Korean document editor. Speech recognition can reduce typical tedious and repetitive actions which are inevitable in standard GUIs (graphic user interfaces). Our system consists of general speech recognition engine called ABrain {Auditory Brain} and speech commandable document editor called SHE {Simple Hearing Editor}. ABrain is a phoneme-based speech recognition engine which shows up to 97% of discrete command recognition rate. SHE is a EuroBridge widget-based document editor that supports speech commands as well as direct manipulation interfaces. ',\n",
       " ' A Framework for Natural Language Interfaces to Temporal Databases Over the past thirty years, there has been considerable progress in the design of natural language interfaces to databases. Most of this work has concerned snapshot databases, in which there are only limited facilities for manipulating time-varying information. The database community is becoming increasingly interested in temporal databases, databases with special support for time-dependent entries. We have developed a framework for constructing natural language interfaces to temporal databases, drawing on research on temporal phenomena within logic and linguistics. The central part of our framework is a logic-like formal language, called TOP, which can capture the semantics of a wide range of English sentences. We have implemented an HPSG-based sentence analyser that converts a large set of English queries involving time into TOP formulae, and have formulated a provably correct procedure for translating TOP expressions into queries in the TSQL2 temporal database language. In this way we have established a sound route from English to a general-purpose temporal database language. ',\n",
       " ' Characterizations of Decomposable Dependency Models Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs. We also briefly discuss a potential application of our results to the problem of learning graphical models from data. ',\n",
       " \" Exploiting Causal Independence in Bayesian Network Inference A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as ``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms. \",\n",
       " ' Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer Efficiently entering information into a computer is key to enjoying the benefits of computing. This paper describes three intelligent user interfaces: handwriting recognition, adaptive menus, and predictive fillin. In the context of adding a personUs name and address to an electronic organizer, tests show handwriting recognition is slower than typing on an on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast. This paper also presents strategies for applying these three interfaces to other information collection domains. ',\n",
       " \" Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word ``line'' using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems. \",\n",
       " ' Specialized Language Models using Dialogue Predictions This paper analyses language modeling in spoken dialogue systems for accessing a database. The use of several language models obtained by exploiting dialogue predictions gives better results than the use of a single model for the whole dialogue interaction. For this reason several models have been created, each one for a specific system question, such as the request or the confirmation of a parameter. The use of dialogue-dependent language models increases the performance both at the recognition and at the understanding level, especially on answers to system requests. Moreover other methods to increase performance, like automatic clustering of vocabulary words or the use of better acoustic models during recognition, does not affect the improvements given by dialogue-dependent language models. The system used in our experiments is Dialogos, the Italian spoken dialogue system used for accessing railway timetable information over the telephone. The experiments were carried out on a large corpus of dialogues collected using Dialogos. ',\n",
       " ' Metrics for Evaluating Dialogue Strategies in a Spoken Language System In this paper, we describe a set of metrics for the evaluation of different dialogue management strategies in an implemented real-time spoken language system. The set of metrics we propose offers useful insights in evaluating how particular choices in the dialogue management can affect the overall quality of the man-machine dialogue. The evaluation makes use of established metrics: the transaction success, the contextual appropriateness of system answers, the calculation of normal and correction turns in a dialogue. We also define a new metric, the implicit recovery, which allows to measure the ability of a dialogue manager to deal with errors by different levels of analysis. We report evaluation data from several experiments, and we compare two different approaches to dialogue repair strategies using the set of metrics we argue for. ',\n",
       " ' Dialogos: a Robust System for Human-Machine Spoken Dialogue on the Telephone This paper presents Dialogos, a real-time system for human-machine spoken dialogue on the telephone in task-oriented domains. The system has been tested in a large trial with inexperienced users and it has proved robust enough to allow spontaneous interactions both to users which get good recognition performance and to the ones which get lower scores. The robust behavior of the system has been achieved by combining the use of specific language models during the recognition phase of analysis, the tolerance toward spontaneous speech phenomena, the activity of a robust parser, and the use of pragmatic-based dialogue knowledge. This integration of the different modules allows to deal with partial or total breakdowns of the different levels of analysis. We report the field trial data of the system and the evaluation results of the overall system and of the submodules. ',\n",
       " ' Improved Heterogeneous Distance Functions Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric VDM was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric HVDM, the Interpolated Value Difference Metric IVDM, and the Windowed Value Difference Metric WVDM. These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes. ',\n",
       " ' Maximum Entropy Modeling Toolkit The Maximum Entropy Modeling Toolkit supports parameter estimation and prediction for statistical language models in the maximum entropy framework. The maximum entropy framework provides a constructive method for obtaining the unique conditional distribution p*(y|x) that satisfies a set of linear constraints and maximizes the conditional entropy H(p|f) with respect to the empirical distribution f(x). The maximum entropy distribution p*(y|x) also has a unique parametric representation in the class of exponential models, as m(y|x) = r(y|x)/Z(x) where the numerator m(y|x) = prod_i alpha_i^g_i(x,y) is a product of exponential weights, with alpha_i = exp(lambda_i), and the denominator Z(x) = sum_y r(y|x) is required to satisfy the axioms of probability. This manual explains how to build maximum entropy models for discrete domains with the Maximum Entropy Modeling Toolkit MEMT. First we summarize the steps necessary to implement a language model using the toolkit. Next we discuss the executables provided by the toolkit and explain the file formats required by the toolkit. Finally, we review the maximum entropy framework and apply it to the problem of statistical language modeling. Keywords: statistical language models, maximum entropy, exponential models, improved iterative scaling, Markov models, triggers. ',\n",
       " \" SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken- language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component. In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the SCREEN system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of SCREEN's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework. \",\n",
       " ' Exploiting Context to Identify Lexical Atoms -- A Statistical View of Linguistic Context Interpretation of natural language is inherently context-sensitive. Most words in natural language are ambiguous and their meanings are heavily dependent on the linguistic context in which they are used. The study of lexical semantics can not be separated from the notion of context. This paper takes a contextual approach to lexical semantics and studies the linguistic context of lexical atoms, or sticky phrases such as hot dog . Since such lexical atoms may occur frequently in unrestricted natural language text, recognizing them is crucial for understanding naturally-occurring text. The paper proposes several heuristic approaches to exploiting the linguistic context to identify lexical atoms from arbitrary natural language text. ',\n",
       " \" Hybrid language processing in the Spoken Language Translator The paper presents an overview of the Spoken Language Translator SLT system's hybrid language-processing architecture, focussing on the way in which rule-based and statistical methods are combined to achieve robust and efficient performance within a linguistically motivated framework. In general, we argue that rules are desirable in order to encode domain-independent linguistic constraints and achieve high-quality grammatical output, while corpus-derived statistics are needed if systems are to be efficient and robust; further, that hybrid architectures are superior from the point of view of portability to architectures which only make use of one type of information. We address the topics of ``multi-engine'' strategies for robust translation; robust bottom-up parsing using pruning and grammar specialization; rational development of linguistic rule-sets using balanced domain corpora; and efficient supervised training by interactive disambiguation. All work described is fully implemented in the current version of the SLT-2 system. \",\n",
       " \" Generating Information-Sharing Subdialogues in Expert-User Consultation In expert-consultation dialogues, it is inevitable that an agent will at times have insufficient information to determine whether to accept or reject a proposal by the other agent. This results in the need for the agent to initiate an information-sharing subdialogue to form a set of shared beliefs within which the agents can effectively re-evaluate the proposal. This paper presents a computational strategy for initiating such information-sharing subdialogues to resolve the system's uncertainty regarding the acceptance of a user proposal. Our model determines when information-sharing should be pursued, selects a focus of information-sharing among multiple uncertain beliefs, chooses the most effective information-sharing strategy, and utilizes the newly obtained information to re-evaluate the user proposal. Furthermore, our model is capable of handling embedded information-sharing subdialogues. \",\n",
       " \" An Efficient Implementation of the Head-Corner Parser This paper describes an efficient and robust implementation of a bi-directional, head-driven parser for constraint-based grammars. This parser is developed for the OVIS system: a Dutch spoken dialogue system in which information about public transport can be obtained by telephone. After a review of the motivation for head-driven parsing strategies, and head-corner parsing in particular, a non-deterministic version of the head-corner parser is presented. A memoization technique is applied to obtain a fast parser. A goal-weakening technique is introduced which greatly improves average case efficiency, both in terms of speed and space requirements. I argue in favor of such a memoization strategy with goal-weakening in comparison with ordinary chart-parsers because such a strategy can be applied selectively and therefore enormously reduces the space requirements of the parser, while no practical loss in time-efficiency is observed. On the contrary, experiments are described in which head-corner and left-corner parsers implemented with selective memoization and goal weakening outperform `standard' chart parsers. The experiments include the grammar of the OVIS system and the Alvey NL Tools grammar. Head-corner parsing is a mix of bottom-up and top-down processing. Certain approaches towards robust parsing require purely bottom-up processing. Therefore, it seems that head-corner parsing is unsuitable for such robust parsing techniques. However, it is shown how underspecification (which arises very naturally in a logic programming environment) can be used in the head-corner parser to allow such robust parsing techniques. A particular robust parsing model is described which is implemented in OVIS. \",\n",
       " \" SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks In this paper, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the SCREEN system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of SCREEN's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework. \",\n",
       " ' Automatic Extraction of Subcategorization from Corpora We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount. ',\n",
       " ' A Robust Text Processing Technique Applied to Lexical Error Recovery This thesis addresses automatic lexical error recovery and tokenization of corrupt text input. We propose a technique that can automatically correct misspellings, segmentation errors and real-word errors in a unified framework that uses both a model of language production and a model of the typing behavior, and which makes tokenization part of the recovery process. The typing process is modeled as a noisy channel where Hidden Markov Models are used to model the channel characteristics. Weak statistical language models are used to predict what sentences are likely to be transmitted through the channel. These components are held together in the Token Passing framework which provides the desired tight coupling between orthographic pattern matching and linguistic expectation. The system, CTR (Connected Text Recognition), has been tested on two corpora derived from two different applications, a natural language dialogue system and a transcription typing scenario. Experiments show that CTR can automatically correct a considerable portion of the errors in the test sets without introducing too much noise. The segmentation error correction rate is virtually faultless. ',\n",
       " \" Some New Applications of Toric Geometry This paper reexamines univariate reduction from a toric geometric point of view. We begin by constructing a binomial variant of the resultant and then retailor the generalized characteristic polynomial to fully exploit sparsity in the monomial structure of any given polynomial system. We thus obtain a fast new algorithm for univariate reduction and a better understanding of the underlying projections. As a corollary, we show that a refinement of Hilbert's Tenth Problem is decidable within single-exponential time. We also show how certain multisymmetric functions of the roots of polynomial systems can be calculated with sparse resultants. \",\n",
       " \" Toric Generalized Characteristic Polynomials We illustrate an efficient new method for handling polynomial systems with degenerate solution sets. In particular, a corollary of our techniques is a new algorithm to find an isolated point in every excess component of the zero set (over an algebraically closed field) of any by system of polynomial equations. Since we use the sparse resultant, we thus obtain complexity bounds (for converting any input polynomial system into a multilinear factorization problem) which are close to cubic in the degree of the underlying variety -- significantly better than previous bounds which were pseudo-polynomial in the classical B\\\\'ezout bound. By carefully taking into account the underlying toric geometry, we are also able to improve the reliability of certain sparse resultant based algorithms for polynomial system solving. \",\n",
       " ' An Annotation Scheme for Free Word Order Languages We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata. ',\n",
       " ' Software Infrastructure for Natural Language Processing We classify and review current approaches to software infrastructure for research, development and delivery of NLP systems. The task is motivated by a discussion of current trends in the field of NLP and Language Engineering. We describe a system called GATE (a General Architecture for Text Engineering) that provides a software infrastructure on top of which heterogeneous NLP processing modules may be evaluated and refined individually, or may be combined into larger application systems. GATE aims to support both researchers and developers working on component technologies (e.g. parsing, tagging, morphological analysis) and those working on developing end-user applications (e.g. information extraction, text summarisation, document generation, machine translation, and second language learning). GATE promotes reuse of component technology, permits specialisation and collaboration in large-scale projects, and allows for the comparison and evaluation of alternative technologies. The first release of GATE is now available - see http://www.dcs.shef.ac.uk/research/groups/nlp/gate/ ',\n",
       " ' Information Extraction - A User Guide This technical memo describes Information Extraction from the point-of-view of a potential user of the technology. No knowledge of language processing is assumed. Information Extraction is a process which takes unseen texts as input and produces fixed-format, unambiguous data as output. This data may be used directly for display to users, or may be stored in a database or spreadsheet for later analysis, or may be used for indexing purposes in Information Retrieval applications. See also http://www.dcs.shef.ac.uk/~hamish ',\n",
       " ' Natural Language Dialogue Service for Appointment Scheduling Agents Appointment scheduling is a problem faced daily by many individuals and organizations. Cooperating agent systems have been developed to partially automate this task. In order to extend the circle of participants as far as possible we advocate the use of natural language transmitted by e-mail. We describe COSMA, a fully implemented German language server for existing appointment scheduling agent systems. COSMA can cope with multiple dialogues in parallel, and accounts for differences in dialogue behaviour between human and machine agents. NL coverage of the sublanguage is achieved through both corpus-based grammar development and the use of message extraction techniques. ',\n",
       " ' Sequential Model Selection for Word Sense Disambiguation Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation. ',\n",
       " \" Fast Statistical Parsing of Noun Phrases for Document Indexing Information Retrieval IR is an important application area of Natural Language Processing NLP where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment's results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance. \",\n",
       " ' How much has information technology contributed to linguistics? Information technology should have much to offer linguistics, not only through the opportunities offered by large-scale data analysis and the stimulus to develop formal computational models, but through the chance to use language in systems for automatic natural language processing. The paper discusses these possibilities in detail, and then examines the actual work that has been done. It is evident that this has so far been primarily research within a new field, computational linguistics, which is largely motivated by the demands, and interest, of practical processing systems, and that information technology has had rather little influence on linguistics at large. There are different reasons for this, and not all good ones: information technology deserves more attention from linguists. ',\n",
       " ' Selective Sampling of Effective Example Sentence Sets for Word Sense Disambiguation This paper proposes an efficient example selection method for example-based word sense disambiguation systems. To construct a practical size database, a considerable overhead for manual sense disambiguation is required. Our method is characterized by the reliance on the notion of the training utility: the degree to which each example is informative for future example selection when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectivity of our method through experiments on about one thousand sentences. Compared to experiments with random example selection, our method reduced the overhead without the degeneration of the performance of the system. ',\n",
       " ' Design and Implementation of a Computational Lexicon for Turkish All natural language processing systems (such as parsers, generators, taggers) need to have access to a lexicon about the words in the language. This thesis presents a lexicon architecture for natural language processing in Turkish. Given a query form consisting of a surface form and other features acting as restrictions, the lexicon produces feature structures containing morphosyntactic, syntactic, and semantic information for all possible interpretations of the surface form satisfying those restrictions. The lexicon is based on contemporary approaches like feature-based representation, inheritance, and unification. It makes use of two information sources: a morphological processor and a lexical database containing all the open and closed-class words of Turkish. The system has been implemented in SICStus Prolog as a standalone module for use in natural language processing applications. ',\n",
       " ' Knowledge Acquisition for Content Selection An important part of building a natural-language generation NLG system is knowledge acquisition, that is deciding on the specific schemas, plans, grammar rules, and so forth that should be used in the NLG system. We discuss some experiments we have performed with KA for content-selection rules, in the context of building an NLG system which generates health-related material. These experiments suggest that it is useful to supplement corpus analysis with KA techniques developed for building expert systems, such as structured group discussions and think-aloud protocols. They also raise the point that KA issues may influence architectural design issues, in particular the decision on whether a planning approach is used for content selection. We suspect that in some cases, KA may be easier if other constructive expert-system techniques (such as production rules, or case-based reasoning) are used to determine the content of a generated text. ',\n",
       " ' Building a Generation Knowledge Source using Internet-Accessible Newswire In this paper, we describe a method for automatic creation of a knowledge source for text generation using information extraction over the Internet. We present a prototype system called PROFILE which uses a client-server architecture to extract noun-phrase descriptions of entities such as people, places, and organizations. The system serves two purposes: as an information extraction tool, it allows users to search for textual descriptions of entities; as a utility to generate functional descriptions FD, it is used in a functional-unification based generation system. We present an evaluation of the approach and its applications to natural language generation and summarization. ',\n",
       " ' Improvising Linguistic Style: Social and Affective Bases for Agent Personality This paper introduces Linguistic Style Improvisation, a theory and set of algorithms for improvisation of spoken utterances by artificial agents, with applications to interactive story and dialogue systems. We argue that linguistic style is a key aspect of character, and show how speech act representations common in AI can provide abstract representations from which computer characters can improvise. We show that the mechanisms proposed introduce the possibility of socially oriented agents, meet the requirements that lifelike characters be believable, and satisfy particular criteria for improvisation proposed by Hayes-Roth. ',\n",
       " ' Instructions for Temporal Annotation of Scheduling Dialogs Human annotation of natural language facilitates standardized evaluation of natural language processing systems and supports automated feature extraction. This document consists of instructions for annotating the temporal information in scheduling dialogs, dialogs in which the participants schedule a meeting with one another. Task-oriented dialogs, such as these are, would arise in many useful applications, for instance, automated information providers and automated phone operators. Explicit instructions support good inter-rater reliability and serve as documentation for the classes being annotated. ',\n",
       " ' A Uniform Framework for Concept Definitions in Description Logics Most modern formalisms used in Databases and Artificial Intelligence for describing an application domain are based on the notions of class (or concept) and relationship among classes. One interesting feature of such formalisms is the possibility of defining a class, i.e., providing a set of properties that precisely characterize the instances of the class. Many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion. In this paper, we argue that, instead of choosing a single style of semantics, we achieve better results by adopting a formalism that allows for different semantics to coexist. We demonstrate the feasibility of our argument, by presenting a knowledge representation formalism, the description logic muALCQ, with the above characteristics. In addition to the constructs for conjunction, disjunction, negation, quantifiers, and qualified number restrictions, muALCQ includes special fixpoint constructs to express (suitably interpreted) recursive definitions. These constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs, lists, streams, etc. We establish several properties of muALCQ, including the decidability and the computational complexity of reasoning, by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus. ',\n",
       " \" Domain Adaptation with Clustered Language Models In this paper, a method of domain adaptation for clustered language models is developed. It is based on a previously developed clustering algorithm, but with a modified optimisation criterion. The results are shown to be slightly superior to the previously published 'Fillup' method, which can be used to adapt standard n-gram models. However, the improvement both methods give compared to models built from scratch on the adaptation data is quite small (less than 11% relative improvement in word error rate). This suggests that both methods are still unsatisfactory from a practical point of view. \",\n",
       " \" Concept Clustering and Knowledge Integration from a Children's Dictionary Knowledge structures called Concept Clustering Knowledge Graphs (CCKGs) are introduced along with a process for their construction from a machine readable dictionary. CCKGs contain multiple concepts interrelated through multiple semantic relations together forming a semantic cluster represented by a conceptual graph. The knowledge acquisition is performed on a children's first dictionary. A collection of conceptual clusters together can form the basis of a lexical knowledge base, where each CCKG contains a limited number of highly connected words giving useful information about a particular domain or situation. \",\n",
       " ' A Semantics-based Communication System for Dysphasic Subjects Dysphasic subjects do not have complete linguistic abilities and only produce a weakly structured, topicalized language. They are offered artificial symbolic languages to help them communicate in a way more adapted to their linguistic abilities. After a structural analysis of a corpus of utterances from children with cerebral palsy, we define a semantic lexicon for such a symbolic language. We use it as the basis of a semantic analysis process able to retrieve an interpretation of the utterances. This semantic analyser is currently used in an application designed to convert iconic languages into natural language; it might find other uses in the field of language rehabilitation. ',\n",
       " ' Insights into the Dialogue Processing of VERBMOBIL We present the dialogue module of the speech-to-speech translation system VERBMOBIL. We follow the approach that the solution to dialogue processing in a mediating scenario can not depend on a single constrained processing tool, but on a combination of several simple, efficient, and robust components. We show how our solution to dialogue processing works when applied to real data, and give some examples where our module contributes to the correct translation from German to English. ',\n",
       " ' A Potts Neuron Approach to Communication Routing A feedback neural network approach to communication routing problems is developed with emphasis on Multiple Shortest Path problems, with several requests for transmissions between distinct start- and endnodes. The basic ingredients are a set of Potts neurons for each request, with interactions designed to minimize path lengths and to prevent overloading of network arcs. The topological nature of the problem is conveniently handled using a propagator matrix approach. Although the constraints are global, the algorithmic steps are based entirely on local information, facilitating distributed implementations. In the polynomially solvable single-request case the approach reduces to a fuzzy version of the Bellman-Ford algorithm. The approach is evaluated for synthetic problems of varying sizes and load levels, by comparing with exact solutions from a branch-and-bound method. With very few exceptions, the Potts approach gives legal solutions of very high quality. The computational demand scales merely as the product of the numbers of requests, nodes, and arcs. ',\n",
       " ' Semi-Automatic Acquisition of Domain-Specific Translation Lexicons We investigate the utility of an algorithm for translation lexicon acquisition SABLE, used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons. ',\n",
       " ' Lifeworld Analysis We argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity. We refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity. As one specific example, we apply the tools to the analysis of the Toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment. ',\n",
       " ' Is Quantum Bit Commitment Really Possible? We show that all proposed quantum bit commitment schemes are insecure because the sender, Alice, can almost always cheat successfully by using an Einstein-Podolsky-Rosen type of attack and delaying her measurement until she opens her commitment. ',\n",
       " ' Evaluating Multilingual Gisting of Web Pages We describe a prototype system for multilingual gisting of Web pages, and present an evaluation methodology based on the notion of gisting as decision support. This evaluation paradigm is straightforward, rigorous, permits fair comparison of alternative approaches, and should easily generalize to evaluation in other situations where the user is faced with decision-making on the basis of information in restricted or alternative form. ',\n",
       " ' A simple polynomial time algorithm to approximate the permanent within a simply exponential factor We present a simple randomized polynomial time algorithm to approximate the mixed discriminant of positive semidefinite matrices within a factor . Consequently, the algorithm allows us to approximate in randomized polynomial time the permanent of a given non-negative matrix within a factor . When applied to approximating the permanent, the algorithm turns out to be a simple modification of the well-known Godsil-Gutman estimator. ',\n",
       " ' A Maximum Entropy Approach to Identifying Sentence Boundaries We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. ',\n",
       " ' Machine Transliteration It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as konpyuutaa in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. ',\n",
       " \" PARADISE: A Framework for Evaluating Spoken Dialogue Agents This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity. \",\n",
       " \" Tracking Initiative in Collaborative Dialogue Interactions In this paper, we argue for the need to distinguish between task and dialogue initiatives, and present a model for tracking shifts in both types of initiatives in dialogue interactions. Our model predicts the initiative holders in the next dialogue turn based on the current initiative holders and the effect that observed cues have on changing them. Our evaluation across various corpora shows that the use of cues consistently improves the accuracy in the system's prediction of task and dialogue initiative holders by 2-4 and 8-13 percentage points, respectively, thus illustrating the generality of our model. \",\n",
       " ' Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation This paper presents a method to combine a set of unsupervised algorithms that can accurately disambiguate word senses in a large, completely untagged corpus. Although most of the techniques for word sense resolution have been presented as stand-alone, it is our belief that full-fledged lexical ambiguity resolution should combine several information sources and techniques. The set of techniques have been applied in a combined way to disambiguate the genus terms of two machine-readable dictionaries MRD, enabling us to construct complete taxonomies for Spanish and French. Tested accuracy is above 80% overall and 95% for two-way ambiguous genus terms, showing that taxonomy building is not limited to structured dictionaries such as LDOCE. ',\n",
       " \" Intonational Boundaries, Speech Repairs and Discourse Markers: Modeling Spoken Dialog To understand a speaker's turn of a conversation, one needs to segment it into intonational phrases, clean up any speech repairs that might have occurred, and identify discourse markers. In this paper, we argue that these problems must be resolved together, and that they must be resolved early in the processing stream. We put forward a statistical language model that resolves these problems, does POS tagging, and can be used as the language model of a speech recognizer. We find that by accounting for the interactions between these tasks that the performance on each task improves, as does POS tagging and perplexity. \",\n",
       " ' Developing a hybrid NP parser We describe the use of energy function optimization in very shallow syntactic parsing. The approach can use linguistic rules and corpus-based statistics, so the strengths of both linguistic and statistical approaches to NLP can be combined in a single framework. The rules are contextual constraints for resolving syntactic ambiguities expressed as alternative tags, and the statistical language model consists of corpus-based n-grams of syntactic tags. The success of the hybrid syntactic disambiguator is evaluated against a held-out benchmark corpus. Also the contributions of the linguistic and statistical language models to the hybrid model are estimated. ',\n",
       " \" Emphatic generation: employing the theory of semantic emphasis for text generation The paper deals with the problem of text generation and planning approaches making only limited formally specifiable contact with accounts of grammar. We propose an enhancement of a systemically-based generation architecture for German (the KOMET system) by aspects of Kunze's theory of semantic emphasis. Doing this, we gain more control over both concept selection in generation and choice of fine-grained grammatical variation. \",\n",
       " ' Morphological Disambiguation by Voting Constraints We present a constraint-based morphological disambiguation system in which individual constraints vote on matching morphological parses, and disambiguation of all the tokens in a sentence is performed at the end by selecting parses that receive the highest votes. This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing. Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95-96% and a precision of 94-95% with about 1.01 parses per token. Our system is implemented in Prolog and we are currently investigating an efficient implementation based on finite state transducers. ',\n",
       " \" The Theoretical Status of Ontologies in Natural Language Processing This paper discusses the use of `ontologies' in Natural Language Processing. It classifies various kinds of ontologies that have been employed in NLP and discusses various benefits and problems with those designs. Particular focus is then placed on experiences gained in the use of the Upper Model, a linguistically-motivated `ontology' originally designed for use with the Penman text generation system. Some proposals for further NLP ontology design criteria are then made. \",\n",
       " \" Insecurity of Quantum Secure Computations It had been widely claimed that quantum mechanics can protect private information during public decision in for example the so-called two-party secure computation. If this were the case, quantum smart-cards could prevent fake teller machines from learning the PIN (Personal Identification Number) from the customers' input. Although such optimism has been challenged by the recent surprising discovery of the insecurity of the so-called quantum bit commitment, the security of quantum two-party computation itself remains unaddressed. Here I answer this question directly by showing that all ``one-sided'' two-party computations (which allow only one of the two parties to learn the result) are necessarily insecure. As corollaries to my results, quantum one-way oblivious password identification and the so-called quantum one-out-of-two oblivious transfer are impossible. I also construct a class of functions that cannot be computed securely in any ``two-sided'' two-party computation. Nevertheless, quantum cryptography remains useful in key distribution and can still provide partial security in ``quantum money'' proposed by Wiesner. \",\n",
       " ' A Theory of Parallelism and the Case of VP Ellipsis We provide a general account of parallelism in discourse, and apply it to the special case of resolving possible readings for instances of VP ellipsis. We show how several problematic examples are accounted for in a natural and straightforward fashion. The generality of the approach makes it directly applicable to a variety of other types of ellipsis and reference. ',\n",
       " ' Centering in-the-large: Computing referential discourse segments We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied. ',\n",
       " \" Connectionist Theory Refinement: Genetically Searching the Space of Network Topologies An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the REGENT algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks. \",\n",
       " ' Query DAGs: A Practical Paradigm for Implementing Belief-Network Inference We describe a new paradigm for implementing inference in belief networks, which consists of two steps: (1) compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG); and (2) answering queries using a simple evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest. It appears that Q-DAGs can be generated using any of the standard algorithms for exact inference in belief networks (we show how they can be generated using clustering and conditioning algorithms). The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based. The complexity of a Q-DAG evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents. The intended value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications. The proposed framework also facilitates the development of on-line inference on different software and hardware platforms due to the simplicity of the Q-DAG evaluation algorithm. Interestingly enough, Q-DAGs were found to serve other purposes: simple techniques for reducing Q-DAGs tend to subsume relatively complex optimization techniques for belief-network inference, such as network-pruning and computation-caching. ',\n",
       " ' Co-evolution of Language and of the Language Acquisition Device A new account of parameter setting during grammatical acquisition is presented in terms of Generalized Categorial Grammar embedded in a default inheritance hierarchy, providing a natural partial ordering on the setting of parameters. Experiments show that several experimentally effective learners can be defined in this framework. Evolutionary simulations suggest that a learner with default initial settings for parameters will emerge, provided that learning is memory limited and the environment of linguistic adaptation contains an appropriate language. ',\n",
       " ' Computing Parallelism in Discourse Although much has been said about parallelism in discourse, a formal, computational theory of parallelism structure is still outstanding. In this paper, we present a theory which given two parallel utterances predicts which are the parallel elements. The theory consists of a sorted, higher-order abductive calculus and we show that it reconciles the insights of discourse theories of parallelism with those of Higher-Order Unification approaches to discourse semantics, thereby providing a natural framework in which to capture the effect of parallelism on discourse semantics. ',\n",
       " ' Grammatical analysis in the OVIS spoken-dialogue system We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system. We discuss the structure of the grammar, the properties of the parser, and a method for achieving robustness. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input. ',\n",
       " ' Sloppy Identity Although sloppy interpretation is usually accounted for by theories of ellipsis, it often arises in non-elliptical contexts. In this paper, a theory of sloppy interpretation is provided which captures this fact. The underlying idea is that sloppy interpretation results from a semantic constraint on parallel structures and the theory is shown to predict sloppy readings for deaccented and paycheck sentences as well as relational-, event-, and one-anaphora. It is further shown to capture the interaction of sloppy/strict ambiguity with quantification and binding. ',\n",
       " ' Document Classification Using a Finite Mixture Model We propose a new method of classifying documents into categories. The simple method of conducting hypothesis testing over word-based distributions in categories suffers from the data sparseness problem. In order to address this difficulty, Guthrie et.al. have developed a method using distributions based on hard clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly. This method might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories. We propose here the use of soft clustering of words, i.e., in which a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution. We define for each document category a finite mixture model, which is a linear combination of the probability distributions of the clusters. We thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models. In order to accomplish this testing, we employ the EM algorithm which helps efficiently estimate parameters in a finite mixture model. Experimental results indicate that our method outperforms not only the method using distributions based on hard clustering, but also the method using word-based distributions and the method based on cosine-similarity. ',\n",
       " ' Quantitative Constraint Logic Programming for Weighted Grammar Applications Constraint logic grammars provide a powerful formalism for expressing complex logical descriptions of natural language phenomena in exact terms. Describing some of these phenomena may, however, require some form of graded distinctions which are not provided by such grammars. Recent approaches to weighted constraint logic grammars attempt to address this issue by adding numerical calculation schemata to the deduction scheme of the underlying CLP framework. Currently, these extralogical extensions are not related to the model-theoretic counterpart of the operational semantics of CLP, i.e., they do not come with a formal semantics at all. The aim of this paper is to present a clear formal semantics for weighted constraint logic grammars, which abstracts away from specific interpretations of weights, but nevertheless gives insights into the parsing problem for such weighted grammars. Building on the formalization of constraint logic grammars in the CLP scheme of Hoehfeld and Smolka 1988, this formal semantics will be given by a quantitative version of CLP. Such a quantitative CLP scheme can also be valuable for CLP tasks independent of grammars. ',\n",
       " ' Recycling Lingware in a Multilingual MT System We describe two methods relevant to multi-lingual machine translation systems, which can be used to port linguistic data (grammars, lexicons and transfer rules) between systems used for processing related languages. The methods are fully implemented within the Spoken Language Translator system, and were used to create versions of the system for two new language pairs using only a month of expert effort. ',\n",
       " \" Charts, Interaction-Free Grammars, and the Compact Representation of Ambiguity Recently researchers working in the LFG framework have proposed algorithms for taking advantage of the implicit context-free components of a unification grammar [Maxwell 96]. This paper clarifies the mathematical foundations of these techniques, provides a uniform framework in which they can be formally studied and eliminates the need for special purpose runtime data-structures recording ambiguity. The paper posits the identity: Ambiguous Feature Structures = Grammars, which states that (finitely) ambiguous representations are best seen as unification grammars of a certain type, here called ``interaction-free'' grammars, which generate in a backtrack-free way each of the feature structures subsumed by the ambiguous representation. This work extends a line of research [Billot and Lang 89, Lang 94] which stresses the connection between charts and grammars: a chart can be seen as a specialization of the reference grammar for a given input string. We show how this specialization grammar can be transformed into an interaction-free form which has the same practicality as a listing of the individual solutions, but is produced in less time and space. \",\n",
       " ' Memory-Based Learning: Using Similarity for Smoothing This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling. We show that the two approaches are closely related, and we argue that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters. We report two applications of this approach: PP-attachment and POS-tagging. Our method achieves state-of-the-art performance in both domains, and allows the easy integration of diverse information sources, such as rich lexical representations. ',\n",
       " ' A Lexicon for Underspecified Semantic Tagging The paper defends the notion that semantic tagging should be viewed as more than disambiguation between senses. Instead, semantic tagging should be a first step in the interpretation process by assigning each lexical item a representation of all of its systematically related senses, from which further semantic processing steps can derive discourse dependent interpretations. This leads to a new type of semantic lexicon (CoreLex) that supports underspecified semantic tagging through a design based on systematic polysemous classes and a class-based acquisition of lexical knowledge for specific domains. ',\n",
       " ' Choiceless polynomial time Turing machines define polynomial time (PTime) on strings but cannot deal with structures like graphs directly, and there is no known, easily computable string encoding of isomorphism classes of structures. Is there a computation model whose machines do not distinguish between isomorphic structures and compute exactly PTime properties? This question can be recast as follows: Does there exist a logic that captures polynomial time (without presuming the presence of a linear order)? Earlier, one of us conjectured the negative answer. The problem motivated a quest for stronger and stronger PTime logics. All these logics avoid arbitrary choice. Here we attempt to capture the choiceless fragment of PTime. Our computation model is a version of abstract state machines (formerly called evolving algebras). The idea is to replace arbitrary choice with parallel execution. The resulting logic is more expressive than other PTime logics in the literature. A more difficult theorem shows that the logic does not capture all PTime. ',\n",
       " \" A Comparative Study of the Application of Different Learning Techniques to Natural Language Interfaces In this paper we present first results from a comparative study. Its aim is to test the feasibility of different inductive learning techniques to perform the automatic acquisition of linguistic knowledge within a natural language database interface. In our interface architecture the machine learning module replaces an elaborate semantic analysis component. The learning module learns the correct mapping of a user's input to the corresponding database command based on a collection of past input data. We use an existing interface to a production planning and control system as evaluation and compare the results achieved by different instance-based and model-based learning algorithms. \",\n",
       " \" FASTUS: A Cascaded Finite-State Transducer for Extracting Information from Natural-Language Text FASTUS is a system for extracting information from natural language text for entry into a database and for other applications. It works essentially as a cascaded, nondeterministic finite-state automaton. There are five stages in the operation of FASTUS. In Stage 1, names and other fixed form expressions are recognized. In Stage 2, basic noun groups, verb groups, and prepositions and some other particles are recognized. In Stage 3, certain complex noun groups and verb groups are constructed. Patterns for events of interest are identified in Stage 4 and corresponding ``event structures'' are built. In Stage 5, distinct event structures that describe the same event are identified and merged, and these are used in generating database entries. This decomposition of language processing enables the system to do exactly the right amount of domain-independent syntax, so that domain-dependent semantic and pragmatic processing can be applied to the right larger-scale structures. FASTUS is very efficient and effective, and has been used successfully in a number of applications. \",\n",
       " ' Incorporating POS Tagging into Language Modeling Language models for speech recognition tend to concentrate solely on recognizing the words that were spoken. In this paper, we redefine the speech recognition problem so that its goal is to find both the best sequence of words and their syntactic role (part-of-speech) in the utterance. This is a necessary first step towards tightening the interaction between speech recognition and natural language understanding. ',\n",
       " ' Translation Methodology in the Spoken Language Translator: An Evaluation In this paper we describe how the translation methodology adopted for the Spoken Language Translator SLT addresses the characteristics of the speech translation task in a context where it is essential to achieve easy customization to new languages and new domains. We then discuss the issues that arise in any attempt to evaluate a speech translator, and present the results of such an evaluation carried out on SLT for several language pairs. ',\n",
       " ' Sense Tagging: Semantic Tagging with a Lexicon Sense tagging, the automatic assignment of the appropriate sense from some lexicon to each of the words in a text, is a specialised instance of the general problem of semantic tagging by category or type. We discuss which recent word sense disambiguation algorithms are appropriate for sense tagging. It is our belief that sense tagging can be carried out effectively by combining several simple, independent, methods and we include the design of such a tagger. A prototype of this system has been implemented, correctly tagging 86% of polysemous word tokens in a small test set, providing evidence that our hypothesis is correct. ',\n",
       " ' A Complete Classification of Tractability in RCC-5 We investigate the computational properties of the spatial algebra RCC-5 which is a restricted version of the RCC framework for spatial reasoning. The satisfiability problem for RCC-5 is known to be NP-complete but not much is known about its approximately four billion subclasses. We provide a complete classification of satisfiability for all these subclasses into polynomial and NP-complete respectively. In the process, we identify all maximal tractable subalgebras which are four in total. ',\n",
       " \" Flaw Selection Strategies for Partial-Order Planning Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link POCL planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair LCFR strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness. \",\n",
       " ' Assigning Grammatical Relations with a Back-off Model This paper presents a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs. It makes use of an unsupervised learning procedure to collect training and test data, and the back-off model to make assignment decisions. ',\n",
       " ' An Empirical Comparison of Probability Models for Dependency Grammar This technical report is an appendix to Eisner (1996): it gives superior experimental results that were reported only in the talk version of that paper. Eisner (1996) trained three probability models on a small set of about 4,000 conjunction-free, dependency-grammar parses derived from the Wall Street Journal section of the Penn Treebank, and then evaluated the models on a held-out test set, using a novel O(n^3) parsing algorithm. The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences. As reported at the talk, the more extensive training yields greatly improved performance. Nearly half the sentences are parsed with no misattachments; two-thirds are parsed with at most one misattachment. Of the models described in the original written paper, the best score is still obtained with the generative (top-down) model C. However, slightly better models are also explored, in particular, two variants on the comprehension (bottom-up) model B. The better of these has an attachment accuracy of 90%, and (unlike model C) tags words more accurately than the comparable trigram tagger. Differences are statistically significant. If tags are roughly known in advance, search error is all but eliminated and the new model attains an attachment accuracy of 93%. We find that the parser of Collins (1996), when combined with a highly-trained tagger, also achieves 93% when trained and tested on the same sentences. Similarities and differences are discussed. ',\n",
       " ' Learning Parse and Translation Decisions From Examples With Rich Context We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state. ',\n",
       " ' Comparing a Linguistic and a Stochastic Tagger Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed. ',\n",
       " \" Three New Probabilistic Models for Dependency Parsing: An Exploration After presenting a novel O(n^3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative (i.e., top-down) model performs significantly better than the others, and does about equally well at assigning part-of-speech tags. \",\n",
       " ' Aggregate and mixed-order Markov models for statistical language processing We consider the use of language models whose size and accuracy are intermediate between different order n-gram models. Two types of models are studied in particular. Aggregate Markov models are class-based bigram models in which the mapping from words to classes is probabilistic. Mixed-order Markov models combine bigram models whose predictions are conditioned on different words. Both types of models are trained by Expectation-Maximization EM algorithms for maximum likelihood estimation. We examine smoothing procedures in which these models are interposed between different order n-grams. This is found to significantly reduce the perplexity of unseen word combinations. ',\n",
       " \" Distinguishing Word Senses in Untagged Text This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. \",\n",
       " ' Library of Practical Abstractions, Release 1.2 The library of practical abstractions LIBPA provides efficient implementations of conceptually simple abstractions, in the C programming language. We believe that the best library code is conceptually simple so that it will be easily understood by the application programmer; parameterized by type so that it enjoys wide applicability; and at least as efficient as a straightforward special-purpose implementation. You will find that our software satisfies the highest standards of software design, implementation, testing, and benchmarking. The current LIBPA release is a source code distribution only. It consists of modules for portable memory management, one dimensional arrays of arbitrary types, compact symbol tables, hash tables for arbitrary types, a trie module for length-delimited strings over arbitrary alphabets, single precision floating point numbers with extended exponents, and logarithmic representations of probability values using either fixed or floating point numbers. We have used LIBPA to implement a wide range of statistical models for both continuous and discrete domains. The time and space efficiency of LIBPA has allowed us to build larger statistical models than previously reported, and to investigate more computationally-intensive techniques than previously possible. We have found LIBPA to be indispensible in our own research, and hope that you will find it useful in yours. ',\n",
       " \" Mistake-Driven Learning in Text Categorization Learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words. Three characteristic properties of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance. In this work we study three mistake-driven learning algorithms for a typical task of this nature -- text categorization. We argue that these algorithms -- which categorize documents by learning a linear separator in the feature space -- have a few properties that make them ideal for this domain. We then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain. In particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training. Overall, we present an algorithm, a variation of Littlestone's Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set. \",\n",
       " ' A Corpus-Based Approach for Building Semantic Lexicons Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon. ',\n",
       " \" Applying Reliability Metrics to Co-Reference Annotation Studies of the contextual and linguistic factors that constrain discourse phenomena such as reference are coming to depend increasingly on annotated language corpora. In preparing the corpora, it is important to evaluate the reliability of the annotation, but methods for doing so have not been readily available. In this report, I present a method for computing reliability of coreference annotation. First I review a method for applying the information retrieval metrics of recall and precision to coreference annotation proposed by Marc Vilain and his collaborators. I show how this method makes it possible to construct contingency tables for computing Cohen's Kappa, a familiar reliability metric. By comparing recall and precision to reliability on the same data sets, I also show that recall and precision can be misleadingly high. Because Kappa factors out chance agreement among coders, it is a preferable measure for developing annotated corpora where no pre-existing target annotation exists. \",\n",
       " ' Exemplar-Based Word Sense Disambiguation: Some Recent Improvements In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of , the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best , we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in \\\\cite{ng96}. The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in \\\\cite{mooney96} to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. ',\n",
       " \" Probabilistic Coreference in Information Extraction Certain applications require that the output of an information extraction system be probabilistic, so that a downstream system can reliably fuse the output with possibly contradictory information from other sources. In this paper we consider the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions. We present the results of initial experiments with several approaches to estimating such distributions in an application using SRI's FASTUS information extraction system. \",\n",
       " ' A Linear Observed Time Statistical Parser Based on Maximum Entropy Models This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall. ',\n",
       " ' Determining Internal and External Indices for Chart Generation This paper presents a compilation procedure which determines internal and external indices for signs in a unification based grammar to be used in improving the computational efficiency of lexicalist chart generation. The procedure takes as input a grammar and a set of feature paths indicating the position of semantic indices in a sign, and calculates the fixed-point of a set of equations derived from the grammar. The result is a set of independent constraints stating which indices in a sign can be bound to other signs within a complete sentence. Based on these constraints, two tests are formulated which reduce the search space during generation. ',\n",
       " ' Text Segmentation Using Exponential Models This paper introduces a new statistical approach to partitioning text automatically into coherent segments. Our approach enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text. To aid its search, the system consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data. We also propose a new probabilistically motivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmentation algorithms. Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approach in two very different domains, Wall Street Journal articles and the TDT Corpus, a collection of newswire articles and broadcast news transcripts. ',\n",
       " ' Evaluating Competing Agent Strategies for a Voice Email Agent This paper reports experimental results comparing a mixed-initiative to a system-initiative dialog strategy in the context of a personal voice email agent. To independently test the effects of dialog strategy and user expertise, users interact with either the system-initiative or the mixed-initiative agent to perform three successive tasks which are identical for both agents. We report performance comparisons across agent strategies as well as over tasks. This evaluation utilizes and tests the PARADISE evaluation framework, and discusses the performance function derivable from the experimental data. ',\n",
       " \" A Model of Lexical Attraction and Repulsion This paper introduces new methods based on exponential families for modeling the correlations between words in text and speech. While previous work assumed the effects of word co-occurrence statistics to be constant over a window of several hundred words, we show that their influence is nonstationary on a much smaller time scale. Empirical data drawn from English and Japanese text, as well as conversational speech, reveals that the ``attraction'' between words decays exponentially, while stylistic and syntactic contraints create a ``repulsion'' between words that discourages close co-occurrence. We show that these characteristics are well described by simple mixture models based on two-stage exponential distributions which can be trained using the EM algorithm. The resulting distance distributions can then be incorporated as penalizing features in an exponential language model. \",\n",
       " ' An Empirical Approach to Temporal Reference Resolution This paper presents the results of an empirical investigation of temporal reference resolution in scheduling dialogs. The algorithm adopted is primarily a linear-recency based approach that does not include a model of global focus. A fully automatic system has been developed and evaluated on unseen test data with good results. This paper presents the results of an intercoder reliability study, a model of temporal reference resolution that supports linear recency and has very good coverage, the results of the system evaluated on unseen test data, and a detailed analysis of the dialogs assessing the viability of the approach. ',\n",
       " ' An Efficient Distribution of Labor in a Two Stage Robust Interpretation Process Although Minimum Distance Parsing MDP offers a theoretically attractive solution to the problem of extragrammaticality, it is often computationally infeasible in large scale practical applications. In this paper we present an alternative approach where the labor is distributed between a more restrictive partial parser and a repair module. Though two stage approaches have grown in popularity in recent years because of their efficiency, they have done so at the cost of requiring hand coded repair heuristics. In contrast, our two stage approach does not require any hand coded knowledge sources dedicated to repair, thus making it possible to achieve a similar run time advantage over MDP without losing the quality of domain independence. ',\n",
       " ' Three Generative, Lexicalised Models for Statistical Parsing In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96). ',\n",
       " \" A Lexicalist Approach to the Translation of Colloquial Text Colloquial English CE as found in television programs or typical conversations is different than text found in technical manuals, newspapers and books. Phrases tend to be shorter and less sophisticated. In this paper, we look at some of the theoretical and implementational issues involved in translating CE. We present a fully automatic large-scale multilingual natural language processing system for translation of CE input text, as found in the commercially transmitted closed-caption television signal, into simple target sentences. Our approach is based on the Whitelock's Shake and Bake machine translation paradigm, which relies heavily on lexical resources. The system currently translates from English to Spanish with the translation modules for Brazilian Portuguese under development. \",\n",
       " ' An Information Extraction Core System for Real World German Text Processing This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner. ',\n",
       " ' Name Searching and Information Retrieval The main application of name searching has been name matching in a database of names. This paper discusses a different application: improving information retrieval through name recognition. It investigates name recognition accuracy, and the effect on retrieval performance of indexing and searching personal names differently from non-name terms in the context of ranked retrieval. The main conclusions are: that name recognition in text can be effective; that names occur frequently enough in a variety of domains, including those of legal documents and news databases, to make recognition worthwhile; and that retrieval performance can be improved using name searching. ',\n",
       " ' A Portable Algorithm for Mapping Bitext Correspondence The first step in most empirical work in multilingual NLP is to construct maps of the correspondence between texts and their translations ({\\\\bf bitext maps}). The Smooth Injective Map Recognizer SIMR algorithm presented here is a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence. SIMR is faster and significantly more accurate than other algorithms in the literature. The algorithm is robust enough to use on noisy texts, such as those resulting from OCR input, and on translations that are not very literal. SIMR encapsulates its language-specific heuristics, so that it can be ported to any language pair with a minimal effort. ',\n",
       " \" Automatic Discovery of Non-Compositional Compounds in Parallel Data Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English. Spaces between words offer an easy first approximation, but this approximation is not good enough for machine translation MT, where many word sequences are not translated word-for-word. This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit. The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages. It can discover hundreds of non-compositional compounds on each iteration, and constructs longer compounds out of shorter ones. Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output. The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations. \",\n",
       " \" A Word-to-Word Model of Translational Equivalence Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model. For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter. This feature makes the model more suitable for applications that are not fully statistical. The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc.. Our model can link word tokens in parallel texts as well as other translation models in the literature. Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. \",\n",
       " ' Representing Constraints with Automata In this paper we describe an approach to constraint-based syntactic theories in terms of finite tree automata. The solutions to constraints expressed in weak monadic second order MSO logic are represented by tree automata recognizing the assignments which make the formulas true. We show that this allows an efficient representation of knowledge about the content of constraints which can be used as a practical tool for grammatical theory verification. We achieve this by using the intertranslatability of formulas of MSO logic and tree automata and the embedding of MSO logic into a constraint logic programming scheme. The usefulness of the approach is discussed with examples from the realm of Principles-and-Parameters based parsing. ',\n",
       " \" Efficient Construction of Underspecified Semantics under Massive Ambiguity We investigate the problem of determining a compact underspecified semantical representation for sentences that may be highly ambiguous. Due to combinatorial explosion, the naive method of building semantics for the different syntactic readings independently is prohibitive. We present a method that takes as input a syntactic parse forest with associated constraint-based semantic construction rules and directly builds a packed semantic structure. The algorithm is fully implemented and runs in in sentence length, if the grammar meets some reasonable `normality' restrictions. \",\n",
       " ' A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search Difficulty The easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning. We test the generality of this explanation by examining one of its predictions: if the number of solutions is held fixed by the choice of problems, then increased pruning should lead to a monotonic decrease in search cost. Instead, we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant, for some search methods. This generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost. In these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems, rather than changing numbers of solutions. ',\n",
       " ' Defining Relative Likelihood in Partially-Ordered Preferential Structures Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic. Lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds. Complications arise when examining partial orders that are not present for total orders. There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning. ',\n",
       " \" Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time This paper combines two important directions of research in temporal resoning: that of finding maximal tractable subclasses of Allen's interval algebra, and that of reasoning with metric temporal information. Eight new maximal tractable subclasses of Allen's interval algebra are presented, some of them subsuming previously reported tractable algebras. The algebras allow for metric temporal constraints on interval starting or ending points, using the recent framework of Horn DLRs. Two of the algebras can express the notion of sequentiality between intervals, being the first such algebras admitting both qualitative and metric time. \",\n",
       " ' Learning Parse and Translation Decisions From Examples With Rich Context We propose a system for parsing and translating natural language that learns from examples and uses some background knowledge. As our parsing model we choose a deterministic shift-reduce type parser that integrates part-of-speech tagging and syntactic and semantic processing. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a parser in the form of a decision structure, a generalization of decision trees. To learn good parsing and translation decisions, our system relies heavily on context, as encoded in currently 205 features describing the morphological, syntactical and semantical aspects of a given parse state. Compared with recent probabilistic systems that were trained on 40,000 sentences, our system relies on more background knowledge and a deeper analysis, but radically fewer examples, currently 256 sentences. We test our parser on lexically limited sentences from the Wall Street Journal and achieve accuracy rates of 89.8% for labeled precision, 98.4% for part of speech tagging and 56.3% of test sentences without any crossing brackets. Machine translations of 32 Wall Street Journal sentences to German have been evaluated by 10 bilingual volunteers and been graded as 2.4 on a 1.0 (best) to 6.0 (worst) scale for both grammatical correctness and meaning preservation. ',\n",
       " ' The TreeBanker: a Tool for Supervised Training of Parsed Corpora I describe the TreeBanker, a graphical tool for the supervised training involved in domain customization of the disambiguation component of a speech- or language-understanding system. The TreeBanker presents a user, who need not be a system expert, with a range of properties that distinguish competing analyses for an utterance and that are relatively easy to judge. This allows training on a corpus to be completed in far less time, and with far less expertise, than would be needed if analyses were inspected directly: it becomes possible for a corpus of about 20,000 sentences of the complexity of those in the ATIS corpus to be judged in around three weeks of work by a linguistically aware non-expert. ',\n",
       " ' Reluctant Paraphrase: Textual Restructuring under an Optimisation Model This paper develops a computational model of paraphrase under which text modification is carried out reluctantly; that is, there are external constraints, such as length or readability, on an otherwise ideal text, and modifications to the text are necessary to ensure conformance to these constraints. This problem is analogous to a mathematical optimisation problem: the textual constraints can be described as a set of constraint equations, and the requirement for minimal change to the text can be expressed as a function to be minimised; so techniques from this domain can be used to solve the problem. The work is done as part of a computational paraphrase system using the XTAG system as a base. The paper will present a theoretical computational framework for working within the Reluctant Paraphrase paradigm: three types of textual constraints are specified, effects of paraphrase on text are described, and a model incorporating mathematical optimisation techniques is outlined. ',\n",
       " ' Automatic Detection of Text Genre As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification. We propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties. ',\n",
       " ' A Flexible POS tagger Using an Automatically Acquired Language Model We present an algorithm that automatically learns context constraints using statistical decision trees. We then use the acquired constraints in a flexible POS tagger. The tagger is able to use information of any degree: n-grams, automatically learned context constraints, linguistically motivated manually written constraints, etc. The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results. The tagger has been tested and evaluated on the WSJ corpus. ',\n",
       " \" Discourse Preferences in Dynamic Logic In order to enrich dynamic semantic theories with a `pragmatic' capacity, we combine dynamic and nonmonotonic (preferential) logics in a modal logic setting. We extend a fragment of Van Benthem and De Rijke's dynamic modal logic with additional preferential operators in the underlying static logic, which enables us to define defeasible (pragmatic) entailments over a given piece of discourse. We will show how this setting can be used for a dynamic logical analysis of preferential resolutions of ambiguous pronouns in discourse. \",\n",
       " ' Intrasentential Centering: A Case Study One of the necessary extensions to the centering model is a mechanism to handle pronouns with intrasentential antecedents. Existing centering models deal only with discourses consisting of simple sentences. It leaves unclear how to delimit center-updating utterance units and how to process complex utterances consisting of multiple clauses. In this paper, I will explore the extent to which a straightforward extension of an existing intersentential centering model contributes to this effect. I will motivate an approach that breaks a complex sentence into a hierarchy of center-updating units and proposes the preferred interpretation of a pronoun in its local context arbitrarily deep in the given sentence structure. This approach will be substantiated with examples from naturally occurring written discourses. ',\n",
       " ' Finite State Transducers Approximating Hidden Markov Models This paper describes the conversion of a Hidden Markov Model into a sequential transducer that closely approximates the behavior of the stochastic model. This transformation is especially advantageous for part-of-speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors. The speed of tagging is also improved. The described methods have been implemented and successfully tested on six languages. ',\n",
       " ' Recognizing Referential Links: An Information Extraction Perspective We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types. ',\n",
       " \" Stressed and Unstressed Pronouns: Complementary Preferences I present a unified account of interpretation preferences of stressed and unstressed pronouns in discourse. The central intuition is the Complementary Preference Hypothesis that predicts the interpretation preference of a stressed pronoun from that of an unstressed pronoun in the same discourse position. The base preference must be computed in a total pragmatics module including commonsense preferences. The focus constraint in Rooth's theory of semantic focus is interpreted to be the salient subset of the domain in the local attentional state in the discourse context independently motivated for other purposes in Centering Theory. \",\n",
       " ' Tailored Patient Information: Some Issues and Questions Tailored patient information TPI systems are computer programs which produce personalised heath-information material for patients. TPI systems are of growing interest to the natural-language generation NLG community; many TPI systems have also been developed in the medical community, usually with mail-merge technology. No matter what technology is used, experience shows that it is not easy to field a TPI system, even if it is shown to be effective in clinical trials. In this paper we discuss some of the difficulties in fielding TPI systems. This is based on our experiences with 2 TPI systems, one for generating asthma-information booklets and one for generating smoking-cessation letters. ',\n",
       " ' Experiences with the GTU grammar development environment In this paper we describe our experiences with a tool for the development and testing of natural language grammars called GTU (German: Grammatik-Testumgebumg; grammar test environment). GTU supports four grammar formalisms under a window-oriented user interface. Additionally, it contains a set of German test sentences covering various syntactic phenomena as well as three types of German lexicons that can be attached to a grammar via an integrated lexicon interface. What follows is a description of the experiences we gained when we used GTU as a tutoring tool for students and as an experimental tool for CL researchers. From these we will derive the features necessary for a future grammar workbench. ',\n",
       " ' Adjunction As Substitution: An Algebraic Formulation of Regular, Context-Free and Tree Adjoining Languages This note presents a method of interpreting the tree adjoining languages as the natural third step in a hierarchy that starts with the regular and the context-free languages. The central notion in this account is that of a higher-order substitution. Whereas in traditional presentations of rule systems for abstract language families the emphasis has been on a first-order substitution process in which auxiliary variables are replaced by elements of the carrier of the proper algebra - concatenations of terminal and auxiliary category symbols in the string case - we lift this process to the level of operations defined on the elements of the carrier of the algebra. Our own view is that this change of emphasis provides the adequate platform for a better understanding of the operation of adjunction. To put it in a nutshell: Adjoining is not a first-order, but a second-order substitution operation. ',\n",
       " \" A lexical database tool for quantitative phonological research A lexical database tool tailored for phonological research is described. Database fields include transcriptions, glosses and hyperlinks to speech files. Database queries are expressed using HTML forms, and these permit regular expression search on any combination of fields. Regular expressions are passed directly to a Perl CGI program, enabling the full flexibility of Perl extended regular expressions. The regular expression notation is extended to better support phonological searches, such as search for minimal pairs. Search results are presented in the form of HTML or LaTeX tables, where each cell is either a number (representing frequency) or a designated subset of the fields. Tables have up to four dimensions, with an elegant system for specifying which fragments of which fields should be used for the row/column labels. The tool offers several advantages over traditional methods of analysis: (i) it supports a quantitative method of doing phonological research; (ii) it gives universal access to the same set of informants; (iii) it enables other researchers to hear the original speech data without having to rely on published transcriptions; (iv) it makes the full power of regular expression search available, and search results are full multimedia documents; and (v) it enables the early refutation of false hypotheses, shortening the analysis-hypothesis-test loop. A life-size application to an African tone language (Dschang) is used for exemplification throughout the paper. The database contains 2200 records, each with approximately 15 fields. Running on a PC laptop with a stand-alone web server, the `Dschang HyperLexicon' has already been used extensively in phonological fieldwork and analysis in Cameroon. \",\n",
       " \" On Cloning Context-Freeness To Rogers (1994) we owe the insight that monadic second order predicate logic with multiple successors MSO is well suited in many respects as a realistic formal base for syntactic theorizing. However, the agreeable formal properties of this logic come at a cost: MSO is equivalent with the class of regular tree automata/grammars, and, thereby, with the class of context-free languages. This paper outlines one approach towards a solution of MSO's expressivity problem. On the background of an algebraically refined Chomsky hierarchy, which allows the definition of several classes of languages--in particular, a whole hierarchy between CF and CS--via regular tree grammars over unambiguously derivable alphabets of varying complexity plus their respective yield-functions, it shows that not only some non-context-free string languages can be captured by context-free means in this way, but that this approach can be generalized to the corresponding structures. I.e., non-recognizable sets of structures can--up to homomorphism--be coded context-freely. Since the class of languages covered--Fischer's (1968} OI family of indexed languages--includes all attested instances of non-context-freeness in natural language, there exists an indirect, to be sure, but completely general way to formally describe the natural languages using a weak framework like MSO. \",\n",
       " ' Towards a PURE Spoken Dialogue System for Information Access With the rapid explosion of the World Wide Web, it is becoming increasingly possible to easily acquire a wide variety of information such as flight schedules, yellow pages, used car prices, current stock prices, entertainment event schedules, account balances, etc. It would be very useful to have spoken dialogue interfaces for such information access tasks. We identify portability, usability, robustness, and extensibility as the four primary design objectives for such systems. In other words, the objective is to develop a PURE (Portable, Usable, Robust, Extensible) system. A two-layered dialogue architecture for spoken dialogue systems is presented where the upper layer is domain-independent and the lower layer is domain-specific. We are implementing this architecture in a mixed-initiative system that accesses flight arrival/departure information from the World Wide Web. ',\n",
       " ' Tagging Grammatical Functions This paper addresses issues in automated treebank construction. We show how standard part-of-speech tagging techniques extend to the more general problem of structural annotation, especially for determining grammatical functions and syntactic categories. Annotation is viewed as an interactive process where manual and automatic processing alternate. Efficiency and accuracy results are presented. We also discuss further automation steps. ',\n",
       " ' On aligning trees The increasing availability of corpora annotated for linguistic structure prompts the question: if we have the same texts, annotated for phrase structure under two different schemes, to what extent do the annotations agree on structuring within the text? We suggest the term tree alignment to indicate the situation where two markup schemes choose to bracket off the same text elements. We propose a general method for determining agreement between two analyses. We then describe an efficient implementation, which is also modular in that the core of the implementation can be reused regardless of the format of markup used in the corpora. The output of the implementation on the Susanne and Penn treebank corpora is discussed. ',\n",
       " ' Generating Coherent Messages in Real-time Decision Support: Exploiting Discourse Theory for Discourse Practice This paper presents a message planner, TraumaGEN, that draws on rhetorical structure and discourse theory to address the problem of producing integrated messages from individual critiques, each of which is designed to achieve its own communicative goal. TraumaGEN takes into account the purpose of the messages, the situation in which the messages will be received, and the social role of the system. ',\n",
       " ' Multilingual phonological analysis and speech synthesis We give an overview of multilingual speech synthesis using the IPOX system. The first part discusses work in progress for various languages: Tashlhit Berber, Urdu and Dutch. The second part discusses a multilingual phonological grammar, which can be adapted to a particular language by setting parameters and adding language-specific details. ',\n",
       " ' Stochastic phonological grammars and acceptability In foundational works of generative phonology it is claimed that subjects can reliably discriminate between possible but non-occurring words and words that could not be English. In this paper we examine the use of a probabilistic phonological parser for words to model experimentally-obtained judgements of the acceptability of a set of nonsense words. We compared various methods of scoring the goodness of the parse as a predictor of acceptability. We found that the probability of the worst part is not the best score of acceptability, indicating that classical generative phonology and Optimality Theory miss an important fact, as these approaches do not recognise a mechanism by which the frequency of well-formed parts may ameliorate the unacceptability of low-frequency parts. We argue that probabilistic generative grammars are demonstrably a more psychologically realistic model of phonological competence than standard generative phonology or Optimality Theory. ',\n",
       " ' A Czech Morphological Lexicon In this paper, a treatment of Czech phonological rules in two-level morphology approach is described. First the possible phonological alternations in Czech are listed and then their treatment in a practical application of a Czech morphological lexicon. ',\n",
       " ' Expectations in Incremental Discourse Processing The way in which discourse features express connections back to the previous discourse has been described in the literature in terms of adjoining at the right frontier of discourse structure. But this does not allow for discourse features that express expectations about what is to come in the subsequent discourse. After characterizing these expectations and their distribution in text, we show how an approach that makes use of substitution as well as adjoining on a suitably defined right frontier, can be used to both process expectations and constrain discouse processing in general. ',\n",
       " ' Epistemic NP Modifiers The paper considers participles such as unknown , identified and unspecified , which in sentences such as Solange is staying in an unknown hotel have readings equivalent to an indirect question Solange is staying in a hotel, and it is not known which hotel it is. We discuss phenomena including disambiguation of quantifier scope and a restriction on the set of determiners which allow the reading in question. Epistemic modifiers are analyzed in a DRT framework with file (information state) discourse referents. The proposed semantics uses a predication on files and discourse referents which is related to recent developments in dynamic modal predicate calculus. It is argued that a compositional DRT semantics must employ a semantic type of discourse referents, as opposed to just a type of individuals. A connection is developed between the scope effects of epistemic modifiers and the scope-disambiguating effect of a certain . ',\n",
       " ' Natural Language Generation in Healthcare: Brief Review Good communication is vital in healthcare, both among healthcare professionals, and between healthcare professionals and their patients. And well-written documents, describing and/or explaining the information in structured databases may be easier to comprehend, more edifying and even more convincing, than the structured data, even when presented in tabular or graphic form. Documents may be automatically generated from structured data, using techniques from the field of natural language generation. These techniques are concerned with how the content, organisation and language used in a document can be dynamically selected, depending on the audience and context. They have been used to generate health education materials, explanations and critiques in decision support systems, and medical reports and progress notes. ',\n",
       " \" Structure and Ostension in the Interpretation of Discourse Deixis This paper examines demonstrative pronouns used as deictics to refer to the interpretation of one or more clauses. Although this usage is frowned upon in style manuals (for example Strunk and White (1959) state that ``This. The pronoun 'this', referring to the complete sense of a preceding sentence or clause, cannot always carry the load and so may produce an imprecise statement.''), it is nevertheless very common in written text. Handling this usage poses a problem for Natural Language Understanding systems. The solution I propose is based on distinguishing between what can be pointed to and what can be referred to by virtue of pointing. I argue that a restricted set of discourse segments yield what such demonstrative pronouns can point to and a restricted set of what Nunberg (1979) has called referring functions yield what they can refer to by virtue of that pointing. \",\n",
       " \" Centering, Anaphora Resolution, and Discourse Structure Centering was formulated as a model of the relationship between attentional state, the form of referring expressions, and the coherence of an utterance within a discourse segment (Grosz, Joshi and Weinstein, 1986; Grosz, Joshi and Weinstein, 1995). In this chapter, I argue that the restriction of centering to operating within a discourse segment should be abandoned in order to integrate centering with a model of global discourse structure. The within-segment restriction causes three problems. The first problem is that centers are often continued over discourse segment boundaries with pronominal referring expressions whose form is identical to those that occur within a discourse segment. The second problem is that recent work has shown that listeners perceive segment boundaries at various levels of granularity. If centering models a universal processing phenomenon, it is implausible that each listener is using a different centering algorithm.The third issue is that even for utterances within a discourse segment, there are strong contrasts between utterances whose adjacent utterance within a segment is hierarchically recent and those whose adjacent utterance within a segment is linearly recent. This chapter argues that these problems can be eliminated by replacing Grosz and Sidner's stack model of attentional state with an alternate model, the cache model. I show how the cache model is easily integrated with the centering algorithm, and provide several types of data from naturally occurring discourses that support the proposed integrated model. Future work should provide additional support for these claims with an examination of a larger corpus of naturally occurring discourses. \",\n",
       " ' Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication Valiant showed that Boolean matrix multiplication BMM can be used for CFG parsing. We prove a dual result: CFG parsers running in time on a grammar and a string can be used to multiply Boolean matrices in time . In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. ',\n",
       " ' Global Thresholding and Multiple Pass Parsing We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms. ',\n",
       " ' DIA-MOLE: An Unsupervised Learning Approach to Adaptive Dialogue Models for Spoken Dialogue Systems The DIAlogue MOdel Learning Environment supports an engineering-oriented approach towards dialogue modelling for a spoken-language interface. Major steps towards dialogue models is to know about the basic units that are used to construct a dialogue model and possible sequences. In difference to many other approaches a set of dialogue acts is not predefined by any theory or manually during the engineering process, but is learned from data that are available in an avised spoken dialogue system. The architecture is outlined and the approach is applied to the domain of appointment scheduling. Even though based on a word correctness of about 70% predictability of dialogue acts in DIA-MOLE turns out to be comparable to human-assigned dialogue acts. ',\n",
       " ' Similarity-Based Methods For Word Sense Disambiguation We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates. ',\n",
       " ' A complexity measure for diachronic Chinese phonology This paper addresses the problem of deriving distance measures between parent and daughter languages with specific relevance to historical Chinese phonology. The diachronic relationship between the languages is modelled as a Probabilistic Finite State Automaton. The Minimum Message Length principle is then employed to find the complexity of this structure. The idea is that this measure is representative of the amount of dissimilarity between the two languages. ',\n",
       " ' Encoding Frequency Information in Lexicalized Grammars We address the issue of how to associate frequency information with lexicalized grammar formalisms, using Lexicalized Tree Adjoining Grammar as a representative framework. We consider systematically a number of alternative probabilistic frameworks, evaluating their adequacy from both a theoretical and empirical perspective using data from existing large treebanks. We also propose three orthogonal approaches for backing off probability estimates to cope with the large number of parameters involved. ',\n",
       " ' Similarity-Based Approaches to Natural Language Processing This thesis presents two similarity-based approaches to sparse data problems. The first approach is to build soft, hierarchical clusters: soft, because each event belongs to each cluster with some probability; hierarchical, because cluster centroids are iteratively split to model finer distinctions. Our second approach is a nearest-neighbor approach: instead of calculating a centroid for each class, as in the hierarchical clustering approach, we in essence build a cluster around each word. We compare several such nearest-neighbor approaches on a word sense disambiguation task and find that as a whole, their performance is far superior to that of standard methods. In another set of experiments, we show that using estimation techniques based on the nearest-neighbor model enables us to achieve perplexity reductions of more than 20 percent over standard techniques in the prediction of low-frequency events, and statistically significant speech recognition error-rate reduction. ',\n",
       " ' explanation-based learning of data oriented parsing This paper presents a new view of Explanation-Based Learning EBL of natural language parsing. Rather than employing EBL for specializing parsers by inferring new ones, this paper suggests employing EBL for learning how to reduce ambiguity only partially. The present method consists of an EBL algorithm for learning partial-parsers, and a parsing algorithm which combines partial-parsers with existing ``full-parsers . The learned partial-parsers, implementable as Cascades of Finite State Transducers (CFSTs), recognize and combine constituents efficiently, prohibiting spurious overgeneration. The parsing algorithm combines a learned partial-parser with a given full-parser such that the role of the full-parser is limited to combining the constituents, recognized by the partial-parser, and to recognizing unrecognized portions of the input sentence. Besides the reduction of the parse-space prior to disambiguation, the present method provides a way for refining existing disambiguation models that learn stochastic grammars from tree-banks. We exhibit encouraging empirical results using a pilot implementation: parse-space is reduced substantially with minimal loss of coverage. The speedup gain for disambiguation models is exemplified by experiments with the DOP model. ',\n",
       " \" Identifying Hierarchical Structure in Sequences: A linear-time algorithm SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences. \",\n",
       " \" Towards Flexible Teamwork Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results. \",\n",
       " ' The Complexity of Recognition of Linguistically Adequate Dependency Grammars Results of computational complexity exist for a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependency-based formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted dependency grammars, we prove that recognition (and thus, parsing) of linguistically adequate dependency grammars is NP-complete. ',\n",
       " ' Learning Methods for Combining Linguistic Indicators to Classify Verbs Fourteen linguistically-motivated numerical indicators are evaluated for their ability to categorize verbs as either states or events. The values for each indicator are computed automatically across a corpus of text. To improve classification performance, machine learning techniques are employed to combine multiple indicators. Three machine learning methods are compared for this task: decision tree induction, a genetic algorithm, and log-linear regression. ',\n",
       " ' Integrating a Lexical Database and a Training Collection for Text Categorization Automatic text categorization is a complex and useful task for many natural language processing applications. Recent approaches to text categorization focus more on algorithms than on resources involved in this operation. In contrast to this trend, we present an approach based on the integration of widely available resources as lexical databases and training collections to overcome current limitations of the task. Our approach makes use of WordNet synonymy information to increase evidence for bad trained categories. When testing a direct categorization, a WordNet based one, a training algorithm, and our integrated approach, the latter exhibits a better perfomance than any of the others. Incidentally, WordNet based approach perfomance is comparable with the training approach one. ',\n",
       " ' Combining Multiple Methods for the Automatic Construction of Multilingual WordNets This paper explores the automatic construction of a multilingual Lexical Knowledge Base from preexisting lexical resources. First, a set of automatic and complementary techniques for linking Spanish words collected from monolingual and bilingual MRDs to English WordNet synsets are described. Second, we show how resulting data provided by each method is then combined to produce a preliminary version of a Spanish WordNet with an accuracy over 85%. The application of these combinations results on an increment of the extracted connexions of a 40% without losing accuracy. Both coarse-grained (class level) and fine-grained (synset assignment level) confidence ratios are used and evaluated. Finally, the results for the whole process are presented. ',\n",
       " ' A generation algorithm for f-structure representations This paper shows that previously reported generation algorithms run into problems when dealing with f-structure representations. A generation algorithm that is suitable for this type of representations is presented: the Semantic Kernel Generation SKG algorithm. The SKG method has the same processing strategy as the Semantic Head Driven generation SHDG algorithm and relies on the assumption that it is possible to compute the Semantic Kernel SK and non Semantic Kernel (Non-SK) information for each input structure. ',\n",
       " \" Semantic Processing of Out-Of-Vocabulary Words in a Spoken Dialogue System One of the most important causes of failure in spoken dialogue systems is usually neglected: the problem of words that are not covered by the system's vocabulary (out-of-vocabulary or OOV words). In this paper a methodology is described for the detection, classification and processing of OOV words in an automatic train timetable information system. The various extensions that had to be effected on the different modules of the system are reported, resulting in the design of appropriate dialogue strategies, as are encouraging evaluation results on the new versions of the word recogniser and the linguistic processor. \",\n",
       " ' Using WordNet to Complement Training Information in Text Categorization Automatic Text Categorization TC is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection. We suggest the utilization of additional resources like lexical databases to increase the amount of information that TC systems make use of, and thus, to improve their performance. Our approach integrates WordNet information with two training approaches through the Vector Space Model. The training approaches we test are the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning) algorithms. Results obtained from evaluation show that the integration of WordNet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories. ',\n",
       " ' Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task. ',\n",
       " \" Evaluating Parsing Schemes with Entropy Indicators This paper introduces an objective metric for evaluating a parsing scheme. It is based on Shannon's original work with letter sequences, which can be extended to part-of-speech tag sequences. It is shown that this regular language is an inadequate model for natural language, but a representation is used that models language slightly higher in the Chomsky hierarchy. We show how the entropy of parsed and unparsed sentences can be measured. If the entropy of the parsed sentence is lower, this indicates that some of the structure of the language has been captured. We apply this entropy indicator to support one particular parsing scheme that effects a top down segmentation. This approach could be used to decompose the parsing task into computationally more tractable subtasks. It also lends itself to the extraction of predicate/argument structure. \",\n",
       " ' An Abstract Machine for Unification Grammars This work describes the design and implementation of an abstract machine, Amalia, for the linguistic formalism ALE, which is based on typed feature structures. This formalism is one of the most widely accepted in computational linguistics and has been used for designing grammars in various linguistic theories, most notably HPSG. Amalia is composed of data structures and a set of instructions, augmented by a compiler from the grammatical formalism to the abstract instructions, and a (portable) interpreter of the abstract instructions. The effect of each instruction is defined using a low-level language that can be executed on ordinary hardware. The advantages of the abstract machine approach are twofold. From a theoretical point of view, the abstract machine gives a well-defined operational semantics to the grammatical formalism. This ensures that grammars specified using our system are endowed with well defined meaning. It enables, for example, to formally verify the correctness of a compiler for HPSG, given an independent definition. From a practical point of view, Amalia is the first system that employs a direct compilation scheme for unification grammars that are based on typed feature structures. The use of amalia results in a much improved performance over existing systems. In order to test the machine on a realistic application, we have developed a small-scale, HPSG-based grammar for a fragment of the Hebrew language, using Amalia as the development platform. This is the first application of HPSG to a Semitic language. ',\n",
       " ' Message-Passing Protocols for Real-World Parsing -- An Object-Oriented Model and its Preliminary Evaluation We argue for a performance-based design of natural language grammars and their associated parsers in order to meet the constraints imposed by real-world NLP. Our approach incorporates declarative and procedural knowledge about language and language use within an object-oriented specification framework. We discuss several message-passing protocols for parsing and provide reasons for sacrificing completeness of the parse in favor of efficiency based on a preliminary empirical evaluation. ',\n",
       " ' Off-line Parsability and the Well-foundedness of Subsumption Typed feature structures are used extensively for the specification of linguistic information in many formalisms. The subsumption relation orders TFSs by their information content. We prove that subsumption of acyclic TFSs is well-founded, whereas in the presence of cycles general TFS subsumption is not well-founded. We show an application of this result for parsing, where the well-foundedness of subsumption is used to guarantee termination for grammars that are off-line parsable. We define a new version of off-line parsability that is less strict than the existing one; thus termination is guaranteed for parsing with a larger set of grammars. ',\n",
       " ' Using Single Layer Networks for Discrete, Sequential Data: An Example from Natural Language Processing A natural language parser which has been successfully implemented is described. This is a hybrid system, in which neural networks operate within a rule based framework. It can be accessed via telnet for users to try on their own text. (For details, contact the author.) Tested on technical manuals, the parser finds the subject and head of the subject in over 90% of declarative sentences. The neural processing components belong to the class of Generalized Single Layer Networks GSLN. In general, supervised, feed-forward networks need more than one layer to process data. However, in some cases data can be pre-processed with a non-linear transformation, and then presented in a linearly separable form for subsequent processing by a single layer net. Such networks offer advantages of functional transparency and operational speed. For our parser, the initial stage of processing maps linguistic data onto a higher order representation, which can then be analysed by a single layer network. This transformation is supported by information theoretic analysis. ',\n",
       " \" Amalia -- A Unified Platform for Parsing and Generation Contemporary linguistic theories (in particular, HPSG) are declarative in nature: they specify constraints on permissible structures, not how such structures are to be computed. Grammars designed under such theories are, therefore, suitable for both parsing and generation. However, practical implementations of such theories don't usually support bidirectional processing of grammars. We present a grammar development system that includes a compiler of grammars (for parsing and generation) to abstract machine instructions, and an interpreter for the abstract machine language. The generation compiler inverts input grammars (designed for parsing) to a form more suitable for generation. The compiled grammars are then executed by the interpreter using one control strategy, regardless of whether the grammar is the original or the inverted version. We thus obtain a unified, efficient platform for developing reversible grammars. \",\n",
       " ' Segmentation of Expository Texts by Hierarchical Agglomerative Clustering We propose a method for segmentation of expository texts based on hierarchical agglomerative clustering. The method uses paragraphs as the basic segments for identifying hierarchical discourse structure in the text, applying lexical similarity between them as the proximity test. Linear segmentation can be induced from the identified structure through application of two simple rules. However the hierarchy can be used also for intelligent exploration of the text. The proposed segmentation algorithm is evaluated against an accepted linear segmentation method and shows comparable results. ',\n",
       " \" Analysis of Three-Dimensional Protein Images A fundamental goal of research in molecular biology is to understand protein structure. Protein crystallography is currently the most successful method for determining the three-dimensional (3D) conformation of a protein, yet it remains labor intensive and relies on an expert's ability to derive and evaluate a protein scene model. In this paper, the problem of protein structure determination is formulated as an exercise in scene analysis. A computational methodology is presented in which a 3D image of a protein is segmented into a graph of critical points. Bayesian and certainty factor approaches are described and used to analyze critical point graphs and identify meaningful substructures, such as alpha-helices and beta-sheets. Results of applying the methodologies to protein images at low and medium resolution are reported. The research is related to approaches to representation, segmentation and classification in vision, as well as to top-down approaches to protein structure prediction. \",\n",
       " \" Tagging French Without Lexical Probabilities -- Combining Linguistic Knowledge And Statistical Learning This paper explores morpho-syntactic ambiguities for French to develop a strategy for part-of-speech disambiguation that a) reflects the complexity of French as an inflected language, b) optimizes the estimation of probabilities, c) allows the user flexibility in choosing a tagset. The problem in extracting lexical probabilities from a limited training corpus is that the statistical model may not necessarily represent the use of a particular word in a particular context. In a highly morphologically inflected language, this argument is particularly serious since a word can be tagged with a large number of parts of speech. Due to the lack of sufficient training data, we argue against estimating lexical probabilities to disambiguate parts of speech in unrestricted texts. Instead, we use the strength of contextual probabilities along with a feature we call ``genotype'', a set of tags associated with a word. Using this knowledge, we have built a part-of-speech tagger that combines linguistic and statistical approaches: contextual information is disambiguated by linguistic rules and n-gram probabilities on parts of speech only are estimated in order to disambiguate the remaining ambiguous tags. \",\n",
       " ' Use of Weighted Finite State Transducers in Part of Speech Tagging This paper addresses issues in part of speech disambiguation using finite-state transducers and presents two main contributions to the field. One of them is the use of finite-state machines for part of speech tagging. Linguistic and statistical information is represented in terms of weights on transitions in weighted finite-state transducers. Another contribution is the successful combination of techniques -- linguistic and statistical -- for word disambiguation, compounded with the notion of word classes. ',\n",
       " ' Disambiguating with Controlled Disjunctions In this paper, we propose a disambiguating technique called controlled disjunctions. This extension of the so-called named disjunctions relies on the relations existing between feature values (covariation, control, etc.). We show that controlled disjunctions can implement different kind of ambiguities in a consistent and homogeneous way. We describe the integration of controlled disjunctions into a HPSG feature structure representation. Finally, we present a direct implementation by means of delayed evaluation and we develop an example within the functionnal programming paradigm. ',\n",
       " ' Parsing syllables: modeling OT computationally In this paper, I propose to implement syllabification in OT as a parser. I propose several innovations that result in a finite and small candidate set. The candidate set problem is handled with several moves: i) MAX and DEP violations are not hypothesized by the parser, ii) candidates are encoded locally, and iii) EVAL is applied constraint by constraint. The parser I propose is implemented in Prolog. It has a number of desirable consequences. First, it runs and thus provides an existence proof that syllabification can be implemented in OT. There are a number of other desirable consequences as well. First, constraints are implemented as finite-state transducers. Second, the parser makes several interesting claims about the phonological properties of so-called nonrecoverable insertions and deletions. Third, the implementation suggests some particular reformulations of some of the benchmark constraints in the OT arsenal, e.g. *COMPLEX, PARSE, ONSET, and NOCODA. ',\n",
       " ' Attaching Multiple Prepositional Phrases: Generalized Backed-off Estimation There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments. To our knowledge, however, these investigations have only considered the problem of attaching the first PP, i.e., in a [V NP PP] configuration. In this paper, we consider one technique which has been successfully applied to this problem, backed-off estimation, and demonstrate how it can be extended to deal with the problem of multiple PP attachment. The multiple PP attachment introduces two related problems: sparser data (since multiple PPs are naturally rarer), and greater syntactic ambiguity (more attachment configurations which must be distinguished). We present and algorithm which solves this problem through re-use of the relatively rich data obtained from first PP training, in resolving subsequent PP attachments. ',\n",
       " ' Learning Features that Predict Cue Usage Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations. Previous attempts to devise rules for text generation were based on intuition or small numbers of constructed examples. We apply a machine learning program, C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. ',\n",
       " \" A Corpus-Based Investigation of Definite Description Use We present the results of a study of definite descriptions use in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation. We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of 1412 definite descriptions. We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text. The most interesting result of this study from a corpus annotation perspective was the rather low agreement (K=0.63) that we obtained using versions of Hawkins' and Prince's classification schemes; better results (K=0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, first-mention and subsequent-mention. The agreement about antecedents was also not complete. These findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation. From a linguistic point of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites which did not seem to require a complete disambiguation. \",\n",
       " ' Probabilistic Event Categorization This paper describes the automation of a new text categorization task. The categories assigned in this task are more syntactically, semantically, and contextually complex than those typically assigned by fully automatic systems that process unseen test data. Our system for assigning these categories is a probabilistic classifier, developed with a recent method for formulating a probabilistic model from a predefined set of potential features. This paper focuses on feature selection. It presents a number of fully automatic features. It identifies and evaluates various approaches to organizing collocational properties into features, and presents the results of experiments covarying type of organization and type of property. We find that one organization is not best for all kinds of properties, so this is an experimental parameter worth investigating in NLP systems. In addition, the results suggest a way to take advantage of properties that are low frequency but strongly indicative of a class. The problems of recognizing and organizing the various kinds of contextual information required to perform a linguistically complex categorization task have rarely been systematically investigated in NLP. ',\n",
       " ' A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms. ',\n",
       " ' Dynamic Non-Bayesian Decision Making The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them -- the perfect monitoring case -- the agent is able to observe the previous environment state as part of his feedback, while in the other -- the imperfect monitoring case -- all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area. ',\n",
       " ' Storing and Indexing Plan Derivations through Explanation-based Analysis of Retrieval Failures Case-Based Planning CBP provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure. ',\n",
       " ' Learning string edit distance In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string edit distance. Our stochastic model allows us to learn a string edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string edit distance with one fourth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes. Keywords: string edit distance, Levenshtein distance, stochastic transduction, syntactic pattern recognition, prototype dictionary, spelling correction, string correction, string similarity, string classification, speech recognition, pronunciation modeling, Switchboard corpus. ',\n",
       " ' Approximating Context-Free Grammars with a Finite-State Calculus Although adequate models of human language for syntactic analysis and semantic interpretation are of at least context-free complexity, for applications such as speech processing in which speed is important finite-state models are often preferred. These requirements may be reconciled by using the more complex grammar to automatically derive a finite-state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing. A method is presented here for calculating such finite-state approximations from context-free grammars. It is essentially different from the algorithm introduced by Pereira and Wright (1991; 1996), is faster in some cases, and has the advantage of being open-ended and adaptable. ',\n",
       " ' Probabilistic Constraint Logic Programming This paper addresses two central problems for probabilistic processing models: parameter estimation from incomplete data and efficient retrieval of most probable analyses. These questions have been answered satisfactorily only for probabilistic regular and context-free models. We address these problems for a more expressive probabilistic constraint logic programming model. We present a log-linear probability model for probabilistic constraint logic programming. On top of this model we define an algorithm to estimate the parameters and to select the properties of log-linear models from incomplete data. This algorithm is an extension of the improved iterative scaling algorithm of Della-Pietra, Della-Pietra, and Lafferty (1995). Our algorithm applies to log-linear models in general and is accompanied with suitable approximation methods when applied to large data spaces. Furthermore, we present an approach for searching for most probable analyses of the probabilistic constraint logic programming model. This method can be applied to the ambiguity resolution problem in natural language processing applications. ',\n",
       " ' Probabilistic Parsing Using Left Corner Language Models We introduce a novel parser based on a probabilistic version of a left-corner parser. The left-corner strategy is attractive because rule probabilities can be conditioned on both top-down goals and bottom-up derivations. We develop the underlying theory and explain how a grammar can be induced from analyzed data. We show that the left-corner approach provides an advantage over simple top-down probabilistic context-free grammars in parsing the Wall Street Journal using a grammar induced from the Penn Treebank. We also conclude that the Penn Treebank provides a fairly weak testbed due to the flatness of its bracketings and to the obvious overgeneration and undergeneration of its induced grammar. ',\n",
       " ' Variation and Synthetic Speech We describe the approach to linguistic variation taken by the Motorola speech synthesizer. A pan-dialectal pronunciation dictionary is described, which serves as the training data for a neural network based letter-to-sound converter. Subsequent to dictionary retrieval or letter-to-sound generation, pronunciations are submitted a neural network based postlexical module. The postlexical module has been trained on aligned dictionary pronunciations and hand-labeled narrow phonetic transcriptions. This architecture permits the learning of individual postlexical variation, and can be retrained for each speaker whose voice is being modeled for synthesis. Learning variation in this way can result in greater naturalness for the synthetic speech that is produced by the system. ',\n",
       " ' Application-driven automatic subgrammar extraction The space and run-time requirements of broad coverage grammars appear for many applications unreasonably large in relation to the relative simplicity of the task at hand. On the other hand, handcrafted development of application-dependent grammars is in danger of duplicating work which is then difficult to re-use in other contexts of application. To overcome this problem, we present in this paper a procedure for the automatic extraction of application-tuned consistent subgrammars from proved large-scale generation grammars. The procedure has been implemented for large-scale systemic grammars and builds on the formal equivalence between systemic grammars and typed unification based grammars. Its evaluation for the generation of encyclopedia entries is described, and directions of future development, applicability, and extensions are discussed. ',\n",
       " \" Contextual Information and Specific Language Models for Spoken Language Understanding In this paper we explain how contextual expectations are generated and used in the task-oriented spoken language understanding system Dialogos. The hard task of recognizing spontaneous speech on the telephone may greatly benefit from the use of specific language models during the recognition of callers' utterances. By 'specific language models' we mean a set of language models that are trained on contextually appropriated data, and that are used during different states of the dialogue on the basis of the information sent to the acoustic level by the dialogue management module. In this paper we describe how the specific language models are obtained on the basis of contextual information. The experimental result we report show that recognition and understanding performance are improved thanks to the use of specific language models. \",\n",
       " ' Language Modelling For Task-Oriented Domains This paper is focused on the language modelling for task-oriented domains and presents an accurate analysis of the utterances acquired by the Dialogos spoken dialogue system. Dialogos allows access to the Italian Railways timetable by using the telephone over the public network. The language modelling aspects of specificity and behaviour to rare events are studied. A technique for getting a language model more robust, based on sentences generated by grammars, is presented. Experimental results show the benefit of the proposed technique. The increment of performance between language models created using grammars and usual ones, is higher when the amount of training material is limited. Therefore this technique can give an advantage especially for the development of language models in a new domain. ',\n",
       " \" On the use of expectations for detecting and repairing human-machine miscommunication In this paper I describe how miscommunication problems are dealt with in the spoken language system DIALOGOS. The dialogue module of the system exploits dialogic expectations in a twofold way: to model what future user utterance might be about (predictions), and to account how the user's next utterance may be related to previous ones in the ongoing interaction (pragmatic-based expectations). The analysis starts from the hypothesis that the occurrence of miscommunication is concomitant with two pragmatic phenomena: the deviation of the user from the expected behaviour and the generation of a conversational implicature. A preliminary evaluation of a large amount of interactions between subjects and DIALOGOS shows that the system performance is enhanced by the uses of both predictions and pragmatic-based expectations. \",\n",
       " \" Some apparently disjoint aims and requirements for grammar development environments: the case of natural language generation Grammar development environments (GDE's) for analysis and for generation have not yet come together. Despite the fact that analysis-oriented GDE's (such as ALEP) may include some possibility of sentence generation, the development techniques and kinds of resources suggested are apparently not those required for practical, large-scale natural language generation work. Indeed, there is no use of `standard' (i.e., analysis-oriented) GDE's in current projects/applications targetting the generation of fluent, coherent texts. This unsatisfactory situation requires some analysis and explanation, which this paper attempts using as an example an extensive GDE for generation. The support provided for distributed large-scale grammar development, multilinguality, and resource maintenance are discussed and contrasted with analysis-oriented approaches. \",\n",
       " ' Towards an Improved Performance Measure for Language Models In this paper a first attempt at deriving an improved performance measure for language models, the probability ratio measure PRM is described. In a proof of concept experiment, it is shown that PRM correlates better with recognition accuracy and can lead to better recognition results when used as the optimisation criterion of a clustering algorithm. Inspite of the approximations and limitations of this preliminary work, the results are very encouraging and should justify more work along the same lines. ',\n",
       " ' Features as Resources in R-LFG This paper introduces a non-unification-based version of LFG called R-LFG (Resource-based Lexical Functional Grammar), which combines elements from both LFG and Linear Logic. The paper argues that a resource sensitive account provides a simpler treatment of many linguistic uses of non-monotonic devices in LFG, such as existential constraints and constraint equations. ',\n",
       " \" Proof Nets and the Complexity of Processing Center-Embedded Constructions This paper shows how proof nets can be used to formalize the notion of ``incomplete dependency'' used in psycholinguistic theories of the unacceptability of center-embedded constructions. Such theories of human language processing can usually be restated in terms of geometrical constraints on proof nets. The paper ends with a discussion of the relationship between these constraints and incremental semantic interpretation. \",\n",
       " ' The effect of alternative tree representations on tree bank grammars The performance of PCFGs estimated from tree banks is sensitive to the particular way in which linguistic constructions are represented as trees in the tree bank. This paper presents a theoretical analysis of the effect of different tree representations for PP attachment on PCFG models, and introduces a new methodology for empirically examining such effects using tree transformations. It shows that one transformation, which copies the label of a parent node onto the labels of its children, can improve the performance of a PCFG model in terms of labelled precision and recall on held out data from 73% (precision) and 69% (recall) to 80% and 79% respectively. It also points out that if only maximum likelihood parses are of interest then many productions can be ignored, since they are subsumed by combinations of other productions in the grammar. In the Penn II tree bank grammar, almost 9% of productions are subsumed in this way. ',\n",
       " \" Type-driven semantic interpretation and feature dependencies in R-LFG Once one has enriched LFG's formal machinery with the linear logic mechanisms needed for semantic interpretation as proposed by Dalrymple et. al., it is natural to ask whether these make any existing components of LFG redundant. As Dalrymple and her colleagues note, LFG's f-structure completeness and coherence constraints fall out as a by-product of the linear logic machinery they propose for semantic interpretation, thus making those f-structure mechanisms redundant. Given that linear logic machinery or something like it is independently needed for semantic interpretation, it seems reasonable to explore the extent to which it is capable of handling feature structure constraints as well. R-LFG represents the extreme position that all linguistically required feature structure dependencies can be captured by the resource-accounting machinery of a linear or similiar logic independently needed for semantic interpretation, making LFG's unification machinery redundant. The goal is to show that LFG linguistic analyses can be expressed as clearly and perspicuously using the smaller set of mechanisms of R-LFG as they can using the much larger set of unification-based mechanisms in LFG: if this is the case then we will have shown that positing these extra f-structure mechanisms is not linguistically warranted. \",\n",
       " ' Bidirectional Heuristic Search Reconsidered The assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago. For quite a long time, this search strategy did not achieve the expected results, and there was a major misunderstanding about the reasons behind it. Although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other, we demonstrate that this conjecture is wrong. Based on this finding, we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only. These approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding. Empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory. These results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search. This provides some evidence for the usefulness of a search strategy that was long neglected. In summary, we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered. ',\n",
       " ' When Gravity Fails: Local Search Topology Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms. ',\n",
       " ' Applying Explanation-based Learning to Control and Speeding-up Natural Language Generation This paper presents a method for the automatic extraction of subgrammars to control and speeding-up natural language generation NLG. The method is based on explanation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. ',\n",
       " \" Context as a Spurious Concept I take issue with AI formalizations of context, primarily the formalization by McCarthy and Buvac, that regard context as an undefined primitive whose formalization can be the same in many different kinds of AI tasks. In particular, any theory of context in natural language must take the special nature of natural language into account and cannot regard context simply as an undefined primitive. I show that there is no such thing as a coherent theory of context simpliciter -- context pure and simple -- and that context in natural language is not the same kind of thing as context in KR. In natural language, context is constructed by the speaker and the interpreter, and both have considerable discretion in so doing. Therefore, a formalization based on pre-defined contexts and pre-defined `lifting axioms' cannot account for how context is used in real-world language. \",\n",
       " ' Multi-document Summarization by Graph Search and Matching We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out. ',\n",
       " \" Topic Graph Generation for Query Navigation: Use of Frequency Classes for Topic Extraction To make an interactive guidance mechanism for document retrieval systems, we developed a user-interface which presents users the visualized map of topics at each stage of retrieval process. Topic words are automatically extracted by frequency analysis and the strength of the relationships between topic words is measured by their co-occurrence. A major factor affecting a user's impression of a given topic word graph is the balance between common topic words and specific topic words. By using frequency classes for topic word extraction, we made it possible to select well-balanced set of topic words, and to adjust the balance of common and specific topic words. \",\n",
       " \" Foreground and Background Lexicons and Word Sense Disambiguation for Information Extraction Lexicon acquisition from machine-readable dictionaries and corpora is currently a dynamic field of research, yet it is often not clear how lexical information so acquired can be used, or how it relates to structured meaning representations. In this paper I look at this issue in relation to Information Extraction (hereafter IE), and one subtask for which both lexical and general knowledge are required, Word Sense Disambiguation WSD. The analysis is based on the widely-used, but little-discussed distinction between an IE system's foreground lexicon, containing the domain's key terms which map onto the database fields of the output formalism, and the background lexicon, containing the remainder of the vocabulary. For the foreground lexicon, human lexicography is required. For the background lexicon, automatic acquisition is appropriate. For the foreground lexicon, WSD will occur as a by-product of finding a coherent semantic interpretation of the input. WSD techniques as discussed in recent literature are suited only to the background lexicon. Once the foreground/background distinction is developed, there is a match between what is possible, given the state of the art in WSD, and what is required, for high-quality IE. \",\n",
       " \" I don't believe in word senses Word sense disambiguation assumes word senses. Within the lexicography and linguistics literature, they are known to be very slippery entities. The paper looks at problems with existing accounts of `word sense' and describes the various kinds of ways in which a word's meaning can deviate from its core meaning. An analysis is presented in which word senses are abstractions from clusters of corpus citations, in accordance with current lexicographic practice. The corpus citations, not the word senses, are the basic objects in the ontology. The corpus citations will be clustered into senses according to the purposes of whoever or whatever does the clustering. In the absence of such purposes, word senses do not exist. Word sense disambiguation also needs a set of word senses to disambiguate between. In most recent work, the set has been taken from a general-purpose lexical resource, with the assumption that the lexical resource describes the word senses of English/French/..., between which NLP applications will need to disambiguate. The implication of the paper is, by contrast, that word senses exist only relative to a task. \",\n",
       " \" Speech Repairs, Intonational Boundaries and Discourse Markers: Modeling Speakers' Utterances in Spoken Dialog In this thesis, we present a statistical language model for resolving speech repairs, intonational boundaries and discourse markers. Rather than finding the best word interpretation for an acoustic signal, we redefine the speech recognition problem to so that it also identifies the POS tags, discourse markers, speech repairs and intonational phrase endings (a major cue in determining utterance units). Adding these extra elements to the speech recognition problem actually allows it to better predict the words involved, since we are able to make use of the predictions of boundary tones, discourse markers and speech repairs to better account for what word will occur next. Furthermore, we can take advantage of acoustic information, such as silence information, which tends to co-occur with speech repairs and intonational phrase endings, that current language models can only regard as noise in the acoustic signal. The output of this language model is a much fuller account of the speaker's turn, with part-of-speech assigned to each word, intonation phrase endings and discourse markers identified, and speech repairs detected and corrected. In fact, the identification of the intonational phrase endings, discourse markers, and resolution of the speech repairs allows the speech recognizer to model the speaker's utterances, rather than simply the words involved, and thus it can return a more meaningful analysis of the speaker's turn for later processing. \",\n",
       " ' What is word sense disambiguation good for? Word sense disambiguation has developed as a sub-area of natural language processing, as if, like parsing, it was a well-defined task which was a pre-requisite to a wide range of language-understanding applications. First, I review earlier work which shows that a set of senses for a word is only ever defined relative to a particular human purpose, and that a view of word senses as part of the linguistic furniture lacks theoretical underpinnings. Then, I investigate whether and how word sense ambiguity is in fact a problem for different varieties of NLP application. ',\n",
       " ' Orthographic Structuring of Human Speech and Texts: Linguistic Application of Recurrence Quantification Analysis A methodology based upon recurrence quantification analysis is proposed for the study of orthographic structure of written texts. Five different orthographic data sets (20th century Italian poems, 20th century American poems, contemporary Swedish poems with their corresponding Italian translations, Italian speech samples, and American speech samples) were subjected to recurrence quantification analysis, a procedure which has been found to be diagnostically useful in the quantitative assessment of ordered series in fields such as physics, molecular dynamics, physiology, and general signal processing. Recurrence quantification was developed from recurrence plots as applied to the analysis of nonlinear, complex systems in the physical sciences, and is based on the computation of a distance matrix of the elements of an ordered series (in this case the letters consituting selected speech and poetic texts). From a strictly mathematical view, the results show the possibility of demonstrating invariance between different language exemplars despite the apparent low-level of coding (orthography). Comparison with the actual texts confirms the ability of the method to reveal recurrent structures, and their complexity. Using poems as a reference standard for judging speech complexity, the technique exhibits language independence, order dependence and freedom from pure statistical characteristics of studied sequences, as well as consistency with easily identifiable texts. Such studies may provide phenomenological markers of hidden structure as coded by the purely orthographic level. ',\n",
       " \" Incremental Recompilation of Knowledge Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ``knowledge compilation,'' supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates. \",\n",
       " ' Monotonicity and Persistence in Preferential Logics An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics. ',\n",
       " \" Linear probing and graphs Mallows and Riordan showed in 1968 that labeled trees with a small number of inversions are related to labeled graphs that are connected and sparse. Wright enumerated sparse connected graphs in 1977, and Kreweras related the inversions of trees to the so-called ``parking problem'' in 1980. A~combination of these three results leads to a surprisingly simple analysis of the behavior of hashing by linear probing, including higher moments of the cost of successful search. \",\n",
       " \" On the classifiability of cellular automata Based on computer simulations Wolfram presented in several papers conjectured classifications of cellular automata into 4 types. He distinguishes the 4 classes of cellular automata by the evolution of the pattern generated by applying a cellular automaton to a finite input. Wolfram's qualitative classification is based on the examination of a large number of simulations. In addition to this classification based on the rate of growth, he conjectured a similar classification according to the eventual pattern. We consider here one formalization of his rate of growth suggestion. After completing our major results (based only on Wolfram's work), we investigated other contributions to the area and we report the relation of some of them to our discoveries. \",\n",
       " ' Identifying Discourse Markers in Spoken Dialog In this paper, we present a method for identifying discourse marker usage in spontaneous speech based on machine learning. Discourse markers are denoted by special POS tags, and thus the process of POS tagging can be used to identify discourse markers. By incorporating POS tagging into language modeling, discourse markers can be identified during speech recognition, in which the timeliness of the information can be used to help predict the following words. We contrast this approach with an alternative machine learning approach proposed by Litman (1996). This paper also argues that discourse markers can be used to help the hearer predict the role that the upcoming utterance plays in the dialog. Thus discourse markers should provide valuable evidence for automatic dialog act prediction. ',\n",
       " ' Hierarchical Non-Emitting Markov Models We describe a simple variant of the interpolated Markov model with non-emitting state transitions and prove that it is strictly more powerful than any Markov model. More importantly, the non-emitting model outperforms the classic interpolated model on the natural language texts under a wide range of experimental conditions, with only a modest increase in computational requirements. The non-emitting model is also much less prone to overfitting. Keywords: Markov model, interpolated Markov model, hidden Markov model, mixture modeling, non-emitting state transitions, state-conditional interpolation, statistical language model, discrete time series, Brown corpus, Wall Street Journal. ',\n",
       " ' A General, Sound and Efficient Natural Language Parsing Algorithm based on Syntactic Constraints Propagation This paper presents a new context-free parsing algorithm based on a bidirectional strictly horizontal strategy which incorporates strong top-down predictions (derivations and adjacencies). From a functional point of view, the parser is able to propagate syntactic constraints reducing parsing ambiguity. From a computational perspective, the algorithm includes different techniques aimed at the improvement of the manipulation and representation of the structures used. ',\n",
       " ' Do not forget: Full memory in memory-based learning of word pronunciation Memory-based learning, keeping full memory of learning material, appears a viable approach to learning NLP tasks, and is often superior in generalisation accuracy to eager learning approaches that abstract from learning material. Here we investigate three partial memory-based learning approaches which remove from memory specific task instance types estimated to be exceptional. The three approaches each implement one heuristic function for estimating exceptionality of instance types: (i) typicality, (ii) class prediction strength, and (iii) friendly-neighbourhood size. Experiments are performed with the memory-based learning algorithm IB1-IG trained on English word pronunciation. We find that removing instance types with low prediction strength (ii) is the only tested method which does not seriously harm generalisation accuracy. We conclude that keeping full memory of types rather than tokens, and excluding minority ambiguities appear to be the only performance-preserving optimisations of memory-based learning. ',\n",
       " ' Modularity in inductively-learned word pronunciation systems In leading morpho-phonological theories and state-of-the-art text-to-speech systems it is assumed that word pronunciation cannot be learned or performed without in-between analyses at several abstraction levels (e.g., morphological, graphemic, phonemic, syllabic, and stress levels). We challenge this assumption for the case of English word pronunciation. Using IGTree, an inductive-learning decision-tree algorithms, we train and test three word-pronunciation systems in which the number of abstraction levels (implemented as sequenced modules) is reduced from five, via three, to one. The latter system, classifying letter strings directly as mapping to phonemes with stress markers, yields significantly better generalisation accuracies than the two multi-module systems. Analyses of empirical results indicate that positive utility effects of sequencing modules are outweighed by cascading errors passed on between modules. ',\n",
       " ' Look-Back and Look-Ahead in the Conversion of Hidden Markov Models into Finite State Transducers This paper describes the conversion of a Hidden Markov Model into a finite state transducer that closely approximates the behavior of the stochastic model. In some cases the transducer is equivalent to the HMM. This conversion is especially advantageous for part-of-speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors. The speed of tagging is also improved. The described methods have been implemented and successfully tested. ',\n",
       " ' A Hybrid Environment for Syntax-Semantic Tagging The thesis describes the application of the relaxation labelling algorithm to NLP disambiguation. Language is modelled through context constraint inspired on Constraint Grammars. The constraints enable the use of a real value statind compatibility . The technique is applied to POS tagging, Shallow Parsing and Word Sense Disambigation. Experiments and results are reported. The proposed approach enables the use of multi-feature constraint models, the simultaneous resolution of several NL disambiguation tasks, and the collaboration of linguistic and statistical models. ',\n",
       " ' Analogue Quantum Computers for Data Analysis Analogue computers use continuous properties of physical system for modeling. In the paper is described possibility of modeling by analogue quantum computers for some model of data analysis. It is analogue associative memory and a formal neural network. A particularity of the models is combination of continuous internal processes with discrete set of output states. The modeling of the system by classical analogue computers was offered long times ago, but now it is not very effectively in comparison with modern digital computers. The application of quantum analogue modelling looks quite possible for modern level of technology and it may be more effective than digital one, because number of element may be about Avogadro number (N=6.0E23). ',\n",
       " ' Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets. ',\n",
       " ' Synthesizing Customized Planners from Specifications Existing plan synthesis approaches in artificial intelligence fall into two categories -- domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System KIDS is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati & Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners. ',\n",
       " \" Tractability of Theory Patching In this paper we consider the problem of `theory patching', in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is `stable' regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms. \",\n",
       " ' Automating Coreference: The Role of Annotated Training Data We report here on a study of interannotator agreement in the coreference task as defined by the Message Understanding Conference (MUC-6 and MUC-7). Based on feedback from annotators, we clarified and simplified the annotation specification. We then performed an analysis of disagreement among several annotators, concluding that only 16% of the disagreements represented genuine disagreement about coreference; the remainder of the cases were mostly typographical errors or omissions, easily reconciled. Initially, we measured interannotator agreement in the low 80s for precision and recall. To try to improve upon this, we ran several experiments. In our final experiment, we separated the tagging of candidate noun phrases from the linking of actual coreferring expressions. This method shows promise - interannotator agreement climbed to the low 90s - but it needs more extensive validation. These results position the research community to broaden the coreference task to multiple languages, and possibly to different kinds of coreference. ',\n",
       " ' Time, Tense and Aspect in Natural Language Database Interfaces Most existing natural language database interfaces (NLDBs) were designed to be used with database systems that provide very limited facilities for manipulating time-dependent data, and they do not support adequately temporal linguistic mechanisms (verb tenses, temporal adverbials, temporal subordinate clauses, etc.). The database community is becoming increasingly interested in temporal database systems, that are intended to store and manipulate in a principled manner information not only about the present, but also about the past and future. When interfacing to temporal databases, supporting temporal linguistic mechanisms becomes crucial. We present a framework for constructing natural language interfaces for temporal databases (NLTDBs), that draws on research in tense and aspect theories, temporal logics, and temporal databases. The framework consists of a temporal intermediate representation language, called TOP, an HPSG grammar that maps a wide range of questions involving temporal mechanisms to appropriate TOP expressions, and a provably correct method for translating from TOP to TSQL2, TSQL2 being a recently proposed temporal extension of the SQL database language. This framework was employed to implement a prototype NLTDB using ALE and Prolog. ',\n",
       " ' Nymble: a High-Performance Learning Name-finder This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. ',\n",
       " ' Graph Interpolation Grammars: a Rule-based Approach to the Incremental Parsing of Natural Languages Graph Interpolation Grammars are a declarative formalism with an operational semantics. Their goal is to emulate salient features of the human parser, and notably incrementality. The parsing process defined by GIGs incrementally builds a syntactic representation of a sentence as each successive lexeme is read. A GIG rule specifies a set of parse configurations that trigger its application and an operation to perform on a matching configuration. Rules are partly context-sensitive; furthermore, they are reversible, meaning that their operations can be undone, which allows the parsing process to be nondeterministic. These two factors confer enough expressive power to the formalism for parsing natural languages. ',\n",
       " ' Treatment of Epsilon-Moves in Subset Construction The paper discusses the problem of determinising finite-state automata containing large numbers of epsilon-moves. Experiments with finite-state approximations of natural language grammars often give rise to very large automata with a very large number of epsilon-moves. The paper identifies three subset construction algorithms which treat epsilon-moves. A number of experiments has been performed which indicate that the algorithms differ considerably in practice. Furthermore, the experiments suggest that the average number of epsilon-moves per state can be used to predict which algorithm is likely to perform best for a given input automaton. ',\n",
       " \" Corpus-Based Word Sense Disambiguation Resolution of lexical ambiguity, commonly termed ``word sense disambiguation'', is expected to improve the analytical accuracy for tasks which are sensitive to lexical semantics. Such tasks include machine translation, information retrieval, parsing, natural language understanding and lexicography. Reflecting the growth in utilization of machine readable texts, word sense disambiguation techniques have been explored variously in the context of corpus-based approaches. Within one corpus-based framework, that is the similarity-based method, systems use a database, in which example sentences are manually annotated with correct word senses. Given an input, systems search the database for the most similar example to the input. The lexical ambiguity of a word contained in the input is resolved by selecting the sense annotation of the retrieved example. In this research, we apply this method of resolution of verbal polysemy, in which the similarity between two examples is computed as the weighted average of the similarity between complements governed by a target polysemous verb. We explore similarity-based verb sense disambiguation focusing on the following three methods. First, we propose a weighting schema for each verb complement in the similarity computation. Second, in similarity-based techniques, the overhead for manual supervision and searching the large-sized database can be prohibitive. To resolve this problem, we propose a method to select a small number of effective examples, for system usage. Finally, the efficiency of our system is highly dependent on the similarity computation used. To maximize efficiency, we propose a method which integrates the advantages of previous methods for similarity computation. \",\n",
       " ' Integrative Windowing In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise. ',\n",
       " ' On the existence of certain total recursive functions in nontrivial axiom systems, I We investigate the existence of a class of ZFC-provably total recursive unary functions, given certain constraints, and apply some of those results to show that, for sound set theory, ZFC. ',\n",
       " ' Valence Induction with a Head-Lexicalized PCFG This paper presents an experiment in learning valences (subcategorization frames) from a 50 million word text corpus, based on a lexicalized probabilistic context free grammar. Distributions are estimated using a modified EM algorithm. We evaluate the acquired lexicon both by comparison with a dictionary and by entropy measures. Results show that our model produces highly accurate frame distributions. ',\n",
       " ' Group Theory and Grammatical Description This paper presents a model for linguistic description based on group theory. A grammar in this model, or G-grammar , is a collection of lexical expressions which are products of logical forms, phonological forms, and their inverses. Phrasal descriptions are obtained by forming products of lexical expressions and by cancelling contiguous elements which are inverses of each other. We show applications of this model to parsing and generation, long-distance movement, and quantifier scoping. We believe that by moving from the free monoid over a vocabulary V --- standard in formal language studies --- to the free group over V, deep affinities between linguistic phenomena and classical algebra come to the surface, and that the consequences of tapping the mathematical connections thus established could be considerable. ',\n",
       " \" Annotation Style Guide for the Blinker Project This annotation style guide was created by and for the Blinker project at the University of Pennsylvania. The Blinker project was so named after the ``bilingual linker'' GUI, which was created to enable bilingual annotators to ``link'' word tokens that are mutual translations in parallel texts. The parallel text chosen for this project was the Bible, because it is probably the easiest text to obtain in electronic form in multiple languages. The languages involved were English and French, because, of the languages with which the project co-ordinator was familiar, these were the two for which a sufficient number of annotators was likely to be found. \",\n",
       " ' Models of Co-occurrence A model of co-occurrence in bitext is a boolean predicate that indicates whether a given pair of word tokens co-occur in corresponding regions of the bitext space. Co-occurrence is a precondition for the possibility that two tokens might be mutual translations. Models of co-occurrence are the glue that binds methods for mapping bitext correspondence with methods for estimating translation models into an integrated system for exploiting parallel texts. Different models of co-occurrence are possible, depending on the kind of bitext map that is available, the language-specific information that is available, and the assumptions made about the nature of translational equivalence. Although most statistical translation models are based on models of co-occurrence, modeling co-occurrence correctly is more difficult than it may at first appear. ',\n",
       " ' Manual Annotation of Translational Equivalence: The Blinker Project Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the research community from http://www.cis.upenn.edu/~melamed . The annotations can be used for several purposes. First, they can be used as a standard data set for developing and testing translation lexicons and statistical translation models. Second, researchers in lexical semantics will be able to mine the annotations for insights about cross-linguistic lexicalization patterns. Third, the annotations can be used in research into certain recently proposed methods for monolingual word-sense disambiguation. This paper describes the annotated texts, the specially-designed annotation tool, and the strategies employed to increase the consistency of the annotations. The annotation process was repeated five times by different annotators. Inter-annotator agreement rates indicate that the annotations are reasonably reliable and that the method is easy to replicate. ',\n",
       " ' Word-to-Word Models of Translational Equivalence Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is noisy. This article presents methods for biasing statistical translation models to reflect these properties. Analysis of the expected behavior of these biases in the presence of sparse data predicts that they will result in more accurate models. The prediction is confirmed by evaluation with respect to a gold standard -- translation models that are biased in this fashion are significantly more accurate than a baseline knowledge-poor model. This article also shows how a statistical translation model can take advantage of various kinds of pre-existing knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, is shown to reliably boost translation model performance on some tasks. Statistical models that are informed by pre-existing knowledge about the model domain combine the best of both the rationalist and empiricist traditions. ',\n",
       " ' The Proper Treatment of Optimality in Computational Phonology This paper presents a novel formalization of optimality theory. Unlike previous treatments of optimality in computational linguistics, starting with Ellison (1994), the new approach does not require any explicit marking and counting of constraint violations. It is based on the notion of lenient composition, defined as the combination of ordinary composition and priority union. If an underlying form has outputs that can meet a given constraint, lenient composition enforces the constraint; if none of the output candidates meet the constraint, lenient composition allows all of them. For the sake of greater efficiency, we may leniently compose the GEN relation and all the constraints into a single finite-state transducer that maps each underlying form directly into its optimal surface realizations, and vice versa, without ever producing any failing candidates. Seen from this perspective, optimality theory is surprisingly similar to the two older strains of finite-state phonology: classical rewrite systems and two-level models. In particular, the ranking of optimality constraints corresponds to the ordering of rewrite rules. ',\n",
       " ' Parsing Inside-Out The inside-outside probabilities are typically used for reestimating Probabilistic Context Free Grammars (PCFGs), just as the forward-backward probabilities are typically used for reestimating HMMs. I show several novel uses, including improving parser accuracy by matching parsing algorithms to evaluation criteria; speeding up DOP parsing by 500 times; and 30 times faster PCFG thresholding at a given accuracy level. I also give an elegant, state-of-the-art grammar formalism, which can be used to compute inside-outside probabilities; and a parser description formalism, which makes it easy to derive inside-outside formulas and many others. ',\n",
       " \" A Descriptive Characterization of Tree-Adjoining Languages (Full Version) Since the early Sixties and Seventies it has been known that the regular and context-free languages are characterized by definability in the monadic second-order theory of certain structures. More recently, these descriptive characterizations have been used to obtain complexity results for constraint- and principle-based theories of syntax and to provide a uniform model-theoretic framework for exploring the relationship between theories expressed in disparate formal terms. These results have been limited, to an extent, by the lack of descriptive characterizations of language classes beyond the context-free. Recently, we have shown that tree-adjoining languages (in a mildly generalized form) can be characterized by recognition by automata operating on three-dimensional tree manifolds, a three-dimensional analog of trees. In this paper, we exploit these automata-theoretic results to obtain a characterization of the tree-adjoining languages by definability in the monadic second-order theory of these three-dimensional tree manifolds. This not only opens the way to extending the tools of model-theoretic syntax to the level of TALs, but provides a highly flexible mechanism for defining TAGs in terms of logical constraints. This is the full version of a paper to appear in the proceedings of COLING-ACL'98 as a project note. \",\n",
       " \" Discovery of Linguistic Relations Using Lexical Attraction This work has been motivated by two long term goals: to understand how humans learn language and to build programs that can understand language. Using a representation that makes the relevant features explicit is a prerequisite for successful learning and understanding. Therefore, I chose to represent relations between individual words explicitly in my model. Lexical attraction is defined as the likelihood of such relations. I introduce a new class of probabilistic language models named lexical attraction models which can represent long distance relations between words and I formalize this new class of models using information theory. Within the framework of lexical attraction, I developed an unsupervised language acquisition program that learns to identify linguistic relations in a given sentence. The only explicitly represented linguistic knowledge in the program is lexical attraction. There is no initial grammar or lexicon built in and the only input is raw text. Learning and processing are interdigitated. The processor uses the regularities detected by the learner to impose structure on the input. This structure enables the learner to detect higher level regularities. Using this bootstrapping procedure, the program was trained on 100 million words of Associated Press material and was able to achieve 60% precision and 50% recall in finding relations between content-words. Using knowledge of lexical attraction, the program can identify the correct relations in syntactically ambiguous sentences such as ``I saw the Statue of Liberty flying over New York.'' \",\n",
       " \" Integrating Text Plans for Conciseness and Coherence Our experience with a critiquing system shows that when the system detects problems with the user's performance, multiple critiques are often produced. Analysis of a corpus of actual critiques revealed that even though each individual critique is concise and coherent, the set of critiques as a whole may exhibit several problems that detract from conciseness and coherence, and consequently assimilation. Thus a text planner was needed that could integrate the text plans for individual communicative goals to produce an overall text plan representing a concise, coherent message. This paper presents our general rule-based system for accomplishing this task. The system takes as input a \\\\emph{set} of individual text plans represented as RST-style trees, and produces a smaller set of more complex trees representing integrated messages that still achieve the multiple communicative goals of the individual text plans. Domain-independent rules are used to capture strategies across domains, while the facility for addition of domain-dependent rules enables the system to be tuned to the requirements of a particular domain. The system has been tested on a corpus of critiques in the domain of trauma care. \",\n",
       " ' Automatic summarising: factors and directions This position paper suggests that progress with automatic summarising demands a better research methodology and a carefully focussed research strategy. In order to develop effective procedures it is necessary to identify and respond to the context factors, i.e. input, purpose, and output factors, that bear on summarising and its evaluation. The paper analyses and illustrates these factors and their implications for evaluation. It then argues that this analysis, together with the state of the art and the intrinsic difficulty of summarising, imply a nearer-term strategy concentrating on shallow, but not surface, text analysis and on indicative summarising. This is illustrated with current work, from which a potentially productive research programme can be developed. ',\n",
       " \" Recognizing Syntactic Errors in the Writing of Second Language Learners This paper reports on the recognition component of an intelligent tutoring system that is designed to help foreign language speakers learn standard English. The system models the grammar of the learner, with this instantiation of the system tailored to signers of American Sign Language ASL. We discuss the theoretical motivations for the system, various difficulties that have been encountered in the implementation, as well as the methods we have used to overcome these problems. Our method of capturing ungrammaticalities involves using mal-rules (also called 'error productions'). However, the straightforward addition of some mal-rules causes significant performance problems with the parser. For instance, the ASL population has a strong tendency to drop pronouns and the auxiliary verb `to be'. Being able to account for these as sentences results in an explosion in the number of possible parses for each sentence. This explosion, left unchecked, greatly hampers the performance of the system. We discuss how this is handled by taking into account expectations from the specific population (some of which are captured in our unique user model). The different representations of lexical items at various points in the acquisition process are modeled by using mal-rules, which obviates the need for multiple lexicons. The grammar is evaluated on its ability to correctly diagnose agreement problems in actual sentences produced by ASL native speakers. \",\n",
       " ' A Selective Macro-learning Algorithm and its Application to the NxN Sliding-Tile Puzzle One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of NxN sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size. ',\n",
       " ' Learning Correlations between Linguistic Indicators and Semantic Constraints: Reuse of Context-Dependent Descriptions of Entities This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators. We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning. Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation. We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself. We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method. ',\n",
       " ' Model-Based Diagnosis using Structured System Descriptions This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form NNF and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions. ',\n",
       " ' Computing Dialogue Acts from Features with Transformation-Based Learning To interpret natural language at the discourse level, it is very useful to accurately recognize dialogue acts, such as SUGGEST, in identifying speaker intentions. Our research explores the utility of a machine learning method called Transformation-Based Learning TBL in computing dialogue acts, because TBL has a number of advantages over alternative approaches for this application. We have identified some extensions to TBL that are necessary in order to address the limitations of the original algorithm and the particular demands of discourse processing. We use a Monte Carlo strategy to increase the applicability of the TBL method, and we select features of utterances that can be used as input to improve the performance of TBL. Our system is currently being tested on the VerbMobil corpora of spoken dialogues, producing promising preliminary results. ',\n",
       " ' Lazy Transformation-Based Learning We introduce a significant improvement for a relatively new machine learning method called Transformation-Based Learning. By applying a Monte Carlo strategy to randomly sample from the space of rules, rather than exhaustively analyzing all possible rules, we drastically reduce the memory and time costs of the algorithm, without compromising accuracy on unseen data. This enables Transformation- Based Learning to apply to a wider range of domains, as it can effectively consider a larger number of different features and feature interactions in the data. In addition, the Monte Carlo improvement decreases the labor demands on the human developer, who no longer needs to develop a minimal set of rule templates to maintain tractability. ',\n",
       " \" Eliminating deceptions and mistaken belief to infer conversational implicature Conversational implicatures are usually described as being licensed by the disobeying or flouting of some principle by the speaker in cooperative dialogue. However, such work has failed to distinguish cases of the speaker flouting such a principle from cases where the speaker is either deceptive or holds a mistaken belief. In this paper, we demonstrate how the three different cases can be distinguished in terms of the beliefs ascribed to the speaker of the utterance. We argue that in the act of distinguishing the speaker's intention and ascribing such beliefs, the intended inference can be made by the hearer. This theory is implemented in ViewGen, a pre-existing belief modelling system used in a medical counselling domain. \",\n",
       " \" Rationality, Cooperation and Conversational Implicature Conversational implicatures are usually described as being licensed by the disobeying or flouting of a Principle of Cooperation. However, the specification of this principle has proved computationally elusive. In this paper we suggest that a more useful concept is rationality. Such a concept can be specified explicitely in planning terms and we argue that speakers perform utterances as part of the optimal plan for their particular communicative goals. Such an assumption can be used by the hearer to infer conversational implicatures implicit in the speaker's utterance. \",\n",
       " ' Dialogue Act Tagging with Transformation-Based Learning For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning TBL machine learning algorithm. To circumvent a sparse data problem, we extract values of well-motivated features of utterances, such as speaker direction, punctuation marks, and a new feature, called dialogue act cues, which we find to be more effective than cue phrases and word n-grams in practice. We present strategies for constructing a set of dialogue act cues automatically by minimizing the entropy of the distribution of dialogue acts in a training corpus, filtering out irrelevant dialogue act cues, and clustering semantically-related words. In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. ',\n",
       " ' An Investigation of Transformation-Based Learning in Discourse This paper presents results from the first attempt to apply Transformation-Based Learning to a discourse-level Natural Language Processing task. To address two limitations of the standard algorithm, we developed a Monte Carlo version of Transformation-Based Learning to make the method tractable for a wider range of problems without degradation in accuracy, and we devised a committee method for assigning confidence measures to tags produced by Transformation-Based Learning. The paper describes these advances, presents experimental evidence that Transformation-Based Learning is as effective as alternative approaches (such as Decision Trees and N-Grams) for a discourse task called Dialogue Act Tagging, and argues that Transformation-Based Learning has desirable features that make it particularly appealing for the Dialogue Act Tagging task. ',\n",
       " ' Unlimited Vocabulary Grapheme to Phoneme Conversion for Korean TTS This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules. The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check. The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes. The phrase-break detector assigns phrase breaks using part-of-speech POS information. In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes. Graphemes within a morpheme are grouped into CCV patterns and converted into phonemes by the CCV conversion rules. The phoneme connectivity table supports grammaticality checking of the adjacent two phonetic morphemes. In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the grapheme-to-phoneme conversion performance and 97.5% of the sentence conversion performance. The full Korean TTS system is now being implemented using this conversion method. ',\n",
       " ' Methods and Tools for Building the Catalan WordNet In this paper we introduce the methodology used and the basic phases we followed to develop the Catalan WordNet, and shich lexical resources have been employed in its building. This methodology, as well as the tools we made use of, have been thought in a general way so that they could be applied to any other language. ',\n",
       " ' Towards a single proposal is spelling correction The study presented here relies on the integrated use of different kinds of knowledge in order to improve first-guess accuracy in non-word context-sensitive correction for general unrestricted texts. State of the art spelling correction systems, e.g. ispell, apart from detecting spelling errors, also assist the user by offering a set of candidate corrections that are close to the misspelled word. Based on the correction proposals of ispell, we built several guessers, which were combined in different ways. Firstly, we evaluated all possibilities and selected the best ones in a corpus with artificially generated typing errors. Secondly, the best combinations were tested on texts with genuine spelling errors. The results for the latter suggest that we can expect automatic non-word correction for all the errors in a free running text with 80% precision and a single proposal 98% of the times (1.02 proposals on average). ',\n",
       " ' Bayesian Stratified Sampling to Assess Corpus Utility This paper describes a method for asking statistical questions about a large text corpus. We exemplify the method by addressing the question, What percentage of Federal Register documents are real documents, of possible interest to a text researcher or analyst? We estimate an answer to this question by evaluating 200 documents selected from a corpus of 45,820 Federal Register documents. Stratified sampling is used to reduce the sampling uncertainty of the estimate from over 3100 documents to fewer than 1000. The stratification is based on observed characteristics of real documents, while the sampling procedure incorporates a Bayesian version of Neyman allocation. A possible application of the method is to establish baseline statistics used to estimate recall rates for information retrieval systems. ',\n",
       " ' Can Subcategorisation Probabilities Help a Statistical Parser? Research into the automatic acquisition of lexical information from corpora is starting to produce large-scale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, the empirical question of whether this type of frequency information can in practice improve the accuracy of a statistical parser has not yet been answered. In this paper we describe an experiment with a wide-coverage statistical grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy. ',\n",
       " ' Word Sense Disambiguation using Optimised Combinations of Knowledge Sources Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs unrestricted word sense disambiguation (on all content words in free text) by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags. The usefulness of these sources is optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample. ',\n",
       " ' Building Accurate Semantic Taxonomies from Monolingual MRDs This paper presents a method that combines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary MRD. Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic exraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE. ',\n",
       " ' Using WordNet for Building WordNets This paper summarises a set of methodologies and techniques for the fast construction of multilingual WordNets. The English WordNet is used in this approach as a backbone for Catalan and Spanish WordNets and as a lexical knowledge resource for several subtasks. ',\n",
       " \" Anchoring a Lexicalized Tree-Adjoining Grammar for Discourse We here explore a ``fully'' lexicalized Tree-Adjoining Grammar for discourse that takes the basic elements of a (monologic) discourse to be not simply clauses, but larger structures that are anchored on variously realized discourse cues. This link with intra-sentential grammar suggests an account for different patterns of discourse cues, while the different structures and operations suggest three separate sources for elements of discourse meaning: (1) a compositional semantics tied to the basic trees and operations; (2) a presuppositional semantics carried by cue phrases that freely adjoin to trees; and (3) general inference, that draws additional, defeasible conclusions that flesh out what is conveyed compositionally. \",\n",
       " ' An Empirical Investigation of Proposals in Collaborative Dialogues We describe a corpus-based investigation of proposals in dialogue. First, we describe our DRI compliant coding scheme and report our inter-coder reliability results. Next, we test several hypotheses about what constitutes a well-formed proposal. ',\n",
       " \" Never Look Back: An Alternative to Centering I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word. \",\n",
       " \" Textual Economy through Close Coupling of Syntax and Semantics We focus on the production of efficient descriptions of objects, actions and events. We define a type of efficiency, textual economy, that exploits the hearer's recognition of inferential links to material elsewhere within a sentence. Textual economy leads to efficient descriptions because the material that supports such inferences has been included to satisfy independent communicative goals, and is therefore overloaded in Pollack's sense. We argue that achieving textual economy imposes strong requirements on the representation and reasoning used in generating sentences. The representation must support the generator's simultaneous consideration of syntax and semantics. Reasoning must enable the generator to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its (incomplete) syntax and semantics. We show that these representational and reasoning requirements are met in the SPUD system for sentence planning and realization. \",\n",
       " ' Evaluating a Focus-Based Approach to Anaphora Resolution We present an approach to anaphora resolution based on a focusing algorithm, and implemented within an existing MUC (Message Understanding Conference) Information Extraction system, allowing quantitative evaluation against a substantial corpus of annotated real-world texts. Extensions to the basic focusing mechanism can be easily tested, resulting in refinements to the mechanism and resolution rules. Results are compared with the results of a simpler heuristic-based approach. ',\n",
       " \" The Role of Verbs in Document Analysis We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations EVCA Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall's Tau. We present convincing evidence for using verb semantic classes as a discriminant in document classification. \",\n",
       " \" Centering in Dynamic Semantics Centering theory posits a discourse center, a distinguished discourse entity that is the topic of a discourse. A simplified version of this theory is developed in a Dynamic Semantics framework. In the resulting system, the mechanism of center shift allows a simple, elegant analysis of a variety of phenomena involving sloppy identity in ellipsis and ``paycheck pronouns''. \"]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "absent-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(InfoTheoryLabels)\n",
    "y_test = np.asarray(InfoTheory_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "coupled-grace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[16062     0]\n",
      " [ 3616     0]]\n",
      "Accuracy: 0.8162414879560931\n",
      "Macro Precision: 0.40812074397804654\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.44941242305540013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtkUlEQVR4nO3dd3hUZfrG8e+TAqGGFhABCUgNEAJEpCbq0lWwL9jQVREFUeKq+FvXtWzBsqEoroJlFVFEbKggRTEBBCRI70VKACUgvZf390cGN2IggeRkMpn7c11zMeecd2aeQ7tzyjyvOecQEZHgFeLvAkRExL8UBCIiQU5BICIS5BQEIiJBTkEgIhLkwvxdwLmqVKmSi46O9ncZIiIBZf78+Tucc1HZbQu4IIiOjiYtLc3fZYiIBBQz23imbTo1JCIS5BQEIiJBTkEgIhLkAu4agYgEr2PHjpGens7hw4f9XUqhFRERQfXq1QkPD8/1axQEIhIw0tPTKVOmDNHR0ZiZv8spdJxz7Ny5k/T0dGrVqpXr13l2asjM3jSz7Wa29AzbzcyGm9laM1tsZs29qkVEiobDhw9TsWJFhcAZmBkVK1Y85yMmL68R/BfocpbtXYG6vkcf4D8e1sL8jbsYMX0t8zfu8vJjRMRjCoGzO5/fH89ODTnnUs0s+ixDegDvuMw+2HPMrJyZVXXObcvvWuZv3MXNo+Zw9PhJioeFMOaeVrSoWT6/P0ZEJCD5866hasDmLMvpvnW/Y2Z9zCzNzNIyMjLO+YPmrN/J0eMnccDh4yf5+If08ypYRKR06dJ5fo+0tDQGDBhwxu0bNmzgvffey/X4vAqI20edcyOdc/HOufioqGy/IX1WrWpXpHh4CCEGBoyZu4mnJizjwJHj+V+siEgO4uPjGT58+Bm3nx4EOY3PK38GwRagRpbl6r51+a5FzfKMubsVD3eqz+i7L6V365q8PXsDnYakkrr63I8wRCRwFMT1wYULF9KqVStiY2O59tpr2bUr87PmzZtHbGwscXFxPPLIIzRu3BiAb7/9lquuugqAlJQU4uLiiIuLo1mzZuzbt49BgwYxY8YM4uLiGDJkyG/G79+/nzvvvJMmTZoQGxvLRx99lOf6/Xn76ASgv5mNBS4F9nhxfeCUFjXL/3pdoF2dSlzV9EIe+2gxt7/5PTe0qM4TVzakXMliXn28iOSzpz9fxvKte886Zt/hY6z8aR8nHYQYNLigDGUiznx/fcyFZfnb1Y3OuZbbb7+dl156icTERJ588kmefvpphg4dyp133smoUaNo3bo1gwYNyva1L774IiNGjKBt27bs37+fiIgIBg8ezIsvvsgXX3wBZAbHKc8++yyRkZEsWbIE4NfQyQsvbx99H5gN1DezdDO7y8z6mllf35CJwHpgLTAKuN+rWrJzSXQFJg5oz/2XXcwnC7bQITmVSUs8yyER8YO9h49z0jct+0mXuZzf9uzZw+7du0lMTASgd+/epKamsnv3bvbt20fr1q0BuPnmm7N9fdu2bUlKSmL48OHs3r2bsLCz/3w+bdo0+vXr9+ty+fJ5v/HFy7uGeuWw3QH9zjbGaxHhoTzapQHdmlTl0fGLuW/MD3RtfAFP92hE5TIR/ixNRHKQm5/c52/cxS2vz+HY8ZOEh4UwrGezQnfH4KBBg7jyyiuZOHEibdu2ZfLkyQVeQ0BcLPZa42qRfNa/LY92qc/XK7fTMTmVD9M2k5lVIhKoTl0fTOpUnzF3e3PbeGRkJOXLl2fGjBkAjB49msTERMqVK0eZMmWYO3cuAGPHjs329evWraNJkyY89thjXHLJJaxcuZIyZcqwb9++bMd37NiRESNG/LpcqE8NBZrw0BDuv6wOkx5sT70qpXlkfOb1g82/HPR3aSKSBy1qlqff5XXyLQQOHjxI9erVf30kJyfz9ttv88gjjxAbG8vChQt58sknAXjjjTe45557iIuL48CBA0RGRv7u/YYOHUrjxo2JjY0lPDycrl27EhsbS2hoKE2bNmXIkCG/Gf/EE0+wa9cuGjduTNOmTZk+fXqe98kC7afe+Ph45/XENCdPOt6du5HnJq3EAY92rs/traMJCdE3GkX8acWKFTRs2NDfZeTa/v37f/3eweDBg9m2bRvDhg3z/HOz+30ys/nOufjsxuuIIBshIcbtraOZPDCB+OgKPPX5cm58bTZrt2d/qCYikp0vv/ySuLg4GjduzIwZM3jiiSf8XVK2dESQA+ccH/+whWe+WM6hoyd4sENd+iTUJjxUGSpS0ALtiMBfdESQz8yM61tUZ1pSIh1iKvPC5FX0eHkWS7fs8XdpIkEp0H54LWjn8/ujIMilqDLFeeWWFrx6awsy9h+hx4hZPPfVSg4fO+Hv0kSCRkREBDt37lQYnMGp+QgiIs7t9nedGjoPew4e4x8TlzMuLZ3alUox+PpYWtaq4NeaRIKBZijL2ZlmKDvbqSEFQR7MXLODQR8vJn3XIW5rVZPHujagdHFN+iYihY+uEXikXd1KTH4ogTvbRvPu3I10Sk5h+qrt/i5LROScKAjyqFTxMP52dSPG921DyeJh3PnWPJI+WMiuA0f9XZqISK4oCPJJi5rl+XJAOx64og4TFm2l45AUvly8TRe1RKTQUxDko+JhoTzcqT4T+rejamQJ+r33A/eOns/2vbqwJSKFl4LAAzEXluWT+9vweNcGpKzO4A/JKYybpyZ2IlI4KQg8EhYawr2JFzPpwfY0rFqWRz9azK1vzGXTTjWxE5HCxdMgMLMuZrbKzNaa2e+m5zGzmmb2tZktNrNvzay6l/X4Q+2o0oy9pxV/v6YxizbvofPQVN6Y+SMnTuroQEQKBy9nKAsFRgBdgRigl5nFnDbsReAd51ws8AzwL6/q8aeQEOPWVjWZMjCBS2tX4NkvlnPDq9+x5mc1sRMR//PyiKAlsNY5t945dxQYC/Q4bUwM8I3v+fRsthcpF5YrwVt3XMLQP8axYccBrhw+k+Ffr+Ho8ZP+Lk1EgpiXQVAN2JxlOd23LqtFwHW+59cCZcys4ulvZGZ9zCzNzNIyMjI8KbagmBnXNKvG1KREOje+gOSpq+n+8kwWp+/2d2kiEqT8fbH4z0CimS0AEoEtwO+6uDnnRjrn4p1z8VFRUQVdoycqlS7OS72aMer2eHYdPMo1I2bxr4krOHRUTexEpGB52RhnC1Ajy3J137pfOee24jsiMLPSwPXOud0e1lTodIypQstaFRg8aQWvpa5n8rKfGHx9LK1q/+7ASETEE14eEcwD6ppZLTMrBvQEJmQdYGaVzOxUDY8Db3pYT6EVWSKcf10Xy3t3X8pJBz1HzuEvnyxh3+Fj/i5NRIKAZ0HgnDsO9AcmAyuAcc65ZWb2jJl19w27DFhlZquBKsA/vKonELSpU4mvHmrP3e1q8f73m+g0JJVvVv7s77JEpIhTG+pCasGmXTz20WJW/7yfa+Iu5MmrG1GhVDF/lyUiAUptqANQs4vK88UD7XnwD3X5csk2OiSnMGHRVrWpEJF8pyAoxIqFhTCwYz0+f6AdNcqXYMD7C7jnnfn8tEdN7EQk/ygIAkCDC8ry8f1t+Uu3hsxcm0HH5BTe/36Tjg5EJF8oCAJEaIhxT0JtvnowgUbVyvL4x0u4edRcNu484O/SRCTAKQgCTHSlUrx3dyv+dV0Tlm7JbGL3+oz1amInIudNQRCAQkKMXi0vYmpSIu3qVOLvX67guv98x6qf1MRORM6dgiCAXRAZwajb4xneqxmbfznIVS/NYMjU1WpiJyLnREEQ4MyM7k0vZFpSIt2aVGXY12u46qUZLNy829+liUiAUBAUERVKFWNYz2a80TuevYeOc90rs/j7F8vVxE5EcqQgKGL+0LAKU5IS6NnyIl6f+SOdh6by3bod/i5LRAoxBUERVDYinH9e24T372lFiMHNo+by+MeL2asmdiKSDQVBEdb64opMejCBexNq88G8zXRMTmHacjWxE5HfUhAUcSWKhfJ4t4Z82q8t5UsW4+530njg/QXs2H/E36WJSCGhIAgSsdXLMaF/O5I61uOrpdvomJzCpwu2qE2FiCgIgkmxsBAG/KEuXw5oT82KpXjog4Xc9XYaW3cf8ndpIuJHCoIgVK9KGT66rw1/vSqG2et20mlIKu/O2chJtakQCUqeBoGZdTGzVWa21swGZbP9IjObbmYLzGyxmXXzsh75n9AQ4652tZj8UAJNa0TyxKdL6TVqDj/uUBM7kWDjWRCYWSgwAugKxAC9zCzmtGFPkDmFZTMy5zR+xat6JHsXVSzJu3ddyvPXx7J82166DE3ltZR1HD+hNhUiwcLLI4KWwFrn3Hrn3FFgLNDjtDEOKOt7Hgls9bAeOQMz46ZLajAtKZGEelH8a9JKrn3lO5Zv3evv0kSkAHgZBNWAzVmW033rsnoKuNXM0oGJwAPZvZGZ9TGzNDNLy8jI8KJWAaqUjWDkbS0YcXNztu05RPeXZ/LvKas4clxtKkSKMn9fLO4F/Nc5Vx3oBow2s9/V5Jwb6ZyLd87FR0VFFXiRwcTMuDK2KlMHJtK96YW89M1arhw+k/kbd/m7NBHxiJdBsAWokWW5um9dVncB4wCcc7OBCKCShzVJLpUvVYzkP8bx1p2XcPDIcW549Tue/nwZB48e93dpIpLPvAyCeUBdM6tlZsXIvBg84bQxm4A/AJhZQzKDQOd+CpHL61dmSlIit7WqyVuzNtBpSCoz16iJnUhR4lkQOOeOA/2BycAKMu8OWmZmz5hZd9+wh4F7zGwR8D5wh9NXXQud0sXDeKZHY8bd25rw0BBufWMuj45fxJ5DamInUhRYoP2/Gx8f79LS0vxdRtA6fOwEw75ew8jU9VQsVYxnr2lM50YX+LssEcmBmc13zsVnt83fF4slwESEh/JYlwZ8en9bKpYuzr2j59NvzA9k7FMTO5FApSCQ89KkeiQT+rflkc71mbr8Zzokp/DR/HQ1sRMJQAoCOW/hoSH0u7wOEx9sR53KpXn4w0Xc8dY8tqiJnUhAURBIntWpXIYP723NU1fHMG/DL3RKTuGd2RvUxE4kQCgIJF+EhBh3tM1sYte8Znme/GwZfxw5m3UZ+/1dmojkQEEg+apGhZK886eWvHBDLKt+2kfXYTN45du1HFMTO5FCS0Eg+c7MuDG+BtMeTuSK+pV5/qtVXDNiFku37PF3aSKSDQWBeKZymQheva0F/7mlOT/vPUKPEbN4YfJKDh9TEzuRwkRBIJ7r2qQq05ISuLZZNUZMX0e34TNI2/CLv8sSER8FgRSIciWL8eKNTXnnTy05cuwkN742m6cmLOPAETWxE/E3BYEUqIR6UUwZmEDv1tG8PTuziV3KavUZFPEnBYEUuFLFw3iqeyM+vLc1xcND6P3m9zw8bhG7Dx71d2kiQUlBIH4TH12BiQPa0+/yi/l04RY6JKcyack2f5clEnQUBOJXEeGhPNK5ARP6t6VK2eLcN+YH+o6ez/a9h/1dmkjQUBBIodDowkg+69eWx7o04JtV2+mQnMKHaZvVxE6kAHgaBGbWxcxWmdlaMxuUzfYhZrbQ91htZru9rEcKt7DQEO677GImPdie+heU4ZHxi7n9ze/Z/MtBf5cmUqR5NjGNmYUCq4GOQDqZU1f2cs4tP8P4B4Bmzrk/ne19NTFNcDh50jFm7kYGT1qJAx7tXJ/bWkcTGmL+Lk0kIPlrYpqWwFrn3Hrn3FFgLNDjLON7kTldpQghIcZtraOZPDCBS6Ir8NTny7nptdms3b7P36WJFDleBkE1YHOW5XTfut8xs5pALeCbM2zvY2ZpZpaWkaF7zoNJ9fIl+e+dl5B8U1PWZeyn27CZvPzNGjWxE8lHheVicU9gvHMu2yY0zrmRzrl451x8VFRUAZcm/mZmXNe8OlMHJtKxURVenLKa7i+riZ1IfvEyCLYANbIsV/ety05PdFpIchBVpjgjbm7Oa7e1YMf+zCZ2gyepiZ1IXnkZBPOAumZWy8yKkfmf/YTTB5lZA6A8MNvDWqQI6dzoAqYNTOSG5tV5NWUd3YbN4Psf1cRO5Hx5FgTOueNAf2AysAIY55xbZmbPmFn3LEN7AmOdbhiXcxBZMpznbojl3bsu5eiJk9z02mz++ulS9h0+5u/SRAKOZ7ePekW3j8rpDh49zouTV/PWdz9StWwE/7iuCZfXr+zvskQKFX/dPipSIEoWC+PJq2MY37cNpYqHcedb80j6YCG7DqiJnUhu5CoIzKytmU31fft3vZn9aGbrvS5O5Fy0qFmeLwa0Y8AVdZiwaCsdklP4YvFWtakQyUGuTg2Z2UpgIDAf+PUWDefcTu9Ky55ODUlurNi2l0fHL2bJlj10iqnCs9c0pkrZCH+XJeI3+XFqaI9zbpJzbrtzbuepRz7WKJKvGlYtyyf3t+Hxrg1IWZ1Bh+QUPpi3SUcHItnIbRBMN7MXzKy1mTU/9fC0MpE8CgsN4d7Ei/nqoQQaVi3LYx8t4ZbX57Jpp5rYiWSV21ND07NZ7ZxzV+R/SWenU0NyPk6edLw/bxP/mriSEycdf+5cnzvaqImdBI+znRrS7aMSVLbtOcRfPlnKNyu3E1ejHM/fEEu9KmX8XZaI5/J8jcDMIs0s+VTjNzP7t5lF5m+ZIt6rGlmCN3rHM6xnHBt3HuDK4TMY/vUajh5XEzsJXrm9RvAmsA+4yffYC7zlVVEiXjIzesRVY1pSIl0aVyV56mq6vzyTRZt3+7s0Eb/IbRBc7Jz7m29ugfXOuaeB2l4WJuK1iqWL81KvZoy6PZ5dB49y7Suz+OfEFRw6qiZ2ElxyGwSHzKzdqQUzawsc8qYkkYLVMaYKU5MS+eMlNRiZup6uw1KZvU53R0vwyG0Q3AeMMLMNZrYReBno611ZIgWrbEQ4/7oulvfuvpSTDnqNmsP/fbKEvWpiJ0HgnO4aMrOyAM65vZ5VlAPdNSReO3T0BMlTV/HGzB+pXCaCf17XmCsaVPF3WSJ5ct63j5rZrc65d80sKbvtzrnkfKox1xQEUlAWbt7NY+MXs+rnffSIu5Anr4qhYuni/i5L5Lzk5fbRUr5fy5zhIVJkxdUox+cPtOOhDnWZuGQbHYekMmGRmthJ0ePpF8rMrAswDAgFXnfODc5mzE3AU4ADFjnnbj7be+qIQPxh1U/7ePSjxSzavJsODSvz7DWNqRpZwt9lieRafnyh7HkzK2tm4Wb2tZllmNmtObwmFBgBdAVigF5mFnPamLrA40Bb51wj4KHc1CNS0OpfUIaP72vDE1c2ZObaHXRKTuW9uZs4eVJHBxL4cnvXUCffBeKrgA1AHeCRHF7TEljr+97BUWAs0OO0MfcAI5xzuwCcc9tzW7hIQQsNMe5uX5vJDyXQuFok//fJEm5+fQ4bdhzwd2kieZLbIAjz/Xol8KFzbk8uXlMN2JxlOd23Lqt6QD0zm2Vmc3ynkn7HzPqcam+RkZGRy5JFvFGzYineu+dSBl/XhGVb9tJlWCqjUtdzQkcHEqByGwRf+CanaQF8bWZRwOF8+PwwoC5wGdALGGVm5U4f5Jwb6ZyLd87FR0VF5cPHiuSNmdGz5UVMTUqkXZ1K/GPiCq57ZRarftrn79JEzlmugsA5NwhoA8Q7544BB/j9aZ7TbQFqZFmu7luXVTowwTl3zDn3I7CazGAQCQgXREYw6vZ4XurVjPRdh7jqpRkMmbqaI8fVpkICx1mDwMyu8P16HZk/tffwPe9CZjCczTygrpnVMrNiQE9gwmljPvW9L2ZWicxTRZoLWQKKmXF10wuZmpTIlU2qMuzrNVz90kwWbNrl79JEciWnI4JE369XZ/O46mwvdM4dB/oDk4EVwDjn3DIze8bMuvuGTQZ2mtlyYDrwiKbAlEBVoVQxhvZsxpt3xLPv8HGu+893PPvFcg4ePe7v0kTOShPTiHhg3+FjPPfVSt6ds4mLKpRk8HVNaFOnkr/LkiCWH98j+GfWi7hmVt7M/p5P9YkUOWUiwvn7NU0Y26cVIQY3vz6XQR8tZs8hNbGTwie3dw11dc7tPrXgu++/mycViRQhrWpX5KuHErg3sTbj0jbTaUgKU5f/7O+yRH4jt0EQama/dtsysxKAum+J5EJEeCiPd23Ip/3aUr5kMe55J43+7/3Ajv1H/F2aCJD7IBhD5vcH7jKzu4CpwNvelSVS9MRWL8eE/u14uGM9piz7mQ7JKXyyIF1N7MTvcn2x2Pet3w6+xanOucmeVXUWulgsRcGanzOb2C3YtJvL60fxj2ubcGE5NbET7+T5YrHPCuAr59yfgRlmpjbUIuepbpUyjO/bhievimHO+l/oNCSV0XM2qomd+EVu7xq6BxgPvOZbVY3ML4OJyHkKDTH+1K4WUwYmEFejHH/9dCk9R83hRzWxkwKW2yOCfkBbYC+Ac24NUNmrokSCSY0KJRl9V0uevz6WFdv20mVoKq+mrOP4iZP+Lk2CRG6D4IivlTQAZhZG5kQyIpIPzIybLqnBtKREEutFMXjSSq595TuWb/Xb9OASRHIbBClm9n9ACTPrCHwIfO5dWSLBqUrZCF67rQWv3NKcbXsO0f3lmfx7yio1sRNP5TYIHgMygCXAvcBE4AmvihIJZmZGtyZVmTowke5xF/LSN2u5cvhM5m9UEzvxRo63j/qmnFzmnGtQMCWdnW4flWDz7art/OWTpWzdc4g72kTz5071KVU8LOcXimSRp9tHnXMngFVmdlG+VyYiObqsfmUmD0zgtlY1eWvWBjoPTWXGGs3UJ/knt6eGygPLfBPXTzj18LIwEfmf0sXDeKZHY8bd25pioSHc9sb3PDp+EXsOqomd5F1ujy//6mkVIpIrLWtVYOKD7Rn29RpGpq5n+qoMnu3RmC6NL/B3aRLAcpqhLMLMHgJuBBoAs5xzKaceOb25mXUxs1VmttbMBmWz/Q4zyzCzhb7H3ee7IyLBIiI8lMe6NOCzfm2JKl2cvu/O5/4x89m+Lz+mEZdglNOpobeBeDLvFuoK/Du3b+y7yDzC97oYoJeZxWQz9APnXJzv8Xpu318k2DWuFsln/dvySOf6TFuxnY7JqXw0X03s5NzlFAQxzrlbnXOvATcA7c/hvVsCa51z631fRhtLzhPei8g5CA8Nod/ldZg4oD11Kpfm4Q8X0futeaTvOujv0iSA5BQEv16J8s1BfC6qAZuzLKf71p3uejNbbGbjzaxGdm9kZn3MLM3M0jIydLeEyOnqVC7Nh/e25unujUjb8Audh6TyzuwNamInuZJTEDQ1s72+xz4g9tRzM8uP775/DkQ752I5yxwHzrmRzrl451x8VFRUPnysSNETEmL0bhPN5IcSaF6zPE9+tow/jpzNuoz9/i5NCrmzBoFzLtQ5V9b3KOOcC8vyvGwO770FyPoTfnXfuqzvv9M5d2qapteBFue6AyLyWzUqlOSdP7XkxRubsvrn/XQdNoMR09dyTE3s5AzOZT6CczUPqGtmtcysGNAT+M13D8ysapbF7mTOeSAieWRm3NCiOlOTEujQsDIvTF7FNSNmsXTLHn+XJoWQZ0Hgu6bQH5hM5n/w45xzy8zsGTPr7hs2wMyWmdkiYABwh1f1iASjymUieOWWFrx6a3N+3nuEHiNm8fxXKzl8TE3s5H9yPVVlYaFeQyLnZ8/BY/z9y+V8OD+d2lGleP76WOKjK/i7LCkg+TVVpYgEsMiS4bxwY1Pe+VNLjhw7yY2vzeZvny1l/5FzvSFQihoFgUiQSagXxZSBCfRuHc07czbSeUgqKat1W3YwUxCIBKFSxcN4qnsjxvdtTUR4CL3f/J6kcQvZffBozi+WIkdBIBLEWtSswJcD2tP/8jpMWLiVDskpTFyyzd9lSQFTEIgEuYjwUP7cuT6f9W/LBZER3D/mB/qOns/2vWpiFywUBCICQKMLI/n0/rY81qUB36zaTofkFMalbVYTuyCgIBCRX4WFhnDfZRfz1YPtaXBBWR4dv5jb3/yezb+oiV1RpiAQkd+pHVWasX1a8WyPRvywcRedh6by1qwfOaEmdkWSgkBEshUSYtzWOpopSYm0rFWBpz9fzo2vfsfa7fv8XZrkMwWBiJxVtXIleOuOSxjyx6as33GAbsNm8vI3a9TErghREIhIjsyMa5tVZ1pSIh0bVeHFKau5+qWZLElXE7uiQEEgIrlWqXRxRtzcnNdua8EvB45yzSuzGDxJTewCnYJARM5Z50YXMDUpkRuaV+fVlHV0HTaDuet3+rssOU8KAhE5L5ElwnnuhljG3H0px0+e5I8j5/DEp0vYd/hYzi+WQkVBICJ50rZOJSY/lMBd7WoxZu4mOg9JZfrK7f4uS86Bp0FgZl3MbJWZrTWzQWcZd72ZOTPLtle2iBRuJYuF8derYvjovjaUKh7Gnf+dx8APFvLLATWxCwSeBYGZhQIjgK5ADNDLzGKyGVcGeBCY61UtIlIwml9Uni8GtGPAH+ry+aKtdExO4YvFW9WmopDz8oigJbDWObfeOXcUGAv0yGbcs8BzgDpciRQBxcNCSepYj88faEe18iXo/94C+oyez89qYldoeRkE1YDNWZbTfet+ZWbNgRrOuS89rENE/KBh1bJ8fF8b/q9bA1JXZ9AhOYWx32/S0UEh5LeLxWYWAiQDD+dibB8zSzOztIwMzaQkEijCQkPok3Axkx9KIKZqWQZ9vIRbXp/Lpp1qYleYeBkEW4AaWZar+9adUgZoDHxrZhuAVsCE7C4YO+dGOufinXPxUVFRHpYsIl6IrlSK9+9pxT+vbcLi9D10GprC6zPWq4ldIeFlEMwD6ppZLTMrBvQEJpza6Jzb45yr5JyLds5FA3OA7s65NA9rEhE/CQkxbr70IqYmJdDm4kr8/csVXP+f71j9s5rY+ZtnQeCcOw70ByYDK4BxzrllZvaMmXX36nNFpHCrGlmCN3rHM6xnHJt+OciVw2cwbNoajh5XEzt/sUC7cBMfH+/S0nTQIFIU7Nx/hKc/X86ERVtpcEEZnrs+lqY1yvm7rCLJzOY757L9rpa+WSwiflOxdHGG92rG67fHs/vgMa59ZRb/nLiCQ0fVxK4gKQhExO86xFRhSlICPVtexMjU9XQZlsrsdWpiV1AUBCJSKJSNCOef1zbhvXsuBaDXqDk8/vES9qqJnecUBCJSqLS5uBJfPZhAn4TafDBvE52SU/l6xc/+LqtIUxCISKFTolgo/9etIR/f35bIEuHc9XYaA95fwM79R/xdWpGkIBCRQiuuRjk+f6AdAzvUY9LSbXQckspnC7eoTUU+UxCISKFWLCyEBzvU5csB7bmoQkkeHLuQu99OY9ueQ/4urchQEIhIQKhXpQwf3deGJ65syKx1O+iYnMqYuRs5qTYVeaYgEJGAERpi3N2+NlMeSiS2eiR/+WQpN78+hw07Dvi7tICmIBCRgHNRxZKMuftSBl/XhGVb9tJ5aCojU9dx/ITaVJwPBYGIBCQzo2fLi5ialEj7ulH8c+JKrv/Pd6z8aa+/Sws4CgIRCWgXREYw6vYWvHxzM9J3HeKq4TNJnrqaI8fVpiK3FAQiEvDMjKtiL2RaUiJXN72Q4V+v4eqXZrJg0y5/lxYQFAQiUmSUL1WMIX+M4607LmHf4eNc95/vePaL5Rw8etzfpRVqCgIRKXIub1CZKQMTuOXSi3hj5o90HprKrLU7/F1WoaUgEJEiqUxEOH+/pgkf9GlFWEgIt7w+l0EfLWbPITWxO52nQWBmXcxslZmtNbNB2Wzva2ZLzGyhmc00sxgv6xGR4HNp7YpMerA99ybWZlzaZjompzBl2U/+LqtQ8SwIzCwUGAF0BWKAXtn8R/+ec66Jcy4OeB5I9qoeEQleEeGhPN61IZ/2a0uFUsXoM3o+/d/7gR1qYgd4e0TQEljrnFvvnDsKjAV6ZB3gnMt6w28pQN8VFxHPxFbPbGL35071mLLsZzokp/DJgvSgb2LnZRBUAzZnWU73rfsNM+tnZuvIPCIYkN0bmVkfM0szs7SMjAxPihWR4BAeGkL/K+oy8cF21K5UioEfLOLO/85jy+7gbWLn94vFzrkRzrmLgceAJ84wZqRzLt45Fx8VFVWwBYpIkVSnchk+7NuGv10dw9z1v9ApOYXRc4KziZ2XQbAFqJFlubpv3ZmMBa7xsB4Rkd8IDTHubFuLKQMTaHZRef766VJ6jpzD+oz9/i6tQHkZBPOAumZWy8yKAT2BCVkHmFndLItXAms8rEdEJFs1KpRk9F0tef6GWFb+tJeuw2bwakrwNLHzLAicc8eB/sBkYAUwzjm3zMyeMbPuvmH9zWyZmS0EkoDeXtUjInI2ZsZN8TWYlpTIZfWjGDxpJde8MovlW4t+EzsLtKvl8fHxLi0tzd9liEgRN2nJNv762TJ2HzxK38SL6X9FHSLCQ/1d1nkzs/nOufjstvn9YrGISGHUtUlVpiUl0COuGi9PX8uVw2cwf+Mv/i7LEwoCEZEzKFeyGP++qSlv/6klh4+d5IZXZ/PUhGUcOFK0mtgpCEREcpBYL4rJAxO4vVVN/vvdBjoPTWXGmqLznSYFgYhILpQuHsbTPRrzYd/WFAsL4bY3vueRDxex52DgN7FTEIiInINLoiswcUB77r/sYj5esIUOQ1L4auk2f5eVJwoCEZFzFBEeyqNdGvBZv7ZElS5O33d/4L5357N932F/l3ZeFAQiIuepcbVIPuvflkc61+frldvpmJzK+PmB18ROQSAikgfhoSH0u7wOEwe0p27l0vz5w0X0fmse6bsO+ru0XFMQiIjkgzqVSzPu3tY83b0RaRt+odOQVN7+bkNANLFTEIiI5JOQEKN3m2imDEwgProCf5uwjJtem83a7YW7iZ2CQEQkn1UvX5K377yEf9/YlDXb99Nt2AxGTF/LsULaxE5BICLiATPj+hbVmZaUSIeYyrwweRU9Xp7F0i17/F3a7ygIREQ8FFWmOK/c0oJXb21Oxv4j9Bgxi+e+WsnhYyf8XdqvFAQiIgWgS+OqTBuYyHXNqvGfb9fRbdgM5m0oHE3sFAQiIgUksmQ4L9zYlNF3teToiZPc+OpsnvxsKfv93MTO0yAwsy5mtsrM1prZoGy2J5nZcjNbbGZfm1lNL+sRESkM2teNYvJDCdzZNprRczbSeUgq367a7rd6PAsCMwsFRgBdgRigl5nFnDZsARDvnIsFxgPPe1WPiEhhUqp4GH+7uhHj+7ahRLFQ7nhrHknjFrLrwNECr8XLI4KWwFrn3Hrn3FEyJ6fvkXWAc266c+7U1+/mkDnBvYhI0GhRszxfDmjHA1fUYcLCrXQcksLEJdsKtE2Fl0FQDdicZTndt+5M7gImZbfBzPqYWZqZpWVkFJ0e4CIiAMXDQnm4U30m9G9H1cgS3D/mB/q+O5/tewumiV2huFhsZrcC8cAL2W13zo10zsU75+KjoqIKtjgRkQISc2FZPrm/DYO6NuDbVRl0SE5hXNpmz48Owjx87y1AjSzL1X3rfsPMOgB/ARKdc0c8rEdEpNALCw2hb+LFdIqpwqCPl/Do+MVMWLiVWy69iPU7DtCqdkVa1Cyfv5+Zr+/2W/OAumZWi8wA6AncnHWAmTUDXgO6OOf8d8lcRKSQqR1VmrH3tOK97zfxjy9XMHPtDgwoHh7CmLtb5WsYeHZqyDl3HOgPTAZWAOOcc8vM7Bkz6+4b9gJQGvjQzBaa2QSv6hERCTQhIcatrWrSu03mnfUOOHb8JHPW78zXz/HyiADn3ERg4mnrnszyvIOXny8iUhR0jLmA/363gWPHTxIeFkKr2hXz9f09DQIREcm7FjXLM+buVsxZvzPgrhGIiEg+aVGzfL4HwCmF4vZRERHxHwWBiEiQUxCIiAQ5BYGISJBTEIiIBDkFgYhIkLOCbHWaH8wsA9h4ni+vBOzIx3ICgfY5OGifg0Ne9rmmcy7brp0BFwR5YWZpzrl4f9dRkLTPwUH7HBy82medGhIRCXIKAhGRIBdsQTDS3wX4gfY5OGifg4Mn+xxU1whEROT3gu2IQERETqMgEBEJckUyCMysi5mtMrO1ZjYom+3FzewD3/a5ZhbthzLzVS72OcnMlpvZYjP72sxq+qPO/JTTPmcZd72ZOTML+FsNc7PPZnaT7896mZm9V9A15rdc/N2+yMymm9kC39/vbv6oM7+Y2Ztmtt3Mlp5hu5nZcN/vx2Iza57nD3XOFakHEAqsA2oDxYBFQMxpY+4HXvU97wl84O+6C2CfLwdK+p7fFwz77BtXBkgF5gDx/q67AP6c6wILgPK+5cr+rrsA9nkkcJ/veQywwd9153GfE4DmwNIzbO8GTAIMaAXMzetnFsUjgpbAWufceufcUWAs0OO0MT2At33PxwN/MDMrwBrzW4777Jyb7pw76FucA1Qv4BrzW27+nAGeBZ4DDhdkcR7JzT7fA4xwzu0CcM5tL+Aa81tu9tkBZX3PI4GtBVhfvnPOpQK/nGVID+Adl2kOUM7MqublM4tiEFQDNmdZTvety3aMc+44sAfI30lAC1Zu9jmru8j8iSKQ5bjPvkPmGs65LwuyMA/l5s+5HlDPzGaZ2Rwz61Jg1XkjN/v8FHCrmaWTOUf6AwVTmt+c67/3HGmqyiBjZrcC8UCiv2vxkpmFAMnAHX4upaCFkXl66DIyj/pSzayJc263P4vyWC/gv865f5tZa2C0mTV2zp30d2GBoigeEWwBamRZru5bl+0YMwsj83ByZ4FU543c7DNm1gH4C9DdOXekgGrzSk77XAZoDHxrZhvIPJc6IcAvGOfmzzkdmOCcO+ac+xFYTWYwBKrc7PNdwDgA59xsIILM5mxFVa7+vZ+LohgE84C6ZlbLzIqReTF4wmljJgC9fc9vAL5xvqswASrHfTazZsBrZIZAoJ83hhz22Tm3xzlXyTkX7ZyLJvO6SHfnXJp/ys0Xufm7/SmZRwOYWSUyTxWtL8Aa81tu9nkT8AcAM2tIZhBkFGiVBWsCcLvv7qFWwB7n3La8vGGROzXknDtuZv2ByWTecfCmc26ZmT0DpDnnJgBvkHn4uJbMizI9/Vdx3uVyn18ASgMf+q6Lb3LOdfdb0XmUy30uUnK5z5OBTma2HDgBPOKcC9ij3Vzu88PAKDMbSOaF4zsC+Qc7M3ufzDCv5Lvu8TcgHMA59yqZ10G6AWuBg8Cdef7MAP79EhGRfFAUTw2JiMg5UBCIiAQ5BYGISJBTEIiIBDkFgYhIkFMQiGTDzE6Y2UIzW2pmn5tZuXx+/w2++/wxs/35+d4i50pBIJK9Q865OOdcYzK/a9LP3wWJeEVBIJKz2fiaepnZxWb2lZnNN7MZZtbAt76KmX1iZot8jza+9Z/6xi4zsz5+3AeRMypy3ywWyU9mFkpm+4I3fKtGAn2dc2vM7FLgFeAKYDiQ4py71vea0r7xf3LO/WJmJYB5ZvZRIH/TV4omBYFI9kqY2UIyjwRWAFPNrDTQhv+16QAo7vv1CuB2AOfcCTJbmwMMMLNrfc9rkNkATkEghYqCQCR7h5xzcWZWksw+N/2A/wK7nXNxuXkDM7sM6AC0ds4dNLNvyWyIJlKo6BqByFn4ZnUbQGZjs4PAj2Z2I/w6d2xT39CvyZwCFDMLNbNIMtub7/KFQAMyW2GLFDoKApEcOOcWAIvJnADlFuAuM1sELON/0yY+CFxuZkuA+WTOnfsVEGZmK4DBZLbCFil01H1URCTI6YhARCTIKQhERIKcgkBEJMgpCEREgpyCQEQkyCkIRESCnIJARCTI/T+CFK7/hoMIggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "    \n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-bidder",
   "metadata": {},
   "source": [
    "## Considering first 1000 CompVis records using LemmaTokenizer and Statistical model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "authentic-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(CompVisLabels)\n",
    "y_test = np.asarray(CompVis_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dense-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[17526     0]\n",
      " [ 2152     0]]\n",
      "Accuracy: 0.8906392926110377\n",
      "Macro Precision: 0.44531964630551885\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4710783786689603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-phoenix",
   "metadata": {},
   "source": [
    "## Considering first 1000 Math records using LemmaTokenizer and Statistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "other-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(MathLabels)\n",
    "y_test = np.asarray(Math_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "heard-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[13748     0]\n",
      " [ 5930     0]]\n",
      "Accuracy: 0.6986482366094116\n",
      "Macro Precision: 0.3493241183047058\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4112965954646084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-biology",
   "metadata": {},
   "source": [
    "## Using Bert Tokenizer and 1000 records as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "outer-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Loading the BERT tokenizer.\n",
    "print('Loading BERT tokenizer')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "perceived-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "\n",
    "for abstract in trainDocs:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "brave-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "victorian-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "other-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "\n",
    "for abstract in testDocs:\n",
    "    encoded_abstract = tokenizer.encode(\n",
    "                        abstract,                      # Sentence to encode.\n",
    "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Adding the encoded sentence to the list\n",
    "    input_ids.append(encoded_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "final-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 16 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 16 #performing truncate\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "solved-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-export",
   "metadata": {},
   "source": [
    "## Using Bert Tokenizer and statistical model on 1000 InfoTheory records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "minute-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(InfoTheoryLabels)\n",
    "y_test = np.asarray(InfoTheory_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "empirical-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[16031    31]\n",
      " [ 3607     9]]\n",
      "Accuracy: 0.8151234881593657\n",
      "Macro Precision: 0.5206627456971179\n",
      "Macro Recall: 0.5002794584425616\n",
      "Macro F1 score:0.45150932583098885\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-turkish",
   "metadata": {},
   "source": [
    "## Using Bert Tokenizer and statistical model on 1000 CompVis records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "formed-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(CompVisLabels)\n",
    "y_test = np.asarray(CompVis_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "endangered-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[17482    44]\n",
      " [ 2148     4]]\n",
      "Accuracy: 0.8886065657078971\n",
      "Macro Precision: 0.4869544914246901\n",
      "Macro Recall: 0.49967409015686515\n",
      "Macro F1 score:0.4723209270006557\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-receipt",
   "metadata": {},
   "source": [
    "## Using Bert Tokenizer and statistical model on 1000 Math records.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "chemical-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(MathLabels)\n",
    "y_test = np.asarray(Math_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "dense-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[13748     0]\n",
      " [ 5930     0]]\n",
      "Accuracy: 0.6986482366094116\n",
      "Macro Precision: 0.3493241183047058\n",
      "Macro Recall: 0.5\n",
      "Macro F1 score:0.4112965954646084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "for clf in models:\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(model_name)\n",
    "    # Do the prediction\n",
    "    y_predict=clf.predict(x_test)\n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test,y_predict)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-burst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "written-flooring",
   "metadata": {},
   "source": [
    "# 2: Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simple-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import smart_open\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collected-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\".//assignment1_data/axcs_train.csv\"\n",
    "test_path=\".//assignment1_data/axcs_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "painful-express",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_train_1000 = df_train[:1000]\n",
    "df_train_20000 = df_train[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stone-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54731"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "democratic-orange",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-advisory",
   "metadata": {},
   "source": [
    "## LDA with K=10 and 20K training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "revised-circumstances",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "docs = df_train_20000['Abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "recent-details",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "present-pledge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "exterior-acoustic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "#only ones that appear 20 times or more.\n",
    "bigram = Phrases(docs, min_count=25)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eligible-agreement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "logical-james",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "guided-showcase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6303\n",
      "Number of documents: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "planned-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in model10.gensim\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "#training parameters.\n",
    "NUM_TOPICS = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None \n",
    "\n",
    "temp = dictionary[0]  \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "outputfile = f'model{NUM_TOPICS}.gensim'\n",
    "print(\"Saving model in \" + outputfile)\n",
    "print(\"\")\n",
    "model.save(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "economic-renewal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -2.1161.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"problem\" + 0.015*\"be\" + 0.012*\"it\" + 0.010*\"set\" + 0.010*\"can\" + 0.010*\"a\" + 0.009*\"which\" + 0.008*\"function\" + 0.008*\"one\" + 0.007*\"show\" + 0.007*\"can_be\" + 0.007*\"two\" + 0.007*\"result\" + 0.007*\"number\" + 0.006*\"graph\" + 0.006*\"such\" + 0.006*\"quantum\" + 0.006*\"complexity\" + 0.006*\"class\" + 0.006*\"all\"'),\n",
       " (1,\n",
       "  '0.092*\"network\" + 0.025*\"node\" + 0.016*\"wireless\" + 0.014*\"power\" + 0.010*\"transmission\" + 0.010*\"link\" + 0.009*\"mobile\" + 0.009*\"energy\" + 0.008*\"delay\" + 0.008*\"system\" + 0.008*\"a\" + 0.008*\"distributed\" + 0.008*\"performance\" + 0.008*\"communication\" + 0.007*\"routing\" + 0.007*\"throughput\" + 0.006*\"at\" + 0.006*\"control\" + 0.006*\"traffic\" + 0.006*\"user\"'),\n",
       " (2,\n",
       "  '0.052*\"channel\" + 0.039*\"code\" + 0.022*\"rate\" + 0.016*\"capacity\" + 0.015*\"coding\" + 0.012*\"interference\" + 0.012*\"error\" + 0.012*\"source\" + 0.011*\"decoding\" + 0.010*\"information\" + 0.010*\"scheme\" + 0.009*\"at\" + 0.009*\"multiple\" + 0.009*\"receiver\" + 0.008*\"feedback\" + 0.007*\"mimo\" + 0.007*\"bit\" + 0.007*\"region\" + 0.007*\"fading\" + 0.007*\"a\"'),\n",
       " (3,\n",
       "  '0.019*\"system\" + 0.013*\"data\" + 0.011*\"a\" + 0.010*\"it\" + 0.009*\"paper\" + 0.009*\"be\" + 0.008*\"application\" + 0.007*\"this_paper\" + 0.007*\"have\" + 0.007*\"ha\" + 0.006*\"based\" + 0.006*\"from\" + 0.006*\"been\" + 0.005*\"design\" + 0.005*\"information\" + 0.005*\"can\" + 0.005*\"these\" + 0.005*\"used\" + 0.005*\"or\" + 0.005*\"which\"'),\n",
       " (4,\n",
       "  '0.018*\"logic\" + 0.017*\"language\" + 0.016*\"model\" + 0.014*\"system\" + 0.014*\"program\" + 0.012*\"a\" + 0.009*\"rule\" + 0.009*\"theory\" + 0.008*\"programming\" + 0.008*\"based\" + 0.008*\"which\" + 0.008*\"order\" + 0.007*\"constraint\" + 0.007*\"semantics\" + 0.007*\"type\" + 0.007*\"calculus\" + 0.006*\"can\" + 0.006*\"be\" + 0.006*\"proof\" + 0.006*\"structure\"'),\n",
       " (5,\n",
       "  '0.033*\"word\" + 0.019*\"cognitive\" + 0.018*\"text\" + 0.016*\"recognition\" + 0.016*\"decoder\" + 0.015*\"language\" + 0.014*\"spectrum\" + 0.013*\"information\" + 0.012*\"document\" + 0.012*\"from\" + 0.011*\"based\" + 0.010*\"feature\" + 0.010*\"semantic\" + 0.009*\"cognitive_radio\" + 0.008*\"alignment\" + 0.008*\"character\" + 0.008*\"retrieval\" + 0.008*\"page\" + 0.007*\"radio\" + 0.007*\"search\"'),\n",
       " (6,\n",
       "  '0.027*\"protocol\" + 0.026*\"game\" + 0.023*\"user\" + 0.018*\"security\" + 0.016*\"scheme\" + 0.015*\"packet\" + 0.015*\"key\" + 0.014*\"communication\" + 0.014*\"message\" + 0.013*\"attack\" + 0.012*\"gaussian\" + 0.010*\"strategy\" + 0.009*\"secure\" + 0.009*\"equilibrium\" + 0.009*\"player\" + 0.008*\"two\" + 0.008*\"access\" + 0.007*\"one\" + 0.007*\"which\" + 0.007*\"based\"'),\n",
       " (7,\n",
       "  '0.024*\"model\" + 0.014*\"information\" + 0.012*\"a\" + 0.011*\"distribution\" + 0.010*\"learning\" + 0.010*\"time\" + 0.009*\"process\" + 0.008*\"agent\" + 0.008*\"it\" + 0.007*\"which\" + 0.007*\"social\" + 0.007*\"between\" + 0.007*\"or\" + 0.007*\"can\" + 0.006*\"problem\" + 0.006*\"dynamic\" + 0.006*\"be\" + 0.006*\"decision\" + 0.006*\"study\" + 0.006*\"measure\"'),\n",
       " (8,\n",
       "  '0.024*\"algorithm\" + 0.017*\"method\" + 0.014*\"based\" + 0.010*\"using\" + 0.009*\"problem\" + 0.009*\"based_on\" + 0.008*\"approach\" + 0.008*\"proposed\" + 0.008*\"result\" + 0.008*\"paper\" + 0.007*\"technique\" + 0.007*\"this_paper\" + 0.007*\"image\" + 0.007*\"a\" + 0.007*\"which\" + 0.006*\"our\" + 0.006*\"from\" + 0.006*\"performance\" + 0.006*\"analysis\" + 0.006*\"data\"'),\n",
       " (9,\n",
       "  '0.031*\"bound\" + 0.026*\"graph\" + 0.024*\"algorithm\" + 0.016*\"matrix\" + 0.015*\"time\" + 0.013*\"number\" + 0.013*\"lower\" + 0.012*\"tree\" + 0.011*\"random\" + 0.011*\"edge\" + 0.011*\"approximation\" + 0.010*\"vertex\" + 0.009*\"distance\" + 0.009*\"probability\" + 0.009*\"maximum\" + 0.009*\"lower_bound\" + 0.009*\"minimum\" + 0.009*\"weight\" + 0.009*\"at\" + 0.008*\"problem\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "model.num_topics\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / NUM_TOPICS\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "model.print_topics( num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "caroline-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el201221403462709028647742056071\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el201221403462709028647742056071_data = {\"mdsDat\": {\"x\": [-0.06559843247184595, -0.03761912640820221, -0.1386831306894761, 0.14964380392989987, 0.15366391103069058, 0.23248782247238403, -0.028016995257469708, -0.012477803447757035, 0.03665320702114808, -0.29005325617937194], \"y\": [0.15029615241617228, -0.22115001177284185, -0.10820258020600125, -0.01136442434765382, 0.1355006440138507, 0.015851190024439205, -0.2152091059833705, 0.06783823204419918, 0.03863455285832867, 0.1478053509528775], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [20.270815012530843, 6.7446420022959614, 8.509875736126379, 19.2602169746716, 7.146188463792277, 2.866237747567458, 3.553074435718339, 8.204967491848784, 17.14377876675084, 6.30020336869751]}, \"tinfo\": {\"Term\": [\"network\", \"channel\", \"algorithm\", \"code\", \"graph\", \"bound\", \"model\", \"method\", \"node\", \"user\", \"information\", \"system\", \"rate\", \"language\", \"based\", \"problem\", \"protocol\", \"scheme\", \"logic\", \"word\", \"capacity\", \"matrix\", \"game\", \"coding\", \"data\", \"time\", \"wireless\", \"power\", \"number\", \"communication\", \"quantum\", \"relay\", \"np\", \"algebra\", \"correlated\", \"note\", \"inverse\", \"topological\", \"exactly\", \"coloring\", \"rational\", \"ring\", \"closed_form\", \"intersection\", \"coding_scheme\", \"bipartite\", \"covering\", \"computable\", \"tensor\", \"shortest_path\", \"cellular_automaton\", \"sat\", \"randomness\", \"np_complete\", \"arc\", \"only_if\", \"turing\", \"predicate\", \"alpha\", \"prime\", \"boolean\", \"union\", \"polynomial\", \"cite\", \"destination\", \"polynomial_time\", \"combinatorial\", \"classical\", \"exists\", \"complete\", \"class\", \"every\", \"whether\", \"if\", \"function\", \"theorem\", \"algebraic\", \"set\", \"prove\", \"complexity\", \"property\", \"finite\", \"problem\", \"existence\", \"group\", \"family\", \"sequence\", \"one\", \"there\", \"be\", \"any\", \"all\", \"some\", \"can_be\", \"two\", \"can\", \"it\", \"number\", \"show\", \"graph\", \"not\", \"which\", \"such\", \"show_that\", \"a\", \"result\", \"also\", \"or\", \"from\", \"network\", \"wireless\", \"routing\", \"throughput\", \"traffic\", \"ad\", \"hoc\", \"mobile\", \"wireless_network\", \"wireless_sensor\", \"modulation\", \"ad_hoc\", \"mobility\", \"sensor_network\", \"social_network\", \"band\", \"multicast\", \"base_station\", \"tcp\", \"hoc_network\", \"downlink\", \"routing_protocol\", \"constellation\", \"trust\", \"wireless_communication\", \"mobile_ad\", \"manet\", \"multi_hop\", \"p2p\", \"resource_allocation\", \"buffer\", \"node\", \"delay\", \"energy\", \"transmission\", \"link\", \"network_coding\", \"allocation\", \"topology\", \"power\", \"layer\", \"distributed\", \"communication\", \"control\", \"load\", \"peer\", \"path\", \"performance\", \"protocol\", \"multi\", \"simulation\", \"system\", \"a\", \"at\", \"user\", \"proposed\", \"scheme\", \"this_paper\", \"paper\", \"coding\", \"decoding\", \"mimo\", \"achievable\", \"fading\", \"transmit\", \"snr\", \"distortion\", \"ldpc\", \"interference_channel\", \"achievable_rate\", \"secret\", \"csi\", \"parity\", \"capacity_region\", \"erasure\", \"ldpc_code\", \"noise_ratio\", \"relay_channel\", \"space_time\", \"decode\", \"quantization\", \"sum_rate\", \"encoder\", \"side_information\", \"parity_check\", \"fading_channel\", \"precoding\", \"correcting\", \"dmt\", \"channel\", \"code\", \"multiple_access\", \"receiver\", \"diversity\", \"capacity\", \"feedback\", \"rate\", \"interference\", \"bit\", \"source\", \"error\", \"symbol\", \"gain\", \"multiple\", \"region\", \"noise\", \"sum\", \"scheme\", \"block\", \"information\", \"at\", \"shown\", \"optimal\", \"over\", \"case\", \"a\", \"it\", \"signal\", \"software\", \"research\", \"technology\", \"project\", \"neural\", \"interface\", \"platform\", \"mining\", \"cloud\", \"scientific\", \"organization\", \"engineering\", \"fault\", \"file\", \"hardware\", \"journal\", \"processor\", \"business\", \"site\", \"library\", \"neural_network\", \"xml\", \"life\", \"integrated\", \"spreadsheet\", \"virtual\", \"overview\", \"electronic\", \"data_mining\", \"multiple_output\", \"development\", \"review\", \"de\", \"modern\", \"architecture\", \"grid\", \"database\", \"web\", \"community\", \"device\", \"science\", \"human\", \"year\", \"management\", \"data\", \"service\", \"will\", \"tool\", \"application\", \"system\", \"environment\", \"challenge\", \"computer\", \"design\", \"been\", \"issue\", \"have\", \"component\", \"ha\", \"paper\", \"a\", \"it\", \"ha_been\", \"this_paper\", \"be\", \"used\", \"from\", \"based\", \"information\", \"these\", \"their\", \"present\", \"or\", \"approach\", \"process\", \"can\", \"new\", \"which\", \"using\", \"such\", \"at\", \"logic\", \"semantics\", \"calculus\", \"fuzzy\", \"formal\", \"grammar\", \"hop\", \"reasoning\", \"lambda\", \"termination\", \"formalism\", \"fragment\", \"smart\", \"rewriting\", \"rayleigh\", \"higher_order\", \"java\", \"relational\", \"context_free\", \"pixel\", \"reed\", \"programming_language\", \"modal\", \"molecular\", \"syntax\", \"axiom\", \"hamming\", \"logic_programming\", \"propositional\", \"semantic_web\", \"abstract\", \"program\", \"checking\", \"language\", \"programming\", \"rule\", \"logical\", \"specification\", \"interpretation\", \"symbolic\", \"proof\", \"net\", \"theory\", \"type\", \"model\", \"constraint\", \"relation\", \"operator\", \"context\", \"representation\", \"system\", \"order\", \"a\", \"framework\", \"structure\", \"term\", \"based\", \"which\", \"can\", \"approach\", \"be\", \"it\", \"paper\", \"can_be\", \"word\", \"cognitive\", \"text\", \"recognition\", \"decoder\", \"cognitive_radio\", \"alignment\", \"speech\", \"tag\", \"corpus\", \"cdma\", \"annotation\", \"dof\", \"english\", \"interference_alignment\", \"rfid\", \"primary_user\", \"search_engine\", \"gaussian_noise\", \"lexical\", \"spectrum_sensing\", \"discourse\", \"wideband\", \"simulator\", \"spatially\", \"intensity\", \"arxiv\", \"information_retrieval\", \"compound\", \"radar\", \"sentence\", \"retrieval\", \"page\", \"character\", \"document\", \"dictionary\", \"spectrum\", \"semantic\", \"face\", \"similarity\", \"extraction\", \"language\", \"radio\", \"feature\", \"translation\", \"information\", \"from\", \"based\", \"search\", \"user\", \"method\", \"using\", \"game\", \"security\", \"packet\", \"attack\", \"secure\", \"equilibrium\", \"player\", \"secondary\", \"outer\", \"sparsity\", \"secrecy\", \"authentication\", \"vehicle\", \"broadcast_channel\", \"client\", \"outer_bound\", \"nash\", \"ml\", \"party\", \"nash_equilibrium\", \"payoff\", \"model_checking\", \"scheduler\", \"gaussian_interference\", \"malicious\", \"session\", \"patient\", \"financial\", \"directional\", \"password\", \"protocol\", \"route\", \"key\", \"message\", \"cooperation\", \"sharing\", \"gaussian\", \"server\", \"user\", \"communication\", \"bandwidth\", \"scheme\", \"strategy\", \"et\", \"primary\", \"access\", \"tradeoff\", \"two\", \"one\", \"which\", \"based\", \"not\", \"where\", \"can\", \"agent\", \"qos\", \"worst\", \"item\", \"worst_case\", \"price\", \"stochastic\", \"consensus\", \"multiple_input\", \"divergence\", \"opinion\", \"regression\", \"uncertainty\", \"time_series\", \"auction\", \"regularization\", \"qualitative\", \"power_law\", \"spreading\", \"reward\", \"home\", \"incentive\", \"multi_agent\", \"long_term\", \"repair\", \"csp\", \"pricing\", \"competition\", \"assisted\", \"regret\", \"social\", \"emergence\", \"entropy\", \"learning\", \"preference\", \"utility\", \"state_information\", \"law\", \"competitive\", \"prediction\", \"online\", \"bayesian\", \"decision\", \"distribution\", \"market\", \"markov\", \"model\", \"behavior\", \"measure\", \"individual\", \"policy\", \"process\", \"information\", \"dynamic\", \"mechanism\", \"probability\", \"time\", \"between\", \"a\", \"optimal\", \"study\", \"or\", \"strategy\", \"it\", \"which\", \"case\", \"can\", \"problem\", \"our\", \"be\", \"state\", \"show\", \"from\", \"when\", \"image\", \"optimization\", \"sensor\", \"antenna\", \"sensing\", \"transmitter\", \"clustering\", \"filter\", \"reconstruction\", \"mac\", \"recovery\", \"heuristic\", \"citation\", \"compressed\", \"ensemble\", \"beamforming\", \"planar\", \"optimization_problem\", \"nonlinear\", \"segmentation\", \"3d\", \"motion\", \"experimental_result\", \"compressed_sensing\", \"filtering\", \"genetic\", \"soft\", \"wavelet\", \"convolutional\", \"eavesdropper\", \"differential\", \"method\", \"measurement\", \"numerical\", \"detection\", \"estimation\", \"station\", \"algorithm\", \"array\", \"technique\", \"equation\", \"signal\", \"based_on\", \"based\", \"solution\", \"proposed\", \"using\", \"approach\", \"search\", \"novel\", \"vector\", \"simulation\", \"analysis\", \"performance\", \"problem\", \"result\", \"efficient\", \"this_paper\", \"paper\", \"our\", \"used\", \"data\", \"from\", \"which\", \"new\", \"a\", \"can\", \"it\", \"be\", \"model\", \"lower\", \"lower_bound\", \"weight\", \"log\", \"moment\", \"approximation_algorithm\", \"covariance\", \"epsilon\", \"cr\", \"eigenvalue\", \"error_probability\", \"outage_probability\", \"packing\", \"running_time\", \"angle\", \"fixed_point\", \"superposition\", \"covariance_matrix\", \"normalized\", \"handoff\", \"eps\", \"hilbert\", \"forest\", \"mu\", \"sqrt\", \"triangle\", \"blind\", \"sorting\", \"crossing\", \"subgraph\", \"rank\", \"vertex\", \"edge\", \"bound\", \"upper\", \"approximation\", \"upper_bound\", \"tree\", \"tight\", \"matrix\", \"minimum\", \"distance\", \"maximum\", \"graph\", \"constant\", \"minimization\", \"cut\", \"random\", \"at_most\", \"probability\", \"matching\", \"size\", \"algorithm\", \"number\", \"time\", \"factor\", \"metric\", \"gaussian\", \"at\", \"distribution\", \"where\", \"our\", \"problem\", \"given\", \"show\"], \"Freq\": [12139.0, 8796.0, 12111.0, 6542.0, 5681.0, 4606.0, 9606.0, 6639.0, 3603.0, 5049.0, 6838.0, 12513.0, 4676.0, 3258.0, 10009.0, 11335.0, 2634.0, 4331.0, 2512.0, 1852.0, 3014.0, 2790.0, 1844.0, 2427.0, 7413.0, 8086.0, 2151.0, 3264.0, 6210.0, 2698.0, 2438.7594171523824, 2244.6656196326553, 801.3145279735019, 633.0449722062003, 466.00023986899595, 424.05300530803464, 357.9108010672401, 349.21094683596186, 344.57171709391577, 343.54043450076813, 331.37803800719865, 324.18579432084164, 311.3526015635907, 304.2205278151616, 286.81008382996134, 279.4626272093118, 260.78012813175656, 245.7211577824013, 234.95755583425574, 231.63274360329075, 231.25217744097466, 230.42867242538836, 221.33227627563517, 218.4619499611728, 215.01552257628424, 214.67748114081198, 213.07236979088623, 204.32352169492916, 241.96662321632365, 200.03320803224287, 532.996486238383, 222.5621776311297, 2173.041365968219, 345.5971797816993, 575.4656573321985, 645.9768946727758, 445.064486753693, 1258.4323401656534, 479.4569740407966, 1103.7244159879506, 2407.236769255388, 924.6347518606614, 683.2256338418475, 2359.684386156928, 3330.8063161262976, 943.7409214992256, 672.8276657091874, 4038.6143954987347, 1357.1879260414253, 2432.0348527138117, 2235.1426777495217, 1715.4131643064457, 6048.394013990177, 531.1792845523393, 1384.2699165849724, 795.0498056193186, 1337.7577893998828, 3319.700359984123, 1625.4621909585064, 5960.00973134201, 1896.5939134750017, 2398.0315802930118, 2241.0022790418648, 2870.5537313221625, 2831.772107915552, 3995.515376160652, 4674.66114040788, 2703.483586171097, 2985.759662293046, 2500.6668866673745, 2331.9180118415584, 3584.50188041437, 2479.336261088522, 2292.5062619481937, 3880.5181270554144, 2826.5884503729103, 2194.4912648267255, 2198.8155240728897, 2189.205770533648, 12138.523889678847, 2150.7234883047477, 991.3131719199848, 916.2399824884448, 775.5151541044165, 684.8774462619892, 605.6155020947608, 1237.384120337331, 555.5286284297283, 401.8758308063886, 391.7309628415569, 341.32154493993704, 337.9982160147017, 310.62383234269197, 307.50340509709315, 275.96041798928906, 271.3318526470333, 258.28660160795624, 252.71435058307137, 247.24601916368857, 221.9755176074056, 216.24054406438785, 211.0820735978919, 203.0851243566716, 198.1025843047168, 194.72227318154242, 182.29442958041045, 179.7422041867217, 174.5555897914305, 171.78659134091936, 266.8407975533647, 3354.223305878909, 1108.7297045727782, 1210.107345488544, 1381.781731429135, 1297.120102958319, 421.69403249044785, 668.1821097296415, 582.1492053597634, 1871.524358334035, 584.1118123592353, 1074.4069490241675, 995.8504546510824, 790.2370210682838, 348.02134297175144, 387.05237413463334, 610.7454737473945, 1005.3297742471351, 726.1481725127775, 678.4572976630063, 664.9671596179246, 1104.4054146010499, 1076.074353367, 841.0481071466564, 766.249506178296, 672.8322767716304, 657.373112936299, 687.4332976552761, 658.66187034238, 2427.0526485426476, 1761.4362436222073, 1250.3453834320626, 1023.1093377688557, 1144.032269662903, 787.4039553391056, 778.4602192163549, 685.4039665112248, 566.217319128944, 519.2083300014968, 471.23376063483494, 453.3457043454774, 417.6818839321679, 390.19284750062565, 382.17388641967455, 378.12143971951616, 370.20963121803675, 354.1912939934435, 348.09621231084867, 346.1723329124924, 326.8455429879389, 319.78324574333226, 314.42366995689315, 278.01258489779116, 267.81885864601793, 261.8659916423913, 257.2706177735725, 253.65433502517365, 232.44445143155048, 228.7506855315714, 8683.261284637376, 6440.5939291411205, 363.59720348768013, 1483.2971158889209, 947.7654998974157, 2666.232157446214, 1302.34514411893, 3707.3499813475614, 2083.032750857544, 1246.95162139285, 1968.6104606997899, 2077.532770928091, 610.1055283155348, 787.3287304904358, 1484.931504215921, 1179.7020862598022, 1029.6732119866392, 874.7797974994946, 1648.4067812525568, 795.6895908447666, 1709.7522559802221, 1584.5597298803063, 969.4451299645259, 1075.6186961883268, 1021.1985020166196, 976.9968948968284, 1096.0950401667767, 1027.9203016931012, 874.7565914902777, 1707.421781518817, 1670.9932538697497, 1164.9411865345737, 696.0256757937368, 612.7985894838658, 604.9830408980506, 525.4202654460821, 518.7402773791258, 506.9064695554612, 495.49111657610837, 483.6989471464497, 478.63668779476205, 475.73416126910206, 468.95988096598404, 465.49530221393246, 461.6869533560243, 421.788956095927, 411.74260301262206, 389.1578059184109, 364.68205293302526, 362.98741435877525, 355.69142040039, 325.7068858571194, 362.74641838415556, 301.00791583042775, 536.3746439512653, 248.13193614735394, 245.25672793310528, 242.62888262250684, 230.3398283577104, 1303.1478726606715, 401.87421550962404, 595.4380228909359, 284.8232042656884, 1314.405396298935, 790.8282796990576, 1116.311507896945, 1420.969458564473, 895.9546937956786, 793.3500242130779, 733.9074054245084, 892.3358106459457, 561.2263762128972, 1016.1589651960583, 5094.9648341093425, 1629.7790868029306, 1549.4348754592206, 1382.7726430548266, 3131.851941397593, 7227.718800775927, 1167.306952302794, 660.3064108201341, 1126.4554137890639, 2075.477866220103, 2232.1296999639526, 832.5519773354903, 2571.219562210721, 1060.989283581469, 2537.191873516783, 3391.2676840424956, 4205.0142645439455, 3819.065743351268, 1204.7056325013039, 2598.743906564232, 3249.233261869746, 1816.3411228459365, 2360.5111350820066, 2398.459442081395, 1913.5172914567675, 1816.5254773861702, 1548.7790597381786, 1641.2171756672644, 1737.8244184120583, 1657.270649182748, 1448.199822637764, 1825.9813143250647, 1584.4137701317838, 1722.6371997390693, 1573.4778094559072, 1545.4999811329683, 1556.2043530269452, 2511.6597152065383, 994.837734661312, 927.4412943140537, 663.4517889633717, 623.2866946630207, 623.1177829176298, 608.4236381539569, 454.9352091941818, 392.25348606075204, 320.8384436873098, 319.0618949505564, 292.56354609841725, 255.30926938953954, 252.62842484514468, 252.3350526741188, 248.77482729497993, 244.49317939352116, 227.51643729003348, 216.0060958248156, 208.74197526219257, 207.54085776956111, 206.39901960220914, 191.89858711797254, 191.11907870411434, 188.0077642441284, 187.11756076367695, 183.4827866467349, 181.51296703754525, 179.1713846437845, 177.70777712173032, 551.5515177756175, 1939.0037767269898, 401.7684880528086, 2438.83972339684, 1186.0879439412859, 1329.5129145371138, 343.2878805659213, 520.6404721881972, 407.7744024841141, 244.5587365072472, 869.6272440942083, 328.9544797602231, 1216.32948954833, 963.7562732750189, 2256.378679516601, 1039.1978956974626, 595.9705330784549, 548.7265783682848, 699.1460515916808, 675.255404509171, 2005.7332674454283, 1055.3609775640064, 1749.2841707426485, 759.0082956858537, 847.4571688014697, 723.9636533745066, 1126.5173554353605, 1094.9576050417934, 907.7605583212744, 775.0143020539737, 877.2806529341492, 764.4901782659437, 729.0453269411269, 701.7422440824913, 1851.7882531639273, 1089.3243349340098, 995.8855705242506, 921.9855578968011, 901.4311343002099, 492.46900556598587, 468.3414755945478, 381.11191134370665, 343.93807114755555, 341.2336783911575, 313.6195698135769, 300.66870504212727, 288.3789816667781, 276.9781940710393, 246.37587431173424, 240.54890185009756, 228.85962715888093, 214.35734755903664, 210.62924395801215, 206.0026049522346, 199.90933469258118, 198.90170190864458, 191.39234268377663, 189.36130068277947, 174.27639465367832, 173.0695171910621, 172.63158019377207, 168.5051052635663, 164.6425282250916, 153.68612983370593, 280.0271504216997, 442.9534571392045, 435.10327187904363, 452.45404352636285, 700.7444660274236, 375.50150747442933, 774.676215120509, 554.837245548618, 407.8874428309503, 383.8651939942742, 316.406582294564, 818.6308528273146, 412.731712682689, 586.5221984418476, 304.91619819749303, 747.5644606969139, 657.787874593654, 599.2018869172572, 408.3950164532511, 384.3851212325374, 347.09941655214107, 343.8804996251832, 1843.4695343850983, 1288.9021987739777, 1064.9649877349113, 880.7319750153522, 659.9175017947126, 639.5957759529242, 602.8081140936698, 404.7934570516126, 367.24136754618536, 365.60609518583203, 346.3058821587882, 330.76271510164014, 301.2618424866563, 297.996857594434, 293.46888125499186, 286.18693653963464, 265.55956644248283, 262.31955200801485, 259.99577129687435, 201.13097342115267, 197.60542440335388, 184.91191329656203, 165.25263614164146, 157.29310765076457, 155.7732748870995, 150.07064659746655, 144.39646772259272, 140.6834777035525, 138.30455389820466, 136.34361702290175, 1907.9857841175035, 268.37586342193293, 1048.5175194518263, 979.7235832026254, 438.209261760001, 444.74290558315164, 830.3207175251929, 393.9480199275722, 1616.3187424820774, 994.0107167265631, 396.5957338035641, 1147.7432907077796, 701.2515427005562, 412.9142530821525, 362.03223093336806, 546.6821982756367, 331.6073538406253, 556.046965287529, 482.5919744650471, 481.7665512736306, 460.92561548408435, 421.00126746476707, 408.443078031378, 429.4086060007607, 1321.872780264911, 437.9753423551297, 428.76889507059343, 397.1949659746832, 365.9259838331038, 293.85762947716296, 768.0484916500385, 264.32607162709047, 258.64276964559093, 247.59627927111578, 242.50498282898852, 233.99822794071213, 430.82611385064735, 212.33896037203814, 205.06759165877634, 179.374205871222, 172.81927291771035, 169.64047403084993, 168.4163689785457, 160.790142320459, 155.99030616599472, 149.22938479214412, 134.49526246186838, 129.6190675600067, 127.2161602668151, 124.48565930623094, 122.61950632995236, 120.78792877469294, 113.31268333211533, 110.89119315791994, 1135.4278444835074, 148.60449318457123, 864.3535403992921, 1678.7349732737096, 313.6951928192725, 485.22755430100324, 390.8884241972556, 552.8543373088221, 280.12065703331587, 479.8802645951086, 574.1264052114898, 308.51696978933865, 964.6992207610709, 1750.7350402346083, 328.90212746228, 542.347463765758, 3811.930782070378, 772.5019260090639, 915.6670691444243, 681.7599536323913, 561.5468083836151, 1447.5794005501448, 2243.282748004155, 1000.1097827313757, 692.1500741251084, 856.4897907349207, 1554.0877649115603, 1100.2677572244295, 1998.5485327370127, 877.3452642325398, 924.3554482312791, 1049.813696111016, 698.928808785901, 1276.7244132368826, 1180.2012502681703, 894.9032711930905, 1048.294329727207, 1010.0801929016234, 892.5917109609139, 979.1673638753631, 760.758971779271, 859.0591086660305, 879.0021814769505, 763.1149056788141, 2303.3611600218023, 1753.9596181085622, 1693.2782840860373, 1239.1764681231675, 1052.1387408999237, 952.5140691583068, 777.7228639911082, 614.6434946803787, 608.7879101221065, 566.4723195241061, 525.0802813817368, 508.7254647899616, 493.0824911177425, 486.4438793479586, 444.70721510453484, 444.4233015566193, 440.95601862844995, 366.26004670969104, 354.330226117856, 351.5718838295707, 345.178828452037, 326.71381921873046, 406.5469438169501, 299.06305560348744, 296.6187674297115, 292.3554166415042, 287.2375994198584, 282.3010962377227, 281.0844723652109, 274.05505294959136, 499.5932064819797, 5824.983235170396, 920.3520255528745, 969.0540825691533, 1316.920696537449, 993.6419606305267, 422.3520529508466, 8194.73694496931, 475.4040781155443, 2484.6399015358575, 840.9380450472881, 1828.0138724598403, 2915.5862321154823, 4837.404607414665, 1813.231699250344, 2760.014001376386, 3496.33239448653, 2858.2754645120563, 1190.7924068309526, 973.9206564991915, 942.424097778554, 1390.9062374564924, 1935.721549968013, 1936.346089177435, 3181.9097688987968, 2699.3362755934536, 1254.8344238336933, 2381.3242360113527, 2579.092632461945, 2080.925425798816, 1654.3356523268656, 1927.2719941228802, 2018.836564575353, 2190.0618093055336, 1668.7810042144256, 2267.6438238181463, 1760.6684336704125, 1643.7335462449882, 1576.9202443554248, 1522.6910702437542, 1619.4443098166016, 1108.7296957826811, 1091.250600089386, 1017.8542179289104, 316.0799006133109, 314.87387518037605, 267.6203816280171, 264.1506731504115, 244.32568243924567, 214.48793927474497, 212.5468632770814, 204.33476058457893, 197.97172105017006, 195.4582312173685, 183.44451659024017, 179.31708350095917, 161.78128691310434, 158.11979854149527, 156.6126703154126, 156.14787864084303, 155.1607872669727, 148.16239215707466, 143.06390940608517, 142.77017167575377, 142.6291112652163, 141.97402423963763, 139.2457945420565, 138.15781189672788, 137.66906575170637, 134.1251157113462, 758.6370414745766, 1266.738340969107, 1363.1510777134392, 3812.699597098345, 1040.306619131393, 1352.5045695319195, 761.8469405151383, 1536.9024509852854, 390.8212499005579, 1985.3820247854628, 1100.398267542121, 1173.7495657172933, 1141.3099663792325, 3180.163442603106, 853.5205548043767, 343.8422670847982, 464.4450121608228, 1387.3676348625638, 383.01952966123145, 1171.2679863523501, 551.6284460007713, 1035.0753778802032, 3031.959122916221, 1650.5799327101804, 1831.0262201473784, 659.8197486445401, 557.1606891538376, 644.6371112461009, 1083.2330301812938, 765.9758725604524, 868.4544808056326, 959.0560532704327, 1048.8363707855624, 742.0067584610987, 766.5000459547946], \"Total\": [12139.0, 8796.0, 12111.0, 6542.0, 5681.0, 4606.0, 9606.0, 6639.0, 3603.0, 5049.0, 6838.0, 12513.0, 4676.0, 3258.0, 10009.0, 11335.0, 2634.0, 4331.0, 2512.0, 1852.0, 3014.0, 2790.0, 1844.0, 2427.0, 7413.0, 8086.0, 2151.0, 3264.0, 6210.0, 2698.0, 2439.6412811239143, 2245.5478242690797, 802.1963916788833, 633.9268933868642, 466.88254291227275, 424.93717093604783, 358.79285587626964, 350.0928974312864, 345.454035914484, 344.42229152812087, 332.25992353682614, 325.06772585666096, 312.2351720962808, 305.1024733565772, 287.69242925723796, 280.344554236734, 261.66209648946136, 246.60302043489446, 235.839480645633, 232.5147793360645, 232.1340466058401, 231.31052499029082, 222.2142006963716, 219.34378299379594, 215.89743113387013, 215.5594080306387, 213.95418818885517, 205.20557087934006, 243.020488221409, 200.9151090091931, 537.0093375453644, 223.56909137738657, 2239.404682371007, 349.87573514614695, 599.4010031037836, 678.003229543854, 463.745178654746, 1401.817827837346, 503.83708772005656, 1255.085152931046, 3001.1383597057443, 1053.118164165938, 755.6331648328393, 3031.523802472513, 4640.875394144369, 1110.9601313839357, 762.7915284189693, 6155.539015123346, 1738.586398171948, 3486.484172432835, 3184.013192926647, 2357.6872072310875, 11335.945593179014, 586.8849489045328, 1940.4019369858506, 972.9969726869697, 1883.214104882982, 6211.984315882001, 2491.5311044027344, 14042.709153909911, 3119.5764351393436, 4284.513992039736, 4056.718171565115, 6512.789039266248, 6641.418917922996, 11203.663484910658, 14523.270918665925, 6210.319550450303, 7549.095939045213, 5681.621334363959, 5150.6864843452095, 12135.11073941231, 6069.608322076625, 5366.852985425262, 16714.980530009245, 8487.342766237967, 6209.402634373996, 6311.107457331156, 9379.50984299206, 12139.416502714643, 2151.616047343695, 992.2057184721178, 917.1325503731973, 776.4077179591206, 685.7700239788768, 606.508034666306, 1239.2698846524793, 556.4211599615164, 402.76836585163227, 392.62364073700974, 342.2140943047941, 338.8907608552787, 311.51639392433975, 308.3960548808936, 276.85311054201674, 272.22440189573695, 259.1791725568156, 253.60764002144577, 248.13852847552056, 222.86808798612213, 217.13304797376117, 211.9748679577494, 203.97781671992954, 198.9952115635411, 195.61478281015224, 183.18694615506064, 180.6347560282022, 175.448169820975, 172.6791393066521, 268.785143661532, 3603.0053961824765, 1174.1864810614377, 1394.851763041369, 1649.869739502771, 1579.920647062502, 461.7254867207327, 798.4458185952337, 747.1439663625672, 3264.3649492280233, 805.3360084790122, 2115.7242853603893, 2698.643932170592, 2199.9706240312353, 450.10344149719776, 559.8585985166043, 1468.4373385888398, 4652.453011118853, 2634.9349484857607, 2297.8362431993155, 2367.7077028019585, 12513.792623236475, 16714.980530009245, 6820.470342413688, 5049.723323402877, 4909.042864429817, 4331.414837045168, 8991.646694004343, 10737.020671498827, 2427.937100468336, 1762.3206249109103, 1251.2297791028873, 1023.9938013704566, 1145.0558162294346, 788.2884309970374, 779.344623695494, 686.2883853885701, 567.1016757251647, 520.0927183830138, 472.1181330251214, 454.23027896911907, 418.5662925892905, 391.07729321082616, 383.05826607220985, 379.0058553770603, 371.0939831108386, 355.07573758988497, 348.9805855854035, 347.05678208017946, 327.72993834322324, 320.66767063103447, 315.3080433588251, 278.8969802463503, 268.70323904642873, 262.75034707383935, 258.154994563229, 254.53872928887068, 233.32888768375435, 229.63505075181638, 8796.612342359933, 6542.060463772642, 368.5921892193226, 1587.9974998047837, 1001.6803678954337, 3014.932328743753, 1438.0984554791544, 4676.003809490938, 2517.9780082059465, 1650.8434775993305, 2975.8939678347215, 3227.0067163684075, 731.5393061780374, 1061.5705371340596, 2654.59109913311, 1916.721665123208, 1701.056961015987, 1434.7095044764321, 4331.414837045168, 1264.2298438031005, 6838.950748941384, 6820.470342413688, 2227.8335957448367, 3525.414532618261, 3977.64264394854, 4839.766865162802, 16714.980530009245, 14523.270918665925, 2703.5813862488544, 1708.3011198165411, 1672.0963002011113, 1165.8205613621715, 696.9050222388782, 613.6779993012531, 605.8624527226312, 526.2996318898583, 519.6196283135764, 507.7858176496558, 496.3704612411263, 484.57832769261057, 479.51606916418416, 476.61358703465555, 469.8392940012651, 466.3747510898743, 462.5663310158093, 422.6686530922408, 412.6219535976614, 390.0398031857311, 365.561439118868, 363.8668252222593, 356.5708066161472, 326.58652667542026, 363.73204542234333, 301.8872255170742, 538.1776859335818, 249.01136338088725, 246.13608224074207, 243.50823016870103, 231.22171081075507, 1308.358422727547, 403.98623452161314, 601.660462731666, 286.08110582079615, 1342.9950911788421, 803.0811127475007, 1148.870264798251, 1503.078716419878, 949.437586326458, 835.069356054246, 778.4437430521031, 974.741550616627, 589.9639376449463, 1158.4922957775027, 7413.019582428107, 2018.4860434830691, 1922.4064137832909, 1701.1485856259676, 4523.144066012586, 12513.792623236475, 1495.4540725550357, 721.6855756614451, 1441.1283200679468, 3342.5768323175807, 3839.1107494914027, 1041.9208729677298, 5758.169859511509, 1521.4402366446807, 6058.34116437107, 10737.020671498827, 16714.980530009245, 14523.270918665925, 2005.74772036784, 8991.646694004343, 14042.709153909911, 4767.549713464186, 9379.50984299206, 10009.247810659745, 6838.950748941384, 5971.068930330839, 4035.3540656062573, 4854.5292680078965, 6311.107457331156, 5733.709172835075, 3429.697824135907, 11203.663484910658, 5647.881543006377, 12135.11073941231, 7280.925708211998, 6069.608322076625, 6820.470342413688, 2512.5284263174008, 995.7064453927347, 928.3099988871469, 664.3208741379951, 624.1554787829187, 623.9864978313528, 609.2930686935791, 455.8039569772628, 393.1222356767543, 321.70719380792525, 319.93065675040435, 293.4323513525618, 256.1785197581461, 253.49716169273896, 253.204981008386, 249.64357734278386, 245.36663150632307, 228.38601074146882, 216.874802015069, 209.6111234403318, 208.4098252920597, 207.26772916539767, 192.7673368798717, 191.98794564320698, 188.87646330184663, 187.98628375773214, 184.35196862307023, 182.38164649090382, 180.04007368500882, 178.57673566638144, 563.8319903653963, 2057.1186599455614, 425.98585014281804, 3258.2205026201086, 1544.4952429787938, 1840.4270550304282, 383.50138016483413, 665.9626553487135, 513.5998746885917, 263.47031417857187, 1654.8885852875173, 407.03278916788696, 2841.484474418449, 2307.734199720328, 9606.666383991465, 2796.459948446796, 1111.3460427294656, 1000.6442013165569, 1614.3075034090434, 1524.842989667394, 12513.792623236475, 3698.8317539783825, 16714.980530009245, 2495.0385638093812, 3429.2466202071246, 2504.9055354489096, 10009.247810659745, 12135.11073941231, 11203.663484910658, 5733.709172835075, 14042.709153909911, 14523.270918665925, 10737.020671498827, 6512.789039266248, 1852.648989049425, 1090.185102553891, 996.746255413037, 922.8462648362547, 902.2920686791019, 493.3297447966081, 469.2022381887021, 381.9725794982908, 344.7987720086494, 342.0943465349647, 314.4803828177833, 301.5293975615015, 289.2397954774041, 277.83885790050675, 247.2366536656258, 241.40965713641336, 229.72042597161575, 215.2180155848053, 211.49059764577285, 206.86326810037377, 200.77004630889547, 199.76316882999782, 192.25316586053543, 190.22231691664794, 175.13772371299225, 173.93046846746674, 173.49245504515915, 169.3657735127554, 165.50331241105164, 154.5469031481179, 285.8574227428383, 473.0893992123619, 468.37636362515263, 493.22908560749283, 796.0744320127656, 423.2265777092722, 1153.0109784163503, 908.5006674887904, 578.32018110652, 556.7126707358036, 438.10338884552806, 3258.2205026201086, 838.5992712698768, 1970.6151907998922, 454.90555058236725, 6838.950748941384, 9379.50984299206, 10009.247810659745, 1748.1622725629486, 5049.723323402877, 6639.529959480251, 7280.925708211998, 1844.3575147644306, 1289.790166232674, 1065.85305029645, 881.6199477534022, 660.8054533842756, 640.483776638689, 603.6960695082279, 405.6815382433765, 368.1293589995262, 366.49439870300057, 347.1938194597163, 331.65065822906547, 302.1498693566031, 298.8848538369747, 294.35688426932944, 287.0748854656786, 266.44751036142696, 263.20776455956684, 260.8837291091451, 202.01890731188755, 198.4933965145667, 185.800055430428, 166.14066120441018, 158.1810891977349, 156.661265663886, 150.95871674577737, 145.28459838599218, 141.57154367285494, 139.19266255442068, 137.23153137966827, 2634.9349484857607, 292.3264293060911, 1557.3461697236617, 1480.0121608682261, 555.5234796908446, 650.2618643220457, 1673.0288487037944, 576.8207658528074, 5049.723323402877, 2698.643932170592, 644.6570052981854, 4331.414837045168, 2029.305470686022, 787.6530268047629, 597.0072963560943, 1804.4643544866626, 523.8201611963715, 6641.418917922996, 6211.984315882001, 12135.11073941231, 10009.247810659745, 5150.6864843452095, 4148.9249791629045, 11203.663484910658, 1322.7576762477302, 438.86042270627115, 429.6539396817968, 398.0799277469212, 366.81098726108166, 294.74251219968835, 770.4137759089564, 265.21101316912984, 259.5285899587977, 248.48119544090466, 243.38986595466247, 234.8831561299949, 432.4820565169015, 213.22385616813665, 205.95244279985064, 180.25923092061217, 173.7139520946316, 170.52536265489667, 169.3015061850701, 161.67508331559006, 156.87549264338764, 150.1143005954859, 135.380138528128, 130.5041269479236, 128.10113841256455, 125.37067527368252, 123.50439714067876, 121.67312360479104, 114.1979471536922, 111.77608610064362, 1161.8190686520131, 149.83872924499133, 908.4693062566885, 1855.0276322420048, 325.8994147337178, 514.252270044962, 412.38776752230694, 609.8180563157671, 306.8693271162142, 566.3134945767146, 709.4709375137579, 346.580339541256, 1337.3543233821788, 2769.566040608145, 382.58415971532173, 732.2001529774572, 9606.666383991465, 1230.5404967321617, 1561.0424077698774, 1059.644956556704, 834.0531732423018, 3429.697824135907, 6838.950748941384, 2273.8097574125495, 1487.9992467733753, 2452.0516299180545, 8086.289977131964, 4668.500287749761, 16714.980530009245, 3525.414532618261, 4135.120246225907, 6311.107457331156, 2029.305470686022, 14523.270918665925, 12135.11073941231, 4839.766865162802, 11203.663484910658, 11335.945593179014, 7459.362737674788, 14042.709153909911, 3405.1735585550355, 7549.095939045213, 9379.50984299206, 3710.5490509843175, 2304.2478677900517, 1754.8463879712249, 1694.1650963176521, 1240.0634400223985, 1053.0255205709948, 953.4012636056027, 778.609612875305, 615.5302152827278, 609.674828021712, 567.3593832277782, 525.9670063465161, 509.61227491838736, 493.96938064372006, 487.33057415682606, 445.594014366515, 445.3101271652842, 441.84285122605996, 367.14680214613566, 355.2169705016125, 352.4585667656602, 346.0655623978897, 327.6008107154532, 407.65270590024164, 299.9497084416473, 297.50550884246945, 293.2421174167596, 288.1243558897112, 283.1877481152938, 281.97121567859574, 274.94233287413743, 506.01743682343226, 6639.529959480251, 1002.7013624564086, 1066.779996131463, 1500.7420598731285, 1112.3044579559862, 438.3928535027975, 12111.34945970323, 500.15766140172207, 3837.3308289352794, 1028.5804783005726, 2703.5813862488544, 5158.73852778199, 10009.247810659745, 2831.3507498647396, 4909.042864429817, 7280.925708211998, 5733.709172835075, 1748.1622725629486, 1318.346183108623, 1292.4078866961147, 2367.7077028019585, 4399.010014113649, 4652.453011118853, 11335.945593179014, 8487.342766237967, 2294.7942429164136, 8991.646694004343, 10737.020671498827, 7459.362737674788, 4767.549713464186, 7413.019582428107, 9379.50984299206, 12135.11073941231, 5647.881543006377, 16714.980530009245, 11203.663484910658, 14523.270918665925, 14042.709153909911, 9606.666383991465, 1620.3389045326765, 1109.618315369424, 1092.1450938691223, 1018.7429089871033, 316.968652147284, 315.76248060972847, 268.50910153278005, 265.03928373980165, 245.21481451600795, 215.3766030811199, 213.43558003992206, 205.22368760280904, 198.86033605306616, 196.3468651609383, 184.33328802671966, 180.2058938248688, 162.67043642668236, 159.00849759358073, 157.50176475933537, 157.03691645109765, 156.0493942133116, 149.0511778443813, 143.95263393925006, 143.65901345914475, 143.5177299458385, 142.86271798847088, 140.1347409338323, 139.04652327082005, 138.55776734704645, 135.01373649977356, 775.9010264493298, 1320.0643221873293, 1444.6064785623307, 4606.482744163697, 1175.7017337336297, 1619.470761004639, 893.4671905285627, 1991.5684177650128, 437.16439871126903, 2790.1615856808608, 1438.9983725935517, 1579.1965287286266, 1614.1961407730473, 5681.621334363959, 1187.4319458721752, 410.41365374824727, 613.0366994331529, 2801.4645444987955, 490.7399989227831, 2452.0516299180545, 842.5133336795386, 2233.341317745946, 12111.34945970323, 6210.319550450303, 8086.289977131964, 1328.784630548791, 1061.7230205342205, 1673.0288487037944, 6820.470342413688, 2769.566040608145, 4148.9249791629045, 7459.362737674788, 11335.945593179014, 3239.4741408671593, 7549.095939045213], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.095799922943115, -5.178800106048584, -6.208799839019775, -6.444499969482422, -6.750899791717529, -6.845200061798096, -7.014800071716309, -7.039400100708008, -7.052800178527832, -7.055799961090088, -7.091800212860107, -7.113800048828125, -7.154200077056885, -7.177299976348877, -7.236299991607666, -7.262199878692627, -7.331399917602539, -7.390900135040283, -7.435699939727783, -7.449900150299072, -7.451600074768066, -7.455100059509277, -7.4953999519348145, -7.508500099182129, -7.524400234222412, -7.525899887084961, -7.533400058746338, -7.575399875640869, -7.406300067901611, -7.59660005569458, -6.616600036621094, -7.4899001121521, -5.21120023727417, -7.049799919128418, -6.539899826049805, -6.424300193786621, -6.796899795532227, -5.757500171661377, -6.722400188446045, -5.888599872589111, -5.108799934387207, -6.065700054168701, -6.368299961090088, -5.128799915313721, -4.78410005569458, -6.045199871063232, -6.383600234985352, -4.591400146484375, -5.6819000244140625, -5.098599910736084, -5.183000087738037, -5.447700023651123, -4.1875, -6.619999885559082, -5.662199974060059, -6.216700077056885, -5.696300029754639, -4.787499904632568, -5.501500129699707, -4.202300071716309, -5.347300052642822, -5.11269998550415, -5.1803998947143555, -4.932799816131592, -4.946400165557861, -4.602200031280518, -4.445199966430664, -4.992800235748291, -4.893499851226807, -5.070799827575684, -5.140600204467773, -4.710700035095215, -5.0792999267578125, -5.157700061798096, -4.631400108337402, -4.948299884796143, -5.201399803161621, -5.199399948120117, -5.203800201416016, -2.390500068664551, -4.121099948883057, -4.895599842071533, -4.974400043487549, -5.14109992980957, -5.265399932861328, -5.388400077819824, -4.673900127410889, -5.474699974060059, -5.798500061035156, -5.824100017547607, -5.9618000984191895, -5.97160005569458, -6.056099891662598, -6.066199779510498, -6.1743998527526855, -6.191299915313721, -6.240600109100342, -6.262400150299072, -6.284299850463867, -6.392099857330322, -6.418300151824951, -6.442399978637695, -6.480999946594238, -6.505899906158447, -6.523099899291992, -6.589000225067139, -6.603099822998047, -6.632400035858154, -6.648399829864502, -6.208000183105469, -3.6767001152038574, -4.783699989318848, -4.696199893951416, -4.563499927520752, -4.626699924468994, -5.750400066375732, -5.29010009765625, -5.4278998374938965, -4.2600998878479, -5.424600124359131, -4.815100193023682, -4.890999794006348, -5.122300148010254, -5.942399978637695, -5.836100101470947, -5.380000114440918, -4.8815999031066895, -5.206900119781494, -5.274799823760986, -5.294899940490723, -4.787600040435791, -4.813600063323975, -5.059999942779541, -5.15310001373291, -5.283199787139893, -5.306399822235107, -5.26170015335083, -5.3043999671936035, -4.232699871063232, -4.553199768066406, -4.895999908447266, -5.096499919891357, -4.984799861907959, -5.358399868011475, -5.369800090789795, -5.497099876403809, -5.6880998611450195, -5.774799823760986, -5.871799945831299, -5.9105000495910645, -5.992400169372559, -6.060500144958496, -6.081200122833252, -6.091899871826172, -6.113100051879883, -6.157299995422363, -6.174600124359131, -6.180200099945068, -6.237599849700928, -6.259500026702881, -6.276400089263916, -6.399499893188477, -6.436800003051758, -6.4593000411987305, -6.4770002365112305, -6.491199970245361, -6.578499794006348, -6.5945000648498535, -2.9579999446868896, -3.256700038909912, -6.131100177764893, -4.725100040435791, -5.172999858856201, -4.138700008392334, -4.855199813842773, -3.8090999126434326, -4.385499954223633, -4.89870023727417, -4.441999912261963, -4.388199806213379, -5.613500118255615, -5.358500003814697, -4.723999977111816, -4.954100131988525, -5.090099811553955, -5.253200054168701, -4.61959981918335, -5.347899913787842, -4.583000183105469, -4.65910005569458, -5.150400161743164, -5.046500205993652, -5.098400115966797, -5.142600059509277, -5.027599811553955, -5.091800212860107, -5.253200054168701, -5.401199817657471, -5.422800064086914, -5.7835001945495605, -6.298600196838379, -6.425899982452393, -6.438700199127197, -6.579699993133545, -6.59250020980835, -6.615600109100342, -6.638400077819824, -6.662499904632568, -6.672999858856201, -6.679100036621094, -6.693399906158447, -6.700799942016602, -6.709099769592285, -6.7993998527526855, -6.823500156402588, -6.880000114440918, -6.944900035858154, -6.9496002197265625, -6.969900131225586, -7.0578999519348145, -6.950200080871582, -7.1367998123168945, -6.559100151062012, -7.329999923706055, -7.341599941253662, -7.352399826049805, -7.404399871826172, -5.67140007019043, -6.847799777984619, -6.454599857330322, -7.1921000480651855, -5.662799835205078, -6.170899868011475, -5.826200008392334, -5.584799766540527, -6.046000003814697, -6.167699813842773, -6.24560022354126, -6.050099849700928, -6.513800144195557, -5.920199871063232, -4.3078999519348145, -5.447700023651123, -5.498300075531006, -5.612100124359131, -4.794600009918213, -3.9583001136779785, -5.781499862670898, -6.351200103759766, -5.8171000480651855, -5.205999851226807, -5.133200168609619, -6.1194000244140625, -4.991799831390381, -5.876999855041504, -5.005099773406982, -4.715000152587891, -4.499899864196777, -4.596199989318848, -5.749899864196777, -4.981200218200684, -4.757800102233887, -5.339399814605713, -5.077300071716309, -5.061399936676025, -5.287199974060059, -5.339300155639648, -5.498700141906738, -5.440700054168701, -5.383600234985352, -5.431000232696533, -5.565899848937988, -5.334099769592285, -5.47599983215332, -5.392300128936768, -5.482900142669678, -5.500800132751465, -5.493899822235107, -4.023799896240234, -4.949900150299072, -5.019999980926514, -5.355000019073486, -5.417500019073486, -5.417699813842773, -5.4415998458862305, -5.7322998046875, -5.8805999755859375, -6.081500053405762, -6.087100028991699, -6.173799991607666, -6.309999942779541, -6.3206000328063965, -6.321700096130371, -6.335899829864502, -6.353300094604492, -6.425300121307373, -6.477200031280518, -6.51140022277832, -6.517099857330322, -6.52269983291626, -6.5954999923706055, -6.599599838256836, -6.616000175476074, -6.620699882507324, -6.640399932861328, -6.651199817657471, -6.664100170135498, -6.672299861907959, -5.539700031280518, -4.28249979019165, -5.856599807739258, -4.053199768066406, -4.774099826812744, -4.659900188446045, -6.013899803161621, -5.597400188446045, -5.841800212860107, -6.353000164031982, -5.084400177001953, -6.056600093841553, -4.748899936676025, -4.981599807739258, -4.13100004196167, -4.906300067901611, -5.462299823760986, -5.544899940490723, -5.302599906921387, -5.337399959564209, -4.248700141906738, -4.8907999992370605, -4.385499954223633, -5.2204999923706055, -5.110199928283691, -5.2677001953125, -4.8256001472473145, -4.854000091552734, -5.041500091552734, -5.1996002197265625, -5.0756001472473145, -5.2133002281188965, -5.260700225830078, -5.298900127410889, -3.4149999618530273, -3.9456000328063965, -4.035299777984619, -4.112400054931641, -4.134900093078613, -4.739500045776367, -4.789700031280518, -4.995800018310547, -5.098400115966797, -5.106299877166748, -5.190700054168701, -5.232900142669678, -5.274600028991699, -5.315000057220459, -5.432000160217285, -5.455999851226807, -5.505799770355225, -5.571300029754639, -5.588799953460693, -5.611000061035156, -5.640999794006348, -5.646100044250488, -5.684599876403809, -5.695199966430664, -5.778299808502197, -5.785200119018555, -5.787700176239014, -5.8119001388549805, -5.835100173950195, -5.9039998054504395, -5.303999900817871, -4.845399856567383, -4.86329984664917, -4.82420015335083, -4.3867998123168945, -5.0106000900268555, -4.286499977111816, -4.620200157165527, -4.9278998374938965, -4.98859977722168, -5.1819000244140625, -4.231299877166748, -4.916100025177002, -4.564700126647949, -5.218900203704834, -4.3221001625061035, -4.449999809265137, -4.543300151824951, -4.926700115203857, -4.987299919128418, -5.089300155639648, -5.098599910736084, -3.6342999935150146, -3.9921998977661133, -4.183000087738037, -4.373000144958496, -4.661600112915039, -4.69290018081665, -4.752099990844727, -5.150300025939941, -5.247700214385986, -5.252200126647949, -5.306399822235107, -5.35230016708374, -5.445700168609619, -5.456600189208984, -5.47189998626709, -5.497099876403809, -5.571899890899658, -5.584099769592285, -5.5929999351501465, -5.849800109863281, -5.867400169372559, -5.933800220489502, -6.046199798583984, -6.095600128173828, -6.105299949645996, -6.142600059509277, -6.181099891662598, -6.207200050354004, -6.224299907684326, -6.238500118255615, -3.599900007247925, -5.561299800872803, -4.198599815368652, -4.26639986038208, -5.071000099182129, -5.05620002746582, -4.4319000244140625, -5.177499771118164, -3.7657999992370605, -4.251999855041504, -5.17080020904541, -4.1082000732421875, -4.600800037384033, -5.130499839782715, -5.26200008392334, -4.849800109863281, -5.349800109863281, -4.832900047302246, -4.9745001792907715, -4.97629976272583, -5.020500183105469, -5.111100196838379, -5.14139986038208, -5.091300010681152, -4.803800106048584, -5.9085001945495605, -5.929699897766113, -6.006199836730957, -6.088200092315674, -6.307499885559082, -5.346799850463867, -6.41349983215332, -6.435200214385986, -6.478799819946289, -6.499599933624268, -6.535299777984619, -5.924900054931641, -6.632500171661377, -6.667300224304199, -6.801199913024902, -6.838399887084961, -6.85699987411499, -6.864200115203857, -6.9105000495910645, -6.940800189971924, -6.985199928283691, -7.089099884033203, -7.125999927520752, -7.144800186157227, -7.166399955749512, -7.181600093841553, -7.196599960327148, -7.260499954223633, -7.282100200653076, -4.955900192260742, -6.989299774169922, -5.228700160980225, -4.564799785614014, -6.242199897766113, -5.806000232696533, -6.022200107574463, -5.67549991607666, -6.355400085449219, -5.8171000480651855, -5.637800216674805, -6.258900165557861, -5.118800163269043, -4.522799968719482, -6.194900035858154, -5.694699764251709, -3.7446999549865723, -5.341000080108643, -5.171000003814697, -5.466000080108643, -5.659900188446045, -4.7129998207092285, -4.274899959564209, -5.082799911499023, -5.450799942016602, -5.237800121307373, -4.642000198364258, -4.987299919128418, -4.390500068664551, -5.213699817657471, -5.161499977111816, -5.034299850463867, -5.441100120544434, -4.838600158691406, -4.917200088500977, -5.193900108337402, -5.035699844360352, -5.07289981842041, -5.196499824523926, -5.103899955749512, -5.356299877166748, -5.234799861907959, -5.2118000984191895, -5.3531999588012695, -4.985400199890137, -5.257900238037109, -5.293099880218506, -5.605299949645996, -5.769000053405762, -5.8684000968933105, -6.071199893951416, -6.30649995803833, -6.316100120544434, -6.3881001472473145, -6.464000225067139, -6.49560022354126, -6.526899814605713, -6.54040002822876, -6.630099773406982, -6.630799770355225, -6.638599872589111, -6.82420015335083, -6.8572998046875, -6.865099906921387, -6.883500099182129, -6.938399791717529, -6.719799995422363, -7.026899814605713, -7.035099983215332, -7.049600124359131, -7.067200183868408, -7.08459997177124, -7.088900089263916, -7.114200115203857, -6.513700008392334, -4.057600021362305, -5.9028000831604, -5.851200103759766, -5.54449987411499, -5.826200008392334, -6.681700229644775, -3.7163000106811523, -6.563399791717529, -4.9096999168396, -5.993000030517578, -5.2164998054504395, -4.74970006942749, -4.2434000968933105, -5.224699974060059, -4.804500102996826, -4.5680999755859375, -4.769599914550781, -5.645199775695801, -5.846199989318848, -5.8790998458862305, -5.489799976348877, -5.159299850463867, -5.158999919891357, -4.662300109863281, -4.8267998695373535, -5.592800140380859, -4.952099800109863, -4.872300148010254, -5.086999893188477, -5.316400051116943, -5.163700103759766, -5.117300033569336, -5.035799980163574, -5.307700157165527, -5.000999927520752, -5.2540998458862305, -5.322800159454346, -5.364299774169922, -5.3993000984191895, -4.336599826812744, -4.7154998779296875, -4.731400012969971, -4.801000118255615, -5.9704999923706055, -5.974299907684326, -6.136899948120117, -6.150000095367432, -6.228000164031982, -6.3582000732421875, -6.367300033569336, -6.406700134277344, -6.438399791717529, -6.451099872589111, -6.514599800109863, -6.537300109863281, -6.640200138092041, -6.663099765777588, -6.672699928283691, -6.6757001876831055, -6.682000160217285, -6.7281999588012695, -6.763199806213379, -6.765200138092041, -6.766200065612793, -6.7708001136779785, -6.790200233459473, -6.798099994659424, -6.801599979400635, -6.827700138092041, -5.09499979019165, -4.582300186157227, -4.508900165557861, -3.4804000854492188, -4.779200077056885, -4.5167999267578125, -5.090700149536133, -4.388999938964844, -5.758200168609619, -4.132900238037109, -4.723100185394287, -4.6585001945495605, -4.686500072479248, -3.661799907684326, -4.977099895477295, -5.886300086975098, -5.585599899291992, -4.491300106048584, -5.77839994430542, -4.660600185394287, -5.413599967956543, -4.784299850463867, -3.7095000743865967, -4.317599773406982, -4.213799953460693, -5.234499931335449, -5.403600215911865, -5.257800102233887, -4.738800048828125, -5.085299968719482, -4.959799766540527, -4.860499858856201, -4.770999908447266, -5.117099761962891, -5.08459997177124], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5956, 1.5956, 1.5949, 1.5946, 1.5941, 1.5939, 1.5935, 1.5935, 1.5934, 1.5934, 1.5933, 1.5933, 1.5932, 1.5931, 1.5929, 1.5928, 1.5926, 1.5924, 1.5922, 1.5922, 1.5922, 1.5922, 1.592, 1.592, 1.5919, 1.5919, 1.5919, 1.5917, 1.5916, 1.5916, 1.5885, 1.5915, 1.5659, 1.5837, 1.5552, 1.5476, 1.5549, 1.4881, 1.5464, 1.4675, 1.3755, 1.4659, 1.4953, 1.3455, 1.2643, 1.4329, 1.4705, 1.1745, 1.3483, 1.2358, 1.2422, 1.278, 0.9678, 1.4963, 1.2583, 1.394, 1.254, 0.9694, 1.1689, 0.739, 1.0984, 1.0156, 1.0025, 0.7767, 0.7436, 0.5649, 0.4624, 0.7643, 0.6684, 0.7753, 0.8035, 0.3765, 0.7007, 0.7454, 0.1357, 0.4965, 0.5559, 0.5416, 0.141, 2.6963, 2.696, 2.6955, 2.6954, 2.6953, 2.6951, 2.6949, 2.6949, 2.6948, 2.6942, 2.6941, 2.6938, 2.6938, 2.6936, 2.6935, 2.6932, 2.6931, 2.693, 2.6929, 2.6928, 2.6924, 2.6923, 2.6922, 2.692, 2.6919, 2.6918, 2.6915, 2.6915, 2.6913, 2.6912, 2.6892, 2.6249, 2.6391, 2.5543, 2.5191, 2.4992, 2.6057, 2.5183, 2.4469, 2.1401, 2.3753, 2.0188, 1.6995, 1.6726, 2.4392, 2.3273, 1.8191, 1.1643, 1.4076, 1.4765, 1.4265, 0.2689, -0.0466, 0.6034, 0.8108, 0.7091, 0.811, 0.1253, -0.0948, 2.4636, 2.4634, 2.4632, 2.4631, 2.463, 2.4628, 2.4628, 2.4627, 2.4624, 2.4622, 2.4621, 2.462, 2.4618, 2.4617, 2.4616, 2.4616, 2.4616, 2.4614, 2.4614, 2.4614, 2.4612, 2.4612, 2.4611, 2.4608, 2.4606, 2.4606, 2.4605, 2.4605, 2.4601, 2.4601, 2.451, 2.4483, 2.4503, 2.3957, 2.4086, 2.341, 2.3648, 2.2318, 2.2743, 2.1834, 2.0507, 2.0236, 2.2824, 2.1651, 1.883, 1.9786, 1.9619, 1.9692, 1.4979, 2.0009, 1.0777, 1.0043, 1.6319, 1.2768, 1.1042, 0.8638, -0.2606, -0.1843, 1.3356, 1.6466, 1.6465, 1.6464, 1.6459, 1.6457, 1.6457, 1.6455, 1.6454, 1.6454, 1.6454, 1.6453, 1.6453, 1.6453, 1.6453, 1.6452, 1.6452, 1.645, 1.645, 1.6449, 1.6447, 1.6447, 1.6447, 1.6444, 1.6444, 1.6442, 1.6438, 1.6436, 1.6435, 1.6435, 1.6433, 1.6431, 1.6419, 1.6367, 1.6427, 1.6256, 1.6318, 1.6184, 1.591, 1.5891, 1.5959, 1.5882, 1.5588, 1.5972, 1.516, 1.2721, 1.4332, 1.4314, 1.4399, 1.2795, 1.0982, 1.3994, 1.5582, 1.4008, 1.1706, 1.1048, 1.4228, 0.8409, 1.2867, 0.7768, 0.4946, 0.2671, 0.3114, 1.1373, 0.4059, 0.1834, 0.6821, 0.2675, 0.2184, 0.3734, 0.4571, 0.6895, 0.5627, 0.3575, 0.4059, 0.785, -0.167, 0.3761, -0.3051, 0.1152, 0.2792, 0.1694, 2.6382, 2.6377, 2.6377, 2.6373, 2.6372, 2.6372, 2.6372, 2.6367, 2.6364, 2.6359, 2.6359, 2.6356, 2.6352, 2.6352, 2.6351, 2.6351, 2.635, 2.6348, 2.6346, 2.6344, 2.6344, 2.6344, 2.6341, 2.6341, 2.634, 2.634, 2.6339, 2.6338, 2.6338, 2.6337, 2.6166, 2.5795, 2.5801, 2.3489, 2.3746, 2.3134, 2.5278, 2.3924, 2.4079, 2.5641, 1.9952, 2.4256, 1.7901, 1.7654, 1.1899, 1.6487, 2.0155, 2.0378, 1.8018, 1.824, 0.8078, 1.3845, 0.3815, 1.4485, 1.2407, 1.3973, 0.4542, 0.2332, 0.1256, 0.6374, -0.1344, -0.3057, -0.0511, 0.4106, 3.5517, 3.5514, 3.5513, 3.5512, 3.5512, 3.5504, 3.5503, 3.5499, 3.5497, 3.5497, 3.5494, 3.5493, 3.5492, 3.5491, 3.5487, 3.5486, 3.5484, 3.5482, 3.5481, 3.548, 3.5479, 3.5478, 3.5477, 3.5476, 3.5472, 3.5472, 3.5472, 3.5471, 3.547, 3.5466, 3.5316, 3.4864, 3.4785, 3.4659, 3.4246, 3.4325, 3.1545, 3.059, 3.203, 3.1804, 3.2267, 2.1709, 2.8432, 2.3403, 3.1521, 1.3386, 0.8948, 0.7365, 2.0981, 0.9767, 0.601, 0.4995, 3.3369, 3.3367, 3.3365, 3.3363, 3.336, 3.336, 3.3359, 3.3352, 3.3349, 3.3349, 3.3348, 3.3347, 3.3344, 3.3344, 3.3343, 3.3343, 3.334, 3.334, 3.3339, 3.333, 3.3329, 3.3326, 3.332, 3.3317, 3.3317, 3.3315, 3.3312, 3.3311, 3.331, 3.3309, 3.0145, 3.2519, 2.9418, 2.9248, 3.1001, 2.9575, 2.6368, 2.956, 2.1982, 2.3386, 2.8516, 2.0093, 2.2748, 2.6915, 2.8372, 2.1432, 2.8802, 0.8571, 0.7823, 0.111, 0.2593, 0.8331, 1.0191, 0.0758, 2.4998, 2.4984, 2.4984, 2.4982, 2.498, 2.4974, 2.4974, 2.4971, 2.497, 2.4969, 2.4968, 2.4967, 2.4966, 2.4963, 2.4961, 2.4955, 2.4953, 2.4952, 2.4952, 2.4949, 2.4948, 2.4945, 2.4939, 2.4936, 2.4935, 2.4933, 2.4932, 2.4931, 2.4926, 2.4925, 2.4775, 2.4922, 2.4507, 2.4006, 2.4623, 2.4423, 2.4469, 2.4024, 2.4092, 2.3348, 2.2888, 2.3841, 2.1738, 2.0418, 2.3492, 2.2003, 1.5761, 2.0349, 1.967, 2.0594, 2.1048, 1.6379, 1.3857, 1.6791, 1.735, 1.4486, 0.8511, 1.0551, 0.3765, 1.1096, 1.0023, 0.7067, 1.4345, 0.069, 0.17, 0.8125, 0.1314, 0.0825, 0.3773, -0.1627, 1.0017, 0.3271, 0.1329, 0.9189, 1.7631, 1.763, 1.763, 1.7628, 1.7627, 1.7626, 1.7624, 1.7621, 1.7621, 1.762, 1.7618, 1.7618, 1.7617, 1.7617, 1.7615, 1.7615, 1.7615, 1.7611, 1.761, 1.761, 1.761, 1.7608, 1.7608, 1.7606, 1.7605, 1.7605, 1.7605, 1.7604, 1.7604, 1.7603, 1.7508, 1.6326, 1.6778, 1.6675, 1.6329, 1.6507, 1.7263, 1.3729, 1.7128, 1.3289, 1.5621, 1.3722, 1.1929, 1.0364, 1.3179, 1.1877, 1.03, 1.0674, 1.3796, 1.4607, 1.4477, 1.2316, 0.9426, 0.8869, 0.493, 0.618, 1.1599, 0.4349, 0.3373, 0.4869, 0.7051, 0.4164, 0.2275, 0.0514, 0.5443, -0.234, -0.087, -0.4152, -0.4231, -0.0784, 2.764, 2.7638, 2.7638, 2.7637, 2.7618, 2.7618, 2.7613, 2.7612, 2.761, 2.7605, 2.7604, 2.7602, 2.7601, 2.7601, 2.7598, 2.7596, 2.7591, 2.759, 2.7589, 2.7589, 2.7589, 2.7586, 2.7584, 2.7584, 2.7584, 2.7583, 2.7582, 2.7582, 2.7582, 2.758, 2.7421, 2.7234, 2.7066, 2.5755, 2.6422, 2.5844, 2.6052, 2.5054, 2.6525, 2.4243, 2.4963, 2.4679, 2.4179, 2.1843, 2.4344, 2.5876, 2.487, 2.0619, 2.5168, 2.0258, 2.3411, 1.9956, 1.3797, 1.4395, 1.2793, 2.0645, 2.1198, 1.8109, 0.9246, 1.4793, 1.2007, 0.7133, 0.3843, 1.2908, 0.4772]}, \"token.table\": {\"Topic\": [9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 2, 3, 4, 7, 3, 3, 2, 2, 8, 1, 1, 3, 5, 1, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 6, 9, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 3, 4, 5, 9, 10, 2, 3, 4, 5, 6, 8, 9, 8, 9, 10, 10, 1, 2, 4, 9, 10, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 10, 7, 8, 7, 5, 2, 4, 7, 9, 10, 2, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 7, 9, 1, 2, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 3, 10, 10, 1, 3, 4, 5, 9, 10, 1, 5, 3, 10, 7, 2, 7, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 3, 1, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 4, 2, 3, 1, 6, 5, 7, 9, 1, 9, 1, 3, 5, 6, 8, 9, 1, 3, 5, 7, 1, 4, 9, 3, 5, 3, 1, 6, 6, 1, 1, 9, 1, 2, 3, 4, 7, 8, 10, 2, 4, 8, 8, 9, 1, 2, 4, 5, 1, 3, 9, 10, 1, 3, 4, 5, 7, 9, 10, 6, 9, 9, 1, 1, 4, 8, 1, 3, 10, 2, 1, 2, 3, 5, 8, 9, 1, 4, 5, 6, 8, 9, 5, 2, 4, 5, 7, 9, 9, 3, 7, 6, 3, 1, 10, 10, 1, 10, 10, 3, 8, 5, 10, 2, 4, 8, 9, 10, 4, 4, 5, 1, 4, 1, 3, 4, 5, 6, 8, 9, 3, 6, 3, 2, 3, 2, 3, 4, 8, 9, 10, 1, 9, 2, 9, 4, 5, 2, 4, 6, 9, 3, 9, 7, 6, 1, 2, 3, 6, 9, 10, 3, 2, 3, 4, 7, 8, 10, 2, 3, 8, 9, 10, 8, 3, 8, 3, 4, 6, 6, 2, 2, 4, 5, 7, 8, 9, 9, 1, 10, 1, 2, 4, 5, 7, 8, 9, 10, 10, 4, 8, 3, 2, 9, 4, 6, 9, 3, 8, 10, 2, 4, 8, 10, 10, 1, 5, 9, 7, 3, 3, 4, 9, 10, 10, 3, 9, 1, 4, 5, 7, 10, 1, 10, 1, 1, 7, 1, 7, 8, 10, 9, 4, 5, 6, 9, 1, 4, 6, 9, 1, 2, 4, 8, 9, 10, 3, 3, 1, 3, 10, 4, 4, 5, 6, 9, 3, 7, 4, 9, 9, 7, 1, 3, 5, 9, 10, 10, 5, 5, 5, 1, 2, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 8, 9, 10, 5, 2, 3, 4, 7, 9, 7, 7, 9, 10, 7, 6, 9, 1, 3, 4, 5, 6, 8, 9, 10, 5, 1, 10, 1, 2, 4, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 9, 5, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 5, 10, 2, 2, 8, 5, 4, 6, 1, 2, 3, 7, 8, 10, 9, 8, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 6, 4, 6, 4, 2, 3, 6, 3, 3, 5, 8, 9, 1, 1, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 5, 4, 1, 2, 3, 4, 7, 9, 5, 5, 6, 2, 8, 2, 3, 4, 9, 3, 3, 4, 8, 9, 6, 4, 4, 2, 3, 5, 6, 7, 9, 2, 4, 8, 10, 5, 5, 4, 5, 8, 10, 10, 9, 7, 2, 4, 2, 4, 8, 3, 8, 9, 10, 1, 4, 5, 9, 10, 1, 3, 9, 10, 1, 2, 3, 9, 10, 1, 3, 4, 6, 8, 9, 1, 4, 9, 2, 4, 5, 7, 8, 3, 7, 4, 5, 6, 9, 1, 2, 4, 6, 8, 9, 10, 3, 8, 10, 1, 2, 3, 9, 10, 4, 7, 2, 7, 2, 2, 5, 1, 2, 4, 5, 6, 7, 8, 9, 7, 4, 2, 5, 10, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 8, 2, 2, 2, 3, 4, 6, 7, 8, 9, 2, 3, 8, 4, 7, 7, 1, 4, 5, 2, 2, 3, 4, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 10, 3, 9, 3, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7, 9, 1, 1, 1, 2, 3, 4, 6, 7, 8, 9, 10, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 1, 1, 4, 5, 8, 9, 8, 2, 3, 7, 8, 9, 10, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 9, 10, 4, 1, 2, 4, 5, 6, 7, 8, 9, 10, 10, 7, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 2, 7, 10, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 3, 7, 7, 1, 2, 10, 7, 7, 2, 4, 2, 3, 4, 9, 5, 9, 4, 7, 2, 4, 7, 8, 1, 10, 1, 10, 1, 2, 3, 4, 5, 8, 8, 3, 1, 4, 8, 9, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 8, 2, 4, 7, 6, 1, 2, 3, 8, 10, 1, 4, 7, 8, 9, 10, 4, 5, 8, 9, 4, 4, 5, 4, 5, 9, 10, 5, 4, 1, 5, 10, 1, 2, 5, 8, 9, 2, 3, 4, 5, 6, 7, 9, 5, 2, 7, 1, 5, 10, 8, 8, 3, 1, 6, 2, 6, 1, 2, 3, 8, 9, 10, 1, 6, 10, 2, 3, 6, 8, 9, 1, 5, 5, 3, 7, 6, 9, 9, 5, 2, 3, 7, 9, 8, 8, 8, 1, 5, 6, 8, 5, 1, 3, 8, 1, 4, 5, 6, 9, 4, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 1, 4, 8, 5, 6, 1, 7, 8, 2, 2, 5, 6, 8, 10, 1, 7, 2, 3, 7, 9, 10, 1, 4, 4, 1, 4, 5, 6, 8, 9, 6, 7, 7, 3, 7, 7, 9, 4, 5, 6, 5, 5, 9, 9, 2, 5, 6, 1, 3, 6, 8, 9, 10, 4, 7, 2, 4, 7, 1, 4, 5, 6, 8, 9, 10, 2, 4, 7, 1, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 9, 10, 3, 3, 9, 6, 8, 9, 2, 3, 4, 9, 6, 4, 1, 2, 4, 8, 9, 10, 5, 3, 4, 8, 2, 9, 4, 1, 2, 4, 7, 8, 9, 1, 3, 4, 5, 7, 8, 9, 10, 10, 2, 3, 4, 5, 6, 3, 7, 6, 4, 5, 2, 3, 6, 6, 6, 8, 4, 10, 1, 3, 4, 5, 7, 8, 9, 6, 8, 8, 9, 8, 9, 2, 3, 4, 5, 7, 8, 9, 1, 2, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 8, 3, 10, 3, 5, 6, 10, 1, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 2, 1, 3, 4, 5, 6, 7, 9, 10, 4, 1, 1, 2, 3, 4, 5, 6, 8, 9, 10, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 5, 8, 9, 1, 2, 3, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 10, 1, 2, 3, 4, 7, 8, 9, 10, 8, 1, 4, 5, 9, 1, 1, 2, 7, 8, 9, 2, 5, 6, 2, 7, 3, 9, 1, 5, 9, 10, 10, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 9, 1, 3, 10, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 8, 1, 3, 9, 7, 1, 10, 4, 7, 9, 4, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 4, 8, 2, 2, 2, 2, 6, 8, 8, 4, 1, 4], \"Freq\": [0.9969209233345657, 0.2321869291461181, 0.06437339236310824, 0.06556992381967158, 0.25157073874244434, 0.10463667587646497, 0.0074783216035209385, 0.004726299253425233, 0.11959331908350684, 0.1356866671742839, 0.014298550905932034, 0.01950935773060935, 0.9790150424814873, 0.24383967403178325, 0.15627906381127926, 0.2964868763795546, 0.3031370493076942, 0.9990294849742971, 0.9976316668500808, 0.9988771396358082, 0.9964522375758352, 0.9994271995080161, 0.998537854448938, 0.8822856244810697, 0.0026219483639853486, 0.11405475383336267, 0.07208115023884312, 0.0009082390064459042, 0.6766380598021986, 0.25034369704945286, 0.9974376972425725, 0.5596900849093458, 0.0413115700704563, 0.052981505118607795, 0.10899719334973498, 0.0028007844115563595, 0.020305686983783607, 0.05928327004460961, 0.04551274668779084, 0.006068366225038779, 0.10316222582565923, 0.8366253344218936, 0.06888382244491639, 0.09393248515215871, 0.9958008140429737, 0.35333511598272915, 0.05153474800112732, 0.09034685508947633, 0.15798621184095593, 0.04944114886358152, 0.009018580900197281, 0.03092084880067639, 0.0718265550265712, 0.11176598472744487, 0.07359806198910995, 0.0004546477488305917, 0.06546927583160521, 0.023187035190360177, 0.32052666292556714, 0.08547377678015124, 0.014776051836994231, 0.00022732387441529585, 0.04705604200396624, 0.4400990208680128, 0.002955210367398846, 0.9927669709524934, 0.9982442920465374, 0.9991424309530653, 0.6080953743052832, 0.007693352126160673, 0.020195049331171763, 0.06924016913544605, 0.010257802834880897, 0.07372795787570645, 0.025644507087202242, 0.0019233380315401682, 0.18367878201208604, 0.07715871856092874, 0.0019897663812273887, 0.6924387006671312, 0.037584476089850674, 0.17156207909249482, 0.019013323198395046, 0.014650202420096863, 0.004011364948359855, 0.2889926834535774, 0.13516555804256034, 0.04342738574528713, 0.014999016763432502, 0.498455696626629, 0.07965565239348615, 0.08521302349070611, 0.8354581216154012, 0.9975852716628772, 0.9958432523761078, 0.0208489220726953, 0.9784101286972009, 0.9497005377639999, 0.04798486927649683, 0.9971615189546371, 0.9895099064076874, 0.1630386093881138, 0.12330527922248534, 0.23238866535985644, 0.22813675917977075, 0.004398523634571415, 0.0024924967262571355, 0.017740711992771375, 0.031815987623399905, 0.03782730325731417, 0.1587867032080281, 0.2180380654417294, 0.7804540099456295, 0.9992968083866727, 0.9953754236322594, 0.9980381217014921, 0.9947534270159667, 0.9969185444933361, 0.012409699940047357, 0.6158313595248501, 0.24664278630844122, 0.1256482118929795, 0.9954503575839717, 0.03786498318049124, 0.01588530956648577, 0.23957844239265963, 0.11259587346810984, 0.05984465679449671, 0.0460574069820751, 0.004795565152146648, 0.48325309668611116, 0.003489225108631186, 0.01725227970378753, 0.04206454714294263, 0.14267053777514183, 0.11572596610293434, 0.06009221020420376, 0.03915685955241664, 0.014344592113261542, 0.5652544675982522, 0.8915681726464967, 0.10675735400621482, 0.42441952864490945, 0.0222891463868887, 0.04934945190451716, 0.2313656121757233, 0.0624523366814741, 0.003916623167025172, 0.015168013355933846, 0.06971589237304805, 0.1123002678981581, 0.00897262761900312, 0.9970579443732303, 0.19718107900385154, 0.007293355630261352, 0.0015628619207702897, 0.5813846345265478, 0.03542487020412657, 0.0015628619207702897, 0.1753010121130675, 0.0560729209507832, 0.0877663110533998, 0.13490007069318857, 0.02600483290471105, 0.6281792448544263, 0.06663738431832207, 0.18399913185269226, 0.0927492713529869, 0.05483559692001073, 0.11009959694095904, 0.08953624809595502, 0.05355038761719798, 0.025061581404848654, 0.2356217055156711, 0.10110313182126979, 0.053336186066729185, 0.9952039224004381, 0.10782366857628986, 0.7553714309810868, 0.13629396308800684, 0.9919025009339542, 0.018983889771026413, 0.629632344072376, 0.007909954071261006, 0.0023729862213783016, 0.33775503884284497, 0.0031639816285044023, 0.9925339518979488, 0.005586495038825227, 0.17214869653092957, 0.8277465067748228, 0.997039482511023, 0.9933584734735936, 0.0037204437208748827, 0.9984926793345856, 0.9985888346686804, 0.3566690489572363, 0.048555546204388525, 0.04596710716040458, 0.1629824032522306, 0.08104491903232497, 0.009014908394564782, 0.03829104654721081, 0.09354083165845437, 0.15718072953295623, 0.0067834954256131026, 0.440824964955944, 0.020728446628022444, 0.04698447902351754, 0.12114625473710895, 0.10778792246571671, 0.00721657030753374, 0.014126052516874554, 0.05481522552743713, 0.1779575529028001, 0.008598466749401902, 0.115425476280923, 0.8842652866808641, 0.997237323493731, 0.35456253323935194, 0.20186922784081984, 0.06570564427162816, 0.023761475129676846, 0.0002066215228667552, 0.004545673503068615, 0.1849262629657459, 0.039877953913283755, 0.12438615676578663, 0.9984724553770921, 0.9951146907469127, 0.08452434419808347, 0.9145256913235261, 0.012845854245031405, 0.9870845345982981, 0.08109821818543692, 0.9164098654954372, 0.9436933171963895, 0.053992403720191436, 0.9980375693682535, 0.9889225380418908, 0.008574472873195585, 0.802029000834204, 0.05664517247271071, 0.0986292414818963, 0.0009996206906948948, 0.0383187931433043, 0.0026656551751863865, 0.8974061928865459, 0.012127110714683052, 0.08988329117941557, 0.9953903430093114, 0.9960440968645905, 0.9984524623919331, 0.9992170493849238, 0.9845521966156878, 0.015438561070980355, 0.9996140342893746, 0.9975931613528181, 0.9989129345547698, 0.997304551751372, 0.9987739134820594, 0.9595787093482613, 0.03881441970397461, 0.0003705564813790284, 0.3690742554535123, 0.13154755088955508, 0.11931918700404714, 0.3683331424907542, 0.003705564813790284, 0.007411129627580568, 0.05582252142035621, 0.9437165885403616, 0.9944677708203051, 0.9124404926073353, 0.08472661717068113, 0.8796215917476107, 0.005577310817240285, 0.0015935173763543672, 0.11313973372116007, 0.6975508505759181, 0.18155826003888, 0.0665426798246764, 0.05420933830544759, 0.024976334321092742, 0.01708907085127398, 0.6973655451231421, 0.01445998302800106, 0.00197181586745469, 0.11436532031237202, 0.1294825752961913, 0.9969588982617967, 0.9972696682141723, 0.9968337744131127, 0.9975546916098958, 0.21788483067572736, 0.7813322272002198, 0.9954337749603274, 0.26275190008538496, 0.0176852240442086, 0.7191991111311498, 0.9954010210401748, 0.05864557441313921, 0.0911867163131128, 0.16127532963613284, 0.3715411696051929, 0.10119937535925852, 0.2159873594240005, 0.09291909978937393, 0.2750405353765468, 0.4330030050184825, 0.0687601338441367, 0.07619366182728662, 0.05389307787783688, 0.9959663270839172, 0.35909570399281093, 0.4486423542289929, 0.04363694630798715, 0.09227395938043116, 0.055909837457108535, 0.9965556211960912, 0.21061216002087235, 0.7884455221294195, 0.9968010388185329, 0.9943046585575145, 0.9981097110490195, 0.9981039691769334, 0.9936575868029494, 0.9974696507505509, 0.9950459171139163, 0.995974477954387, 0.9986470659503244, 0.9890670184978237, 0.24142110926939425, 0.7568878020337766, 0.020234669331720294, 0.687304268300766, 0.032240573135207666, 0.2599480520148334, 0.00026979559108960394, 0.9979128829922959, 0.9713890542688707, 0.027853449584770488, 0.00831033499741522, 0.9889298646924112, 0.045612444610587985, 0.03738724968080982, 0.1046842991062675, 0.07851322432970062, 0.011963919897859143, 0.7215739188396296, 0.0007477449936161964, 0.9977727443915765, 0.9985680150319913, 0.9992506330050033, 0.9444837067085711, 0.05535747604694059, 0.06970669985720242, 0.1809980833202037, 0.6207785502304507, 0.0077784300269839604, 0.11727479117606586, 0.002991703856532292, 0.9592910205731526, 0.0383716408229261, 0.12193967563984358, 0.8775658623916611, 0.9959044688103309, 0.0030572662127715455, 0.049097718294594736, 0.9496217221369178, 0.8884130151634436, 0.11105162689543045, 0.011857298905874693, 0.9881082421562244, 0.9914315702240815, 0.9961796319388221, 0.08358680987367041, 0.0455928053856384, 0.0018997002244016002, 0.04432633856937067, 0.08042064283300107, 0.7434160211491595, 0.9981226763908577, 0.5076275805082308, 0.12147140427431592, 0.2821728729640724, 0.00047265137849928377, 0.06711649574689829, 0.02126931203246777, 0.033579267883996344, 0.03899527883302801, 0.6322290114502968, 0.018775504623309784, 0.2765776257972172, 0.9980634532925085, 0.9464096835518319, 0.052911089903214234, 0.997234521691104, 0.11933557489066125, 0.8805709262984582, 0.9957136068522047, 0.9961049246934977, 0.1328167402815866, 0.2563978794177648, 0.04178010041970439, 0.024628269721088905, 0.4397905307337304, 0.10423035578389411, 0.9965726162854346, 0.05607063321535912, 0.943509544105364, 0.09412608588624025, 0.13073067484200032, 0.10807069120272028, 0.014380374232620037, 0.06754418200170018, 0.0013073067484200033, 0.5468899897557014, 0.0370403578719001, 0.993608390784205, 0.9953843328032219, 0.994402453563124, 0.9967838294786913, 0.8674756931601724, 0.13191365912518324, 0.9989237708653149, 0.9969807754507574, 0.998666915740869, 0.04182860085452692, 0.9510502931134542, 0.00550376327033249, 0.13775080343861165, 0.7803649884119407, 0.08091187968966994, 0.9932752432740808, 0.9960787558540871, 0.16333189628250644, 0.0184720596986168, 0.8176316950808805, 0.9992446699567819, 0.9973460690308871, 0.6439404013198117, 0.017043658360245257, 0.24759787326974472, 0.09141598575040637, 0.9979591966820125, 0.10608606227906466, 0.8936402195372056, 0.0545925661892473, 0.02031351300065016, 0.001269594562540635, 0.5243425543292822, 0.3999222872003, 0.8783439802622656, 0.1215438156471027, 0.998685683572108, 0.9047769941811482, 0.09371513122403607, 0.9507041297169123, 0.023817222456373587, 0.0039695370760622645, 0.01984768538031132, 0.9983988677352202, 0.05706415571420017, 0.041086192114224124, 0.7212909282274902, 0.17804016582830454, 0.06224925426451478, 0.14697740590232658, 0.7054915483311676, 0.08472815163781179, 0.0903080884901865, 0.021071887314376852, 0.19867779467841032, 0.10460686916779936, 0.08880295368201672, 0.4966944866960258, 0.9990779347046056, 0.9955259646818643, 0.8170631793484167, 0.05447087862322778, 0.12744130092981593, 0.9987126111144394, 0.27098136789620714, 0.09337185710281294, 0.29787652238777823, 0.3379655262525729, 0.9053622128856204, 0.09387396216555971, 0.9982136572824348, 0.999138603971725, 0.998300842077055, 0.9959628633125901, 0.727407772642635, 0.16329562242997928, 0.10137052924874039, 0.007634600529193836, 0.9933082442573119, 0.9933823097696699, 0.9981487324517733, 0.997091067296091, 0.9985265723068063, 0.008817498983426847, 0.03526999593370739, 0.016031816333503358, 0.3462872328036726, 0.30420371492822623, 0.09017896687595639, 0.19919531794377923, 0.23338106539069528, 0.005970461243434873, 0.03422353677040347, 0.2517189106383881, 0.05352092043221975, 0.07015291961035976, 0.016951845316181154, 0.09371491844605809, 0.21525645090169657, 0.025054614146557054, 0.7177525180277181, 0.002370242479226929, 0.001939289301185669, 0.15600505045093604, 0.08748349514237573, 0.03469173083232141, 0.9980116925578937, 0.1347060746298212, 0.74135441072496, 0.028260015656605846, 0.01978201095962409, 0.07441804122906205, 0.999263963329472, 0.49610620919242104, 0.11775050989265898, 0.3855283191917007, 0.9925333097418587, 0.9976802862574792, 0.995764191625331, 0.5071810820382697, 0.015434603835613807, 0.017286756295887464, 0.057725418345195637, 0.012347683068491046, 0.08859462601642325, 0.07285133010409717, 0.2290495209205089, 0.9984190397792558, 0.44019125049275737, 0.5596993908704392, 0.012452042317105436, 0.002490408463421087, 0.9849565472830399, 0.7132542869699744, 0.003092142862586594, 0.007730357156466485, 0.06751178583314063, 0.0876107144399535, 0.1211089287846416, 0.2934796756670567, 0.021458019030774673, 0.0448967782797747, 0.4187614944698103, 0.021127895661070446, 0.0013204934788169029, 0.01733147690947185, 0.010729009515387336, 0.13023366934831704, 0.04060517447361976, 0.188956964104386, 0.007478507814157757, 0.0019942687504420687, 0.6007734610706732, 0.02642406094335741, 0.17449851566368102, 0.9926663727370632, 0.9933969892269213, 0.997052260898212, 0.27908172895342986, 0.04741089732687389, 0.03195459746573186, 0.4464960330673728, 0.027786606491940744, 0.004341657264365741, 0.017540295348037593, 0.03646992102067222, 0.10211577885788223, 0.006772985332410556, 0.9987985475458074, 0.9974220152201225, 0.9929475374862264, 0.9991623611934746, 0.9954117223048138, 0.9944191879264543, 0.9978777557795764, 0.9151143699945035, 0.08412486360936018, 0.7784863830114684, 0.013524551569267036, 0.010225880454811662, 0.020781628021068862, 0.06102541561742443, 0.11578335611738365, 0.9994584489769981, 0.9925769857297699, 0.022649095672561438, 0.07172213629644456, 0.2614083125541466, 0.00094371231969006, 0.6436118020286209, 0.002047097649031482, 0.007895948074835717, 0.25003835570313104, 0.2798674928747326, 0.008188390596125928, 0.10937350296253918, 0.014768347325155692, 0.32797428762697245, 0.997840333940152, 0.9979874046525284, 0.994650342313999, 0.9985764875859933, 0.17236052045951902, 0.8272510693944196, 0.994998097380422, 0.9978989931133605, 0.023364491681926315, 0.7943927171854948, 0.12461062230360702, 0.05646418823132193, 0.9963865472985244, 0.9977902127556769, 0.005758594683788268, 0.15068322755912633, 0.7994848952659379, 0.04318946012841201, 0.3218971832296739, 0.042070412610338136, 0.0707829528043005, 0.2629572925677272, 0.05260522951603655, 0.009157716656587515, 0.02396154433452974, 0.08792785090573126, 0.11319764047691636, 0.015492377802497676, 0.9972871585034858, 0.994430247104371, 0.998775676096949, 0.036600725714125704, 0.009631769924769922, 0.0019263539849539845, 0.21446741032487696, 0.67358177673891, 0.06485391749345082, 0.9971453263771193, 0.7485681211688007, 0.25136420304930207, 0.09183066886921253, 0.9068278550834736, 0.7251631540764759, 0.02607607232124314, 0.17632391760078694, 0.07201962831581438, 0.9980573576620878, 0.9970520052584311, 0.054985703839204696, 0.9051078112355362, 0.039352513531979834, 0.9958268661792827, 0.998464173025959, 0.9982040695879558, 0.8209273056918854, 0.08354849988537308, 0.006962374990447756, 0.07278846580922654, 0.010127090895196736, 0.0056964886285481645, 0.7731556080585235, 0.20661917111908817, 0.019995403656685954, 0.9992707591085547, 0.9997896834472136, 0.9979074292932055, 0.10169454926925496, 0.8943905230603705, 0.9961370804148993, 0.9991736885851897, 0.999442767516668, 0.997604017368948, 0.9957790098204317, 0.12257310688863846, 0.8770019478792723, 0.9935205745825583, 0.13853161103020298, 0.8599415099799392, 0.09560227448102589, 0.7402347538388004, 0.05053263079711368, 0.11199123582063032, 0.033233895394526994, 0.028486196052451705, 0.051037767927309304, 0.23263726776168894, 0.6551825092063892, 0.01003526109157846, 0.0035840218184208785, 0.2745360712910393, 0.7114283309565443, 0.0012390068031275239, 0.048321265321973436, 0.20629463272073273, 0.03593119729069819, 0.7068533811842524, 0.18833569064918082, 0.0006405975872421116, 0.009608963808631674, 0.061497368375242716, 0.5867873899137742, 0.15374342093810678, 0.003989223660972033, 0.07778986138895465, 0.9175214420235677, 0.10282263269404877, 0.3077958547312048, 0.040322601056489715, 0.0840054188676869, 0.4650539988515147, 0.33715939178990895, 0.6621567213509234, 0.045635760641061875, 0.02470054371331402, 0.05226273578365832, 0.8773211410369156, 0.06498870106940104, 0.13845418923481093, 0.13657045876903118, 0.006593056630229091, 0.05933750967206182, 0.06781429676807066, 0.5246189347196577, 0.9990171436746262, 0.1608133632914786, 0.8381787420040703, 0.017373195464385215, 0.04100074129594911, 0.15705368699804234, 0.019457978920111443, 0.7644206004329495, 0.9988075348200617, 0.9954113642445623, 0.998168369391857, 0.0008069267335423259, 0.9968571761227837, 0.99737153986426, 0.9960193625523297, 0.0028105483130956605, 0.03560027863254503, 0.14781402239243843, 0.23483692571643738, 0.011450382016315654, 0.012074948308114689, 0.3968077840563206, 0.15853574373498855, 0.99569399789158, 0.9962209813972358, 0.9984116067594935, 0.994854126701043, 0.9969440127888928, 0.998166027995654, 0.9954126549857439, 0.29506019064962147, 0.13012241446052628, 0.12141857402838406, 0.037861705879818686, 0.005657496280892447, 0.09356628464552894, 0.12054818998516983, 0.19540121770159297, 0.989805457852732, 0.996485969576624, 0.9955022331311582, 0.1145185788121097, 0.5594081892631017, 0.08061505219010354, 0.04407458460860801, 0.02712282129760493, 0.02335576278404869, 0.15030563469089397, 0.01085210190826884, 0.9875412736524644, 0.9979632688680596, 0.9947162798576688, 0.9983204558345472, 0.9949563764825512, 0.024568045292968636, 0.16460590346288986, 0.8082886901386681, 0.9999656900548268, 0.9139629761335656, 0.08446577267585086, 0.9988951872121453, 0.9976177404419053, 0.23831944592866264, 0.0008852876891852253, 0.04709730506465398, 0.2804591399338794, 0.06568834653754371, 0.0007082301513481802, 0.02797509097825312, 0.0072593590513188476, 0.2955090306500282, 0.035942680180920145, 0.9308895300444714, 0.013322211521208892, 0.05550921467170372, 0.6055058846382274, 0.39446062970121415, 0.9969703996190034, 0.9965740079932162, 0.9968142277002288, 0.4527551826514364, 0.021744674295437767, 0.04659573063308093, 0.1799760095702751, 0.03125796929969179, 0.03145211817732963, 0.08173667748552947, 0.11784836872616719, 0.03688828675118907, 0.997794565879037, 0.03565072710202363, 0.11302039017450045, 0.04247746207900688, 0.029582518233594077, 0.040201883753345796, 0.7388044297312982, 0.9985086050108261, 0.9938736216935132, 0.43524330399455996, 0.09435907367399635, 0.01883961027279449, 0.027856859634132026, 0.0006440892400955382, 0.005957825470883729, 0.021093922613128875, 0.12994500418927482, 0.2658478338494334, 0.09092783924685288, 0.9083409920639222, 0.5344508020588287, 0.008692874491318298, 0.03654226869498618, 0.10157784822262679, 0.022859040329022192, 0.016258894881910152, 0.07775293295012478, 0.08821657817115607, 0.10962680608495855, 0.004185458088412514, 0.1902826357808097, 0.8090535773198873, 0.9974048544865219, 0.269826177621135, 0.0369761798962296, 0.5486465611629744, 0.001998712426823222, 0.14290793851786038, 0.9983981832886375, 0.13445227380054192, 0.3052123346189517, 0.000850963758231278, 0.2487650719896103, 0.11147625232829743, 0.19912551942611906, 0.9995176854355877, 0.9968764479509774, 0.3484333003149203, 0.03438382272320951, 0.03200706078381715, 0.27538748337759505, 0.014735924024232646, 0.0190140955151389, 0.020440152678774318, 0.16637333575746538, 0.07890849638782643, 0.010457752533326394, 0.18465289730071668, 0.03217232032032984, 0.05758574981706099, 0.10003158418926086, 0.28522519275586544, 0.04433832380280752, 0.27846630193226674, 0.01703240487546874, 0.9988065341358447, 0.155643322469917, 0.024398867088307404, 0.14465042630925104, 0.07641403428755615, 0.02399668796047816, 0.047725256502403494, 0.11971532038383798, 0.2789782550042182, 0.12856326119608133, 0.9940372984371211, 0.9969321680764732, 0.9962557314481377, 0.3504588332289698, 0.1076014208192246, 0.25668469779539327, 0.08195809155856827, 0.05505773635376212, 0.004022483021279424, 0.004525293398939353, 0.05882881418621158, 0.05455492597610219, 0.026146139638316256, 0.9959384850267244, 0.9974455714104495, 0.9991996548715485, 0.9956736669054176, 0.07045615996628367, 0.9287402904646483, 0.17593334853255027, 0.06137643021860806, 0.07329780057973376, 0.31582317886388456, 0.06789592963484867, 0.028685797431458698, 0.02663681190064022, 0.0011176284713555337, 0.24019698563549344, 0.00894102777084427, 0.9972453189445458, 0.9971442584864465, 0.9966125556692905, 0.9910258861991339, 0.2444775752876163, 0.416088575210957, 0.3391360236580304, 0.9911580552910415, 0.9975142925496239, 0.691245970009912, 0.30722043111551645, 0.21601507798104785, 0.18205449855716171, 0.18549354457477044, 0.41612456813065535, 0.9970844894569455, 0.9980924185517065, 0.9975306236008724, 0.9988469868475458, 0.18583947039905424, 0.04316271570558679, 0.09711611033757028, 0.673817950737216, 0.9703471717757151, 0.029472118424849144, 0.9527978213829673, 0.04572249607255725, 0.12345433377334353, 0.573465292366499, 0.2579368462460428, 0.0006126765944086528, 0.025732416965163415, 0.018992974426668235, 0.9969191523963513, 0.997883507588901, 0.9941250577449043, 0.008829032060656087, 0.8475870778229844, 0.14126451297049739, 0.033752745487400634, 0.9634874620948908, 0.10670447563550613, 0.010711646202792122, 0.001029965981037704, 0.33803483497657444, 0.12050601978141136, 0.030280999842508496, 0.02368921756386719, 0.019569353639716375, 0.2817986924119158, 0.06777176155228092, 0.9974808106433412, 0.9959159580358566, 0.29312874577602904, 0.09882626286163264, 0.6063577484052715, 0.9968639011155901, 0.9954452952109678, 0.038335245005889786, 0.13458117927599605, 0.34909542260682613, 0.47755927555209504, 0.5335240849814206, 0.0021171590673865893, 0.0020289441062454816, 0.08909711075251897, 0.28070000635100534, 0.09253749423702218, 0.42219462887078557, 0.11079693299095202, 0.42219462887078557, 0.04461034407267278, 0.9984180206236045, 0.05687566900156175, 0.9425805315728908, 0.01942382156007191, 0.7678884123415095, 0.20265520494341693, 0.009064450061366892, 0.993883615310003, 0.9987013693258074, 0.4362831470461566, 0.525715149487751, 0.038069028066354385, 0.70194432766959, 0.0012562762016457988, 0.17870528968411487, 0.07537657209874792, 0.04208525275513426, 0.13709393431384687, 0.11774189306597843, 0.06355617757194684, 0.024444683681518014, 0.020981686826636294, 0.07374146243924601, 0.5622277246749143, 0.9942230989816828, 0.27552862373973075, 0.72411654834078, 0.7805191628249419, 0.03451079570338726, 0.18463275701312185, 0.9980394160380986, 0.9958900705094622, 0.9979178735738449, 0.9997371412228199, 0.9964612480937663, 0.506797483089187, 0.49248790709608065, 0.12636247733177486, 0.09959076603267, 0.07567470393880302, 0.13778507415272626, 0.06532297556981581, 0.4950981809581122, 0.9945358996294271, 0.020621186793912404, 0.9782175485362197, 0.11077835286374436, 0.7927709537951745, 0.011334464675247976, 0.014328474212105933, 0.07078693976428453, 0.9962080183387314, 0.9952410848965642, 0.9982361781530059, 0.9338805635287893, 0.06549128699055569, 0.9990829839503064, 0.9988931345190982, 0.998161469569673, 0.9980335605987607, 0.0438248294097518, 0.6156345083750848, 0.10069276281050116, 0.23947138927471517, 0.9962400193162165, 0.9930567787107447, 0.99301433322343, 0.34642675206224705, 0.5362866083872708, 0.050389345754508666, 0.06658592117560073, 0.9983098319366601, 0.99975603981213, 0.9971901428735395, 0.9914041481113289, 0.2282202183163182, 0.0006558052250468914, 0.4426685269066517, 0.13116104500937828, 0.19608576228902053, 0.9993443558238964, 0.9960670448707412, 0.33308422646079533, 0.047129002682815044, 0.07281430914494924, 0.053844885565116185, 0.018498133553004906, 0.034757639478576095, 0.0008247575469492632, 0.05714391575291324, 0.3180029456022945, 0.06374197612850735, 0.06129919640617943, 0.9363980692392236, 0.0024753318666517594, 0.9950834103940072, 0.9958244443005958, 0.9980387879319077, 0.9983030623493994, 0.9967153741459656, 0.9167833392148774, 0.07867916717142605, 0.9987848099948723, 0.9947817801834657, 0.7226583614736151, 0.06302884957213485, 0.21462409983614883, 0.9931403785854471, 0.994334347776238, 0.9931343646032155, 0.15168253901263276, 0.3804761404761321, 0.2650404182442959, 0.19924205657214927, 0.0032322002224914135, 0.05652303123085797, 0.9429069300784034, 0.9972390354621433, 0.023453200336997694, 0.010868556253730639, 0.020593053954436998, 0.23338794481695266, 0.02974552237863122, 0.6812868683259574, 0.994340550062709, 0.9983200166161674, 0.996561518688397, 0.9972915082369427, 0.9987811036059849, 0.999387368384904, 0.9986989484469956, 0.003302143969021372, 0.38525012971916006, 0.6108966342689538, 0.9967703762518153, 0.9992905083661922, 0.9990261199268573, 0.9993122887963017, 0.9983423218347052, 0.017491237246961666, 0.9795092858298532, 0.7104874568062668, 0.026019346325491084, 0.06425185521192697, 0.04832164317591202, 0.12478666094878378, 0.026019346325491084, 0.31552262119225194, 0.6830544656579519, 0.1922232760799639, 0.807535927861704, 0.9936491461609872, 0.6561569977993333, 0.021119190322830734, 0.09536126707308955, 0.02680512617897747, 0.01104696109194223, 0.07830345950464934, 0.1112818874703004, 0.1722383029747096, 0.14301930514864278, 0.6843396859263015, 0.9977860360638819, 0.3955440524415511, 0.06954475135023923, 0.05484100392190293, 0.05881498971334517, 0.014173882656143994, 0.024903644293038048, 0.11378845982829618, 0.16690740324057415, 0.1016015700678733, 0.4272522474953366, 0.08161207344219687, 0.08068042876819918, 0.02310478791514249, 0.009689104609575883, 0.021614156436746204, 0.09577307248696162, 0.133784175186067, 0.12670367566368462, 0.30118946104485106, 0.05835265266144656, 0.43495169560724395, 0.03546045815580214, 0.00314206591253943, 0.02199446138777601, 0.14094409950534015, 0.004039799030407839, 0.9973828412008564, 0.32364477890345245, 0.676140178097727, 0.6897633558303418, 0.17782961517500998, 0.1311268879573306, 0.28086237131932934, 0.009291687472218415, 0.12205898543050554, 0.5874880579025371, 0.9935742717444477, 0.9973341100645671, 0.2498498530279174, 0.04208940176455956, 0.02686557559439972, 0.01164174942423988, 0.20596941289039786, 0.4634311790033952, 0.9953996152399557, 0.9982746738033323, 0.02237869966290551, 0.9769163122076059, 0.9987157589254942, 0.9960976714854972, 0.9992383545257637, 0.1621134365009095, 0.028608253500160503, 0.12220315692661153, 0.022957240463091763, 0.023310428777908558, 0.6403304147628518, 0.5524169797418793, 0.020706392814956655, 0.21026848894235747, 0.060393645710290246, 0.007148635614687417, 0.07666295435061334, 0.06335170182671263, 0.009367177702004202, 0.9924735746985796, 0.0665346286326411, 0.661649918069042, 0.24429633846429333, 0.0013441339117705273, 0.02587457780158265, 0.9969550167732054, 0.9986510061142811, 0.9935038340748524, 0.21772992649876838, 0.7823261496955746, 0.23243490740053097, 0.09453509293529058, 0.6721531837142221, 0.9961645358804634, 0.9974537976009474, 0.9923124949423227, 0.9970610696906615, 0.9963925715238535, 0.2845669929409381, 0.19587841516161583, 0.0026430370861387446, 0.13273919588163471, 0.06490124400407361, 0.22348346917239828, 0.0957366766756923, 0.05092294595974907, 0.9481367557267565, 0.034215886231147886, 0.9626069326362938, 0.9968669097250908, 0.0025960075774090906, 0.06406132633942613, 0.08820751857505597, 0.011333926967744622, 0.04040791353717648, 0.3454383827995209, 0.3444528239327605, 0.1059475781767432, 0.27119659301255755, 0.006998621755162775, 0.14988714925640276, 0.24699302610928625, 0.031493797898232485, 0.10731220024582921, 0.17788163627705386, 0.00845666795415502, 0.29938669888255687, 0.07303294269994522, 0.01209154680462669, 0.22901389647962953, 0.012575208676811759, 0.04812435628241423, 0.22345178494950124, 0.0636015361923364, 0.038451118838712876, 0.9924916047355281, 0.4084283315256573, 0.056840570543102764, 0.00691972163133425, 0.2545469028669385, 0.08451945706843976, 0.0008237763846826487, 0.003459860815667125, 0.08122435152970917, 0.08814407316104342, 0.014827974924287677, 0.38823190218096854, 0.6098795590814138, 0.0006970052103787586, 0.0013940104207575171, 0.9958515382452945, 0.995878560103424, 0.8338581329101434, 0.11619334638911835, 0.04374337746413867, 0.005467922183017334, 0.06831889222935442, 0.9298960331217684, 0.9953595949091554, 0.0032763848047048794, 0.08822265425351675, 0.0596941329052328, 0.5776026675221188, 0.1603031199570241, 0.023014605457439153, 0.00463488582128983, 0.0019977956126249266, 0.081270325521582, 0.9976833675943911, 0.9976040153151758, 0.02215081362259934, 0.011466303522286716, 0.23766520028012467, 0.03961086671335411, 0.035180703988834244, 0.002345380265922283, 0.6475855512018748, 0.003908967109870471, 0.9992961512351328, 0.9964404575377506, 0.10738927923355478, 0.07066134730237619, 0.06507231418241423, 0.055890331199619586, 0.28903285563231845, 0.07265743055950546, 0.11856734547347869, 0.15329919414752802, 0.0670683974395435, 0.9978017469875184, 0.9992513085361652, 0.24508382261406747, 0.04460575133521956, 0.027754689719692173, 0.38385727121252833, 0.038410508094216846, 0.0024780972964010867, 0.04088860539061793, 0.17891862480015847, 0.03345431350141467, 0.004956194592802173, 0.8497154608276072, 0.14942030349299026, 0.43428004309351576, 0.4279453260953932, 0.1182480506316218, 0.019356079716485713, 0.6522093973173746, 0.02327885848886629, 0.0204693410850376, 0.14770034351556544, 0.010435350357077993, 0.03411556847506267, 0.09913582839224093, 0.012843508131788299, 0.3332736605822, 0.03265746925302965, 0.05074468299316914, 0.30430062375771727, 0.06497999010346411, 0.009880977876557688, 0.0015072678116782915, 0.08976617189550713, 0.09311565592145889, 0.019761955753115377, 0.15047299410709547, 0.07640424756213939, 0.07028745918380218, 0.2890460544599713, 0.0685080298373768, 0.03403158625038523, 0.029471798550170215, 0.0016682150122737857, 0.2648013296149256, 0.01534757811291883, 0.9987651181143484, 0.10522357295242905, 0.894400370095647, 0.21357136645902602, 0.06443004164745371, 0.011253615719612836, 0.11624614040039634, 0.0019786577089429165, 0.19217712998108075, 0.17387454617335876, 0.22643264156715498, 0.9942602287092511, 0.07700679476613516, 0.8129801310043124, 0.08993923358182199, 0.019986496351515996, 0.9968782644854972, 0.21950254219200316, 0.7789663387545478, 0.6338053106656556, 0.19663237047759796, 0.16799658836920991, 0.9994748661692952, 0.32754051870602835, 0.6704688470156956, 0.8376418858476063, 0.16183095768546374, 0.9983655335453703, 0.9995791241097319, 0.0005021168196281324, 0.22394410155414707, 0.0040169345570250595, 0.7717535517684395, 0.9939612097500448, 0.9952062595057966, 0.9955402219655877, 0.4264149024476332, 0.023187816022929207, 0.13009268210266775, 0.03252317052566694, 0.03327602169524256, 0.026349790935146826, 0.08371705005680934, 0.07543568719147749, 0.16186300145875906, 0.007076800994010862, 0.24136228516590094, 0.011266462144189271, 0.013433089479610286, 0.1408307768023659, 0.41772575026917147, 0.01083313667710507, 0.030332782695894196, 0.04463252310967288, 0.08839839528517736, 0.0012999764012526084, 0.9965731375566478, 0.0023122346579040553, 0.9974545167496971, 0.1148250411873476, 0.8845780950729, 0.14661982150962055, 0.8528572823689379, 0.06271565436550872, 0.004195027047860114, 0.03544797855441797, 0.3809084559456984, 0.10130990320582176, 0.04719405428842629, 0.018877621715370514, 0.0016780108191440457, 0.34692873685803144, 0.00041950270478601144, 0.15169147910539632, 0.15783042930417868, 0.2940755175868323, 0.07604377020427178, 0.3200175329429771, 0.054251343281045715, 0.02101381144810125, 0.05164178499664098, 0.216043956914139, 0.07430373851910312, 0.04724673946501196, 0.03351222217867127, 0.00027469034572681373, 0.4801587243304704, 0.021425846966691472, 0.02139027990880478, 0.03305770531360739, 0.9431168868882107, 0.13540616844065106, 0.13540616844065106, 0.7288720609776761, 0.996194374139391, 0.04014955870648765, 0.9598017147381104, 0.9959535930409227, 0.0018581223750763483, 0.9958057927180867, 0.9453929354974981, 0.05388939322681024, 0.9989515185522964, 0.2032044286815134, 0.0778860475980867, 0.1549635895117642, 0.06117693012029648, 0.0026950189480306817, 0.015361608003774885, 0.04851034106455227, 0.20562994573474103, 0.12504887918862365, 0.10537524086799965, 0.2969443907005932, 0.03711804883757415, 0.12774393431113182, 0.020487234748011705, 0.01084618310188855, 0.008435920190357762, 0.09833872679045619, 0.16221069394602208, 0.02844110235606331, 0.2092108207208725, 0.903877743575605, 0.09528433021587637, 0.2954237564851112, 0.03757691295877565, 0.0637818654168692, 0.1419846952367773, 0.09023403440758626, 0.02398000366448183, 0.03971945624151286, 0.09723850283191944, 0.18046806881517252, 0.02966598391482288, 0.9934816893395425, 0.1888258369288401, 0.8057609405035078, 0.004681632320549755, 0.9997136815630022, 0.9949988165256765, 0.9992430913994257, 0.9980922884794897, 0.9996496967027964, 0.9984779851378038, 0.997789086779714, 0.9983991773707889, 0.0474605280312083, 0.9509055794824234], \"Term\": [\"3d\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"abstract\", \"abstract\", \"access\", \"access\", \"access\", \"access\", \"achievable\", \"achievable_rate\", \"ad\", \"ad_hoc\", \"agent\", \"algebra\", \"algebraic\", \"algebraic\", \"algebraic\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"alignment\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"allocation\", \"allocation\", \"allocation\", \"alpha\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"angle\", \"annotation\", \"antenna\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approximation\", \"approximation\", \"approximation\", \"approximation_algorithm\", \"arc\", \"architecture\", \"architecture\", \"array\", \"array\", \"arxiv\", \"assisted\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at_most\", \"at_most\", \"attack\", \"auction\", \"authentication\", \"axiom\", \"band\", \"bandwidth\", \"bandwidth\", \"bandwidth\", \"bandwidth\", \"base_station\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"bayesian\", \"bayesian\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"beamforming\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"bipartite\", \"bit\", \"bit\", \"bit\", \"blind\", \"block\", \"block\", \"block\", \"block\", \"block\", \"block\", \"boolean\", \"boolean\", \"bound\", \"bound\", \"broadcast_channel\", \"buffer\", \"buffer\", \"business\", \"calculus\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"capacity\", \"capacity\", \"capacity_region\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cdma\", \"cellular_automaton\", \"challenge\", \"challenge\", \"channel\", \"channel\", \"character\", \"character\", \"checking\", \"checking\", \"citation\", \"cite\", \"cite\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classical\", \"classical\", \"classical\", \"client\", \"closed_form\", \"cloud\", \"clustering\", \"code\", \"code\", \"coding\", \"coding_scheme\", \"cognitive\", \"cognitive_radio\", \"coloring\", \"combinatorial\", \"combinatorial\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"community\", \"community\", \"competition\", \"competitive\", \"competitive\", \"complete\", \"complete\", \"complete\", \"complete\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"compound\", \"compressed\", \"compressed_sensing\", \"computable\", \"computer\", \"computer\", \"consensus\", \"constant\", \"constant\", \"constant\", \"constellation\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context_free\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convolutional\", \"cooperation\", \"cooperation\", \"corpus\", \"correcting\", \"correlated\", \"covariance\", \"covariance_matrix\", \"covering\", \"cr\", \"crossing\", \"csi\", \"csp\", \"cut\", \"cut\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data_mining\", \"database\", \"database\", \"de\", \"de\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decode\", \"decoder\", \"decoding\", \"delay\", \"delay\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"destination\", \"destination\", \"detection\", \"detection\", \"development\", \"development\", \"device\", \"device\", \"dictionary\", \"dictionary\", \"differential\", \"differential\", \"directional\", \"discourse\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distortion\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"divergence\", \"diversity\", \"diversity\", \"dmt\", \"document\", \"document\", \"dof\", \"downlink\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"eavesdropper\", \"edge\", \"edge\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"eigenvalue\", \"electronic\", \"emergence\", \"encoder\", \"energy\", \"energy\", \"engineering\", \"english\", \"ensemble\", \"entropy\", \"entropy\", \"entropy\", \"environment\", \"environment\", \"environment\", \"eps\", \"epsilon\", \"equation\", \"equation\", \"equation\", \"equilibrium\", \"erasure\", \"error\", \"error\", \"error\", \"error\", \"error_probability\", \"estimation\", \"estimation\", \"et\", \"et\", \"et\", \"et\", \"et\", \"every\", \"every\", \"exactly\", \"existence\", \"existence\", \"exists\", \"exists\", \"exists\", \"exists\", \"experimental_result\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"face\", \"face\", \"face\", \"face\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"fading\", \"fading_channel\", \"family\", \"family\", \"family\", \"fault\", \"feature\", \"feature\", \"feature\", \"feature\", \"feedback\", \"feedback\", \"file\", \"filter\", \"filtering\", \"financial\", \"finite\", \"finite\", \"finite\", \"finite\", \"fixed_point\", \"forest\", \"formal\", \"formalism\", \"fragment\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"fuzzy\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"game\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian_interference\", \"gaussian_noise\", \"genetic\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"grammar\", \"graph\", \"graph\", \"grid\", \"grid\", \"grid\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"hamming\", \"handoff\", \"hardware\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"heuristic\", \"higher_order\", \"hilbert\", \"hoc\", \"hoc_network\", \"home\", \"hop\", \"human\", \"human\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"image\", \"incentive\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information_retrieval\", \"integrated\", \"intensity\", \"interface\", \"interference\", \"interference\", \"interference_alignment\", \"interference_channel\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"intersection\", \"inverse\", \"issue\", \"issue\", \"issue\", \"issue\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"item\", \"java\", \"journal\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"lambda\", \"language\", \"language\", \"law\", \"law\", \"layer\", \"layer\", \"layer\", \"layer\", \"ldpc\", \"ldpc_code\", \"learning\", \"learning\", \"learning\", \"lexical\", \"library\", \"life\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"load\", \"load\", \"load\", \"log\", \"logic\", \"logic_programming\", \"logical\", \"logical\", \"long_term\", \"lower\", \"lower_bound\", \"mac\", \"malicious\", \"management\", \"management\", \"manet\", \"market\", \"market\", \"markov\", \"markov\", \"markov\", \"markov\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measurement\", \"measurement\", \"measurement\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"message\", \"message\", \"method\", \"method\", \"method\", \"method\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"mimo\", \"minimization\", \"minimization\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"mining\", \"ml\", \"mobile\", \"mobile\", \"mobile_ad\", \"mobility\", \"modal\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model_checking\", \"modern\", \"modulation\", \"molecular\", \"moment\", \"motion\", \"mu\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi_agent\", \"multi_hop\", \"multicast\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple_access\", \"multiple_access\", \"multiple_input\", \"multiple_output\", \"nash\", \"nash_equilibrium\", \"net\", \"net\", \"net\", \"network\", \"network_coding\", \"network_coding\", \"neural\", \"neural_network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise_ratio\", \"nonlinear\", \"normalized\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"note\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"np\", \"np_complete\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numerical\", \"numerical\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"online\", \"online\", \"only_if\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"opinion\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimization\", \"optimization_problem\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"organization\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"outage_probability\", \"outer\", \"outer_bound\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"overview\", \"p2p\", \"packet\", \"packing\", \"page\", \"page\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"parity\", \"parity_check\", \"party\", \"password\", \"path\", \"path\", \"path\", \"patient\", \"payoff\", \"peer\", \"peer\", \"performance\", \"performance\", \"performance\", \"performance\", \"pixel\", \"planar\", \"platform\", \"player\", \"policy\", \"policy\", \"policy\", \"policy\", \"polynomial\", \"polynomial\", \"polynomial_time\", \"polynomial_time\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power_law\", \"precoding\", \"predicate\", \"prediction\", \"prediction\", \"prediction\", \"preference\", \"preference\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"price\", \"pricing\", \"primary\", \"primary\", \"primary\", \"primary_user\", \"prime\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"processor\", \"program\", \"program\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming_language\", \"project\", \"proof\", \"proof\", \"proof\", \"property\", \"property\", \"property\", \"property\", \"property\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"propositional\", \"protocol\", \"protocol\", \"prove\", \"prove\", \"prove\", \"qos\", \"qualitative\", \"quantization\", \"quantum\", \"radar\", \"radio\", \"radio\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"randomness\", \"rank\", \"rank\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rational\", \"rayleigh\", \"reasoning\", \"receiver\", \"receiver\", \"recognition\", \"reconstruction\", \"recovery\", \"reed\", \"region\", \"region\", \"region\", \"region\", \"regression\", \"regret\", \"regularization\", \"relation\", \"relation\", \"relation\", \"relation\", \"relational\", \"relay\", \"relay_channel\", \"repair\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"research\", \"resource_allocation\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retrieval\", \"retrieval\", \"review\", \"review\", \"reward\", \"rewriting\", \"rfid\", \"ring\", \"route\", \"route\", \"routing\", \"routing_protocol\", \"rule\", \"rule\", \"rule\", \"running_time\", \"sat\", \"scheduler\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"science\", \"science\", \"scientific\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search_engine\", \"secondary\", \"secrecy\", \"secret\", \"secure\", \"security\", \"segmentation\", \"semantic\", \"semantic\", \"semantic\", \"semantic_web\", \"semantics\", \"sensing\", \"sensor\", \"sensor_network\", \"sentence\", \"sentence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"server\", \"server\", \"service\", \"service\", \"session\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sharing\", \"sharing\", \"sharing\", \"shortest_path\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"side_information\", \"signal\", \"signal\", \"similarity\", \"similarity\", \"similarity\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulator\", \"site\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"smart\", \"snr\", \"social\", \"social\", \"social_network\", \"soft\", \"software\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"sorting\", \"source\", \"source\", \"source\", \"source\", \"source\", \"space_time\", \"sparsity\", \"spatially\", \"specification\", \"specification\", \"spectrum\", \"spectrum\", \"spectrum\", \"spectrum_sensing\", \"speech\", \"spreading\", \"spreadsheet\", \"sqrt\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state_information\", \"state_information\", \"station\", \"station\", \"stochastic\", \"stochastic\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"subgraph\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum_rate\", \"superposition\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbolic\", \"symbolic\", \"syntax\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tag\", \"tcp\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technology\", \"tensor\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"termination\", \"text\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"theorem\", \"theorem\", \"theory\", \"theory\", \"theory\", \"theory\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"throughput\", \"tight\", \"tight\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time_series\", \"tool\", \"tool\", \"tool\", \"tool\", \"topological\", \"topology\", \"topology\", \"tradeoff\", \"tradeoff\", \"tradeoff\", \"traffic\", \"translation\", \"translation\", \"transmission\", \"transmission\", \"transmit\", \"transmitter\", \"tree\", \"tree\", \"tree\", \"tree\", \"triangle\", \"trust\", \"turing\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"uncertainty\", \"uncertainty\", \"union\", \"upper\", \"upper\", \"upper_bound\", \"upper_bound\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"user\", \"user\", \"user\", \"user\", \"user\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"utility\", \"utility\", \"utility\", \"vector\", \"vector\", \"vector\", \"vehicle\", \"vertex\", \"vertex\", \"virtual\", \"virtual\", \"wavelet\", \"web\", \"web\", \"weight\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"whether\", \"whether\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"wideband\", \"will\", \"will\", \"will\", \"wireless\", \"wireless_communication\", \"wireless_network\", \"wireless_sensor\", \"word\", \"worst\", \"worst_case\", \"xml\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el201221403462709028647742056071\", ldavis_el201221403462709028647742056071_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el201221403462709028647742056071\", ldavis_el201221403462709028647742056071_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el201221403462709028647742056071\", ldavis_el201221403462709028647742056071_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-roberts",
   "metadata": {},
   "source": [
    "## LDA with K=10 and first 1000 training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "italic-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 715\n",
      "Number of documents: 1000\n",
      "Saving model in model10.gensim\n",
      "\n",
      "Average topic coherence: -1.6016.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el201221403468889580161222937532\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el201221403468889580161222937532_data = {\"mdsDat\": {\"x\": [0.00716255589069812, 0.018242333988711952, -0.013743358890045887, -0.011128730267247085, 0.14745540099498794, 0.03493818665330077, -0.2016145579848907, -0.0010616088084247999, 0.040983700060089286, -0.021233921637179427], \"y\": [0.09481927904575563, 0.12479760719708222, -0.08966947801068874, -0.03032543052421833, -0.09174946602426715, 0.06392482618214337, -0.04866066185057219, 0.05866151635854631, -0.07578838764330174, -0.006009804730479481], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [8.282024082393665, 16.3241822311141, 13.542786567355353, 10.450615602011192, 9.571793074394467, 9.518693635903274, 4.374682671124971, 11.77615205775789, 10.626768413626117, 5.53230166431897]}, \"tinfo\": {\"Term\": [\"grammar\", \"discourse\", \"parsing\", \"model\", \"word\", \"learning\", \"agent\", \"parser\", \"dialogue\", \"lexical\", \"strategy\", \"language\", \"theory\", \"speech\", \"algorithm\", \"planning\", \"finite\", \"state\", \"lexicon\", \"plan\", \"logic\", \"sentence\", \"semantic\", \"system\", \"problem\", \"domain\", \"structure\", \"formalism\", \"task\", \"program\", \"symbolic\", \"modeling\", \"language_processing\", \"induction\", \"artificial\", \"acquisition\", \"gram\", \"spoken_language\", \"employ\", \"markov\", \"increase\", \"contrast\", \"technique\", \"classification\", \"during\", \"model\", \"learned\", \"main\", \"verb\", \"utterance\", \"empirical\", \"network\", \"ability\", \"under\", \"generalized\", \"change\", \"detailed\", \"learning\", \"spoken\", \"sequence\", \"class\", \"task\", \"algorithm\", \"representation\", \"language\", \"data\", \"problem\", \"reasoning\", \"processing\", \"approach\", \"analysis\", \"more\", \"time\", \"use\", \"from\", \"this_paper\", \"framework\", \"by\", \"which\", \"these\", \"using\", \"semantic\", \"system\", \"present\", \"a\", \"it\", \"method\", \"word_sense\", \"tagging\", \"tagger\", \"clustering\", \"similarity\", \"disambiguation\", \"sense\", \"frequency\", \"segmentation\", \"word\", \"experimental_result\", \"hidden\", \"accuracy\", \"estimation\", \"distribution\", \"contextual\", \"low\", \"occurrence\", \"part\", \"markov\", \"distance\", \"gram\", \"evaluated\", \"training\", \"experimental\", \"performs\", \"combining\", \"probability\", \"machine_learning\", \"derived\", \"method\", \"corpus\", \"statistical\", \"learning\", \"experiment\", \"model\", \"data\", \"based\", \"automatic\", \"speech\", \"text\", \"result\", \"from\", \"using\", \"based_on\", \"by\", \"set\", \"a\", \"approach\", \"between\", \"problem\", \"it\", \"which\", \"be\", \"this_paper\", \"can\", \"prolog\", \"logic\", \"specification\", \"programming\", \"field\", \"expression\", \"formal\", \"linguistics\", \"generation\", \"world\", \"knowledge_base\", \"answer\", \"base\", \"underlying\", \"situation\", \"inference\", \"content\", \"line\", \"question\", \"expressed\", \"needed\", \"program\", \"direct\", \"constraint\", \"natural_language\", \"what\", \"interest\", \"natural\", \"paradigm\", \"issue\", \"type\", \"can_be\", \"be\", \"can\", \"description\", \"semantics\", \"they\", \"language\", \"a\", \"it\", \"feature\", \"knowledge\", \"some\", \"by\", \"structure\", \"such\", \"which\", \"system\", \"used\", \"or\", \"how\", \"set\", \"from\", \"not\", \"ha\", \"processing\", \"have\", \"centering\", \"discourse\", \"pronoun\", \"anaphora\", \"topic\", \"resolution\", \"preference\", \"subject\", \"reference\", \"structural\", \"difference\", \"capture\", \"interpretation\", \"german\", \"role\", \"matching\", \"distinction\", \"japanese\", \"factor\", \"ambiguity\", \"identify\", \"sentence\", \"account\", \"determining\", \"element\", \"view\", \"grammatical\", \"relationship\", \"help\", \"ability\", \"relation\", \"phenomenon\", \"syntactic\", \"function\", \"text\", \"structure\", \"analysis\", \"between\", \"processing\", \"a\", \"which\", \"propose\", \"it\", \"semantic\", \"order\", \"by\", \"their\", \"information\", \"these\", \"model\", \"this_paper\", \"from\", \"be\", \"based\", \"or\", \"present\", \"system\", \"not\", \"context_free\", \"parse\", \"parser\", \"head\", \"parsing\", \"free\", \"grammar\", \"lexicalized\", \"typed\", \"formalism\", \"driven\", \"feature_structure\", \"abstract\", \"coverage\", \"down\", \"dependency\", \"unification\", \"top\", \"machine_translation\", \"probabilistic\", \"par\", \"hpsg\", \"constituent\", \"novel\", \"style\", \"wide\", \"complexity\", \"implementation\", \"fast\", \"development\", \"machine\", \"context\", \"structure\", \"translation\", \"sentence\", \"based\", \"feature\", \"algorithm\", \"syntactic\", \"from\", \"language\", \"system\", \"a\", \"by\", \"using\", \"which\", \"this_paper\", \"present\", \"based_on\", \"it\", \"database\", \"decision_tree\", \"integrated\", \"average\", \"recall\", \"speech_recognition\", \"recognition\", \"rate\", \"search\", \"morphological\", \"retrieval\", \"mapping\", \"phonological\", \"precision\", \"error\", \"decision\", \"processor\", \"module\", \"interface\", \"speech\", \"indicate\", \"architecture\", \"scheme\", \"par\", \"heuristic\", \"spoken_language\", \"best\", \"dependent\", \"correction\", \"top\", \"very\", \"tree\", \"language\", \"processing\", \"spoken\", \"system\", \"algorithm\", \"our\", \"from\", \"be\", \"can\", \"which\", \"based\", \"application\", \"it\", \"corpus\", \"used\", \"analysis\", \"information\", \"result\", \"can_be\", \"present\", \"been\", \"approach\", \"by\", \"agent\", \"planning\", \"plan\", \"strategy\", \"control\", \"environment\", \"solving\", \"dialogue\", \"goal\", \"effective\", \"partial\", \"evaluating\", \"situation\", \"evidence\", \"furthermore\", \"basic\", \"requirement\", \"might\", \"relative\", \"interaction\", \"making\", \"resource\", \"contribution\", \"choice\", \"behavior\", \"reference\", \"flexible\", \"we_argue\", \"effect\", \"apply\", \"task\", \"domain\", \"case\", \"different\", \"order\", \"provides\", \"where\", \"problem\", \"study\", \"context\", \"their\", \"it\", \"system\", \"decision\", \"approach\", \"show\", \"can\", \"present\", \"from\", \"by\", \"which\", \"a\", \"this_paper\", \"our\", \"be\", \"noun_phrase\", \"graph\", \"speaker\", \"article\", \"hybrid\", \"cost\", \"network\", \"nlp\", \"statistic\", \"extraction\", \"noun\", \"doe_not\", \"document\", \"achieve\", \"variation\", \"japanese\", \"minimum\", \"even\", \"high\", \"doe\", \"independent\", \"produced\", \"boundary\", \"related\", \"fully\", \"number\", \"evaluated\", \"bound\", \"quality\", \"test\", \"phrase\", \"statistical\", \"domain\", \"dialogue\", \"system\", \"spoken\", \"algorithm\", \"new\", \"information\", \"method\", \"we_describe\", \"using\", \"text\", \"be\", \"by\", \"language\", \"it\", \"show\", \"can\", \"result\", \"our\", \"natural\", \"which\", \"problem\", \"a\", \"this_paper\", \"corpus\", \"use\", \"present\", \"used\", \"lexicon\", \"categorial\", \"derivation\", \"categorial_grammar\", \"adjoining\", \"tree_adjoining\", \"bilingual\", \"lexical\", \"construction\", \"transfer\", \"dictionary\", \"argument\", \"morphology\", \"definition\", \"entry\", \"meaning\", \"functional\", \"acquisition\", \"require\", \"handle\", \"hierarchy\", \"multi\", \"providing\", \"treatment\", \"constituent\", \"extended\", \"operation\", \"semantic\", \"linear\", \"via\", \"tag\", \"order\", \"rule\", \"level\", \"tree\", \"morphological\", \"semantics\", \"linguistic\", \"form\", \"representation\", \"a\", \"grammar\", \"be\", \"this_paper\", \"based\", \"which\", \"can\", \"by\", \"it\", \"translation\", \"two\", \"knowledge\", \"approach\", \"used\", \"can_be\", \"language\", \"structure\", \"word\", \"model\", \"finite_state\", \"transducer\", \"finite\", \"automaton\", \"clause\", \"formula\", \"generalization\", \"state\", \"string\", \"incremental\", \"least\", \"program\", \"theory\", \"computer\", \"offer\", \"represented\", \"optimal\", \"solution\", \"scheme\", \"action\", \"efficient\", \"change\", \"capture\", \"correction\", \"computationally\", \"condition\", \"because\", \"perspective\", \"distance\", \"minimum\", \"directly\", \"advantage\", \"example\", \"time\", \"two\", \"any\", \"learning\", \"all\", \"class\", \"by\", \"one\", \"it\", \"result\", \"this_paper\", \"constraint\", \"problem\", \"be\", \"which\", \"set\", \"speech\", \"system\", \"from\", \"based\", \"show\"], \"Freq\": [534.0, 250.0, 289.0, 736.0, 543.0, 357.0, 109.0, 183.0, 146.0, 258.0, 115.0, 825.0, 263.0, 302.0, 548.0, 79.0, 79.0, 142.0, 128.0, 70.0, 133.0, 283.0, 272.0, 650.0, 394.0, 223.0, 336.0, 122.0, 175.0, 110.0, 28.27775401442438, 28.470115138763074, 15.066052489851971, 14.01032329213774, 11.555139111406964, 29.11752489177097, 13.04327559901922, 17.893036068349318, 9.139812603401959, 16.079556784065172, 9.995879696088082, 7.9412760295660085, 76.44602772082206, 24.64392878053665, 11.964092159846905, 244.30848935257046, 9.26091269138081, 11.120849182499342, 31.80781361353073, 28.9135190273034, 19.623148608227655, 32.83813324201243, 7.59037828613189, 20.235304310283578, 7.3941355817502386, 11.694419265214364, 6.9902386426452825, 104.49063252199191, 28.21551062508382, 16.68359072812859, 41.60640449276303, 45.41759273584219, 127.21038793269346, 51.66897189042251, 176.70037841106006, 63.061230017554884, 86.47237672991591, 18.92925196713972, 51.08654104727235, 65.53991689892031, 49.640344765498796, 37.205133702553894, 34.383380080687466, 39.71684551274924, 60.43131658387906, 57.05855304894804, 29.083329673826974, 55.47143748171974, 50.539897306016385, 37.050277557918676, 37.88687435920018, 33.05579490243638, 38.11900835270564, 34.155571068690534, 37.51130604897616, 34.571958966295156, 32.689436258718786, 44.699314918206994, 99.13485184458182, 73.99833323118536, 53.32801816434908, 61.898032300439496, 111.11046766789589, 81.28776521766335, 36.337439616751396, 32.2150943672724, 403.04217414413455, 21.734818708491318, 18.086926845923298, 71.84215674394622, 22.628785330792848, 35.07570971402066, 21.92009367182884, 19.374633210680933, 20.882405829094793, 95.16542589723113, 25.169566757391713, 34.06788220659446, 19.184955011972946, 19.955502098830948, 61.10023004627212, 31.16749208133978, 17.12165479357017, 18.612922442959814, 49.42466940201548, 31.257519619266724, 25.282300567085333, 218.70241597275546, 159.5954283777852, 94.94003946548087, 160.8104921043527, 63.359309475514955, 266.24390948050063, 101.94585438151871, 202.60342333939838, 45.39827838796315, 95.28661516052931, 106.22928545425779, 101.05234145483561, 133.19844304577458, 99.15404078136982, 75.90767064753743, 134.58380022878467, 79.48528983455371, 123.11146179138204, 84.78210976785229, 64.08926751962504, 73.18277476323556, 78.8925818786343, 76.73949529734385, 76.41041315019348, 73.06722087709264, 66.79238123849883, 28.904870908614935, 115.85577108165445, 39.29588061882948, 32.09160361421343, 33.32887177278912, 45.78244212949617, 27.599408317947034, 26.527671723140617, 86.26605960938792, 34.00481758277775, 23.54763011290504, 18.270457948275297, 31.95433914176196, 19.91511969171421, 17.981075359059446, 33.33857478023705, 17.835886829892633, 26.966741724723413, 30.059610016500358, 11.648030561167396, 12.84194026241092, 54.8910621211837, 17.108256359978203, 130.34207989740582, 119.99376104274516, 24.978094698257426, 10.797050262393629, 132.3215171889544, 13.827570094920098, 23.854126683611845, 64.28400519187434, 136.21730835654077, 218.85892819650243, 176.94692817716023, 43.02865016761115, 46.80964948759073, 42.1514887854499, 181.61026725036453, 152.6992670838803, 133.63832900985284, 63.82025373463745, 62.24822785707969, 56.45695874087214, 99.59043229274216, 67.89608343282467, 59.910425100579864, 85.72118562152772, 86.72141552853492, 65.7906615635589, 58.503318653410126, 50.97662961017451, 53.7813138263138, 61.2411753372406, 50.12678245151104, 50.86949636338312, 50.47697028993205, 49.39217484965875, 59.45990640369183, 241.1519085158324, 37.37042073405963, 31.910223718152576, 38.04421229705542, 46.33694733799757, 25.388134121832895, 17.104589607471365, 20.231126646476252, 20.23256973647827, 15.43064036516609, 13.67243698571988, 52.799000415621144, 25.55010110149758, 22.0882718841856, 13.664512220285088, 12.736795671770594, 26.59095116720922, 15.837715570194167, 44.766348716207354, 13.360229661353912, 117.5078959953277, 41.26577620121416, 10.082178873016728, 11.054427894887889, 20.784253566326882, 23.752571990799435, 15.663223975713265, 9.554412272645363, 8.888732740471283, 33.27912911040273, 16.830803025712004, 51.349147457918995, 31.911183739891204, 85.23884427930128, 78.54509576410847, 70.1516929659495, 53.96486426754546, 62.21913729106103, 118.60519472779188, 87.59101983470387, 27.829070413238643, 87.78658910391758, 51.40891339723972, 44.542195869813895, 81.24233492927247, 36.433005367200025, 54.11132567317367, 47.453709441124516, 68.26547649670592, 55.7476124066022, 56.45283703805289, 60.268813382914395, 52.19493883922695, 41.094299197288315, 42.57150360235916, 42.10993493408763, 40.193988364887296, 62.52372038808753, 53.776384983371365, 163.0318146411784, 55.02483780653339, 237.9252514138825, 82.2985096743894, 408.3452588149477, 24.941711641630906, 24.24029526510707, 78.30477117550014, 33.402140641551775, 31.465883646458227, 39.33598093226473, 31.84235449307219, 20.82572652857226, 43.90476156759421, 49.87767709033373, 18.53615847663655, 36.768978452911526, 38.47336872254775, 16.357626082053663, 31.70291970531967, 18.64050004325127, 13.935892792483031, 10.854862972577433, 18.483371629759905, 29.02376005288864, 34.620078716011385, 8.327117744032623, 21.97209481801432, 63.51357772775199, 77.08055205292274, 102.14434789196386, 59.31306589269809, 67.57099479739401, 123.99261673284025, 52.1677477941362, 84.43966661233917, 41.817658607271554, 71.08728963066301, 74.71178762901059, 67.10951277879622, 70.03373691851209, 66.4062370104087, 50.90963957467006, 52.64152868180814, 49.825003655159605, 44.65166583784311, 41.60410017097789, 40.9782103877598, 76.7719576264181, 38.078558638977505, 24.741748947384657, 18.026230234731912, 24.22454208723215, 31.687513936335368, 76.07061723762692, 27.077108013770303, 70.94669703920769, 64.89342633346068, 27.6291020255479, 18.854264416091084, 23.97413697402944, 18.739642533729207, 52.357876802309654, 51.35652925542734, 16.260406347918167, 27.35869123693095, 28.15273888354721, 137.86339780124803, 11.67679533192824, 33.142534172354615, 19.711131944226292, 15.745842219665453, 22.180827561556097, 17.945351221105323, 23.175820298959668, 12.894142351737244, 20.690527272387772, 14.666461678125561, 34.66647986385518, 66.07456876166883, 144.44004081332892, 65.06889927599993, 29.87345810378462, 103.54427427408478, 88.76740961647215, 50.95400971936448, 67.85237266179439, 75.8297972489754, 67.71843711547992, 68.19501440933318, 69.84243610972396, 39.83625939728578, 64.80085552859985, 45.06063125544087, 45.820585154270866, 43.48193729297093, 44.89032960177192, 44.05544074752408, 43.975830189514305, 41.74383006642972, 38.75889362553461, 40.698642131456324, 39.15917143432737, 108.73572158431669, 71.90995360050198, 61.386474335141564, 88.38897955592824, 39.48581050145577, 22.537310621460914, 17.59818441051598, 80.45226933734455, 26.55883289568406, 20.298893883664515, 21.308935634028586, 9.530248869648373, 10.498238876279654, 14.26455760697546, 7.766648753690921, 10.554103306631314, 13.433852330994734, 9.084551835698775, 7.98548688812968, 14.00693580341881, 6.932700359999543, 14.206469452463478, 6.233767538434257, 7.863578810097368, 7.772695363645134, 7.979164472387525, 6.9296114631587145, 8.908699729100453, 9.95548252195803, 6.950613018041486, 33.00775838506182, 40.07411000642204, 26.762447958453777, 23.645425326461744, 30.305468307977748, 13.97529556820068, 14.848083304104039, 42.41821859377687, 14.405268472634466, 27.895629634046887, 20.75866661160849, 49.68524420272959, 44.95112168230341, 15.88581313992677, 31.403909972861328, 25.273181173088037, 34.14316954410084, 22.53570058117678, 25.053609049696217, 25.945312839176303, 24.777805821436516, 24.941016296153634, 21.896742400116175, 19.929356362583018, 20.413064681460146, 30.70831288778573, 49.44585429476467, 24.387889485882496, 22.150193243773902, 21.233492401305472, 22.32570146594594, 59.55833260593223, 32.614060079610496, 15.736284660223763, 30.886405001103345, 51.23154701358963, 16.182653578215945, 27.71367951533371, 15.225301237651234, 15.250080753737297, 27.071744489503434, 17.002520516112806, 21.799101005289554, 17.53040603196559, 19.164319262305376, 22.9253089096235, 9.757412048101315, 14.023016687615172, 18.60166146200954, 15.641917510399168, 54.28430415606432, 13.066057098504587, 11.775282028406018, 13.462492320108533, 34.92582959048634, 42.39130090836296, 67.57857913351658, 75.15865463666994, 49.636631591012964, 181.01999961399284, 32.36463878992224, 119.31662393465002, 57.820689926850385, 80.69407985038879, 85.4899196241038, 30.78461002553227, 69.65997580022935, 68.7421776653211, 105.06294328446937, 103.29287449410465, 107.24593166064976, 96.14143552530845, 55.16430168610691, 73.57468113179772, 56.62550246449417, 51.03568806475505, 49.56939185302871, 66.34513974664128, 55.423596885159455, 70.4989411388802, 58.5463361354209, 45.92196077637042, 43.348808282396405, 45.497696185831586, 44.97825827143073, 105.60187644398373, 39.60774538999404, 32.521066780886265, 26.196545150932053, 28.318451165640905, 25.214053731584155, 28.80669598188946, 166.45169427128616, 44.60985621805154, 21.866822540348295, 55.51946008949138, 19.19829535523542, 29.908680630195256, 42.36229041431304, 21.34278462800388, 39.184244013873695, 20.245090301300824, 31.030985236468783, 9.772140906068831, 13.509938260034476, 17.32762808643973, 20.692172901592713, 11.582705588049693, 17.351263716863492, 17.122440346860266, 16.511521867596134, 15.00642886096213, 105.54806710608509, 34.19892761864081, 8.179931831166263, 35.58956386080709, 67.42089926547986, 75.00583965909996, 47.01886558012945, 55.048973473963706, 35.927439122737205, 36.339748355546064, 48.87826609132492, 42.02642352032159, 50.86462281481695, 118.28613143032155, 85.1524440317092, 95.01710587858743, 77.5064007534658, 89.7003901199016, 82.3461197999311, 75.69654412999581, 82.59427448857622, 80.2747400944162, 46.110700873620225, 50.11237710035144, 46.939653185842445, 56.420570636785676, 51.03572270569121, 49.497347062884685, 54.35014226345224, 44.57531291129213, 46.036173634880555, 43.969982731361995, 56.37180492602702, 39.37433545519764, 75.8514796660399, 28.608029304497595, 50.57246332254277, 22.932141173096845, 21.200453577708032, 75.27286232546294, 29.30275208156025, 19.12144829355907, 11.74355280394853, 40.83658159108281, 96.54492040600864, 15.853406224410014, 8.582763730601835, 8.576581831490412, 8.533690506080733, 15.690321915399617, 12.38921125829532, 7.537199768414176, 29.772657339530582, 9.904132324529437, 6.766210774960859, 13.282418396041376, 5.53267143780271, 7.429381103351587, 10.579263846769672, 4.7678356149075425, 13.022592504706596, 8.691446680027692, 8.765995252634616, 11.25260691541176, 23.15526087515346, 30.381767409328454, 37.01776540963635, 15.948606985172917, 46.01811201477494, 19.168184389577256, 22.23441767841341, 52.93393097949591, 26.17542813112947, 45.993303492827074, 30.640062190162013, 34.66311167996665, 25.86203926841272, 29.002401935754513, 34.52037851624412, 31.571851370326737, 23.952725051272015, 23.177105905211548, 26.84281751057628, 25.05005058316315, 24.127676275921882, 22.356471139309217], \"Total\": [534.0, 250.0, 289.0, 736.0, 543.0, 357.0, 109.0, 183.0, 146.0, 258.0, 115.0, 825.0, 263.0, 302.0, 548.0, 79.0, 79.0, 142.0, 128.0, 70.0, 133.0, 283.0, 272.0, 650.0, 394.0, 223.0, 336.0, 122.0, 175.0, 110.0, 37.27577655734763, 50.20614655223739, 28.03726595959403, 27.886367787867883, 24.10844505191156, 67.73122630178183, 33.04951692266487, 45.849604294580345, 23.8889342606139, 42.056787060282865, 26.988952614801452, 21.892947927632026, 217.65904172158653, 71.45608383175609, 35.75373403855679, 736.6793965931817, 28.81712662105153, 35.5643622706674, 102.36933053240394, 93.30748312641943, 63.50686715457234, 106.44576425654625, 24.76322792731594, 66.32693241329086, 24.771558079858895, 39.51291173497138, 23.869907053586683, 357.18571728191336, 97.44779045006561, 59.57155355269071, 157.70621270078593, 175.70408553947556, 548.3322327494344, 205.23016321128023, 825.2767948844207, 264.6201432031979, 394.5245968404272, 68.90166069532444, 290.3527532085785, 451.96105991941596, 301.34742158551614, 198.3080571188936, 176.78284946204053, 236.0970559849689, 565.1479482649321, 506.9446824452824, 138.70160950149196, 741.2198061786082, 626.4690578890687, 287.9902719500448, 368.26275552047065, 272.9453168256028, 650.411650419115, 345.5624510952468, 771.1432320985787, 712.7632481903408, 427.319278580314, 45.715941922202894, 103.34479667616267, 78.473038271659, 56.64820351014096, 67.52134850328031, 127.0978088063434, 96.3821127194547, 43.673153889630804, 40.72289566219455, 543.3419665930404, 29.819084304692264, 24.913850305305782, 101.10078210356876, 31.877628584481542, 53.477318924182114, 34.835554356190485, 30.7985991870472, 33.62546760584317, 154.51609904891882, 42.056787060282865, 57.092213308986175, 33.04951692266487, 34.69764917935851, 107.27505873138976, 54.738775599260755, 30.790382770011337, 33.56509475638145, 89.19682022578557, 56.67801525369562, 46.61748987971135, 427.319278580314, 312.99128025033934, 194.57016085609257, 357.18571728191336, 129.7615398176358, 736.6793965931817, 264.6201432031979, 675.9549390145351, 99.09100074927261, 302.61726496684577, 364.1850371227228, 342.52189976151726, 565.1479482649321, 368.26275552047065, 255.55709965300025, 741.2198061786082, 278.4140876174588, 771.1432320985787, 451.96105991941596, 235.43459999189835, 394.5245968404272, 712.7632481903408, 626.4690578890687, 731.790628180017, 506.9446824452824, 585.0311971923641, 31.997606352889825, 133.7255026156252, 47.94096889828045, 44.76923303660658, 49.71428475952859, 70.59930385256229, 43.81505364572168, 42.88309173519107, 141.22316045419706, 57.83519506067504, 40.78150453937822, 31.721794518822826, 55.69248950075285, 34.75121044799198, 31.969149842244597, 60.551799199795454, 32.942006590459, 49.820094273520034, 55.6542255115347, 22.80101262448536, 25.806414625390083, 110.46112450124465, 34.8637672250965, 266.37079537175714, 246.7354371653927, 52.5547555695137, 22.92217002919987, 283.4404648707678, 29.84254798397162, 53.54272546718315, 149.71726155407896, 368.0681154403006, 731.790628180017, 585.0311971923641, 110.89850168926077, 123.4376615012014, 108.1185592926672, 825.2767948844207, 771.1432320985787, 712.7632481903408, 236.28933282238813, 237.11205789393526, 202.17229806119119, 741.2198061786082, 336.39947286110555, 264.11977861587536, 626.4690578890687, 650.411650419115, 336.25130494460285, 258.63021781707613, 203.27383304646176, 278.4140876174588, 565.1479482649321, 235.0677186681524, 261.3740628929714, 290.3527532085785, 249.48978778548505, 60.53703979864072, 250.03833358831525, 40.07241909002456, 37.214530630966976, 47.011661371632336, 64.71631664718235, 36.27173694132441, 28.458322753090446, 35.38337021808594, 38.379175785476285, 29.41808862109157, 27.398106830851134, 108.93570901921379, 53.909729738878056, 47.26185578978638, 29.418968116495098, 28.637056438608294, 60.84544177710327, 36.57419352894843, 104.39862195112045, 31.40457267882277, 283.7859984789241, 101.37568813261068, 25.59017914443868, 28.366711941463343, 54.32879172054398, 62.65422578370856, 43.379359093729924, 26.485674778702013, 24.76322792731594, 93.49830076773587, 48.21951034607449, 172.02824589006013, 104.32695447936831, 364.1850371227228, 336.39947286110555, 301.34742158551614, 235.43459999189835, 290.3527532085785, 771.1432320985787, 626.4690578890687, 90.86314221570508, 712.7632481903408, 272.9453168256028, 211.96348632026942, 741.2198061786082, 147.25836850400069, 364.4553617391933, 287.9902719500448, 736.6793965931817, 506.9446824452824, 565.1479482649321, 731.790628180017, 675.9549390145351, 258.63021781707613, 345.5624510952468, 650.411650419115, 235.0677186681524, 63.5194620835886, 57.68613307477577, 183.85864400228382, 64.7165729875054, 289.6741427737804, 102.56163436735238, 534.7904639384728, 34.2297388725033, 33.37287021636727, 122.23350781625984, 53.25673482614331, 50.24465079012082, 64.13974754898683, 53.94995376681991, 35.346007606857405, 76.64849032949118, 94.32798953029368, 38.28599234433601, 76.49722403500029, 85.83606806296065, 37.34522788432031, 74.01828159060094, 44.02920847715902, 33.56468569511135, 26.632215630278484, 51.277869056868376, 80.98636677631714, 99.64075150171381, 24.563206987607423, 65.11590688741875, 191.80251179112187, 235.0598952098677, 336.39947286110555, 200.48962266439648, 283.7859984789241, 675.9549390145351, 236.28933282238813, 548.3322327494344, 172.02824589006013, 565.1479482649321, 825.2767948844207, 650.411650419115, 771.1432320985787, 741.2198061786082, 368.26275552047065, 626.4690578890687, 506.9446824452824, 345.5624510952468, 255.55709965300025, 712.7632481903408, 93.66214510168794, 48.46736626566073, 32.587233438504, 25.661593937460616, 34.56038443794197, 46.52730531361436, 117.37439347414963, 42.408805105896526, 122.61040636961438, 115.18625949572059, 49.311516993059726, 34.61652339945483, 45.21821619821955, 35.56995327075361, 100.34586035112909, 99.17671169297364, 33.5156301364067, 57.16974389195239, 60.06356165527802, 302.61726496684577, 25.73288134997635, 76.10337001647488, 46.11946735303425, 37.34522788432031, 54.42093670781618, 45.849604294580345, 60.2606074429722, 33.584977795323915, 53.95771281991642, 38.28599234433601, 90.84361549272751, 198.85565241957696, 825.2767948844207, 290.3527532085785, 97.44779045006561, 650.411650419115, 548.3322327494344, 294.7831738403678, 565.1479482649321, 731.790628180017, 585.0311971923641, 626.4690578890687, 675.9549390145351, 196.78475081387558, 712.7632481903408, 312.99128025033934, 336.25130494460285, 301.34742158551614, 364.4553617391933, 342.52189976151726, 368.0681154403006, 345.5624510952468, 221.44978796014865, 451.96105991941596, 741.2198061786082, 109.85243272753037, 79.96223217334435, 70.74038424575399, 115.65162955446955, 54.716924784755534, 33.850842996338685, 28.858929371584868, 146.976780135169, 51.910511242396545, 41.81563728876561, 53.21405148858117, 27.758912201502152, 31.969149842244597, 45.52905195305454, 25.909834775939753, 35.47898559919452, 48.820558136207275, 33.637758292002, 29.89166850625378, 53.5037997522656, 26.72427971888256, 55.604774185710546, 25.729681340465525, 32.64792508465889, 32.763755324770116, 35.38337021808594, 31.55061445763449, 40.65062151892986, 46.40317157877338, 32.78074756718168, 175.70408553947556, 223.81245217475006, 147.80899006348133, 149.5304648391073, 211.96348632026942, 76.0480844397179, 83.10227300382411, 394.5245968404272, 80.13478533404071, 235.0598952098677, 147.25836850400069, 712.7632481903408, 650.411650419115, 99.17671169297364, 451.96105991941596, 281.9820107607953, 585.0311971923641, 345.5624510952468, 565.1479482649321, 741.2198061786082, 626.4690578890687, 771.1432320985787, 506.9446824452824, 294.7831738403678, 731.790628180017, 39.46546893289319, 71.08707786507979, 41.39709217956416, 38.549806259993176, 37.63627749277632, 39.57342459935854, 106.44576425654625, 58.44093020457456, 28.68435174406101, 59.06259569625895, 107.65234420105325, 34.36162812042094, 61.06696953476288, 33.6049513805184, 33.70555120469663, 60.84544177710327, 38.343243252033105, 49.28958990451703, 41.52050219127297, 46.24635741833788, 55.34671249507301, 23.774409395742442, 34.484080196996, 47.42970871228434, 41.38345571000612, 144.11619421381977, 34.69764917935851, 31.77382588141476, 36.56217697977908, 96.13998973995464, 119.16886867490712, 194.57016085609257, 223.81245217475006, 146.976780135169, 650.411650419115, 97.44779045006561, 548.3322327494344, 215.95378979835183, 364.4553617391933, 427.319278580314, 97.13325112095981, 368.26275552047065, 364.1850371227228, 731.790628180017, 741.2198061786082, 825.2767948844207, 712.7632481903408, 281.9820107607953, 585.0311971923641, 342.52189976151726, 294.7831738403678, 283.4404648707678, 626.4690578890687, 394.5245968404272, 771.1432320985787, 506.9446824452824, 312.99128025033934, 236.0970559849689, 345.5624510952468, 336.25130494460285, 128.87182810342924, 49.04246948480436, 43.11254252821299, 35.30174813786528, 38.2424344261262, 35.29744903439649, 41.19700582831842, 258.94444195419675, 69.59932775613629, 35.34008165147476, 92.64422228053006, 32.39628375300665, 51.281455810126225, 72.96334975704552, 37.427230216751035, 75.81042498017305, 41.23778363236634, 67.73122630178183, 21.636114655111445, 31.460207255640082, 41.61550044965327, 52.35289776028154, 29.464492949211937, 44.25428254138625, 44.02920847715902, 42.513998092263336, 38.653640223010264, 272.9453168256028, 89.64443493916653, 21.6824282658461, 95.78006524711569, 211.96348632026942, 253.16761808133498, 147.77034276768418, 198.85565241957696, 115.18625949572059, 123.4376615012014, 188.5328932386276, 153.90776776982747, 205.23016321128023, 771.1432320985787, 534.7904639384728, 731.790628180017, 506.9446824452824, 675.9549390145351, 626.4690578890687, 585.0311971923641, 741.2198061786082, 712.7632481903408, 200.48962266439648, 255.64218664974467, 237.11205789393526, 451.96105991941596, 336.25130494460285, 368.0681154403006, 825.2767948844207, 336.39947286110555, 543.3419665930404, 736.6793965931817, 57.93997554656886, 40.55462546041294, 79.21994094629699, 36.94441683140908, 80.68854064223024, 42.11550263760281, 39.969169233479256, 142.21546709454262, 56.60763780621401, 41.82955863956137, 29.49506366970312, 110.46112450124465, 263.8617060928243, 46.25510721224012, 27.600557062249884, 28.388101260037562, 29.493758973901343, 57.93546593931837, 46.11946735303425, 28.54148118234863, 117.8142364750732, 39.51291173497138, 27.398106830851134, 53.95771281991642, 22.574030634119964, 31.49277069490748, 45.22440513595535, 20.622295403818153, 57.092213308986175, 38.343243252033105, 40.19040778624741, 53.1704442994632, 121.46403191744162, 176.78284946204053, 255.64218664974467, 84.76637872729466, 357.18571728191336, 113.40683427546668, 157.70621270078593, 741.2198061786082, 224.39927008672998, 712.7632481903408, 342.52189976151726, 506.9446824452824, 266.37079537175714, 394.5245968404272, 731.790628180017, 626.4690578890687, 278.4140876174588, 302.61726496684577, 650.411650419115, 565.1479482649321, 675.9549390145351, 281.9820107607953], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.215000152587891, -5.208199977874756, -5.844600200653076, -5.917200088500977, -6.109899997711182, -5.185699939727783, -5.988800048828125, -5.672599792480469, -6.344399929046631, -5.7795000076293945, -6.254899978637695, -6.485000133514404, -4.2204999923706055, -5.352499961853027, -6.075099945068359, -3.0585999488830566, -6.331200122833252, -6.148200035095215, -5.097300052642822, -5.192699909210205, -5.5802998542785645, -5.065499782562256, -6.530200004577637, -5.549600124359131, -6.556399822235107, -6.097899913787842, -6.612500190734863, -3.907900094985962, -5.217199802398682, -5.742599964141846, -4.828800201416016, -4.741099834442139, -3.711199998855591, -4.612199783325195, -3.3826000690460205, -4.412899971008301, -4.0971999168396, -5.616300106048584, -4.623499870300293, -4.3744001388549805, -4.652200222015381, -4.9405999183654785, -5.019499778747559, -4.87529993057251, -4.45550012588501, -4.513000011444092, -5.1869001388549805, -4.541200160980225, -4.634300231933594, -4.944799900054932, -4.922399997711182, -5.058800220489502, -4.916299819946289, -5.026100158691406, -4.932400226593018, -5.013999938964844, -5.070000171661377, -5.4355998039245605, -4.639100074768066, -4.931600093841553, -5.259099960327148, -5.110099792480469, -4.525100231170654, -4.837600231170654, -5.6427998542785645, -5.763199806213379, -3.236599922180176, -6.156700134277344, -6.340400218963623, -4.961100101470947, -6.116399765014648, -5.678100109100342, -6.148200035095215, -6.271599769592285, -6.196700096130371, -4.679999828338623, -6.010000228881836, -5.707200050354004, -6.281499862670898, -6.242099761962891, -5.1230998039245605, -5.796199798583984, -6.395299911499023, -6.311699867248535, -5.33519983291626, -5.793300151824951, -6.005499839782715, -3.847899913787842, -4.163000106811523, -4.682400226593018, -4.155399799346924, -5.0868000984191895, -3.65120005607605, -4.611199855804443, -3.9244000911712646, -5.420100212097168, -4.678699970245361, -4.570000171661377, -4.619999885559082, -4.343800067901611, -4.638899803161621, -4.906099796295166, -4.333399772644043, -4.860000133514404, -4.422500133514404, -4.795499801635742, -5.075300216674805, -4.942599773406982, -4.867499828338623, -4.895199775695801, -4.899499893188477, -4.944200038909912, -5.033999919891357, -5.684800148010254, -4.296500205993652, -5.377699851989746, -5.5802001953125, -5.542399883270264, -5.224899768829346, -5.730999946594238, -5.770599842071533, -4.591400146484375, -5.522299766540527, -5.889800071716309, -6.143499851226807, -5.584499835968018, -6.057300090789795, -6.1595001220703125, -5.542099952697754, -6.167600154876709, -5.754199981689453, -5.645599842071533, -6.593699932098389, -6.496099948883057, -5.043499946594238, -6.209199905395508, -4.178599834442139, -4.26140022277832, -5.8308000564575195, -6.66949987411499, -4.163599967956543, -6.422100067138672, -5.8769001960754395, -4.885499954223633, -4.1346001625061035, -3.660399913787842, -3.872999906539917, -5.286900043487549, -5.202700138092041, -5.307499885559082, -3.84689998626709, -4.020299911499023, -4.15369987487793, -4.8927001953125, -4.917699813842773, -5.0152997970581055, -4.447700023651123, -4.8308000564575195, -4.955999851226807, -4.597700119018555, -4.586100101470947, -4.862299919128418, -4.979700088500977, -5.117400169372559, -5.063899993896484, -4.934000015258789, -5.134300231933594, -5.119500160217285, -5.127299785614014, -5.14900016784668, -4.7042999267578125, -3.3041999340057373, -5.168700218200684, -5.326700210571289, -5.150899887084961, -4.953700065612793, -5.555300235748291, -5.950300216674805, -5.782400131225586, -5.782299995422363, -6.053299903869629, -6.174200057983398, -4.8231000900268555, -5.548999786376953, -5.6946001052856445, -6.174799919128418, -6.245100021362305, -5.508999824523926, -6.027200222015381, -4.9882001876831055, -6.197299957275391, -4.023099899291992, -5.0696001052856445, -6.478799819946289, -6.3867998123168945, -5.75540018081665, -5.6219000816345215, -6.038300037384033, -6.532599925994873, -6.604800224304199, -5.2846999168396, -5.966400146484375, -4.85099983215332, -5.326700210571289, -4.344200134277344, -4.425899982452393, -4.539000034332275, -4.801300048828125, -4.658999919891357, -4.013800144195557, -4.31689977645874, -5.463500022888184, -4.314700126647949, -4.849800109863281, -4.993199825286865, -4.392199993133545, -5.1940999031066895, -4.798600196838379, -4.929900169372559, -4.566199779510498, -4.768799781799316, -4.756199836730957, -4.690800189971924, -4.83459997177124, -5.073699951171875, -5.038400173187256, -5.049300193786621, -5.095900058746338, -4.566199779510498, -4.716899871826172, -3.607800006866455, -4.693999767303467, -3.22979998588562, -4.291399955749512, -2.68969988822937, -5.485199928283691, -5.513800144195557, -4.34119987487793, -5.19320011138916, -5.252900123596191, -5.029600143432617, -5.241000175476074, -5.665599822998047, -4.9197998046875, -4.792200088500977, -5.782100200653076, -5.097099781036377, -5.051799774169922, -5.907100200653076, -5.2453999519348145, -5.776400089263916, -6.067299842834473, -6.317200183868408, -5.784900188446045, -5.333700180053711, -5.157299995422363, -6.582300186157227, -5.611999988555908, -4.55049991607666, -4.356900215148926, -4.075399875640869, -4.618899822235107, -4.48859977722168, -3.8815999031066895, -4.747300148010254, -4.265699863433838, -4.968500137329102, -4.437900066375732, -4.3881001472473145, -4.4953999519348145, -4.4527997970581055, -4.50600004196167, -4.771699905395508, -4.73829984664917, -4.793300151824951, -4.902900218963623, -4.973599910736084, -4.988699913024902, -4.355400085449219, -5.056600093841553, -5.48769998550415, -5.8043999671936035, -5.508800029754639, -5.240300178527832, -4.364500045776367, -5.397500038146973, -4.434299945831299, -4.523499965667725, -5.377299785614014, -5.759500026702881, -5.519199848175049, -5.765600204467773, -4.738100051879883, -4.757400035858154, -5.90749979019165, -5.387199878692627, -5.35860013961792, -3.76990008354187, -6.23859977722168, -5.195400238037109, -5.715000152587891, -5.939599990844727, -5.5970001220703125, -5.808899879455566, -5.553100109100342, -6.139400005340576, -5.666500091552734, -6.0106000900268555, -5.150400161743164, -4.50540018081665, -3.723299980163574, -4.5208001136779785, -5.299200057983398, -4.05620002746582, -4.21019983291626, -4.7652997970581055, -4.478899955749512, -4.367700099945068, -4.480899810791016, -4.473800182342529, -4.449999809265137, -5.01140022277832, -4.524899959564209, -4.888199806213379, -4.871500015258789, -4.923900127410889, -4.892000198364258, -4.910799980163574, -4.912600040435791, -4.964700222015381, -5.038899898529053, -4.989999771118164, -5.028600215911865, -3.2298998832702637, -3.643399953842163, -3.8015999794006348, -3.437000036239624, -4.242800235748291, -4.803599834442139, -5.051000118255615, -3.531100034713745, -4.639400005340576, -4.908199787139893, -4.8597002029418945, -5.664299964904785, -5.567599773406982, -5.261000156402588, -5.868899822235107, -5.562300205230713, -5.321000099182129, -5.712200164794922, -5.84119987487793, -5.279200077056885, -5.982500076293945, -5.265100002288818, -6.088799953460693, -5.856500148773193, -5.868199825286865, -5.8420000076293945, -5.982999801635742, -5.731800079345703, -5.620699882507324, -5.980000019073486, -4.421999931335449, -4.228099822998047, -4.631800174713135, -4.7555999755859375, -4.507500171661377, -5.281499862670898, -5.220900058746338, -4.171199798583984, -5.251200199127197, -4.5903000831604, -4.885799884796143, -4.0131001472473145, -4.1132001876831055, -5.15339994430542, -4.47189998626709, -4.689000129699707, -4.388199806213379, -4.803699970245361, -4.697800159454346, -4.662799835205078, -4.708799839019775, -4.702300071716309, -4.832499980926514, -4.926599979400635, -4.902599811553955, -5.484499931335449, -5.008200168609619, -5.714900016784668, -5.811200141906738, -5.853499889373779, -5.803299903869629, -4.8221001625061035, -5.424300193786621, -6.15310001373291, -5.478700160980225, -4.972700119018555, -6.125100135803223, -5.587100028991699, -6.186100006103516, -6.1844000816345215, -5.610499858856201, -6.075699806213379, -5.827199935913086, -6.045100212097168, -5.955999851226807, -5.776800155639648, -6.63100004196167, -6.2683000564575195, -5.985799789428711, -6.15910005569458, -4.91480016708374, -6.339000225067139, -6.442999839782715, -6.309100151062012, -5.355800151824951, -5.162099838256836, -4.695700168609619, -4.589399814605713, -5.004300117492676, -3.710400104522705, -5.432000160217285, -4.127200126647949, -4.8516998291015625, -4.518400192260742, -4.460599899291992, -5.48199987411499, -4.66540002822876, -4.678699970245361, -4.254499912261963, -4.271500110626221, -4.23390007019043, -4.343200206756592, -4.89870023727417, -4.6107001304626465, -4.872600078582764, -4.976500034332275, -5.00570011138916, -4.714200019836426, -4.894000053405762, -4.65339994430542, -4.839200019836426, -5.082099914550781, -5.139800071716309, -5.091400146484375, -5.10290002822876, -4.146699905395508, -5.127299785614014, -5.324399948120117, -5.5406999588012695, -5.462800025939941, -5.57889986038208, -5.445700168609619, -3.6916000843048096, -5.008399963378906, -5.721399784088135, -4.789599895477295, -5.851500034332275, -5.408199787139893, -5.060100078582764, -5.74560022354126, -5.1381001472473145, -5.798399925231934, -5.371300220489502, -6.526800155639648, -6.202899932861328, -5.953999996185303, -5.776599884033203, -6.356800079345703, -5.952700138092041, -5.96589994430542, -6.002299785614014, -6.097899913787842, -4.147200107574463, -5.274099826812744, -6.704599857330322, -5.234300136566162, -4.595399856567383, -4.488800048828125, -4.9558000564575195, -4.798099994659424, -5.224800109863281, -5.213399887084961, -4.916999816894531, -5.067999839782715, -4.877200126647949, -4.033199787139893, -4.3618998527526855, -4.252299785614014, -4.455999851226807, -4.309899806976318, -4.395400047302246, -4.479599952697754, -4.392399787902832, -4.420899868011475, -4.975299835205078, -4.892099857330322, -4.957499980926514, -4.773499965667725, -4.873799800872803, -4.904399871826172, -4.8109002113342285, -5.009200096130371, -4.976900100708008, -5.022799968719482, -4.121600151062012, -4.480400085449219, -3.8248000144958496, -4.799900054931641, -4.230199813842773, -5.020999908447266, -5.0995001792907715, -3.83240008354187, -4.775899887084961, -5.202700138092041, -5.690299987792969, -4.443999767303467, -3.5834999084472656, -5.390200138092041, -6.003799915313721, -6.004499912261963, -6.009500026702881, -5.4004998207092285, -5.63670015335083, -6.133699893951416, -4.760000228881836, -5.860599994659424, -6.241600036621094, -5.5671000480651855, -6.44290018081665, -6.148099899291992, -5.7947001457214355, -6.591700077056885, -5.586900234222412, -5.991199970245361, -5.982699871063232, -5.732999801635742, -5.011300086975098, -4.739699840545654, -4.542200088500977, -5.384200096130371, -4.32450008392334, -5.200300216674805, -5.0518999099731445, -4.184500217437744, -4.888700008392334, -4.325099945068359, -4.731200218200684, -4.607900142669678, -4.9008002281188965, -4.786200046539307, -4.611999988555908, -4.701300144195557, -4.977499961853027, -5.01039981842041, -4.86359977722168, -4.932700157165527, -4.970200061798096, -5.04640007019043], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2148, 1.9238, 1.87, 1.8027, 1.7557, 1.6469, 1.5613, 1.5501, 1.5303, 1.5296, 1.4978, 1.477, 1.4447, 1.4265, 1.3963, 1.3874, 1.3559, 1.3286, 1.3222, 1.3195, 1.3166, 1.315, 1.3086, 1.3039, 1.2821, 1.2736, 1.263, 1.2619, 1.2516, 1.2183, 1.1586, 1.1382, 1.03, 1.1118, 0.9498, 1.0569, 0.9732, 1.1991, 0.7535, 0.5601, 0.6876, 0.8177, 0.8537, 0.7086, 0.2555, 0.3068, 0.9289, -0.1013, -0.0263, 0.4404, 0.2169, 0.38, -0.3458, 0.1768, -0.5321, -0.535, -0.0794, 1.79, 1.7709, 1.7538, 1.7521, 1.7256, 1.6781, 1.6422, 1.6286, 1.5782, 1.5138, 1.4963, 1.4923, 1.4709, 1.4698, 1.3908, 1.3493, 1.349, 1.3361, 1.3278, 1.2991, 1.2962, 1.2686, 1.2594, 1.2496, 1.2493, 1.2257, 1.2229, 1.2221, 1.2174, 1.2007, 1.1427, 1.139, 1.095, 1.0145, 1.0956, 0.7948, 0.8587, 0.6076, 1.032, 0.6569, 0.5805, 0.5918, 0.3673, 0.5004, 0.5986, 0.1064, 0.559, -0.0223, 0.139, 0.5114, 0.1278, -0.3885, -0.2872, -0.4469, -0.1245, -0.3576, 1.8977, 1.8559, 1.8005, 1.6664, 1.5994, 1.5662, 1.5371, 1.519, 1.5064, 1.4682, 1.4501, 1.4476, 1.4438, 1.4426, 1.4239, 1.4025, 1.3858, 1.3855, 1.3833, 1.3276, 1.3014, 1.3, 1.2874, 1.2846, 1.2784, 1.2555, 1.2465, 1.2375, 1.23, 1.1908, 1.1539, 1.0053, 0.7922, 0.8035, 1.0526, 1.0297, 1.0574, 0.4855, 0.3799, 0.3253, 0.6903, 0.6619, 0.7237, -0.0079, 0.399, 0.5158, 0.0103, -0.0156, 0.3679, 0.513, 0.6161, 0.3551, -0.223, 0.454, 0.3626, 0.2497, 0.3797, 2.2406, 2.2223, 2.1887, 2.1047, 2.0469, 1.9244, 1.9018, 1.7494, 1.6995, 1.6183, 1.6133, 1.5634, 1.5342, 1.5118, 1.4979, 1.4917, 1.4483, 1.4307, 1.4216, 1.4117, 1.4038, 1.3768, 1.3597, 1.3271, 1.3161, 1.2977, 1.2886, 1.2398, 1.2389, 1.2339, 1.2255, 1.206, 1.0495, 1.0739, 0.8063, 0.8039, 0.8009, 0.7854, 0.7181, 0.3864, 0.2911, 1.0752, 0.1643, 0.589, 0.6985, 0.0476, 0.8618, 0.3511, 0.4553, -0.1202, 0.0509, -0.0452, -0.2382, -0.3026, 0.419, 0.1645, -0.4788, 0.4924, 2.3305, 2.2762, 2.2261, 2.1841, 2.1495, 2.1262, 2.0766, 2.0298, 2.0266, 1.901, 1.8798, 1.8783, 1.8574, 1.8191, 1.8174, 1.7891, 1.7091, 1.621, 1.6137, 1.5439, 1.5208, 1.4984, 1.4868, 1.4673, 1.4488, 1.326, 1.3202, 1.2892, 1.2646, 1.26, 1.2411, 1.2314, 1.1544, 1.1284, 0.9113, 0.6504, 0.8358, 0.4755, 0.932, 0.2732, -0.0557, 0.0751, -0.0525, -0.0662, 0.3676, -0.1302, 0.0265, 0.3001, 0.5311, -0.5098, 2.1531, 2.1107, 2.0765, 1.9987, 1.9966, 1.9678, 1.9182, 1.9032, 1.8048, 1.7781, 1.7726, 1.7443, 1.7174, 1.7111, 1.7014, 1.6938, 1.6286, 1.6149, 1.5942, 1.5657, 1.5617, 1.5206, 1.5019, 1.4883, 1.4544, 1.4139, 1.3963, 1.3946, 1.3934, 1.3924, 1.3885, 1.2501, 0.6091, 0.8563, 1.1696, 0.5143, 0.5311, 0.5966, 0.2322, 0.0849, 0.1956, 0.1342, 0.082, 0.7546, -0.0459, 0.4137, 0.3588, 0.416, 0.2577, 0.301, 0.2273, 0.2383, 0.6091, -0.0555, -0.5887, 3.1191, 3.0232, 2.9875, 2.8605, 2.8031, 2.7225, 2.6347, 2.5267, 2.4592, 2.4066, 2.2141, 2.0603, 2.0158, 1.9688, 1.9246, 1.9169, 1.839, 1.8203, 1.8094, 1.7891, 1.78, 1.7648, 1.7117, 1.7058, 1.6906, 1.6399, 1.6135, 1.6114, 1.5901, 1.5783, 1.4573, 1.4093, 1.4204, 1.285, 1.1843, 1.4353, 1.4071, 0.8992, 1.4132, 0.998, 1.1701, 0.4659, 0.4573, 1.2979, 0.4627, 0.7172, 0.2882, 0.3993, 0.0133, -0.223, -0.1008, -0.302, -0.0127, 0.4353, -0.45, 1.8882, 1.7761, 1.61, 1.585, 1.5667, 1.5667, 1.5584, 1.5558, 1.5387, 1.4908, 1.3965, 1.3861, 1.349, 1.3474, 1.346, 1.3292, 1.3259, 1.3232, 1.2768, 1.2582, 1.2577, 1.2485, 1.2393, 1.2031, 1.1662, 1.1627, 1.1624, 1.1465, 1.14, 1.1265, 1.1055, 1.0816, 1.0479, 1.0535, 0.8601, 1.0368, 0.614, 0.8214, 0.6314, 0.53, 0.99, 0.4739, 0.4718, 0.1982, 0.1684, 0.0985, 0.1358, 0.5076, 0.0657, 0.3392, 0.3854, 0.3955, -0.1061, 0.1764, -0.2532, -0.0195, 0.2199, 0.4441, 0.1116, 0.1274, 2.0427, 2.0281, 1.9599, 1.9435, 1.9414, 1.9054, 1.884, 1.7999, 1.797, 1.7617, 1.7298, 1.7186, 1.7026, 1.6981, 1.6801, 1.5818, 1.5304, 1.4612, 1.447, 1.3965, 1.3656, 1.3135, 1.3081, 1.3055, 1.2973, 1.296, 1.2956, 1.2917, 1.2781, 1.267, 1.2518, 1.0963, 1.0253, 1.0967, 0.9574, 1.0767, 1.019, 0.8919, 0.9437, 0.8468, 0.367, 0.4044, 0.2004, 0.3638, 0.2221, 0.2126, 0.1969, 0.0474, 0.0581, 0.7721, 0.6123, 0.6221, 0.161, 0.3565, 0.2354, -0.4785, 0.2207, -0.2265, -0.5769, 2.8671, 2.865, 2.8511, 2.6388, 2.4274, 2.2867, 2.2605, 2.2583, 2.2361, 2.1118, 1.9736, 1.8995, 1.8891, 1.8238, 1.7265, 1.6976, 1.6544, 1.5883, 1.5802, 1.5631, 1.519, 1.5109, 1.496, 1.4928, 1.4884, 1.4503, 1.4418, 1.4301, 1.4166, 1.4103, 1.3718, 1.3417, 1.2372, 1.1335, 0.9622, 1.224, 0.8453, 1.1168, 0.9355, 0.2553, 0.746, 0.1539, 0.4805, 0.2118, 0.5625, 0.2843, -0.1594, -0.0933, 0.4415, 0.3253, -0.293, -0.2216, -0.4382, 0.3598]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 8, 9, 1, 2, 3, 4, 5, 7, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 6, 8, 10, 1, 2, 4, 6, 8, 10, 1, 2, 3, 5, 8, 9, 1, 2, 3, 4, 6, 8, 10, 5, 9, 1, 2, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 3, 4, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 8, 9, 10, 1, 4, 5, 7, 9, 2, 5, 6, 7, 8, 9, 1, 3, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 9, 10, 1, 4, 5, 6, 8, 1, 2, 3, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 10, 2, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 9, 1, 2, 3, 4, 5, 8, 2, 4, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 5, 9, 5, 9, 4, 1, 6, 8, 9, 10, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 8, 10, 3, 4, 5, 6, 10, 1, 2, 8, 2, 4, 6, 8, 9, 10, 1, 2, 3, 5, 7, 8, 10, 2, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 3, 4, 5, 6, 8, 10, 4, 5, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 7, 8, 9, 10, 1, 2, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 8, 9, 1, 3, 4, 8, 10, 1, 2, 3, 5, 6, 7, 8, 10, 3, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7, 8, 9, 2, 4, 5, 6, 10, 1, 3, 5, 7, 8, 10, 2, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 6, 9, 10, 1, 2, 3, 4, 6, 7, 8, 1, 2, 6, 8, 1, 2, 5, 6, 7, 9, 2, 3, 5, 9, 2, 3, 4, 6, 8, 9, 5, 6, 9, 10, 2, 3, 5, 6, 8, 9, 1, 2, 3, 4, 5, 8, 9, 10, 1, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 3, 4, 6, 7, 8, 10, 2, 6, 7, 8, 9, 2, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 7, 9, 10, 4, 6, 7, 9, 10, 2, 3, 4, 5, 6, 8, 3, 4, 8, 9, 2, 4, 10, 1, 2, 3, 4, 7, 9, 2, 4, 8, 9, 10, 2, 4, 6, 8, 2, 3, 4, 5, 7, 8, 10, 2, 4, 5, 7, 8, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 1, 3, 5, 6, 8, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 7, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 4, 5, 7, 8, 9, 10, 1, 2, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 9, 2, 3, 6, 8, 9, 3, 6, 7, 9, 10, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 8, 2, 4, 6, 7, 1, 2, 3, 4, 6, 7, 8, 9, 10, 2, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 4, 7, 8, 10, 1, 2, 7, 10, 3, 4, 5, 8, 9, 1, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 4, 5, 6, 8, 9, 1, 2, 3, 4, 6, 8, 2, 5, 6, 8, 9, 10, 2, 3, 4, 5, 6, 8, 9, 10, 3, 5, 9, 1, 2, 3, 6, 10, 2, 3, 10, 3, 10, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 7, 9, 3, 5, 9, 10, 2, 3, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 8, 9, 10, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 3, 4, 5, 8, 9, 1, 3, 4, 7, 8, 9, 1, 2, 5, 9, 10, 1, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6, 7, 9, 1, 3, 4, 6, 7, 8, 10, 1, 2, 1, 2, 3, 5, 6, 9, 10, 2, 4, 5, 8, 9, 1, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 9, 2, 3, 4, 6, 8, 10, 1, 2, 6, 8, 10, 1, 2, 4, 1, 3, 4, 6, 9, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 9, 10, 1, 3, 4, 8, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 8, 4, 5, 6, 9, 10, 2, 3, 4, 5, 6, 8, 2, 3, 6, 8, 1, 2, 6, 10, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 3, 4, 6, 9, 10, 3, 4, 5, 9, 10, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 8, 9, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 3, 4, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 3, 4, 6, 8, 1, 2, 6, 8, 10, 1, 2, 4, 5, 6, 8, 10, 2, 3, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 4, 5, 6, 8, 9, 5, 9, 10, 1, 2, 4, 5, 9, 1, 2, 3, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 9, 10, 3, 5, 9, 10, 1, 2, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 8, 10, 2, 4, 5, 8, 9, 1, 4, 6, 8, 10, 3, 5, 6, 7, 8, 10, 1, 6, 7, 9, 1, 2, 3, 4, 5, 6, 8, 10, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 7, 8, 10, 1, 2, 4, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 8, 9, 10, 1, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 9, 10, 1, 6, 9, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 2, 3, 4, 5, 8, 9, 1, 2, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 8, 2, 3, 4, 5, 8, 1, 2, 5, 6, 7, 10, 1, 2, 3, 5, 6, 8, 9, 10, 2, 4, 7, 8, 9, 1, 2, 3, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 8, 9, 1, 2, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 1, 3, 5, 6, 7, 8, 9, 2, 5, 6, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 4, 5, 6, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 7, 8, 9, 10, 1, 2, 5, 6, 8, 1, 3, 4, 5, 6, 7, 9, 10, 3, 4, 5, 6, 7, 9, 1, 6, 9, 10, 1, 2, 3, 4, 5, 8, 9, 4, 6, 7, 8, 1, 3, 7, 2, 6, 8, 9, 2, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 8, 10, 1, 2, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 10, 2, 3, 4, 6, 1, 2, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 10, 3, 5, 6, 10, 3, 5, 3, 4, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 9, 10, 1, 2, 4, 7, 8, 10, 1, 3, 4, 6, 8, 2, 4, 6, 8, 10, 1, 3, 4, 7, 10, 2, 6, 9, 1, 3, 4, 5, 6, 8, 10, 3, 4, 5, 7, 9, 1, 2, 3, 4, 8, 9, 10, 2, 3, 4, 5, 9, 10, 2, 3, 4, 6, 7, 8, 9, 1, 2, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 9, 10, 2, 3, 4, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 6, 8, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 5, 6, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 4, 8, 1, 2, 3, 4, 5, 9, 10, 3, 4, 5, 9, 10, 2, 3, 9, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 9, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 8, 10, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 8, 9, 1, 3, 5, 8, 1, 2, 4, 5, 6, 7, 8, 10, 1, 6, 8, 10, 1, 2, 5, 6, 7, 8, 10, 1, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 2, 4, 8, 10, 1, 2, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 2, 3, 5, 9, 10, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 8, 2, 4, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 8, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 8, 9, 2, 5, 8, 9, 2, 8, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 4, 5, 6, 1, 3, 4, 8, 1, 2, 6, 8, 10, 10, 5, 7, 8, 9, 10, 2, 3, 4, 5, 6, 8, 9, 10, 3, 5, 9, 10, 1, 2, 4, 5, 6, 8, 9, 10, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 3, 5, 9, 1, 4, 5, 6, 7, 8, 9, 10, 3, 4, 7, 8, 10, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 8, 9, 1, 2, 3, 4, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 8, 9, 1, 2, 3, 4, 5, 7, 8, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 8, 9, 10, 2, 3, 4, 6, 7, 8], \"Freq\": [0.049277486228579505, 0.15950344226619156, 0.1984067208677017, 0.15431633845265688, 0.09077431673685699, 0.04409038241504482, 0.03241939883459178, 0.09077431673685699, 0.1530195624992732, 0.027232295021057097, 0.3230596602139789, 0.36344211774072627, 0.2019122876337368, 0.08076491505349473, 0.10913669397675348, 0.06236382512957342, 0.1715005191063269, 0.015590956282393355, 0.6080472950133408, 0.03118191256478671, 0.009864298022735873, 0.17755736440924572, 0.4044362189321708, 0.009864298022735873, 0.029592894068207618, 0.04932149011367937, 0.009864298022735873, 0.2959289406820762, 0.009891120317700302, 0.7121606628744217, 0.059346721906201806, 0.19782240635400603, 0.019782240635400603, 0.009891120317700302, 0.08927255885688239, 0.23806015695168636, 0.05951503923792159, 0.05951503923792159, 0.44636279428441195, 0.08927255885688239, 0.42816292548415125, 0.04429271642939496, 0.01476423880979832, 0.02952847761959664, 0.01476423880979832, 0.4576914031037479, 0.07007344808849277, 0.03503672404424638, 0.2452570683097247, 0.17518362022123193, 0.07007344808849277, 0.10511017213273915, 0.28029379235397106, 0.23534066633193837, 0.732170961921586, 0.15045952888681743, 0.11284464666511307, 0.1692669699976696, 0.07522976444340872, 0.03761488222170436, 0.13165208777596524, 0.11284464666511307, 0.20688185221937397, 0.992240201638095, 0.23161140712666048, 0.1021278645597873, 0.03829794920992023, 0.04194537294419835, 0.15319179683968093, 0.16231035617537623, 0.012765983069973412, 0.217021712189548, 0.003647423734278118, 0.03465052547564212, 0.026453432186573173, 0.13226716093286586, 0.2204452682214431, 0.08817810728857725, 0.052906864373146345, 0.15872059311943904, 0.026453432186573173, 0.06172467510200407, 0.06172467510200407, 0.16753840384829677, 0.028736011490693845, 0.14368005745346923, 0.16283739844726514, 0.4310401723604077, 0.10536537546587743, 0.05747202298138769, 0.06705069347828564, 0.16592144620627206, 0.04977643386188162, 0.06968700740663426, 0.23229002468878085, 0.046458004937756175, 0.14269244373739395, 0.03982114708950529, 0.08959758095138691, 0.10287129664788867, 0.05973172063425793, 0.10748489722107411, 0.8598791777685929, 0.567433219747997, 0.15762033881888807, 0.12609627105511045, 0.03152406776377761, 0.09457220329133284, 0.02359426024832653, 0.10617417111746938, 0.1297684313657959, 0.04718852049665306, 0.02359426024832653, 0.10617417111746938, 0.04718852049665306, 0.2949282531040816, 0.02359426024832653, 0.18875408198661223, 0.060980334860143356, 0.05589864028846474, 0.22359456115385895, 0.03557186200175029, 0.05589864028846474, 0.2032677828671445, 0.010163389143357225, 0.17277761543707285, 0.09147050229021503, 0.08638880771853642, 0.24404568515725888, 0.24404568515725888, 0.03050571064465736, 0.21353997451260154, 0.18303426386794416, 0.06101142128931472, 0.14603027971429156, 0.18806929963204216, 0.10399125979654096, 0.08186545983983011, 0.05973965988311927, 0.09071577982251446, 0.0685899798658036, 0.09071577982251446, 0.12390447975758072, 0.048676759904763854, 0.15768027089210682, 0.013140022574342235, 0.06570011287171118, 0.05256009029736894, 0.4336207449532938, 0.1839603160407913, 0.05256009029736894, 0.039420067723026704, 0.030867738028970422, 0.2778096422607338, 0.030867738028970422, 0.030867738028970422, 0.586487022550438, 0.05188093518580381, 0.05188093518580381, 0.05188093518580381, 0.18158327315031333, 0.570690287043842, 0.07782140277870572, 0.4977508907837472, 0.2488754453918736, 0.08295848179729119, 0.08295848179729119, 0.08295848179729119, 0.05045866892243192, 0.4541280203018873, 0.04036693513794554, 0.05045866892243192, 0.010091733784486384, 0.15137600676729576, 0.02018346756897277, 0.14128427298280938, 0.07064213649140469, 0.1894738258271491, 0.027067689403878444, 0.7849629927124749, 0.11690622208859057, 0.03896874069619686, 0.07793748139239372, 0.7014373325315434, 0.03896874069619686, 0.03591148497631739, 0.03591148497631739, 0.5745837596210782, 0.16160168239342826, 0.03591148497631739, 0.16160168239342826, 0.04586104518326984, 0.3003158765227025, 0.036984713857475676, 0.07692820482354941, 0.18344418073307936, 0.1035571988009319, 0.02514960542308346, 0.05917554217196108, 0.13314496988691243, 0.03550532530317665, 0.03913019835323757, 0.2973895074846055, 0.046956238023885086, 0.10173851571841769, 0.1643468330835978, 0.0821734165417989, 0.03521717851791381, 0.03913019835323757, 0.16043381324827405, 0.03521717851791381, 0.0563713974969286, 0.3100426862331073, 0.3100426862331073, 0.2536712887361787, 0.0563713974969286, 0.03006324371045116, 0.10385484190883128, 0.29926592602676383, 0.08199066466486679, 0.03142975478819894, 0.10385484190883128, 0.027330221554955597, 0.1434836631635169, 0.1298185523860391, 0.047827887721172295, 0.04422390950168439, 0.08844781900336877, 0.13267172850505315, 0.17689563800673755, 0.04422390950168439, 0.15478368325589534, 0.11055977375421097, 0.24323150225926413, 0.10386101613309741, 0.06773544530419397, 0.11289240884032327, 0.08128253436503276, 0.08128253436503276, 0.1761121577909043, 0.013547089060838792, 0.16256506873006552, 0.1896592468517431, 0.018062785414451724, 0.12208612719604557, 0.061043063598022786, 0.18312919079406836, 0.030521531799011393, 0.09156459539703418, 0.24417225439209114, 0.12208612719604557, 0.12208612719604557, 0.3650809531055551, 0.06637835511010093, 0.38167554188308034, 0.033189177555050464, 0.06637835511010093, 0.049783766332575696, 0.016594588777525232, 0.046722104569075766, 0.2718376993109863, 0.18688841827630306, 0.2293630587936447, 0.012742392155202483, 0.029732248362139127, 0.029732248362139127, 0.046722104569075766, 0.10193913724161986, 0.03822717646560745, 0.1941888697770574, 0.07282082616639653, 0.7039346529418331, 0.12588978157457872, 0.12588978157457872, 0.22030711775551276, 0.09441733618093404, 0.03147244539364468, 0.37766934472373614, 0.2899888859692168, 0.20299222017845178, 0.40598444035690356, 0.057997777193843365, 0.07420201071468253, 0.18213220811785713, 0.13491274675396825, 0.10927932487071428, 0.08904241285761905, 0.05261597123404762, 0.03507731415603174, 0.1389601291565873, 0.11197757980579365, 0.07150375577960318, 0.03418621108751539, 0.11452380714317656, 0.30254796812451124, 0.05469793774002463, 0.04444207441377001, 0.11623311769755233, 0.058116558848776166, 0.12648898102380696, 0.1299076021325585, 0.020511726652509235, 0.021735107346720375, 0.10052487147858173, 0.36949682489424635, 0.04347021469344075, 0.04618710311178079, 0.11954309040696207, 0.029885772601740516, 0.12225997882530211, 0.1331275324986623, 0.010867553673360187, 0.1824943610472327, 0.5109842109322517, 0.036498872209446545, 0.25549210546612583, 0.08118586017566465, 0.21649562713510573, 0.10148232521958081, 0.027061953391888216, 0.06765488347972054, 0.18266818539524546, 0.0744203718276926, 0.20296465043916162, 0.040592930087832324, 0.18351441300868054, 0.8156196133719136, 0.22661767255143545, 0.7365074357921652, 0.9746099280084847, 0.30369819568066037, 0.05061636594677673, 0.17715728081371854, 0.20246546378710692, 0.25308182973388366, 0.24503854316178775, 0.24503854316178775, 0.12251927158089387, 0.0918894536856704, 0.24503854316178775, 0.06125963579044694, 0.2663179800004841, 0.13949989428596787, 0.09511356428588719, 0.012681808571451625, 0.02536361714290325, 0.0063409042857258124, 0.02536361714290325, 0.07609085142870975, 0.22193165000040344, 0.13949989428596787, 0.34986524112996026, 0.22391375432317456, 0.19592453503277776, 0.01399460964519841, 0.08396765787119047, 0.1259514868067857, 0.11154000219071554, 0.2106866708046849, 0.024786667153492342, 0.012393333576746171, 0.6320600124140547, 0.017652810469461464, 0.9355989548814576, 0.03530562093892293, 0.5660642443557438, 0.20854998476264242, 0.08937856489827532, 0.059585709932183546, 0.029792854966091773, 0.029792854966091773, 0.012347757280703574, 0.049391029122814296, 0.3457372038597001, 0.35808496114040367, 0.024695514561407148, 0.12347757280703574, 0.08643430096492502, 0.13289607197863862, 0.08859738131909241, 0.13289607197863862, 0.35438952527636963, 0.26579214395727724, 0.0648577028745084, 0.0432384685830056, 0.2594308114980336, 0.0216192342915028, 0.0864769371660112, 0.10809617145751399, 0.0216192342915028, 0.0432384685830056, 0.3459077486640448, 0.12701327675328414, 0.1905199151299262, 0.15876659594160517, 0.0952599575649631, 0.15876659594160517, 0.031753319188321034, 0.22227323431824722, 0.11356097856253411, 0.4315317185376296, 0.386107327112616, 0.045424391425013644, 0.00750833062314029, 0.4880414905041189, 0.05631247967355218, 0.07883747154297305, 0.09385413278925363, 0.00750833062314029, 0.0412958184272716, 0.12764162059338494, 0.09760829810082378, 0.04310386460213335, 0.05747181946951113, 0.05747181946951113, 0.028735909734755566, 0.028735909734755566, 0.6465579690320002, 0.14367954867377783, 0.06071275574874241, 0.30356377874371204, 0.5464148017386816, 0.06071275574874241, 0.03403387886673474, 0.18718633376704108, 0.1403897503252808, 0.07657622745015318, 0.3275760840923219, 0.008508469716683686, 0.11911857603357161, 0.025525409150051057, 0.04679658344176027, 0.029779644008392902, 0.9918220012174377, 0.1148252144662419, 0.6315386795643305, 0.08611891084968143, 0.028706303616560475, 0.05741260723312095, 0.08611891084968143, 0.3654144716574627, 0.18270723582873136, 0.09135361791436568, 0.18270723582873136, 0.1370304268715485, 0.03886561931209316, 0.15546247724837264, 0.03886561931209316, 0.07773123862418632, 0.03886561931209316, 0.23319371587255897, 0.27205933518465214, 0.11659685793627948, 0.05482764266817528, 0.10965528533635056, 0.05482764266817528, 0.7127593546862786, 0.01827588088939176, 0.01827588088939176, 0.05431461217195225, 0.5111963498536682, 0.11501917871707536, 0.01916986311951256, 0.1437739733963442, 0.00958493155975628, 0.14696895058292964, 0.0031949771865854267, 0.22239637992162373, 0.11119818996081186, 0.01853303166013531, 0.3891936648628415, 0.24092941158175904, 0.1263474175060666, 0.050538967002426646, 0.050538967002426646, 0.10107793400485329, 0.555928637026693, 0.10107793400485329, 0.0926784853534959, 0.01853569707069918, 0.5931423062623737, 0.03707139414139836, 0.03707139414139836, 0.03707139414139836, 0.16682127363629262, 0.23807711399968226, 0.38545818457091413, 0.018895009047593832, 0.09069604342845039, 0.037790018095187665, 0.056685027142781494, 0.007558003619037532, 0.15493907419026942, 0.007558003619037532, 0.003779001809518766, 0.08541337582343955, 0.05338335988964972, 0.8221037423006057, 0.010676671977929944, 0.03203001593378983, 0.10083012260940356, 0.15124518391410535, 0.03024903678282107, 0.020166024521880713, 0.5142336253079581, 0.1613281961750457, 0.03024903678282107, 0.10316219727298272, 0.06189731836378963, 0.7840326992746687, 0.020632439454596543, 0.0822330666009564, 0.1233495999014346, 0.13705511100159398, 0.0137055111001594, 0.0548220444006376, 0.5756314662066948, 0.2478848561572984, 0.03913971413009975, 0.5740491405747963, 0.14351228514369907, 0.05955043389305033, 0.11910086778610066, 0.05955043389305033, 0.3870778203048271, 0.357302603358302, 0.029775216946525164, 0.11597553071076667, 0.023195106142153334, 0.7654385026910601, 0.09278042456861334, 0.5362794106784455, 0.17160941141710256, 0.02145117642713782, 0.08580470570855128, 0.02145117642713782, 0.15015823498996475, 0.0450862718958104, 0.06312078065413455, 0.38774193830396947, 0.1352588156874312, 0.08115528941245873, 0.02705176313748624, 0.21641410509988993, 0.03606901751664832, 0.2932562738633782, 0.33515002727243226, 0.20946876704527015, 0.12568126022716208, 0.04189375340905403, 0.03907749118736914, 0.15630996474947656, 0.1953874559368457, 0.3907749118736914, 0.03907749118736914, 0.03907749118736914, 0.11723247356210742, 0.046071691901439824, 0.0614289225352531, 0.16892953697194601, 0.046071691901439824, 0.33785907394389203, 0.0614289225352531, 0.16892953697194601, 0.09214338380287965, 0.03401897902105141, 0.006803795804210282, 0.013607591608420564, 0.006803795804210282, 0.5443036643368225, 0.3401897902105141, 0.05443036643368226, 0.2698495317311758, 0.05396990634623516, 0.010793981269247032, 0.05396990634623516, 0.6044629510778338, 0.3059342201296983, 0.5098903668828304, 0.13597076450208812, 0.07356360465966481, 0.19394041228457087, 0.06687600423605893, 0.12706440804851196, 0.020062801270817675, 0.12706440804851196, 0.1605024101665414, 0.08693880550687659, 0.11368920720130017, 0.020062801270817675, 0.14341536781489225, 0.4876122505706336, 0.0573661471259569, 0.20078151494084914, 0.02868307356297845, 0.02868307356297845, 0.02868307356297845, 0.09952623574445899, 0.2736971482972622, 0.09952623574445899, 0.298578707233377, 0.2239340304250327, 0.8733431444843291, 0.03147182502646231, 0.03147182502646231, 0.015735912513231157, 0.03147182502646231, 0.007867956256615578, 0.015997547026472932, 0.9638522083449942, 0.0119981602698547, 0.007998773513236466, 0.5955277966890186, 0.15763971088826964, 0.22770180461638947, 0.06983958020572299, 0.06983958020572299, 0.24443853072003047, 0.45395727133719943, 0.034919790102861496, 0.1047593703085845, 0.6544830725269066, 0.01869951635791162, 0.24309371265285104, 0.01869951635791162, 0.056098549073734856, 0.09825278781165733, 0.2292565048938671, 0.19650557562331467, 0.4585130097877342, 0.12973994785632229, 0.043246649285440766, 0.10811662321360191, 0.10811662321360191, 0.06486997392816114, 0.41084316821168726, 0.12973994785632229, 0.058204459724404335, 0.14551114931101083, 0.14551114931101083, 0.029102229862202168, 0.4656356777952347, 0.14551114931101083, 0.10276461285559696, 0.03574421316716416, 0.07595645298022384, 0.03574421316716416, 0.08042447962611936, 0.17872106583582079, 0.335101998442164, 0.05361631975074624, 0.09829658620970144, 0.1131669535210525, 0.08487521514078937, 0.08487521514078937, 0.5941265059855256, 0.1131669535210525, 0.22532361473481274, 0.01877696789456773, 0.6196399405207351, 0.03755393578913546, 0.056330903683703186, 0.01877696789456773, 0.3356292796455668, 0.02796910663713057, 0.05593821327426114, 0.05593821327426114, 0.13984553318565285, 0.0839073199113917, 0.11187642654852228, 0.13984553318565285, 0.0839073199113917, 0.12930150668288876, 0.10775125556907396, 0.21550251113814792, 0.021550251113814794, 0.12930150668288876, 0.21550251113814792, 0.04310050222762959, 0.08620100445525918, 0.04310050222762959, 0.26305948475775864, 0.16740149030039186, 0.4782899722868339, 0.04782899722868339, 0.008487938554111642, 0.09336732409522808, 0.118831139757563, 0.22917434096101436, 0.08487938554111643, 0.042439692770558216, 0.16127083252812122, 0.008487938554111642, 0.25463815662334927, 0.3877784645150011, 0.14101035073272766, 0.035252587683181916, 0.035252587683181916, 0.2820207014654553, 0.10575776304954575, 0.3149265724495756, 0.11022430035735147, 0.20470227209222414, 0.0787316431123939, 0.141716957602309, 0.12597062897983025, 0.01574632862247878, 0.3767434705046033, 0.16744154244649034, 0.08372077122324517, 0.12558115683486776, 0.20930192805811293, 0.08372077122324517, 0.13359257340293876, 0.0534370293611755, 0.213748117444702, 0.02671851468058775, 0.5610888082923428, 0.11816544717756781, 0.08862408538317586, 0.6794513212710149, 0.08862408538317586, 0.029541361794391954, 0.21924172978354914, 0.01993106634395901, 0.009965533171979506, 0.5182077249429343, 0.009965533171979506, 0.029896599515938517, 0.029896599515938517, 0.1594485307516721, 0.15684981041638923, 0.7215091279153905, 0.09410988624983353, 0.02882039629920796, 0.5764079259841592, 0.37466515188970345, 0.2521712648243183, 0.10807339921042214, 0.2521712648243183, 0.3602446640347404, 0.020288259690072148, 0.12172955814043289, 0.08115303876028859, 0.060864779070216445, 0.040576519380144296, 0.060864779070216445, 0.4463417131815873, 0.020288259690072148, 0.16230607752057719, 0.32945996801046173, 0.08785599146945647, 0.30749597014309765, 0.1317839872041847, 0.1317839872041847, 0.1152604584171528, 0.2799182561559425, 0.1152604584171528, 0.07409600898245537, 0.10702756853021332, 0.03293155954775794, 0.01646577977387897, 0.0576302292085764, 0.18935646739960818, 0.03853221846031495, 0.4855059525999684, 0.0077064436920629914, 0.015412887384125983, 0.03853221846031495, 0.20036753599363777, 0.0077064436920629914, 0.20036753599363777, 0.0077064436920629914, 0.14614868367841313, 0.5663261492538509, 0.01826858545980164, 0.16441726913821478, 0.05480575637940493, 0.05480575637940493, 0.06707114073537411, 0.7377825480891151, 0.10060671110306116, 0.06707114073537411, 0.5262924150620197, 0.04385770125516831, 0.13157310376550493, 0.08771540251033662, 0.17543080502067324, 0.04249333685024884, 0.6515644983704822, 0.04249333685024884, 0.014164445616749614, 0.04249333685024884, 0.08498667370049769, 0.12748001055074654, 0.16465165155271186, 0.0705649935225908, 0.02352166450753027, 0.04704332901506054, 0.09408665803012108, 0.02352166450753027, 0.18817331606024215, 0.39986829662801454, 0.02352166450753027, 0.033862378996774786, 0.18624308448226132, 0.06772475799354957, 0.033862378996774786, 0.5248668744500091, 0.13544951598709914, 0.21873346280808653, 0.1367084142550541, 0.027341682851010816, 0.43746692561617306, 0.1367084142550541, 0.05468336570202163, 0.12213388917471378, 0.3256903711325701, 0.08142259278314253, 0.3256903711325701, 0.08142259278314253, 0.04071129639157126, 0.15235558698309992, 0.2708543768588443, 0.033856797107355535, 0.220069181197811, 0.012696298915258326, 0.15658768662151934, 0.14389138770626103, 0.004232099638419442, 0.318441858951997, 0.6169811017194942, 0.039805232368999625, 0.04022988583008159, 0.06034482874512238, 0.6637931161963462, 0.06034482874512238, 0.18103448623536714, 0.02524616878161769, 0.012623084390808845, 0.9593544137014722, 0.01725924097424337, 0.9665174945576286, 0.19017062276415173, 0.06339020758805057, 0.031695103794025285, 0.15847551897012643, 0.221865726558177, 0.06339020758805057, 0.19017062276415173, 0.12678041517610114, 0.09746096780789412, 0.038984387123157646, 0.11695316136947295, 0.11695316136947295, 0.045481784977017256, 0.09746096780789412, 0.038984387123157646, 0.0844661721001749, 0.27289070986210356, 0.09096356995403451, 0.639049771030785, 0.02282320610824232, 0.09129282443296928, 0.2282320610824232, 0.01636212553931103, 0.6381228960331303, 0.27815613416828755, 0.06544850215724413, 0.1662095798840129, 0.26118648267487743, 0.02374422569771613, 0.5461171910474709, 0.2090819284954877, 0.014419443344516393, 0.11535554675613115, 0.15861387678968034, 0.10814582508387295, 0.12256526842838934, 0.09372638173935656, 0.028838886689032787, 0.09372638173935656, 0.05046805170580738, 0.02925070391580037, 0.019500469277200245, 0.79951924036521, 0.019500469277200245, 0.03900093855440049, 0.08775211174740111, 0.8243050202185508, 0.04579472334547505, 0.06869208501821257, 0.022897361672737523, 0.022897361672737523, 0.10616688989884289, 0.23533660594243505, 0.10793633806382359, 0.09908909723892002, 0.12563081971363074, 0.12032247521868861, 0.04423620412451787, 0.053083444949421445, 0.060161237609344304, 0.04423620412451787, 0.12082122950382436, 0.024164245900764873, 0.14498547540458923, 0.0966569836030595, 0.04832849180152975, 0.386627934412238, 0.1691497213053541, 0.024164245900764873, 0.09585250571056973, 0.1533640091369116, 0.047926252855284866, 0.3067280182738232, 0.01917050114211395, 0.01917050114211395, 0.13419350799479762, 0.08626725513951276, 0.1437787585658546, 0.048499211738194825, 0.1939968469527793, 0.12124802934548706, 0.14549763521458448, 0.48499211738194825, 0.11578614938856512, 0.27016768190665197, 0.07719076625904342, 0.3087630650361737, 0.15438153251808684, 0.03859538312952171, 0.1000771363706377, 0.025019284092659425, 0.2501928409265942, 0.07505785227797827, 0.5254049659458478, 0.2825821443057115, 0.2825821443057115, 0.040368877757958783, 0.16147551103183513, 0.16147551103183513, 0.03540495754321864, 0.007080991508643727, 0.6089652697433605, 0.09205288961236846, 0.13453883866423083, 0.06372892357779354, 0.007080991508643727, 0.021242974525931182, 0.021242974525931182, 0.482287708099, 0.1669457451111923, 0.25969338128407693, 0.05564858170373077, 0.018549527234576924, 0.0770556849521664, 0.2696948973325824, 0.0192639212380416, 0.09631960619020799, 0.5201258734271231, 0.0192639212380416, 0.0192639212380416, 0.3933491684740721, 0.5748949385390284, 0.003739782465960534, 0.001869891232980267, 0.06170641068834881, 0.762915623055949, 0.009349456164901335, 0.1589407548033227, 0.001869891232980267, 0.0319212307706792, 0.3830547692481504, 0.31921230770679204, 0.0957636923120376, 0.15960615385339602, 0.028134508550146254, 0.07033627137536563, 0.04220176282521938, 0.07033627137536563, 0.028134508550146254, 0.6892954594785833, 0.028134508550146254, 0.028134508550146254, 0.10712616121923912, 0.06504088359739518, 0.19512265079218555, 0.0765186865851708, 0.09947429256072204, 0.14155957018256599, 0.02295560597555124, 0.14538550451182453, 0.10712616121923912, 0.0382593432925854, 0.31786186018234935, 0.1907171161094096, 0.031786186018234935, 0.4450066042552891, 0.07615542170542257, 0.144294483231327, 0.19640082439819506, 0.07615542170542257, 0.024049080538554497, 0.10421268233373615, 0.008016360179518166, 0.15631902350060423, 0.1683435637698815, 0.04008180089759083, 0.07725996246688359, 0.03090398498675344, 0.01545199249337672, 0.8498595871357195, 0.01545199249337672, 0.1887812956164765, 0.0755125182465906, 0.377562591232953, 0.0377562591232953, 0.11326877736988589, 0.1887812956164765, 0.11025168552709334, 0.09187640460591112, 0.4042561802660089, 0.34913033750246225, 0.01837528092118222, 0.20069158073632537, 0.7224896906507714, 0.040138316147265075, 0.12014753988238194, 0.2883540957177167, 0.12014753988238194, 0.02402950797647639, 0.4085016356000986, 0.264929358255981, 0.07225346134254026, 0.09633794845672036, 0.02408448711418009, 0.02408448711418009, 0.43352076805524165, 0.09633794845672036, 0.09346997454245486, 0.06395314047641648, 0.2508930895613262, 0.1180673362641535, 0.014758417033019187, 0.04427525109905756, 0.09346997454245486, 0.1475841703301919, 0.12790628095283296, 0.04427525109905756, 0.4188154511808671, 0.4323256270254112, 0.0810610550672646, 0.0675508792227205, 0.1859907638672166, 0.05314021824777617, 0.1594206547433285, 0.5579722916016499, 0.03184249663980736, 0.41395245631749567, 0.03184249663980736, 0.03184249663980736, 0.19105497983884415, 0.28658246975826623, 0.06021632624776822, 0.010036054374628036, 0.2408653049910729, 0.08028843499702429, 0.3512619031119813, 0.06021632624776822, 0.020072108749256072, 0.050180271873140184, 0.1103965981209084, 0.03010816312388411, 0.37052197403598897, 0.11115659221079668, 0.1482087896143956, 0.11115659221079668, 0.25936538182519225, 0.16734577718875504, 0.16734577718875504, 0.0956261583935743, 0.0956261583935743, 0.45422425236947794, 0.01806792047655984, 0.19874712524215826, 0.054203761429679524, 0.0903396023827992, 0.19874712524215826, 0.41556217096087633, 0.34974707564212476, 0.07772157236491661, 0.4663294341894997, 0.07772157236491661, 0.5020374150731375, 0.10757944608710089, 0.14343926144946786, 0.21515889217420178, 0.5449879348937905, 0.24772178858808658, 0.19817743087046927, 0.04115730367751896, 0.15639775397457203, 0.10152134907121342, 0.14816629323906824, 0.03566966318718309, 0.12347191103255686, 0.046644944167854814, 0.22224943985860235, 0.08505842760020584, 0.04115730367751896, 0.12274745591854176, 0.09206059193890632, 0.767171599490886, 0.11214156803407092, 0.037380522678023644, 0.13083182937308274, 0.20559287472913004, 0.037380522678023644, 0.2616636587461655, 0.0934513066950591, 0.05607078401703546, 0.05607078401703546, 0.0872517740446153, 0.0872517740446153, 0.4798847572453842, 0.04362588702230765, 0.0872517740446153, 0.04362588702230765, 0.1745035480892306, 0.19978835202733125, 0.06659611734244375, 0.46617282139710625, 0.09989417601366563, 0.14984126402049844, 0.20195397999493167, 0.48652549726051725, 0.045898631817029925, 0.1835945272681197, 0.08261753727065387, 0.09338336732717403, 0.44824016317043536, 0.05603002039630442, 0.11206004079260884, 0.05603002039630442, 0.13073671425804365, 0.09338336732717403, 0.03735334693086961, 0.049104664261046996, 0.11083624218922036, 0.18800071459943707, 0.12346315585634673, 0.05752260670579791, 0.09119437648480157, 0.07014952037292428, 0.13468707911601463, 0.11223923259667885, 0.06453755874309033, 0.44374729168554355, 0.08217542438621177, 0.44374729168554355, 0.016435084877242354, 0.12230504115894542, 0.10965279552181313, 0.2614797431674005, 0.09700054988468085, 0.016869660849509714, 0.0042174152123774285, 0.15604436285796483, 0.19821851498173912, 0.03373932169901943, 0.07356275924305906, 0.5885020739444725, 0.14712551848611813, 0.19616735798149082, 0.21447349676757685, 0.055738874866149916, 0.22053207012259315, 0.04119829881411081, 0.09087860032524443, 0.1744869126244693, 0.12965346979734874, 0.065432592234176, 0.007270288026019554, 0.5350022367236978, 0.07133363156315971, 0.035666815781579854, 0.28533452625263883, 0.035666815781579854, 0.31231427471416623, 0.24291110255546264, 0.20820951647611083, 0.0347015860793518, 0.1388063443174072, 0.2911650577503822, 0.4507459067097263, 0.005599328033661197, 0.0027996640168305984, 0.011198656067322394, 0.10918689665639333, 0.12878454477420753, 0.13561591338786425, 0.20342387008179638, 0.03390397834696606, 0.13561591338786425, 0.06780795669393212, 0.40684774016359276, 0.11504338205891826, 0.013534515536343324, 0.02030177330451499, 0.1759487019724632, 0.0541380621453733, 0.23008676411783652, 0.03383628884085831, 0.31806111510406815, 0.03383628884085831, 0.2239862711950361, 0.011585496785950144, 0.02317099357190029, 0.02703282583388367, 0.0617893161917341, 0.007723664523966762, 0.6410641554892413, 0.7303590627179007, 0.2045005375610122, 0.02921436250871603, 0.015519295639966022, 0.15519295639966021, 0.007759647819983011, 0.007759647819983011, 0.8225226689181991, 0.08028888861669711, 0.1003611107708714, 0.5419499981627055, 0.08028888861669711, 0.06021666646252284, 0.1003611107708714, 0.020072222154174278, 0.020072222154174278, 0.0669310928678579, 0.022310364289285966, 0.10039663930178684, 0.011155182144642983, 0.08924145715714386, 0.011155182144642983, 0.21194846074821666, 0.3792761929178614, 0.10039663930178684, 0.026520571100934944, 0.16973165504598364, 0.11669051284411375, 0.06364937064224387, 0.10077817018355278, 0.12729874128448773, 0.021216456880747955, 0.08486582752299182, 0.25990159678916247, 0.021216456880747955, 0.13991528495778283, 0.6296187823100227, 0.04663842831926094, 0.11659607079815235, 0.02331921415963047, 0.02331921415963047, 0.8674485997889676, 0.014956010341189097, 0.022434015511783645, 0.08973606204713458, 0.12987603675436832, 0.6169111745832495, 0.12987603675436832, 0.03246900918859208, 0.03246900918859208, 0.03128217635926563, 0.2137615384549818, 0.015641088179632816, 0.046923264538898446, 0.3336765478321667, 0.005213696059877605, 0.005213696059877605, 0.16162457785620576, 0.12512870543706253, 0.06777804877840887, 0.1940787790603299, 0.5469492864427479, 0.0352870507382418, 0.1587917283220881, 0.0529305761073627, 0.06536184891771127, 0.039217109350626764, 0.4836776819910634, 0.13072369783542254, 0.27451976545438733, 0.30929839023354355, 0.2249442838062135, 0.08435410642733007, 0.14059017737888344, 0.19682624833043683, 0.22451493784360382, 0.14967662522906922, 0.22451493784360382, 0.2619340941508711, 0.037419156307267304, 0.07483831261453461, 0.1444396926376132, 0.5488708320229301, 0.02888793852752264, 0.2311035082201811, 0.38043800105476694, 0.5944343766480733, 0.06798334979256489, 0.4758834485479542, 0.10197502468884732, 0.20395004937769465, 0.10197502468884732, 0.06798334979256489, 0.026381595941759546, 0.17148037362143706, 0.17148037362143706, 0.026381595941759546, 0.026381595941759546, 0.05276319188351909, 0.5144411208643112, 0.07722562883106081, 0.5124973549697671, 0.004680341141276413, 0.04446324084212592, 0.03042221741829668, 0.05616409369531695, 0.01638119399446744, 0.19891449850424753, 0.046803411412764125, 0.014041023423829237, 0.11891398841970614, 0.1486424855246327, 0.05945699420985307, 0.11891398841970614, 0.26755647394433885, 0.1486424855246327, 0.11891398841970614, 0.02608021427470083, 0.10432085709880332, 0.02608021427470083, 0.15648128564820496, 0.44336364266991407, 0.23472192847230747, 0.3312159958977986, 0.36107973323284603, 0.039365835578017046, 0.09230609721741928, 0.02307652430435482, 0.017646753879800744, 0.010859540849108151, 0.03529350775960149, 0.05972747467009483, 0.02714885212277038, 0.5577006387229335, 0.13942515968073338, 0.09958939977195241, 0.019917879954390483, 0.09958939977195241, 0.039835759908780966, 0.019917879954390483, 0.052475309416635335, 0.017491769805545114, 0.10495061883327067, 0.052475309416635335, 0.47227778474971804, 0.22739300747208646, 0.017491769805545114, 0.03498353961109023, 0.18657839997805548, 0.20170637835465455, 0.1512797837659909, 0.07059723242412909, 0.05546925404753, 0.06555457296526274, 0.010085318917732728, 0.06051191350639637, 0.09581052971846092, 0.10085318917732727, 0.026044773162474784, 0.05208954632494957, 0.008681591054158262, 0.564303418520287, 0.31253727794969743, 0.03472636421663305, 0.1170013585849725, 0.29250339646243123, 0.5850067929248625, 0.15280911548833773, 0.05730341830812665, 0.019101139436042216, 0.05730341830812665, 0.07640455774416886, 0.17191025492437995, 0.03820227887208443, 0.4011239281568865, 0.06703347741354744, 0.017640388793038798, 0.4657062641362243, 0.017640388793038798, 0.09173002172380175, 0.12701079930987935, 0.01411231103443104, 0.176403887930388, 0.01411231103443104, 0.01058423327582328, 0.060793861523608946, 0.02431754460944358, 0.48635089218887156, 0.016211696406295717, 0.09321725433620039, 0.14590526765666145, 0.1661698881645311, 0.004052924101573929, 0.038750055539142306, 0.50375072200885, 0.038750055539142306, 0.15500022215656922, 0.19375027769571154, 0.07750011107828461, 0.31001703290387667, 0.0563667332552503, 0.563667332552503, 0.03757782217016687, 0.02818336662762515, 0.14354922888339416, 0.1852248114624441, 0.06482868401185543, 0.08335116515809984, 0.027783721719366612, 0.07408992458497764, 0.03704496229248882, 0.26857597662054394, 0.06482868401185543, 0.055567443438733224, 0.017111295054672543, 0.06844518021869017, 0.22244683571074306, 0.5646727368041939, 0.1197790653827078, 0.004254093270083208, 0.17441782407341153, 0.2127046635041604, 0.17016373080332833, 0.017016373080332832, 0.034032746160665664, 0.059557305781164915, 0.14038507791274588, 0.1063523317520802, 0.08082777213158095, 0.037156645586180044, 0.21365071212053524, 0.037156645586180044, 0.22293987351708025, 0.47374723122379553, 0.02533860681347517, 0.05067721362695034, 0.10135442725390068, 0.02533860681347517, 0.7854968112177303, 0.11917287223644685, 0.2681389625320054, 0.41710505282756394, 0.08937965417733514, 0.029793218059111712, 0.029793218059111712, 0.027755381841858457, 0.20816536381393844, 0.10408268190696922, 0.0485719182232523, 0.06244960914418153, 0.37469765486508916, 0.027755381841858457, 0.13183806374882767, 0.6245266310096097, 0.14869681690704992, 0.029739363381409985, 0.05947872676281997, 0.11895745352563994, 0.14492461115833494, 0.14492461115833494, 0.18115576394791869, 0.10869345836875122, 0.07246230557916747, 0.32608037510625365, 0.09803953458269732, 0.14705930187404598, 0.15151564435507767, 0.09803953458269732, 0.022281712405158482, 0.053476109772380355, 0.04010708232928527, 0.1292339319499192, 0.14705930187404598, 0.1158649045068241, 0.2069662767554206, 0.025870784594427574, 0.15522470756656545, 0.12935392297213788, 0.1034831383777103, 0.38806176891641364, 0.13562191253883746, 0.13562191253883746, 0.13562191253883746, 0.06781095626941873, 0.23733834694296552, 0.30514930321238426, 0.08506353273676687, 0.16239401704291856, 0.2281249287031475, 0.15852749282761097, 0.01546609686123034, 0.03093219372246068, 0.042531766368383433, 0.1469279201816882, 0.08506353273676687, 0.046398290583691014, 0.0707669054722733, 0.06133131807597019, 0.04246014328336398, 0.21230071641681988, 0.06133131807597019, 0.01887117479260621, 0.1415338109445466, 0.03302455588706087, 0.31609217777615406, 0.04246014328336398, 0.054277182077781636, 0.20353943279168116, 0.10855436415556327, 0.016961619399306762, 0.09159274475625652, 0.17300851787292898, 0.06784647759722705, 0.17300851787292898, 0.047492534318058936, 0.06106182983750434, 0.10710873186770489, 0.42843492747081957, 0.42843492747081957, 0.033509203052537545, 0.46912884273552563, 0.10052760915761264, 0.10052760915761264, 0.16754601526268773, 0.033509203052537545, 0.06701840610507509, 0.01733518866143702, 0.9361001877175992, 0.03467037732287404, 0.010877922062642629, 0.01631688309396394, 0.0054389610313213144, 0.8865506481053742, 0.043511688250570515, 0.010877922062642629, 0.021755844125285258, 0.010877922062642629, 0.003452154860715149, 0.024165084025006042, 0.003452154860715149, 0.8216128568502055, 0.11737326526431507, 0.010356464582145447, 0.017260774303575744, 0.614822666277147, 0.0776618104771133, 0.07118999293735385, 0.08413362801687274, 0.012943635079518883, 0.012943635079518883, 0.058246357857834975, 0.058246357857834975, 0.0064718175397594415, 0.09396014511454583, 0.24429637729781914, 0.39463260948109247, 0.018792029022909166, 0.056376087068727494, 0.18792029022909165, 0.16238836773636378, 0.5521204503036368, 0.16238836773636378, 0.0649553470945455, 0.03247767354727275, 0.09698241446147193, 0.09698241446147193, 0.19396482892294387, 0.1454736216922079, 0.048491207230735967, 0.09698241446147193, 0.1454736216922079, 0.24245603615367986, 0.18664644114812506, 0.35255438883534734, 0.020738493460902785, 0.1244309607654167, 0.020738493460902785, 0.290338908452639, 0.02211498117520555, 0.5307595482049332, 0.33172471762808325, 0.0884599247008222, 0.04195726665527059, 0.11748034663475765, 0.03356581332421647, 0.24335214660056942, 0.1930034266142447, 0.35244103990427295, 0.008391453331054118, 0.0848171813593186, 0.014136196893219766, 0.8623080104864057, 0.014136196893219766, 0.06252952005092681, 0.03751771203055609, 0.900425088733346, 0.33736338950623984, 0.5341587000515464, 0.05622723158437331, 0.05622723158437331, 0.13784837511609477, 0.6892418755804739, 0.027569675023218956, 0.08270902506965687, 0.09839031958547093, 0.10996565130140869, 0.054982825650704344, 0.12443481594633088, 0.13022248180429977, 0.12154098301734645, 0.0665581573666421, 0.13022248180429977, 0.10996565130140869, 0.054982825650704344, 0.12815125678789846, 0.30290297058957816, 0.4427043416309219, 0.058250571267226575, 0.058250571267226575, 0.06726697190339395, 0.5493469372110507, 0.1569562677745859, 0.20180091571018188, 0.21798387398082633, 0.18503282326279447, 0.0836449748996194, 0.06083270901790502, 0.03041635450895251, 0.07857558248146065, 0.1064572407813338, 0.13940829149936568, 0.02027756967263501, 0.0735061900633019, 0.1756484119279679, 0.003444086508391527, 0.17220432541957637, 0.2135333635202747, 0.04477312460908985, 0.22386562304544927, 0.1515398063692272, 0.010332259525174583, 0.11934730105685701, 0.11934730105685701, 0.2685314273779283, 0.47738920422742803, 0.08412406662594794, 0.3364962665037918, 0.04206203331297397, 0.08412406662594794, 0.04206203331297397, 0.42062033312973973, 0.01810591743502905, 0.04526479358757262, 0.49791272946329884, 0.05431775230508715, 0.009052958717514525, 0.009052958717514525, 0.3711713074180955, 0.7147765961019362, 0.20103091765366957, 0.022336768628185507, 0.044673537256371014, 0.9063177939052588, 0.06250467544174199, 0.024954819866338823, 0.9233283350545365, 0.03301668781031293, 0.2861446276893787, 0.13206675124125172, 0.308155752896254, 0.022011125206875284, 0.0770389382240635, 0.022011125206875284, 0.05502781301718821, 0.05502781301718821, 0.013149575132200523, 0.013149575132200523, 0.14464532645420575, 0.06574787566100261, 0.10519660105760419, 0.13149575132200522, 0.18409405185080732, 0.05259830052880209, 0.276141077776211, 0.013149575132200523, 0.10181746569238818, 0.06787831046159212, 0.03393915523079606, 0.30545239707716454, 0.40726986276955274, 0.10181746569238818, 0.027350668986506347, 0.3555586968245825, 0.08205200695951904, 0.08205200695951904, 0.3555586968245825, 0.054701337973012694, 0.035936175225104644, 0.5390426283765697, 0.16171278851297088, 0.21561705135062786, 0.017968087612552322, 0.2593800974239323, 0.02358000885672112, 0.6366602391314703, 0.02358000885672112, 0.04716001771344224, 0.27575532735003166, 0.4208897101658378, 0.17416125937896737, 0.07256719140790306, 0.04354031484474184, 0.23147890656034584, 0.6944367196810376, 0.02893486332004323, 0.11075669586197352, 0.01703949167107285, 0.0340789833421457, 0.059638220848754975, 0.6475006835007683, 0.05111847501321855, 0.08519745835536424, 0.02826186408576924, 0.5652372817153848, 0.02826186408576924, 0.22609491268615392, 0.1413093204288462, 0.1475868224800418, 0.042167663565726224, 0.06325149534858934, 0.1686706542629049, 0.40059280387439916, 0.1686706542629049, 0.042167663565726224, 0.0641722892366239, 0.22460301232818364, 0.3529475908014314, 0.02139076307887463, 0.29947068310424485, 0.04278152615774926, 0.09220975329204811, 0.16136706826108418, 0.36883901316819245, 0.04610487664602406, 0.09220975329204811, 0.06915731496903608, 0.18441950658409623, 0.23417896523693538, 0.26763310312792615, 0.13381655156396308, 0.03345413789099077, 0.26763310312792615, 0.03345413789099077, 0.25337406152363223, 0.004872578106223696, 0.16566765561160568, 0.1461773431867109, 0.04385320295601327, 0.004872578106223696, 0.02923546863734218, 0.02923546863734218, 0.24850148341740852, 0.07796124969957914, 0.035226026243879785, 0.2113561574632787, 0.17613013121939894, 0.2818082099510383, 0.3170342361949181, 0.13865705778607815, 0.13865705778607815, 0.046219019262026054, 0.13865705778607815, 0.4621901926202605, 0.13865705778607815, 0.08193269705848448, 0.3277307882339379, 0.04096634852924224, 0.02048317426462112, 0.02048317426462112, 0.2662812654400745, 0.16386539411696896, 0.08193269705848448, 0.07726026849239252, 0.06180821479391401, 0.7107944701300112, 0.030904107396957006, 0.015452053698478503, 0.04635616109543551, 0.04635616109543551, 0.07193626911676101, 0.08992033639595128, 0.17984067279190255, 0.05395220183757077, 0.25177694190866357, 0.3596813455838051, 0.06422946975162061, 0.29487165658698555, 0.017517128114078347, 0.07006851245631339, 0.11094181138916287, 0.12845893950324122, 0.026275692171117523, 0.1664127170837443, 0.029195213523463916, 0.09050516192273814, 0.02027923821813763, 0.04055847643627526, 0.5678186701078537, 0.36502628792647734, 0.14811098470476797, 0.14811098470476797, 0.46549166621498506, 0.021158712100681137, 0.04231742420136227, 0.06347613630204342, 0.08463484840272455, 0.021158712100681137, 0.023699713436780782, 0.20144756421263665, 0.01974976119731732, 0.06319923583141542, 0.07899904478926928, 0.10269875822605005, 0.1737978985363924, 0.29624641795975976, 0.0434494746340981, 0.08673127053660205, 0.08673127053660205, 0.06504845290245154, 0.4336563526830103, 0.021682817634150513, 0.04336563526830103, 0.26019381160980615, 0.05709140200464059, 0.04893548743254908, 0.008155914572091513, 0.03262365828836605, 0.5790699346184974, 0.09787097486509816, 0.17127420601392176, 0.008155914572091513, 0.07366863164362537, 0.7857987375320039, 0.07366863164362537, 0.049112421095750244, 0.12090333838219032, 0.16120445117625376, 0.1282308134356564, 0.18685061386338506, 0.0073274750534660805, 0.38835617783370224, 0.0036637375267330402, 0.3807589954994615, 0.25924016714856957, 0.04860753134035679, 0.29164518804214073, 0.008101255223392799, 0.8404049020565844, 0.0726275841283468, 0.0726275841283468, 0.010571346070911954, 0.12333237082730612, 0.06695185844910904, 0.4158062787892035, 0.2396171776073376, 0.03523782023637318, 0.08104698654365831, 0.03171403821273586, 0.2853711039273736, 0.10071921315083773, 0.1678653552513962, 0.2517980328770943, 0.18465189077653582, 0.01678653552513962, 0.061060128621645095, 0.2837500094770566, 0.1939557026805197, 0.04669303953419919, 0.02514240590303033, 0.03591772271861476, 0.04310126726233771, 0.14367089087445903, 0.08261076225281395, 0.08620253452467543, 0.08511181239983123, 0.10638976549978904, 0.14894567169970466, 0.07801916136651196, 0.0709265103331927, 0.06738018481653306, 0.08865813791649087, 0.19504790341627992, 0.07801916136651196, 0.07801916136651196, 0.9182281067296505, 0.014810130753704041, 0.059240523014816165, 0.5630428112359273, 0.06256031235954748, 0.3128015617977374, 0.03128015617977374, 0.034521168813845406, 0.034521168813845406, 0.10356350644153621, 0.017260584406922703, 0.08630292203461351, 0.15534525966230434, 0.31069051932460867, 0.27616935051076325, 0.0693026402417147, 0.0693026402417147, 0.03465132012085735, 0.0693026402417147, 0.6237237621754324, 0.1386052804834294, 0.0840866931969811, 0.11376434961944502, 0.27699145994299657, 0.10387179747862371, 0.06924786498574914, 0.11871062568985567, 0.014838828211231959, 0.0840866931969811, 0.0939792453378024, 0.03957020856328523, 0.0241562860420823, 0.0724688581262469, 0.21740657437874067, 0.0724688581262469, 0.5797508650099752, 0.0241562860420823, 0.020858986019280643, 0.813500454751945, 0.12515391611568386, 0.020858986019280643, 0.08591710721742368, 0.31392789175597113, 0.003304504123747065, 0.016522520618735322, 0.45602156907709496, 0.003304504123747065, 0.04295855360871184, 0.07600359484618249, 0.21492755560623233, 0.6877681779399435, 0.021492755560623234, 0.0644782666818697, 0.28733334917786374, 0.01026190532778085, 0.0205238106555617, 0.3078571598334255, 0.030785715983342544, 0.3283809704889872, 0.01026190532778085, 0.3925879029260824, 0.3925879029260824, 0.1962939514630412, 0.15469484753985827, 0.05625267183267573, 0.07734742376992913, 0.0632842558117602, 0.014063167958168933, 0.007031583979084466, 0.0843790077490136, 0.007031583979084466, 0.527368798431335, 0.38348435056676883, 0.034862213687888076, 0.5577954190062092, 0.034862213687888076, 0.10279068440916972, 0.48825575094355617, 0.015418602661375458, 0.020558136881833944, 0.005139534220458486, 0.34948832699117705, 0.020558136881833944, 0.025939971719871543, 0.09511322963952899, 0.008646657239957181, 0.017293314479914362, 0.034586628959828725, 0.017293314479914362, 0.7609058371162319, 0.034586628959828725, 0.1766546068259055, 0.08832730341295275, 0.15898914614331497, 0.052996382047771656, 0.512298359795126, 0.15633477992173458, 0.07816738996086729, 0.5211159330724486, 0.20844637322897944, 0.011890625053540264, 0.020808593843695464, 0.2021406259101845, 0.23483984480742023, 0.3032109388652767, 0.002972656263385066, 0.02378125010708053, 0.02972656263385066, 0.13376953185232798, 0.03567187516062079, 0.09983180171572324, 0.09983180171572324, 0.07487385128679244, 0.2121425786459119, 0.09983180171572324, 0.07487385128679244, 0.1747056530025157, 0.12478975214465406, 0.012478975214465405, 0.02495795042893081, 0.2252910566396334, 0.1501940377597556, 0.4130336038393279, 0.1501940377597556, 0.0750970188798778, 0.17569552652069148, 0.597364790170351, 0.035139105304138295, 0.14055642121655318, 0.035139105304138295, 0.022716965883596875, 0.11358482941798437, 0.22716965883596874, 0.10601250745678542, 0.07572321961198958, 0.04922009274779323, 0.0719370586313901, 0.1628049221657776, 0.14766027824337968, 0.026503126864196355, 0.7511580598977693, 0.026827073567777478, 0.053654147135554955, 0.13413536783888738, 0.11625997752009636, 0.063942987636053, 0.011625997752009636, 0.29646294267624573, 0.24414595279220236, 0.011625997752009636, 0.011625997752009636, 0.20926795953617344, 0.03487799325602891, 0.058424537714712524, 0.03689970803034475, 0.13376144160999973, 0.06457448905310331, 0.10301168491804577, 0.1598987347981606, 0.06918695255689641, 0.27828529806218333, 0.05534956204551713, 0.041512171534137846, 0.010440585913362738, 0.438504608361235, 0.05220292956681369, 0.10440585913362738, 0.010440585913362738, 0.37586109288105857, 0.942999043108613, 0.012743230312278555, 0.012743230312278555, 0.02548646062455711, 0.9579582444796194, 0.029029037711503617, 0.25611242824453173, 0.23334687906724003, 0.0626052602375522, 0.011382774588645854, 0.011382774588645854, 0.18781578071265662, 0.2048899425956254, 0.02276554917729171, 0.005691387294322927, 0.34916996509253045, 0.17917932419221957, 0.15620761596244784, 0.009188683291908696, 0.07810380798122392, 0.06891512468931522, 0.009188683291908696, 0.07350946633526957, 0.05972644139740652, 0.013783024937863044, 0.08321199140585403, 0.34324946454914784, 0.010401498925731754, 0.010401498925731754, 0.06240899355439052, 0.06240899355439052, 0.03120449677719526, 0.36405246240061134, 0.03120449677719526, 0.0659005657937355, 0.29106083225566515, 0.11258013323096482, 0.23339783718614657, 0.013729284540361562, 0.06864642270180782, 0.0027458569080723127, 0.18946412665698956, 0.01922099835650619, 0.09507099760934572, 0.06111706989172225, 0.1222341397834445, 0.244468279566889, 0.09507099760934572, 0.08148942652229633, 0.14260649641401857, 0.1290249253269692, 0.02716314217409878, 0.0037898640723872546, 0.12127565031639215, 0.14780469882310293, 0.12885537846116665, 0.011369592217161764, 0.022739184434323528, 0.04926823294103431, 0.14401483475071566, 0.3676168150215637, 0.1284765618972646, 0.10764252483284333, 0.15972761749389655, 0.1631999570046334, 0.03472339510736881, 0.11458720385431709, 0.02430637657515817, 0.10417018532210644, 0.09722550630063267, 0.05902977168252698, 0.046245529284805305, 0.1202383761404938, 0.38846244599236457, 0.03699642342784425, 0.027747317570883185, 0.009249105856961062, 0.046245529284805305, 0.1202383761404938, 0.08324195271264956, 0.11098927028353274, 0.11243830337672464, 0.14399993239475262, 0.09073968342683042, 0.1104657015630979, 0.0986300906813374, 0.06312325803605594, 0.043397239899788456, 0.11638350700397813, 0.15386294146288634, 0.06904106347693618, 0.1923263489838736, 0.033939943938330636, 0.05656657323055106, 0.016969971969165318, 0.10747648913804701, 0.1923263489838736, 0.1923263489838736, 0.03959660126138574, 0.16969971969165318, 0.07835763986521867, 0.05223842657681244, 0.49626505247971825, 0.3917881993260933, 0.042542636053420464, 0.042542636053420464, 0.8083100850149888, 0.10635659013355116, 0.12118380687678029, 0.568631709191046, 0.14914930077142188, 0.11186197557856642, 0.03728732519285547, 0.96166589032044, 0.16977889466052257, 0.05659296488684085, 0.1131859297736817, 0.6225226137552494, 0.028296482443420427, 0.11970694383606115, 0.0049877893265025475, 0.04987789326502547, 0.2942795702636503, 0.14464589046857387, 0.14464589046857387, 0.2294383090191172, 0.0049877893265025475, 0.24856351449631767, 0.20337014822425992, 0.384143613312491, 0.13558009881617328, 0.030172639937537483, 0.08548914648968954, 0.005028773322922914, 0.17600706630230198, 0.3318990393129123, 0.09051791981261245, 0.2765825327607603, 0.005028773322922914, 0.25497593299815297, 0.708266480550425, 0.11735152320967664, 0.25426163362096604, 0.027382022082257883, 0.10170465344838642, 0.00782343488064511, 0.05476404416451577, 0.03520545696290299, 0.06258747904516088, 0.19558587201612773, 0.14473354529193452, 0.02003776965234151, 0.10686810481248807, 0.4274724192499523, 0.1469436441171711, 0.04675479585546353, 0.02003776965234151, 0.10686810481248807, 0.08683033516014656, 0.04007553930468302, 0.14982229480363418, 0.7191470150574442, 0.08989337688218052, 0.30153663485260657, 0.09046099045578196, 0.12061465394104262, 0.06030732697052131, 0.06030732697052131, 0.10553782219841229, 0.12061465394104262, 0.12061465394104262, 0.5755195212532707, 0.0863279281879906, 0.14387988031331767, 0.02877597606266353, 0.14387988031331767, 0.18022222337878202, 0.06360784354545249, 0.5300653628787707, 0.021202614515150826, 0.20142483789393287, 0.16942184998082566, 0.18636403497890824, 0.11435974873705733, 0.0508265549942477, 0.0804753787408922, 0.021177731247603207, 0.038119916245685774, 0.18212848872938758, 0.11435974873705733, 0.042355462495206414, 0.03866155999644891, 0.15167227383222262, 0.19628176613581752, 0.10408881537505475, 0.05947932307145986, 0.13680244306435768, 0.008921898460718978, 0.13382847691078467, 0.15167227383222262, 0.017843796921437956, 0.10318719292233094, 0.2688297920871254, 0.07331721602376146, 0.03530088178921848, 0.13848807471154942, 0.06517085868778796, 0.010861809781297993, 0.1900816711727149, 0.08960993069570845, 0.024439072007920486, 0.3108003670049571, 0.032151762103961076, 0.24649684279703493, 0.010717254034653694, 0.08573803227722955, 0.021434508069307388, 0.2572140968316886, 0.021434508069307388, 0.010717254034653694, 0.26701833016591975, 0.02966870335176886, 0.11867481340707545, 0.44503055027653293, 0.11867481340707545, 0.3125936238282885, 0.10745405819097416, 0.10745405819097416, 0.19537101489268027, 0.03907420297853606, 0.22467666712658232, 0.009768550744634015, 0.022015856471059435, 0.055039641177648584, 0.1320951388263566, 0.03302378470658915, 0.03302378470658915, 0.3852774882435401, 0.04403171294211887, 0.18713478000400519, 0.03302378470658915, 0.07705549764870802, 0.18448118222536689, 0.04612029555634172, 0.04612029555634172, 0.2306014777817086, 0.04612029555634172, 0.36896236445073377, 0.05521933959863081, 0.11043867919726162, 0.22087735839452324, 0.38653537719041564, 0.11043867919726162, 0.05521933959863081, 0.036812893065753874, 0.04919973976458529, 0.2213988289406338, 0.07379960964687794, 0.024599869882292645, 0.2213988289406338, 0.24599869882292646, 0.1721990891760485, 0.11324649255589855, 0.1956075780510975, 0.06177081412139921, 0.10295135686899869, 0.05147567843449934, 0.041180542747599476, 0.3191492062938959, 0.11324649255589855, 0.03805554755848151, 0.03805554755848151, 0.47569434448101894, 0.0951388688962038, 0.05708332133772227, 0.03805554755848151, 0.0951388688962038, 0.05708332133772227, 0.1331944164546853, 0.03610009559981529, 0.20456720839895334, 0.10830028679944588, 0.024066730399876862, 0.048133460799753724, 0.07220019119963059, 0.18050047799907648, 0.16846711279913804, 0.060166825999692156, 0.10830028679944588, 0.08140864956977774, 0.12291109837005659, 0.1372773306470762, 0.1404698267086361, 0.08460114563133765, 0.10854486609303698, 0.03990620076949889, 0.10535237003147707, 0.13089233852395638, 0.05107993698495858, 0.07800636168332005, 0.11700954252498007, 0.058504771262490034, 0.35102862757494024, 0.13651113294581008, 0.019501590420830012, 0.17551431378747012, 0.039003180841660025, 0.025766461751123872, 0.7417060061216372, 0.05705430816320286, 0.00368092310730341, 0.06073523127050627, 0.020245077090168755, 0.08466123146797844, 0.005521384660955115, 0.9843393378305264, 0.5878773290957265, 0.03458101935857215, 0.05187152903785822, 0.15561458711357468, 0.15561458711357468], \"Term\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"ability\", \"ability\", \"ability\", \"ability\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"adjoining\", \"adjoining\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"agent\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"anaphora\", \"anaphora\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"argument\", \"argument\", \"argument\", \"argument\", \"argument\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automaton\", \"automaton\", \"automaton\", \"average\", \"average\", \"average\", \"average\", \"average\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"bilingual\", \"bilingual\", \"bilingual\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"capture\", \"capture\", \"capture\", \"capture\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"categorial\", \"categorial\", \"categorial_grammar\", \"categorial_grammar\", \"centering\", \"change\", \"change\", \"change\", \"change\", \"change\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clustering\", \"clustering\", \"clustering\", \"combining\", \"combining\", \"combining\", \"combining\", \"combining\", \"combining\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"content\", \"content\", \"content\", \"content\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context_free\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"correction\", \"correction\", \"correction\", \"correction\", \"correction\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"database\", \"database\", \"database\", \"database\", \"database\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"dialogue\", \"dialogue\", \"dialogue\", \"dialogue\", \"dialogue\", \"dialogue\", \"dialogue\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"difference\", \"difference\", \"difference\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"distance\", \"distance\", \"distance\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"document\", \"document\", \"document\", \"document\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"down\", \"down\", \"down\", \"down\", \"down\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effective\", \"effective\", \"effective\", \"effective\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimation\", \"estimation\", \"estimation\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluating\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature_structure\", \"feature_structure\", \"feature_structure\", \"field\", \"field\", \"field\", \"field\", \"field\", \"finite\", \"finite\", \"finite\", \"finite_state\", \"finite_state\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"formal\", \"formal\", \"formal\", \"formal\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formula\", \"formula\", \"formula\", \"formula\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"german\", \"german\", \"german\", \"german\", \"german\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gram\", \"gram\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"handle\", \"handle\", \"handle\", \"handle\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"head\", \"head\", \"head\", \"head\", \"head\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"hidden\", \"hidden\", \"hidden\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"hpsg\", \"hpsg\", \"hpsg\", \"hpsg\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"induction\", \"induction\", \"induction\", \"induction\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"integrated\", \"integrated\", \"integrated\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge_base\", \"knowledge_base\", \"knowledge_base\", \"knowledge_base\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language_processing\", \"language_processing\", \"language_processing\", \"language_processing\", \"language_processing\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexicalized\", \"lexicalized\", \"lexicalized\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"logic\", \"logic\", \"logic\", \"logic\", \"low\", \"low\", \"low\", \"low\", \"low\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"main\", \"main\", \"main\", \"main\", \"main\", \"making\", \"making\", \"making\", \"making\", \"making\", \"making\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"markov\", \"markov\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphology\", \"morphology\", \"morphology\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"network\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun_phrase\", \"noun_phrase\", \"noun_phrase\", \"noun_phrase\", \"noun_phrase\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"par\", \"par\", \"par\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"parse\", \"parse\", \"parse\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"plan\", \"plan\", \"plan\", \"plan\", \"planning\", \"planning\", \"planning\", \"precision\", \"precision\", \"precision\", \"precision\", \"preference\", \"preference\", \"preference\", \"preference\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processor\", \"processor\", \"processor\", \"processor\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"programming\", \"programming\", \"programming\", \"programming\", \"prolog\", \"prolog\", \"pronoun\", \"pronoun\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"provides\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"question\", \"question\", \"question\", \"question\", \"question\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"recall\", \"recall\", \"recall\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"represented\", \"represented\", \"represented\", \"represented\", \"represented\", \"require\", \"require\", \"require\", \"require\", \"require\", \"require\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"sense\", \"sense\", \"sense\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"similarity\", \"similarity\", \"similarity\", \"situation\", \"situation\", \"situation\", \"situation\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"specification\", \"specification\", \"specification\", \"specification\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech_recognition\", \"speech_recognition\", \"speech_recognition\", \"speech_recognition\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken_language\", \"spoken_language\", \"spoken_language\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"string\", \"string\", \"string\", \"string\", \"string\", \"structural\", \"structural\", \"structural\", \"structural\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"style\", \"style\", \"style\", \"style\", \"style\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tagger\", \"tagger\", \"tagger\", \"tagger\", \"tagging\", \"tagging\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"top\", \"top\", \"top\", \"top\", \"topic\", \"topic\", \"topic\", \"topic\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transducer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree_adjoining\", \"tree_adjoining\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"typed\", \"typed\", \"typed\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"unification\", \"unification\", \"unification\", \"unification\", \"unification\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"variation\", \"variation\", \"variation\", \"variation\", \"variation\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"via\", \"via\", \"via\", \"via\", \"via\", \"via\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word_sense\", \"world\", \"world\", \"world\", \"world\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el201221403468889580161222937532\", ldavis_el201221403468889580161222937532_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el201221403468889580161222937532\", ldavis_el201221403468889580161222937532_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el201221403468889580161222937532\", ldavis_el201221403468889580161222937532_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df_train_1000['Abstract'].tolist()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "#!pip3 install gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "#only ones that appear 20 times or more.\n",
    "bigram = Phrases(docs, min_count=25)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "#training parameters.\n",
    "NUM_TOPICS = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None \n",
    "\n",
    "temp = dictionary[0]  \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "outputfile = f'model{NUM_TOPICS}.gensim'\n",
    "print(\"Saving model in \" + outputfile)\n",
    "print(\"\")\n",
    "model.save(outputfile)\n",
    "\n",
    "top_topics = model.top_topics(corpus)\n",
    "model.num_topics\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / NUM_TOPICS\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "model.print_topics( num_words=20)\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-roots",
   "metadata": {},
   "source": [
    "## LDA with K=40 and 20K training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "later-asset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6303\n",
      "Number of documents: 20000\n",
      "Saving model in model40.gensim\n",
      "\n",
      "Average topic coherence: -3.9125.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el201221403475790047365016217701\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el201221403475790047365016217701_data = {\"mdsDat\": {\"x\": [0.11200431778016035, -0.04867291153929795, 0.013520674231992826, -0.16139309315002637, -0.07962027188252088, 0.16365401123924883, -0.20100960770027712, -0.09721595366266896, 0.011093427448433418, -0.1410207532192574, -0.007377849167717547, 0.17987903082672652, 0.16230248195993904, -0.05972812374556031, -0.11426237112127034, 0.016500764299095933, 0.12384571366757362, 0.13599734589282003, 0.003889736261065032, 0.060246805685751276, -0.004018251911500936, -0.02117410961010468, 0.10208014381403821, -0.060233235510849796, 0.012771337015391989, 0.09075325899679955, -0.21210319051687312, -0.11507517012368494, -0.13008597319327003, 0.07769813752415743, -0.06006330729161829, 0.10475160137042083, -0.1481794050433077, 0.19780431528136208, -0.12399050522950461, 0.006062386185656371, -0.08443298329292333, -0.04884721376337745, 0.12932331975152672, 0.21432547144345315], \"y\": [0.11912419679616917, 0.16643090528093524, -0.23650919585377272, 0.03880900263526556, -0.12100158955093106, -0.016174847118208796, -0.05225074737772882, 0.013512101088811432, 0.11125665685877842, 0.07563433056793709, -0.16417667193496624, 0.048759305980189765, 0.07535163168809429, 0.09575609387841484, -0.07081633638542674, 0.12909068107601046, -0.049718347950435705, -0.06600037553609779, 0.0754375977787636, -0.010804587616035612, -0.16801106138296196, -0.12008926878635041, -0.03531868412922808, -0.07256299245643272, 0.0021360623864405985, -0.1310802761698743, 0.16581112927805572, -0.13051137933151458, 0.21276172528872353, -0.16602087172448596, 0.10870512115059591, 0.05243938283890654, -0.025124054220568105, 0.05170430522566198, 0.009809621088938259, 0.05273222340920015, -0.03342779334843796, -0.14120913137586147, 0.1285517791910691, 0.07699435876235435], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [1.1526631618568937, 1.5762074817175373, 2.177502802255056, 3.1080472294269033, 2.985211347012236, 0.673374708081549, 17.77530770746869, 3.292639748453218, 1.4619365794353025, 3.3693927115377384, 2.121916637878984, 0.8511800853607686, 0.7285999234130931, 1.5701240659491147, 2.868088112814821, 1.2579634368304455, 1.097030869910829, 1.0035067414286938, 1.868186104484642, 0.8805989586444373, 1.5896758182181054, 1.5198102735635406, 0.748540664002088, 1.8843614445458166, 1.3245116823668137, 1.1916372572554628, 9.960649764747979, 4.117701485122809, 2.751646958709611, 1.1845825138180923, 1.547697415157679, 0.8108480400528976, 5.010883801288105, 0.6231615097920977, 5.7358806916503084, 1.2936711980569826, 2.084789511555763, 3.2381934432586896, 0.8920981003869738, 0.6701800124892381]}, \"tinfo\": {\"Term\": [\"system\", \"network\", \"algorithm\", \"data\", \"model\", \"code\", \"method\", \"graph\", \"channel\", \"problem\", \"bound\", \"ha\", \"information\", \"based\", \"been\", \"have\", \"user\", \"source\", \"complexity\", \"distribution\", \"state\", \"be\", \"node\", \"random\", \"set\", \"quantum\", \"signal\", \"coding\", \"time\", \"scheme\", \"graph\", \"edge\", \"vertex\", \"et\", \"al\", \"et_al\", \"at_least\", \"log\", \"directed\", \"shortest\", \"epsilon\", \"bipartite\", \"shortest_path\", \"randomness\", \"bipartite_graph\", \"entanglement\", \"kolmogorov\", \"balanced\", \"acyclic\", \"median\", \"gamma\", \"probe\", \"log_log\", \"kolmogorov_complexity\", \"straight\", \"polyhedron\", \"game_theoretic\", \"travel\", \"recently_introduced\", \"interconnection\", \"path\", \"least\", \"completion\", \"connected\", \"factor\", \"at\", \"degree\", \"geometric\", \"pair\", \"matching\", \"bound\", \"lower\", \"upper\", \"sparse\", \"lower_bound\", \"upper_bound\", \"recovery\", \"tight\", \"error_probability\", \"forest\", \"intrusion_detection\", \"bob\", \"denotes\", \"ct\", \"homology\", \"matrix_multiplication\", \"wired\", \"math\", \"probability_density\", \"elliptic\", \"amplifier\", \"displacement\", \"leq\", \"flipping\", \"morphisms\", \"deciding_whether\", \"unconditional\", \"simplicial\", \"von\", \"substring\", \"spectral\", \"recover\", \"measurement\", \"theta\", \"sqrt\", \"constant\", \"denote\", \"error\", \"probability\", \"derive\", \"gap\", \"best\", \"our\", \"every\", \"known\", \"number\", \"size\", \"minimization\", \"random\", \"new\", \"deterministic\", \"optimal\", \"small\", \"from\", \"term\", \"than\", \"also\", \"architecture\", \"memory\", \"cell\", \"storage\", \"processor\", \"module\", \"maximum_likelihood\", \"profile\", \"smart\", \"reversible\", \"chip\", \"instruction\", \"phone\", \"layout\", \"satellite\", \"next_generation\", \"layered\", \"green\", \"pc\", \"main_contribution\", \"film\", \"service_oriented\", \"resource_management\", \"living\", \"modulated\", \"atm\", \"locating\", \"exposure\", \"failed\", \"nation\", \"hybrid\", \"platform\", \"system\", \"likelihood\", \"generation\", \"implementation\", \"execution\", \"intelligent\", \"processing\", \"integration\", \"design\", \"performance\", \"application\", \"machine\", \"requirement\", \"component\", \"area\", \"scheduling\", \"mobile\", \"based\", \"high\", \"this_paper\", \"using\", \"paper\", \"present\", \"level\", \"support\", \"logic\", \"calculus\", \"chain\", \"hop\", \"modulation\", \"markov_chain\", \"monte\", \"carlo\", \"monte_carlo\", \"bit_error\", \"mathematics\", \"modal\", \"axiom\", \"propositional\", \"wave\", \"knowledge_base\", \"recursion\", \"induction\", \"reverse\", \"su\", \"modulo\", \"markovian\", \"csma\", \"quantifier\", \"expressiveness\", \"decidability\", \"concurrency\", \"reasoning_about\", \"revision\", \"inductive\", \"first_order\", \"checking\", \"logic_programming\", \"equivalence\", \"reasoning\", \"higher_order\", \"semantics\", \"logical\", \"interpretation\", \"constructive\", \"fragment\", \"proof\", \"theory\", \"rule\", \"category\", \"notion\", \"definition\", \"operator\", \"verification\", \"order\", \"formula\", \"type\", \"formal\", \"relation\", \"theorem\", \"term\", \"property\", \"extension\", \"first\", \"a\", \"based\", \"set\", \"based_on\", \"system\", \"project\", \"privacy\", \"market\", \"site\", \"business\", \"risk\", \"coding_scheme\", \"people\", \"multiple_output\", \"customer\", \"transaction\", \"industry\", \"open_source\", \"enterprise\", \"financial\", \"disease\", \"developer\", \"fractal\", \"join\", \"software\", \"software_engineering\", \"institution\", \"actor\", \"astronomy\", \"perfectly\", \"educational\", \"association_rule\", \"emotion\", \"authority\", \"laboratory\", \"experience\", \"engineering\", \"development\", \"virtual\", \"organization\", \"supply\", \"technology\", \"coupling\", \"external\", \"effort\", \"management\", \"tool\", \"will\", \"process\", \"research\", \"product\", \"quality\", \"will_be\", \"human\", \"support\", \"open\", \"methodology\", \"study\", \"cost\", \"at\", \"a\", \"paper\", \"from\", \"selection\", \"kernel\", \"face\", \"update\", \"action\", \"visual\", \"tracking\", \"coordinate\", \"conflict\", \"tuning\", \"contact\", \"uniqueness\", \"discrepancy\", \"composite\", \"compatibility\", \"facial\", \"removing\", \"markov_decision\", \"saturation\", \"multiplicity\", \"eye\", \"continuation\", \"dominance\", \"patch\", \"dominated\", \"reinforcement\", \"inferring\", \"partner\", \"envelope\", \"pointed\", \"trading\", \"collaborative\", \"validation\", \"neuron\", \"decision\", \"coordination\", \"preserving\", \"assessment\", \"learning\", \"inference\", \"change\", \"activity\", \"active\", \"human\", \"selecting\", \"belief\", \"antenna\", \"should\", \"broadcast\", \"real_time\", \"beamforming\", \"state_information\", \"secrecy\", \"should_be\", \"lambda\", \"must_be\", \"broadcast_channel\", \"try\", \"trajectory\", \"name\", \"protection\", \"search_engine\", \"day\", \"person\", \"most_important\", \"act\", \"threat\", \"unfortunately\", \"ubiquitous\", \"rise\", \"multi_agent\", \"pressure\", \"copy\", \"bounding\", \"extra\", \"located\", \"do\", \"may_be\", \"because\", \"may\", \"do_not\", \"usually\", \"easily\", \"able_to\", \"make\", \"able\", \"therefore\", \"although\", \"rather\", \"be\", \"but\", \"what\", \"can\", \"advantage\", \"more\", \"not\", \"they\", \"it\", \"can_be\", \"or\", \"however\", \"many\", \"a\", \"their\", \"such_a\", \"such\", \"how\", \"other\", \"which\", \"way\", \"very\", \"real\", \"one\", \"use\", \"have\", \"used\", \"only\", \"work\", \"from\", \"these\", \"paper\", \"different\", \"new\", \"some\", \"this_paper\", \"also\", \"time\", \"system\", \"application\", \"model\", \"stochastic\", \"cellular\", \"outer\", \"modelling\", \"evolutionary\", \"assembly\", \"cellular_automaton\", \"maximizing\", \"time_series\", \"country\", \"carrier\", \"qualitative\", \"sigma\", \"modeling\", \"fpga\", \"placement\", \"hidden_markov\", \"fingerprint\", \"subsequence\", \"pr\", \"artificial_neural\", \"variational\", \"matched\", \"computer_simulation\", \"stabilization\", \"cumulative\", \"recovering\", \"ann\", \"dichotomy\", \"series\", \"markov\", \"fluctuation\", \"population\", \"dynamic\", \"simulated\", \"self\", \"conditional\", \"evolution\", \"hidden\", \"process\", \"evolving\", \"internal\", \"simulation\", \"dynamical\", \"behavior\", \"coupled\", \"observed\", \"between\", \"parameter\", \"relationship\", \"time\", \"state\", \"study\", \"two\", \"structure\", \"a\", \"different\", \"result\", \"system\", \"both\", \"which\", \"distribution\", \"entropy\", \"symmetry\", \"poisson\", \"probability_distribution\", \"collision\", \"temperature\", \"prime\", \"phase_transition\", \"spin\", \"numerical_simulation\", \"replica\", \"odd\", \"outlier\", \"statistical_physic\", \"symmetry_breaking\", \"principal_component\", \"statistical_mechanic\", \"reviewed\", \"tuples\", \"disorder\", \"glass\", \"binomial\", \"entropic\", \"implying\", \"progression\", \"spin_glass\", \"beforehand\", \"nine\", \"threshold\", \"sequence\", \"random\", \"principal\", \"probability\", \"breaking\", \"phase\", \"statistical\", \"transition\", \"calculated\", \"theoretical_analysis\", \"average\", \"physic\", \"critical\", \"shannon\", \"expansion\", \"length\", \"numerical_result\", \"energy\", \"numerical\", \"large\", \"number\", \"exponential\", \"size\", \"cluster\", \"analysis\", \"process\", \"parameter\", \"equation\", \"rank\", \"filter\", \"differential\", \"norm\", \"nonlinear\", \"inverse\", \"subspace\", \"lp\", \"diffusion\", \"divergence\", \"box\", \"downlink\", \"constellation\", \"tensor\", \"fourier\", \"integral\", \"singular\", \"derivative\", \"normalized\", \"uplink\", \"black\", \"simulator\", \"sc\", \"unitary\", \"transforms\", \"poly\", \"residual\", \"determinant\", \"induced_by\", \"matrix\", \"transform\", \"group\", \"vector\", \"geometry\", \"linear\", \"variable\", \"function\", \"field\", \"element\", \"algebraic\", \"random_variable\", \"numerical\", \"space\", \"algebra\", \"finite\", \"discrete\", \"operator\", \"non\", \"representation\", \"zero\", \"order\", \"solution\", \"form\", \"which\", \"a\", \"theory\", \"sum\", \"using\", \"from\", \"reconstruction\", \"resolution\", \"surface\", \"filtering\", \"projection\", \"ranking\", \"column\", \"row\", \"interpolation\", \"by_applying\", \"arxiv\", \"null\", \"pursuit\", \"consecutive\", \"anomaly\", \"reconstruct\", \"rounding\", \"reconstructing\", \"maximum_entropy\", \"correspondence_between\", \"approaching\", \"novelty\", \"mark\", \"radical\", \"tomography\", \"reproduce\", \"randomly_generated\", \"hit\", \"propagate\", \"print\", \"volume\", \"detection\", \"frame\", \"method\", \"target\", \"experimental_result\", \"estimation\", \"identification\", \"based_on\", \"based\", \"parametric\", \"experimental\", \"count\", \"estimated\", \"proposed\", \"accuracy\", \"using\", \"approach\", \"analysis\", \"we_propose\", \"propose\", \"technique\", \"result\", \"novel\", \"compared\", \"new\", \"comparison\", \"parameter\", \"from\", \"program\", \"de\", \"partition\", \"solver\", \"termination\", \"modular\", \"cr\", \"sat\", \"clause\", \"planar_graph\", \"programming_language\", \"undirected\", \"hamiltonian\", \"subgraph\", \"data_stream\", \"thesis\", \"ary\", \"atom\", \"minor\", \"undirected_graph\", \"preprocessing\", \"motif\", \"this_thesis\", \"sequent\", \"arising_from\", \"pac\", \"download\", \"girth\", \"sequent_calculus\", \"fix\", \"programming\", \"window\", \"cycle\", \"assignment\", \"logic_program\", \"java\", \"planar\", \"prolog\", \"core\", \"stream\", \"formula\", \"variable\", \"satisfiability\", \"string\", \"list\", \"page\", \"price\", \"opinion\", \"perturbation\", \"education\", \"pricing\", \"decision_tree\", \"normal_form\", \"contract\", \"longest\", \"leaf\", \"web_page\", \"artificial_intelligence\", \"answering\", \"attracted\", \"edit\", \"stock\", \"similarity_between\", \"cutting\", \"pattern_matching\", \"edit_distance\", \"offs\", \"trade_offs\", \"lookup\", \"observing\", \"similarity_measure\", \"intelligence\", \"committee\", \"tree\", \"pattern\", \"permutation\", \"web\", \"matching\", \"fold\", \"similarity\", \"normal\", \"artificial\", \"occurrence\", \"insertion\", \"composition\", \"decision\", \"structure\", \"distance\", \"length\", \"between\", \"code\", \"convolutional\", \"correcting\", \"error_correcting\", \"distance_between\", \"hamming\", \"cube\", \"convolutional_code\", \"cache\", \"percolation\", \"replacement\", \"visibility\", \"live\", \"clinical\", \"digital_library\", \"caching\", \"driving\", \"screen\", \"hamming_distance\", \"toolkit\", \"bit_per\", \"viterbi\", \"closest\", \"more_importantly\", \"unnecessary\", \"controllable\", \"conducting\", \"coset\", \"reducibility\", \"seminal\", \"distance\", \"binary\", \"decoding\", \"weight\", \"length\", \"redundancy\", \"error\", \"construction\", \"encoding\", \"minimum\", \"coded\", \"perfect\", \"complexity\", \"block\", \"linear\", \"over\", \"generalized\", \"space\", \"family\", \"new\", \"maximum\", \"these\", \"design\", \"from\", \"have\", \"abstract\", \"top\", \"consensus\", \"schema\", \"rewriting\", \"closure\", \"typed\", \"nominal\", \"unification\", \"we_believe\", \"token\", \"register\", \"bisimulation\", \"atomic\", \"safe\", \"notation\", \"write\", \"road\", \"merging\", \"cope\", \"affected\", \"replication\", \"cope_with\", \"tier\", \"declarative\", \"one_hand\", \"rewrite\", \"hypergraph\", \"asp\", \"deduction\", \"query\", \"we_describe\", \"abstraction\", \"high_level\", \"language\", \"describe\", \"relational\", \"rely_on\", \"rely\", \"semantic\", \"answer\", \"domain\", \"representation\", \"constraint\", \"read\", \"description\", \"specification\", \"structure\", \"context\", \"framework\", \"functional\", \"level\", \"view\", \"meaning\", \"how\", \"formal\", \"show_how\", \"natural\", \"a\", \"approach\", \"our\", \"present\", \"from\", \"based\", \"which\", \"tool\", \"quantum\", \"inequality\", \"gate\", \"intersection\", \"correction\", \"adversary\", \"covering\", \"running_time\", \"oracle\", \"computational_complexity\", \"error_correction\", \"branching\", \"thermal\", \"spatially\", \"local_search\", \"card\", \"sided\", \"intuitionistic\", \"key_distribution\", \"quantum_computation\", \"labeling\", \"quantum_computer\", \"wire\", \"withdrawn\", \"open_question\", \"adiabatic\", \"resp\", \"qubits\", \"qubit\", \"universe\", \"relaxation\", \"cryptography\", \"classical\", \"complexity\", \"lemma\", \"amplitude\", \"round\", \"computation\", \"bit\", \"legitimate\", \"computational\", \"communication\", \"state\", \"boolean\", \"query\", \"theory\", \"computer\", \"proof\", \"error\", \"computing\", \"way\", \"data\", \"object\", \"shape\", \"mining\", \"character\", \"attribute\", \"data_mining\", \"format\", \"repository\", \"rotation\", \"warehouse\", \"object_oriented\", \"metadata\", \"normalization\", \"data_warehouse\", \"navigation\", \"nonzero\", \"decision_support\", \"arrangement\", \"ranging\", \"ternary\", \"large_amount\", \"polytope\", \"character_recognition\", \"ranging_from\", \"hub\", \"inconsistent\", \"standardization\", \"post_processing\", \"hyperplane\", \"file\", \"database\", \"post\", \"center\", \"oriented\", \"segment\", \"regression\", \"table\", \"annotation\", \"processing\", \"set\", \"transformation\", \"support\", \"large\", \"from\", \"into\", \"using\", \"grid\", \"curve\", \"fault\", \"heterogeneous\", \"detector\", \"multidimensional\", \"resource\", \"resource_allocation\", \"tolerant\", \"decentralized\", \"handoff\", \"home\", \"cancellation\", \"tolerance\", \"instantaneous\", \"deadline\", \"fault_tolerant\", \"obstacle\", \"epidemic\", \"workspace\", \"taxonomy\", \"extremal\", \"fault_tolerance\", \"curvature\", \"faced\", \"commodity\", \"unreliable\", \"tsp\", \"fire\", \"grid_computing\", \"spectrum\", \"computing\", \"sharing\", \"infrastructure\", \"distributed\", \"radio\", \"allocation\", \"embedded\", \"environment\", \"management\", \"availability\", \"application\", \"challenge\", \"load\", \"user\", \"computational\", \"coefficient\", \"correlation\", \"sampling\", \"law\", \"station\", \"depends\", \"consumption\", \"depends_on\", \"linear_programming\", \"power_law\", \"least_square\", \"intensity\", \"primary\", \"manifold\", \"pi\", \"vulnerability\", \"aided\", \"reaction\", \"music\", \"differential_equation\", \"correlation_between\", \"email\", \"mean_field\", \"innovative\", \"urban\", \"formulated_a\", \"bridge\", \"spam\", \"positive_integer\", \"tv\", \"successive\", \"penalty\", \"force\", \"determined_by\", \"sample\", \"estimate\", \"tends\", \"white\", \"bias\", \"noise\", \"square\", \"robust\", \"estimating\", \"robustness\", \"range\", \"respect\", \"with_respect\", \"power\", \"base\", \"determined\", \"mean\", \"entry\", \"analytical\", \"parameter\", \"statistic\", \"value\", \"estimation\", \"between\", \"degree\", \"frequency\", \"from\", \"ensemble\", \"encryption\", \"formation\", \"equipped\", \"malicious\", \"equipped_with\", \"mixing\", \"voting\", \"intrusion\", \"chaotic\", \"analyse\", \"cipher\", \"bayes\", \"we_conclude\", \"proximity\", \"voter\", \"cnf\", \"multilevel\", \"gf\", \"comparative_study\", \"xor\", \"permanent\", \"chaos\", \"naive\", \"floating\", \"election\", \"vulnerable\", \"shall\", \"enables_u\", \"sends\", \"peer\", \"logarithmic\", \"conclude\", \"spread\", \"map\", \"arithmetic\", \"dependence\", \"comparative\", \"partitioning\", \"multiplication\", \"detecting\", \"very_large\", \"precision\", \"description\", \"hardness\", \"number\", \"operation\", \"value\", \"security\", \"transmitter\", \"attack\", \"secure\", \"server\", \"net\", \"job\", \"student\", \"spreadsheet\", \"pseudo\", \"cryptographic\", \"protein\", \"molecular\", \"password\", \"sent\", \"manage\", \"la\", \"software_development\", \"petri\", \"petri_net\", \"usability\", \"passive\", \"met\", \"instant\", \"elastic\", \"sending\", \"teaching\", \"contraction\", \"child\", \"exercise\", \"party\", \"key\", \"long_term\", \"client\", \"http\", \"multimedia\", \"university\", \"protocol\", \"tag\", \"scheme\", \"exchange\", \"against\", \"communication\", \"message\", \"based\", \"user\", \"based_on\", \"proposed\", \"paper\", \"this_paper\", \"using\", \"our\", \"information\", \"been\", \"image\", \"ha_been\", \"have_been\", \"compressed\", \"aware\", \"moment\", \"wavelet\", \"so_far\", \"hashing\", \"suboptimal\", \"image_processing\", \"voice\", \"inversion\", \"prior_knowledge\", \"evolved\", \"voip\", \"online_social\", \"probing\", \"week\", \"microsoft\", \"succeed\", \"very_little\", \"expertise\", \"undertaken\", \"interdependency\", \"summarizes\", \"breakthrough\", \"contributed\", \"continuing\", \"crossing\", \"drawing\", \"widely_used\", \"ha\", \"ai\", \"have\", \"recently\", \"author\", \"widely\", \"spatial\", \"far\", \"proposed\", \"paper\", \"this_paper\", \"developed\", \"technique\", \"recent\", \"used\", \"work\", \"however\", \"several\", \"so\", \"research\", \"shown\", \"result\", \"using\", \"ldpc\", \"check\", \"parity\", \"ldpc_code\", \"ring\", \"synchronization\", \"soft\", \"passing\", \"asynchronous\", \"parity_check\", \"message_passing\", \"low_density\", \"turbo\", \"belief_propagation\", \"block_length\", \"water\", \"stabilizing\", \"graphical_model\", \"iterative_decoding\", \"turbo_code\", \"repeat\", \"self_stabilizing\", \"persistent\", \"regular_expression\", \"convolution\", \"scan\", \"custom\", \"dna\", \"delivering\", \"propagation\", \"density\", \"seed\", \"belief\", \"request\", \"block\", \"iterative\", \"graphical\", \"message\", \"aggregate\", \"decoding\", \"low\", \"iteration\", \"performance\", \"node\", \"citation\", \"scientific\", \"journal\", \"secondary\", \"ontology\", \"mutual_information\", \"indicator\", \"trust\", \"rayleigh\", \"publication\", \"collaboration\", \"record\", \"international\", \"decision_making\", \"focusing\", \"strategic\", \"brief\", \"irregular\", \"focusing_on\", \"national\", \"placed\", \"heterogeneity\", \"posteriori\", \"fisher\", \"storing\", \"audio\", \"optimisation\", \"aimed\", \"un\", \"publishing\", \"science\", \"mutual\", \"survey\", \"index\", \"review\", \"article\", \"information\", \"discipline\", \"this_article\", \"research\", \"diagram\", \"digital\", \"impact\", \"field\", \"content\", \"published\", \"discussion\", \"computer\", \"measure\", \"knowledge\", \"web\", \"between\", \"document\", \"communication\", \"paper\", \"these\", \"circuit\", \"fuzzy\", \"optical\", \"cyclic\", \"division\", \"quasi\", \"ml\", \"cdma\", \"minimum_distance\", \"angle\", \"peak\", \"reed\", \"pulse\", \"spreading\", \"db\", \"wideband\", \"gene\", \"signal_processing\", \"codewords\", \"bug\", \"border\", \"velocity\", \"comment\", \"board\", \"regarded\", \"formed_by\", \"regarded_a\", \"division_multiple\", \"house\", \"fiber\", \"shift\", \"generator\", \"signal\", \"frequency\", \"analog\", \"orthogonal\", \"formed\", \"output\", \"realization\", \"digital\", \"input\", \"grows\", \"low\", \"power\", \"high\", \"coded\", \"simulation\", \"using\", \"game\", \"player\", \"interface\", \"service\", \"utility\", \"interactive\", \"synthesis\", \"overview\", \"ip\", \"provider\", \"an_overview\", \"thread\", \"interacting\", \"user_interface\", \"telecommunication\", \"slice\", \"backbone\", \"front\", \"archive\", \"debugging\", \"push\", \"service_provider\", \"reproducing\", \"visible\", \"customized\", \"publish\", \"tcp_ip\", \"hierarchical_clustering\", \"inexpensive\", \"front_end\", \"policy\", \"flow\", \"mechanism\", \"strategy\", \"environmental\", \"user\", \"environment\", \"interaction\", \"component\", \"access\", \"package\", \"oriented\", \"framework\", \"end\", \"application\", \"quality\", \"based\", \"dynamic\", \"design\", \"multi\", \"polynomial_time\", \"cut\", \"at_most\", \"exists\", \"secret\", \"worst\", \"worst_case\", \"cite\", \"coloring\", \"rational\", \"there_exists\", \"eavesdropper\", \"counting\", \"delta\", \"np_complete\", \"packing\", \"high_probability\", \"max\", \"boolean_function\", \"pairwise\", \"cardinality\", \"independent_set\", \"polygon\", \"prove_that\", \"there_exist\", \"width\", \"triangulation\", \"semidefinite\", \"confidential\", \"directed_graph\", \"polynomial\", \"we_prove\", \"conjecture\", \"convex\", \"subset\", \"prove\", \"existence\", \"np\", \"integer\", \"point\", \"set\", \"if\", \"complete\", \"maximal\", \"show_that\", \"class\", \"show\", \"given\", \"number\", \"dimension\", \"all\", \"any\", \"case\", \"give\", \"degree\", \"where\", \"there\", \"consider\", \"maximum\", \"result\", \"two\", \"known\", \"problem\", \"one\", \"value\", \"function\", \"each\", \"then\", \"also\", \"which\", \"be\", \"it\", \"can\", \"study\", \"at\", \"time\", \"neural\", \"segmentation\", \"dictionary\", \"neural_network\", \"speech\", \"classifier\", \"corpus\", \"english\", \"machine_learning\", \"medical\", \"linguistic\", \"extracted\", \"sentence\", \"dataset\", \"discourse\", \"patient\", \"predictive\", \"overlap\", \"parser\", \"information_retrieval\", \"trained\", \"supervised\", \"ca\", \"workflow\", \"lexicon\", \"harmonic\", \"interleaved\", \"morphological\", \"script\", \"subgraphs\", \"recognition\", \"text\", \"retrieval\", \"template\", \"extraction\", \"classification\", \"lexical\", \"learning\", \"clustering\", \"expert\", \"prediction\", \"feature\", \"training\", \"document\", \"word\", \"machine\", \"automatic\", \"accuracy\", \"rule\", \"task\", \"from\", \"based\", \"using\", \"approach\", \"experiment\", \"method\", \"used\", \"result\", \"technique\", \"information\", \"this_paper\", \"our\", \"paper\", \"present\", \"distortion\", \"mac\", \"universal\", \"achievable_rate\", \"emph\", \"memoryless\", \"stationary\", \"ieee\", \"causal\", \"multiple_input\", \"turing\", \"source_coding\", \"model_checking\", \"rate_distortion\", \"region\", \"imperfect\", \"mu\", \"compress\", \"asymptotically_optimal\", \"time_varying\", \"turing_machine\", \"reachability\", \"previously_known\", \"lossy\", \"voronoi\", \"genome\", \"constant_factor\", \"th\", \"welfare\", \"well_studied\", \"alphabet\", \"asymptotic\", \"affine\", \"weak\", \"deviation\", \"uncertainty\", \"compression\", \"information_theoretic\", \"infinite\", \"asymptotically\", \"theoretic\", \"additive\", \"optimality\", \"measure\", \"limit\", \"discrete\", \"strong\", \"condition\", \"rate\", \"setting\", \"under\", \"consider\", \"general\", \"function\", \"we_consider\", \"every\", \"information\", \"case\", \"a\", \"problem\", \"result\", \"finite\", \"class\", \"motion\", \"controller\", \"trace\", \"safety\", \"camera\", \"nearest\", \"splitting\", \"graphic\", \"triangle\", \"factorization\", \"designer\", \"health\", \"nearest_neighbor\", \"subsystem\", \"beam\", \"assisted\", \"viewpoint\", \"low_cost\", \"divided\", \"device\", \"preservation\", \"bus\", \"access_control\", \"substrate\", \"divided_into\", \"silicon\", \"fabrication\", \"programmable\", \"traveling\", \"tracing\", \"parallel\", \"video\", \"control\", \"deal\", \"deal_with\", \"test\", \"monitoring\", \"design\", \"mode\", \"configuration\", \"hardware\", \"neighbor\", \"body\", \"wa\", \"moving\", \"low\", \"high\", \"paper\", \"were\", \"integrated\", \"this_paper\", \"speed\", \"using\", \"based\", \"coding\", \"sufficient\", \"destination\", \"array\", \"alignment\", \"sufficient_condition\", \"terminal\", \"tradeoff_between\", \"decay\", \"forwarding\", \"axis\", \"abelian\", \"non_zero\", \"necessary_condition\", \"non_negative\", \"adjacency\", \"converse\", \"newton\", \"puzzle\", \"selfish\", \"joint_source\", \"ising\", \"sufficiently_large\", \"vanishing\", \"interconnected\", \"edge_weight\", \"careful\", \"abelian_group\", \"trip\", \"shot\", \"source\", \"half\", \"opportunistic\", \"side\", \"tradeoff\", \"forward\", \"exponent\", \"condition\", \"diversity\", \"joint\", \"necessary\", \"compound\", \"optimally\", \"information\", \"multiple\", \"scheme\", \"separation\", \"pair\", \"distributed\", \"two\", \"at\", \"single\", \"shown\", \"block\", \"channel\", \"case\", \"problem\", \"between\", \"color\", \"3d\", \"signature\", \"carried\", \"inspired\", \"particle\", \"carried_out\", \"two_dimensional\", \"visualization\", \"overlapping\", \"2d\", \"degraded\", \"oblivious\", \"inspired_by\", \"blind\", \"head\", \"six\", \"one_dimensional\", \"commutative\", \"handwritten\", \"enumeration\", \"scene\", \"coin\", \"car\", \"thresholding\", \"lightweight\", \"massive\", \"mutation\", \"digit\", \"targeted\", \"transfer\", \"dimensional\", \"depth\", \"root\", \"out\", \"display\", \"three\", \"two\", \"wa\", \"one\", \"solving\", \"optimization_problem\", \"approximation_algorithm\", \"faster\", \"vehicle\", \"genetic\", \"np_hard\", \"gradient\", \"algorithm\", \"genetic_algorithm\", \"superposition\", \"sorting\", \"c\", \"csp\", \"constraint_satisfaction\", \"faster_than\", \"subgroup\", \"near_optimal\", \"gas\", \"fluid\", \"dominating\", \"parallelism\", \"solid\", \"quotient\", \"dominating_set\", \"weighted\", \"speed_up\", \"wavelet_transform\", \"licensed\", \"thin\", \"solve\", \"heuristic\", \"optimization\", \"search\", \"competitive\", \"approximation\", \"problem\", \"satisfaction\", \"greedy\", \"solution\", \"cost\", \"efficient\", \"approximate\", \"optimal\", \"objective\", \"fast\", \"time\", \"hard\", \"constraint\", \"our\", \"local\", \"present\", \"finding\", \"best\", \"find\", \"show\", \"performance\", \"complexity\", \"approach\", \"result\", \"using\", \"new\", \"based\", \"which\", \"first\", \"testing\", \"stability\", \"stable\", \"position\", \"robot\", \"estimator\", \"label\", \"alpha\", \"generated_by\", \"balancing\", \"workshop\", \"observer\", \"load_balancing\", \"differs\", \"differs_from\", \"gps\", \"hull\", \"concentrate\", \"planner\", \"embed\", \"subdivision\", \"impose\", \"gateway\", \"red\", \"convex_hull\", \"concerned_with\", \"concentrate_on\", \"leg\", \"really\", \"increasingly_important\", \"grammar\", \"positioning\", \"plan\", \"concerned\", \"generated\", \"planning\", \"dependency\", \"line\", \"load\", \"reliability\", \"disjoint\", \"refinement\", \"parsing\", \"balance\", \"bring\", \"capacity\", \"transmission\", \"receiver\", \"feedback\", \"delay\", \"packet\", \"transmit\", \"decoder\", \"closed_form\", \"selective\", \"localization\", \"tile\", \"session\", \"streaming\", \"rating\", \"send\", \"sender\", \"queueing\", \"pilot\", \"digraph\", \"registration\", \"d\", \"kinematic\", \"cross_layer\", \"congestion_control\", \"floor\", \"en\", \"medium_access\", \"packet_loss\", \"source_destination\", \"channel\", \"gain\", \"receive\", \"bandwidth\", \"throughput\", \"achievable\", \"rate\", \"symbol\", \"multiple\", \"regime\", \"buffer\", \"bit\", \"user\", \"simulation_result\", \"scheme\", \"power\", \"ratio\", \"sum\", \"optimal\", \"noise\", \"diversity\", \"at\", \"performance\", \"access\", \"error\", \"shown\", \"over\", \"proposed\", \"result\", \"two\", \"constraint\", \"information\", \"communication\", \"light\", \"route\", \"neighborhood\", \"micro\", \"hilbert\", \"aggregation\", \"look\", \"team\", \"weakly\", \"dealing\", \"uncertain\", \"transport\", \"dealing_with\", \"chapter\", \"iterated\", \"id\", \"look_at\", \"hilbert_space\", \"away\", \"proxy\", \"projective\", \"closely_related\", \"tie\", \"best_possible\", \"tackle\", \"homomorphism\", \"payment\", \"attachment\", \"this_chapter\", \"preferential\", \"relation_between\", \"agreement\", \"host\", \"fine\", \"ball\", \"double\", \"bottleneck\", \"he\", \"mass\", \"relation\", \"closely\", \"em\", \"open\", \"associated\", \"associated_with\", \"article\", \"minimizing\", \"related\", \"between\", \"area\", \"criterion\", \"this_article\", \"end\", \"some\", \"social\", \"community\", \"large_scale\", \"recommendation\", \"scale_free\", \"workload\", \"small_world\", \"government\", \"hyperbolic\", \"router\", \"updating\", \"updated\", \"consuming\", \"website\", \"cf\", \"feed\", \"time_consuming\", \"converter\", \"interplay\", \"portal\", \"reflection\", \"knowledge_about\", \"artifact\", \"costly\", \"your\", \"empirical_study\", \"advertising\", \"interplay_between\", \"tremendous\", \"suggestion\", \"scale\", \"period\", \"steady_state\", \"book\", \"xml\", \"failure\", \"steady\", \"topology\", \"internet\", \"preference\", \"loop\", \"world\", \"event\", \"connectivity\", \"network\", \"traffic\", \"online\", \"complex\", \"topological\", \"large\", \"response\", \"structure\", \"individual\", \"content\", \"global\", \"behavior\", \"interaction\", \"link\", \"study\", \"local\", \"analysis\", \"our\", \"user\", \"small\", \"a\", \"show\", \"property\", \"relay\", \"sensor\", \"routing\", \"ad\", \"hoc\", \"qos\", \"ad_hoc\", \"mobility\", \"congestion\", \"multicast\", \"hoc_network\", \"mobile_ad\", \"power_constraint\", \"mesh\", \"pixel\", \"schedule\", \"scheduler\", \"voltage\", \"tiling\", \"directional\", \"slot\", \"hole\", \"battery\", \"zone\", \"service_qos\", \"triangular\", \"interoperability\", \"causally\", \"edge_coloring\", \"organisation\", \"node\", \"wireless\", \"network\", \"cooperative\", \"energy\", \"overhead\", \"mobile\", \"maximization\", \"layer\", \"scheduling\", \"link\", \"cooperation\", \"protocol\", \"distributed\", \"location\", \"traffic\", \"communication\", \"path\", \"multi\", \"proposed\", \"performance\", \"based\", \"simulation\", \"efficient\", \"this_paper\", \"scheme\", \"agent\", \"automaton\", \"sparsity\", \"trade\", \"walk\", \"trade_off\", \"finite_state\", \"context_free\", \"reward\", \"orientation\", \"random_walk\", \"finite_automaton\", \"dynamical_system\", \"temporal_logic\", \"regular_language\", \"finitely\", \"rigid\", \"worse\", \"posterior\", \"soundness\", \"univariate\", \"oscillator\", \"coming\", \"cooperating\", \"perceived\", \"sequentially\", \"infty\", \"merge\", \"worse_than\", \"coming_from\", \"decidable\", \"regular\", \"free\", \"interval\", \"temporal\", \"off\", \"word\", \"state\", \"move\", \"finite\", \"language\", \"probabilistic\", \"context\", \"deterministic\", \"omega\", \"over\", \"gaussian\", \"cognitive\", \"lattice\", \"equilibrium\", \"correlated\", \"item\", \"embedding\", \"auction\", \"fractional\", \"hash\", \"gaussian_noise\", \"scalar\", \"revenue\", \"intrinsic\", \"empty\", \"starting_from\", \"burst\", \"geometrical\", \"metric_space\", \"cubic\", \"categorical\", \"dilemma\", \"pervasive\", \"pairing\", \"spike\", \"removal\", \"piecewise\", \"logarithm\", \"crossover\", \"conflicting\", \"metric\", \"high_dimensional\", \"parameterized\", \"compact\", \"positive\", \"incentive\", \"space\", \"variance\", \"false\", \"flat\", \"mechanism\", \"dimensional\"], \"Freq\": [12929.0, 12486.0, 12349.0, 7771.0, 9775.0, 7076.0, 6982.0, 5512.0, 9034.0, 11422.0, 4854.0, 6130.0, 6967.0, 10308.0, 3738.0, 5896.0, 5147.0, 3143.0, 3693.0, 3004.0, 3528.0, 14710.0, 3672.0, 3064.0, 6312.0, 2623.0, 2896.0, 2595.0, 8439.0, 4566.0, 5511.509576289083, 1439.3025938395176, 1316.5828187132686, 779.8804564011824, 689.0285224124766, 654.648946045089, 574.2801693054879, 1017.811212746952, 484.4609950922998, 341.7989316346684, 264.3134482163853, 260.3047007316577, 215.83968384136534, 206.07677506549643, 152.2623773325729, 152.07034341974222, 144.6464912410167, 136.1863563093546, 135.09054156674344, 126.0324497791893, 118.04811720795122, 104.85750962120422, 91.23381841219451, 88.18380598584503, 87.03190839612188, 77.36000108037307, 68.8080577790923, 68.73105329811787, 62.10619446663871, 57.895909468319154, 1050.3420908921048, 679.8016968831037, 181.58431879757958, 343.94767361634416, 246.08045789895652, 400.7898172925904, 243.27064080727584, 156.94958276199424, 143.29704321992995, 136.01178961390545, 4853.792854316895, 1719.8777729528574, 1241.3058837286737, 1232.664864249203, 1177.4497351973116, 941.7465031592401, 545.7759914409758, 461.20255849860763, 225.7085988072157, 151.86801454648665, 93.57025435497644, 76.00991158948854, 75.22816564437225, 75.20394838128898, 74.18825848228398, 68.68853822914518, 67.87549302203324, 67.14243401648667, 66.68521237794695, 65.55960004775815, 55.90877911338838, 54.755747475801535, 49.57227597552041, 43.590136014391945, 42.89578280127181, 41.10232427120365, 40.67326836255426, 40.25609030747788, 38.896606141515576, 37.19210530654336, 452.2836534976585, 171.0253120076587, 706.6940649927094, 98.65990379329156, 124.76227671333022, 573.2476854378896, 71.02722773963414, 585.9548396411715, 483.49911424345146, 261.7851522606403, 164.8371738622805, 215.0873745442791, 375.6808011886404, 175.3393022697802, 230.92084904136632, 300.2040588248059, 206.32666784082693, 122.19191496958128, 208.05460345733252, 246.46166822698268, 147.36536329500888, 189.97589010342907, 160.9586760505898, 221.6081935550627, 171.39122490054706, 169.05938661100183, 164.11058385166714, 1388.7388217073355, 1133.8220864457908, 841.2903604082445, 525.5089604399051, 435.1223216520475, 429.04589389035755, 272.10542406555646, 255.33635648546166, 238.4355383221227, 237.34931334565772, 203.0356390973791, 162.89123439029788, 158.78698055126168, 158.16542944068487, 133.62001881117612, 115.16469520300859, 102.37378985717551, 94.81004705699377, 85.21129375456655, 77.19654132964294, 70.95049864021252, 66.92411055158045, 54.97103290896357, 51.95456823586373, 44.97670405962043, 43.09071498424081, 41.99707256152464, 41.376908797342026, 38.14122595872751, 36.93797071713265, 564.6946365569229, 485.02614284254463, 9122.593384027978, 383.3842342501236, 520.7822388951878, 925.165587279509, 300.6023857823333, 174.11148428408526, 638.1019886701264, 252.73484039384508, 1037.7826860942068, 1306.85109183775, 1181.6618948693854, 527.6437104376781, 433.14466808474884, 511.10299459442206, 406.05382303581655, 351.0669639168195, 420.4084994074233, 983.4100253989722, 463.9060780065485, 665.5433715684886, 614.176106664171, 676.2229797956288, 479.28913423315055, 395.6623843390569, 306.6872820857472, 2346.8854508601553, 866.5577774773885, 748.4969221028491, 568.7174295380111, 466.12141527990883, 276.86197257942416, 227.5086355107805, 227.0033768106249, 226.20824495133695, 185.32641682884284, 182.46154453853472, 179.27389256975403, 174.81036639388185, 167.35809208933807, 165.76430165650277, 128.45216251319405, 127.06597894114037, 119.16589724937735, 115.56471670274203, 112.81139097151134, 110.76733298384838, 110.21038232267207, 104.4513930328842, 98.7616012898162, 92.16918400565683, 88.89991988713808, 88.46352454908504, 87.77851718667313, 87.74008595568043, 87.33660424727866, 318.1403711880213, 393.40324072679704, 166.75586296766937, 373.56228759800746, 384.9175680411216, 217.18416529559144, 759.1733669335053, 319.0013729684778, 426.2873768080244, 180.19755041080404, 243.99928119619383, 1054.3686791782884, 1627.8700966092558, 1082.2129983765283, 370.6329247868661, 576.643776718634, 471.3966687809592, 513.9344741163557, 304.09949783632885, 1090.5153388031997, 474.4212528771059, 710.8485079107766, 339.88666013672776, 455.8731533780925, 453.0245636999904, 644.9206803693253, 680.4440216901214, 380.9427050995148, 491.04678116232157, 637.4621916529615, 549.1396716814556, 424.549702121652, 364.31592539956193, 371.83695641154316, 691.842428422415, 465.517194102671, 398.51224585157905, 386.8603432550601, 409.121724383159, 359.50332231098974, 292.3148844780269, 264.7143853974051, 229.97060438708814, 194.74063198505772, 189.93473197109824, 188.48382026844783, 165.4861588515045, 154.48400933665764, 153.0466333028798, 151.0027336445254, 131.33755117228972, 121.3792462163907, 119.55019627687135, 1683.4305093861844, 106.06214810008699, 105.22163906334494, 100.91870319728005, 100.36354955315731, 98.37052816579214, 97.04643652921823, 94.13916334401814, 93.78041348451008, 91.94952646379662, 91.58946537904961, 274.7057304976056, 439.18098932429, 1100.6863915924635, 462.4034166009377, 405.8548340175897, 182.1139290335776, 854.9257943407964, 163.81108226738422, 228.63512148125113, 348.08544610242495, 685.0713159230323, 888.1190788232865, 762.7360063398627, 974.6407132883068, 608.1750581305923, 408.404402225894, 460.3342485395417, 306.9453717907083, 386.27463533115133, 370.6594451812357, 329.69469261083924, 265.94330393538763, 456.1504292912747, 327.4625228619303, 411.69505854891867, 379.77265181205644, 355.2080216002223, 322.85698079619664, 1144.694168139947, 552.9620997665406, 526.1994440135596, 503.49893922828306, 501.8229889273266, 321.50549070422045, 317.2415483794667, 258.5574816226093, 187.32260618694778, 162.13213359028197, 164.56689037934424, 130.19700609804627, 108.52195978527013, 99.75403042752953, 96.81828290640595, 92.83170818745602, 82.39192005987786, 81.84948408940461, 75.59957171421753, 72.65830309086125, 71.33052729945408, 69.54641638745917, 67.60613715516807, 65.5798063685471, 64.78392346064872, 64.55840245549876, 57.28796732532057, 55.93835976470532, 53.38281037846951, 53.25725459823137, 70.64362454120683, 253.4862506307689, 147.45987835023195, 104.1770618342517, 666.3297017571123, 153.55935301346705, 166.0444876882578, 126.59558371732683, 462.86259158898883, 200.07435587446645, 222.4176527396211, 165.63937681193391, 152.1518170130197, 196.94342260075044, 94.44603288190667, 111.78003862089393, 1373.415197177659, 767.1988370066008, 750.5187255368663, 622.1118046094459, 492.45895638240734, 443.0513654955791, 389.0310092670331, 371.2052904378924, 364.26023897905907, 343.4759968705547, 334.79248665047004, 241.40757403614543, 220.48779631324027, 209.68209441102417, 200.51542595436476, 185.48636436404695, 183.39511855727073, 183.21792190568007, 177.11072697223378, 169.22174498556373, 167.4095000339943, 165.45910675778543, 159.26543823177286, 152.23956558073377, 146.058797946911, 144.27006738748898, 138.53143742287617, 133.2192180351032, 129.0287764627703, 123.96328028426035, 999.9413422871473, 798.149754790948, 749.9011906312884, 1899.489385151773, 609.2520244826806, 374.7118525637248, 395.5842558519534, 731.7826042669197, 1378.9573147340534, 745.03223735956, 673.0865336624247, 524.9177231262454, 480.8786926394171, 12093.319517644704, 2656.3635232609404, 659.0791129387135, 9316.737490335496, 751.3394215374177, 3218.9533561772378, 4381.992555636301, 1820.472894586345, 10653.883633619795, 5140.857639944408, 4857.054440327702, 1632.8753383552569, 2057.6495656231164, 10001.760266969373, 2903.2172663739784, 1693.9918562565917, 4008.6579368525126, 1953.991537347742, 2328.482498985147, 6427.402844565038, 1538.9342336119291, 1366.5067439945415, 1499.404354681792, 3241.6416845769827, 2188.2234297909185, 2985.8984642810815, 2624.136220278629, 2014.879356690604, 1979.3909033535103, 3798.7507223810853, 2804.2350338040546, 3882.6856958847106, 2164.099798347101, 2572.378776102342, 2194.8696766609078, 3007.8421397642624, 2562.835247067687, 2416.191649339588, 2515.4599990961074, 2083.349037489754, 9767.992175104444, 814.0604524185522, 558.5050107219093, 401.3234642676347, 343.8169952737079, 321.15054264947935, 268.9534756089871, 237.02946628953694, 224.89373128725757, 224.61077650645296, 213.73227850356062, 199.629569377068, 182.76375002017537, 143.78840873529444, 828.0206597203387, 132.13855689724198, 123.17977345369312, 117.75696059673943, 110.6586703706459, 105.4408347657222, 100.77584718503378, 96.85819196160145, 95.42901571648325, 94.70348271860445, 93.95648179597607, 87.94626132224289, 86.27617729840854, 83.20456169044931, 82.09686950160733, 79.41042612668139, 601.0504302296192, 721.9658573719889, 98.09013447280985, 339.3427483212682, 1748.3600848172575, 250.77452732230677, 731.4558551977096, 228.03169408160076, 555.0424227749319, 351.5825688618597, 1794.9986014555373, 137.13029382522416, 207.85755453172055, 1158.4442912969112, 263.88740693313514, 604.8856712198287, 212.03938575125548, 303.32463250215454, 868.1238278476952, 566.1722386168926, 315.6197724951534, 893.513046069005, 548.7239530322155, 548.056443692002, 632.3350268261596, 497.6965489233423, 594.5271941478177, 394.79347897159613, 401.69229074743765, 387.3186338127898, 332.59027999877225, 322.6736354159231, 3003.6141595747918, 970.547884986414, 401.62181934665193, 277.5270616338058, 236.1511813972535, 235.45376219882957, 220.94200607344052, 207.19933481527687, 205.23513094070083, 153.76339476231985, 150.38079392401903, 138.79511615273267, 125.56317482509598, 93.73389210887144, 92.56358250655948, 87.70054446950184, 85.7703674063121, 77.80625407402954, 61.7492379016146, 54.9496716204332, 51.63267624037436, 47.38005725975205, 45.49403411477758, 34.67332414714117, 30.609795776127612, 27.78717076812353, 26.93255900546108, 24.32730893416365, 21.479648768244072, 983.5087623265101, 1847.917397165241, 2753.1744732526267, 164.47266976548008, 1826.1160505475552, 133.4343181059812, 773.6167274644372, 790.5392433771506, 446.9032042849439, 132.05894462384413, 70.83802865471168, 469.66027152595916, 187.83425973223737, 278.09503580491537, 174.6393218094737, 165.35944044350376, 388.6023465931994, 142.03344919705276, 298.7332768454246, 257.7372768275182, 327.9921874736957, 411.76463188685534, 208.43631342271706, 297.33233810417244, 212.85064838633156, 260.71116796868387, 244.6983525373031, 184.6159668544031, 1055.189805780449, 817.9441216970874, 637.8348992828342, 524.2754463538023, 442.40391524773, 367.8340168763436, 353.3204131161296, 302.13991855774657, 297.2015563469664, 262.57055366973316, 252.16819210315222, 249.69770060483256, 245.8333364391313, 234.01388512573416, 231.94155266496873, 222.37645509844853, 221.3749603233436, 217.29365675405452, 167.11481113465044, 166.18210428015445, 163.44663591508754, 163.13894023883614, 153.61568274773788, 140.2765147965578, 139.36024181103738, 130.1914070664204, 130.06214065801478, 129.30578956725833, 115.6460378399548, 112.47835563755214, 2787.888398838363, 480.44181621075654, 1629.2617867623553, 1053.6917029178103, 324.4163825310816, 2327.859452025664, 1036.9403796927313, 2330.282661463807, 974.3111208561464, 657.7543517855507, 482.64444797788383, 207.66479700922, 539.384122667917, 803.0753243710219, 373.71882194279914, 703.2488510094757, 453.84305970019, 402.24237608664845, 625.6988107884897, 443.63815122351474, 369.909987604537, 536.0875440504064, 453.4306096715097, 420.6509301251787, 549.9498979410096, 532.4185671786984, 404.01001906670683, 384.2426029765957, 400.0809060738972, 395.59199322157355, 642.1665915143151, 486.66426254991404, 451.61463999651954, 312.86173045286296, 302.2643589097408, 300.07037645923714, 280.33494208844445, 240.77591871219536, 162.75630006665938, 143.51154138264022, 142.33032753010784, 119.01482470862784, 113.56281006644518, 90.55155723847743, 89.85044634175503, 83.79987338059807, 64.84717546720344, 64.82941347335793, 63.3446874854889, 61.32296093867697, 60.94241058461164, 62.178337591730745, 56.26928291605566, 54.71439729521336, 47.021934554784714, 42.90768105661406, 40.097087033484456, 39.90089442805669, 37.467460009415724, 35.3251494756895, 241.5550969456282, 1469.591196309705, 350.3277996967043, 6132.176191914163, 556.4377238548556, 348.0318568967251, 790.5556181541717, 335.0078445699406, 2316.4872570049065, 3590.49244497647, 134.24773198799127, 496.2364578273227, 152.9428057485094, 151.43716730655822, 1399.483485251217, 372.01849998087386, 1249.9837102145977, 1048.7050095477848, 812.9180023859815, 541.7499578460171, 580.3462316416725, 697.9346114496434, 825.6937448539973, 373.5102550387315, 340.40578197478, 482.21198748370716, 278.58083961452047, 314.3681261703101, 319.2181262104854, 1997.6521722861091, 641.7691275419292, 558.30717298244, 313.1617261473146, 309.7813879392866, 292.83109345753513, 287.75053831862147, 252.36003577735957, 212.67529454551175, 212.65843912120928, 199.25117975267506, 196.16179233919132, 176.0564023296236, 157.76071161877664, 148.33737665977026, 145.91591269581696, 145.55328740292188, 139.87434912324505, 136.37057063594474, 119.51409481672698, 110.10702331702906, 91.7361608409589, 88.32694097243484, 87.39741572964093, 77.08090685187922, 75.43066422890698, 73.3048712014237, 64.31932940602901, 64.25786630382447, 60.128637855312235, 1487.3904360182134, 307.42555419953504, 635.0360553882243, 396.64178237319885, 149.23217848225903, 213.27470583600135, 412.680108047636, 141.05519466203847, 365.63873743104955, 352.69613942674226, 276.7820812571539, 243.66626651738898, 143.79870316074647, 610.0652957569687, 556.6206862934895, 400.98069155764125, 312.29952656094144, 257.703927490158, 199.83653685074546, 143.25194837098215, 130.29596792246187, 111.52474036188113, 109.05387839811242, 108.03084097553506, 105.019567549057, 103.62513475896216, 96.86199738049427, 87.86032682930527, 85.26998682112479, 77.20019167793488, 75.30580461115137, 65.98018987782874, 62.48519038672106, 60.389984196182944, 57.71196250485183, 54.922306849424274, 46.45612729237453, 46.03423711629943, 45.78268316113013, 43.99685309876138, 42.612834633743056, 222.40167236711403, 35.21456010529015, 2029.8402324861486, 1332.7782254742415, 344.88076256472954, 1197.2342244548022, 670.4633673899748, 91.67218316096121, 362.6770706182753, 235.37145412991626, 176.75365869705817, 119.9883863443284, 77.38136893121867, 110.18229252224826, 164.4231378102716, 179.57940778961873, 153.68034145384448, 131.5723161341763, 137.05946533521535, 7075.422726222177, 312.16160279730235, 251.98211431155684, 199.48340393100256, 173.94907718900114, 171.12886320602496, 159.15653508753076, 153.3108034172569, 152.0220122510497, 119.23802506779568, 111.93103648739807, 85.1172864856362, 77.9389629393347, 72.3206113357373, 64.43830829830355, 60.43443347346199, 58.783296858358156, 58.60645248025506, 56.951242550001446, 54.69096931256138, 52.84994221651142, 51.193701385510025, 50.82995506103278, 44.8560791950197, 42.806134854048, 40.2170868938046, 37.3765781748529, 34.740773298115116, 33.42155080391064, 32.44910607971469, 1506.1248716172586, 1162.0024720477556, 1295.3600801400285, 847.1024232975208, 1022.9017051136192, 196.1645500811023, 1528.4017531476318, 621.9901371148143, 264.01797752561475, 480.55989368955557, 153.25961039475723, 206.43802560702386, 491.83577830914345, 287.7485799738858, 416.701806504809, 351.84192804844696, 195.71576701689332, 295.8085715067597, 195.39187021470144, 290.58277397675874, 207.62774972576122, 246.4773485736263, 189.31612195806863, 188.22224646495442, 169.18877265449382, 562.8294390741922, 319.1561795586653, 308.48886126604884, 257.6744018338286, 251.9754196131214, 166.05636402824422, 137.10057028898262, 133.77552535510293, 129.61310728639546, 129.34423611748278, 129.14973133610627, 129.07586361148284, 128.76251958438064, 127.96790023296434, 125.45345810912438, 123.00998359138917, 120.7036282519432, 112.03806099535126, 111.49097528926657, 105.3074895580208, 103.32286365623564, 102.37220218126323, 100.17599229334094, 99.02975653627132, 98.99681853861547, 92.86170967125749, 92.59804821751189, 91.42410386201584, 88.74326579782681, 88.02004710188679, 1413.6632706045516, 631.7319508302695, 270.6528390089784, 232.57207824342916, 2128.1660737019183, 901.2053470690037, 197.61048024234822, 164.17007970193035, 176.2005324345865, 512.286383493573, 417.08887768695683, 739.7312621592674, 703.2069858168102, 1028.2661269729008, 190.5646094943894, 376.7182043353731, 340.0176375305618, 913.9417227681536, 582.0437084637149, 678.2880127057922, 292.4599455813036, 562.8804601637883, 313.60157337389825, 180.50320600885007, 493.6378603012372, 258.8990392112149, 265.7056567427349, 285.13219906057674, 563.2843771230899, 375.0172913855081, 366.0541929652304, 320.8881933270341, 326.2489508658361, 318.437921246436, 294.2247398748295, 275.6623016484787, 2622.1674282168983, 455.32175004159916, 402.66790452470923, 327.04900117085435, 295.3698655024147, 289.48414420110186, 280.4019540980703, 225.74970259685333, 211.70374102767644, 353.4190917640087, 167.58116044012243, 166.62360577488658, 157.37118475460622, 154.3280481017754, 145.94887208768776, 142.52972632740395, 118.22600515754313, 117.14378178566643, 114.16163264971311, 103.75369502993074, 99.03132960690311, 98.57472902990324, 97.96204507401666, 92.50608546793698, 89.29719584124457, 88.8386786123836, 86.64810669506406, 78.19197106525642, 77.92532622811191, 75.17109174800181, 281.8624543276501, 253.23895671697312, 1048.7637184307034, 2126.739927920282, 156.19981692619888, 139.74223534743368, 237.70173639705814, 739.9943958745654, 655.8776682739962, 122.30612529147514, 551.7186330318551, 435.8566711743392, 381.77681364994675, 174.90424401161084, 222.4179515568666, 204.37798680614736, 187.5078588646311, 185.51623310820906, 198.2848757154678, 172.52431494841056, 166.61608881593583, 7770.998786797728, 1469.864294410436, 541.6900208009148, 530.3585559638242, 431.90628342374333, 324.35508470632794, 248.0765734179465, 218.4634527047848, 188.29189616405324, 184.22387165701028, 170.8656827020639, 155.89237863555397, 146.75208275149956, 130.848055651958, 119.34213734158934, 108.60990582978991, 101.4223183276907, 96.72420136445213, 92.8208927389574, 79.97808841350557, 72.91356600111263, 71.28510436562667, 68.8458326179461, 61.52537718250757, 59.21928456955152, 58.22628612384408, 49.96410016121755, 49.85533527709794, 43.14050283410113, 29.673528917526436, 463.33951061251145, 1043.9643583406923, 175.76025808813887, 236.34070740019462, 341.0427470100101, 199.02740030977424, 168.6106773100488, 188.96290658302254, 156.5429401118908, 215.46051995788972, 338.8889924944757, 159.00383031708685, 170.4039602340038, 171.55964171350146, 166.70521130611442, 127.8749580830225, 129.61400161716733, 814.9663769893607, 506.66368098474106, 482.9983888366011, 481.10788067348966, 318.0012291487764, 311.0501919262132, 1671.9090289192768, 200.8415147610159, 177.52671668477325, 175.6233340538144, 174.9652301930779, 167.81547436550898, 162.9053616993101, 162.23666073736396, 146.38702458509184, 116.96533012571057, 111.21831276892003, 108.7840282234603, 104.5151871073244, 98.01044028232478, 86.28011041421476, 85.32305626762057, 84.79651624486543, 76.61522352554515, 71.90274587861305, 71.3486983890698, 71.31681984760597, 58.41471138092173, 57.524224293832454, 57.296478447125146, 1063.1427736578923, 1182.1506812673656, 535.1288906391876, 323.127361060084, 1224.8699654393545, 437.0701114133901, 401.44181231702765, 168.4429990693754, 279.05267642142053, 217.7407689941917, 114.89948309735057, 194.79330648344987, 135.63841421353192, 129.24780202539068, 144.38294194385145, 131.49318451800514, 812.1172900322599, 750.376989997175, 702.6540886975649, 632.2316134618657, 459.02616384473447, 435.90166483660533, 365.0108420279416, 318.97526389418033, 195.77196948837945, 174.68846478748796, 149.44458872931048, 142.03899231719683, 635.5329544061756, 134.88278466236181, 131.60502692278632, 126.80134437611358, 123.5478622079211, 107.1518848637291, 104.71488247670113, 102.43359420371709, 90.29412460812972, 88.2835773379577, 86.01516195599578, 84.2384636583227, 82.28951992440334, 81.60170921930529, 79.91000961241843, 73.72070480395871, 69.86593211853405, 63.4956589088293, 192.01523933949122, 109.89662285671899, 252.41238226723672, 146.8120055856575, 659.1144495933266, 635.3512637212045, 142.82726456823318, 190.07623662300793, 134.22736987822915, 948.8035957891258, 405.8801682800463, 479.50038589485746, 185.4255158192575, 278.9910215556065, 540.8447889707254, 493.2921585802527, 454.363612973803, 1109.7712170078862, 420.5001482409502, 253.23530704164872, 574.5952526283374, 196.20144195618548, 247.65573791398683, 395.68196655108886, 241.27186098179754, 373.41241213793535, 290.0254141458725, 405.87124444324206, 313.8424849482759, 273.6888093787321, 258.4827111327104, 491.6068583213244, 305.80356904480686, 211.9925703404942, 189.55168335334557, 174.52775978766158, 168.13585036207147, 157.3022014521015, 156.92111208860533, 148.79083498921025, 139.42320201686482, 133.77911156935082, 123.3316125853459, 114.75975306122301, 117.61391029363966, 110.70351078694839, 90.91696133868228, 87.49428859235162, 86.14391617700433, 85.00993921474272, 83.73462106505565, 83.4786114000325, 81.03000023030454, 80.83258188647696, 80.44087178837262, 80.06813879952637, 78.1409992524815, 77.1244144463438, 76.21917813096123, 74.07171071923074, 69.84864716433573, 622.3111624647532, 186.13129108341408, 152.64708028909996, 247.1157748335089, 552.4636676259479, 267.7813230312364, 192.75796870481105, 168.69167417871094, 152.5308807496088, 159.64372692971452, 153.04330299067968, 105.40740653315044, 130.76076809441457, 175.77989443699386, 116.14371213957995, 272.96960829485465, 169.60992452018775, 120.09849335964284, 1390.8749670721818, 1013.5875889245709, 950.3881319131299, 712.0911472085382, 604.5938537168024, 369.8705984758595, 357.7200928618892, 308.54952055312737, 296.86658042302, 269.3116090197847, 243.7428381030056, 194.15699630672478, 170.48064220785395, 147.0822351471316, 146.1978727544379, 140.37799024392507, 139.48220855856655, 131.7763534933945, 98.30163531959046, 94.59373280384754, 91.94374410899901, 90.42371988035548, 88.60088801335434, 82.35382940252494, 82.25599443078329, 81.26854564640908, 77.66960645767989, 77.30365085822936, 74.86160239140268, 70.63643501168306, 271.48590053064345, 1438.7864205326694, 131.84632393634703, 284.4439901593649, 114.23986212300021, 203.1620759397597, 192.17238142905904, 1678.0987185504243, 218.26392319997237, 1225.8686610334905, 218.59672648352085, 293.0621667006724, 665.8052773362131, 386.5178906194059, 858.7437842015587, 458.53772566010457, 404.08029723805794, 303.3527677503724, 285.20493691006084, 268.1113177123211, 238.2348259306341, 223.58106589584074, 213.51028259322098, 3737.2958845213184, 2379.232720494038, 1953.019987951038, 1320.4516096306024, 502.3794958212405, 344.8963683578389, 333.5794404947303, 291.5772197139198, 152.22154303481202, 113.19332476980536, 108.28990820327576, 102.44872699555536, 101.71559791544752, 95.36652721097653, 71.58371387681403, 64.67820244929503, 62.28671729627213, 59.14138082676694, 56.97128286819144, 37.879653545100055, 35.35606065830478, 34.3243992527636, 33.995172983921954, 32.861145121445134, 29.007798329665516, 27.090268272887403, 24.963118539952053, 22.984530900460577, 22.074543095913313, 20.44532506023586, 121.10536667593358, 120.78325411441108, 138.41734878249875, 2755.5118171452204, 63.511471003707804, 1699.006536322264, 405.1100414717363, 250.12570974918665, 177.93169124229223, 246.57586549037345, 167.7307879295888, 542.4262606804454, 712.1928344895584, 604.2549609226775, 239.21952720761016, 379.08855904715654, 203.248409017023, 307.00780355572084, 240.5401873190983, 201.05893089979796, 189.37432937022544, 168.9003008834142, 164.64203429141878, 171.1974094619156, 182.14712260741965, 162.32585847394463, 619.5323301657681, 539.1702087524178, 426.97810928237783, 405.0509347586285, 345.44125126457294, 343.6687757968465, 321.94857410831696, 298.4116473983305, 290.7077863943024, 286.4994483412505, 222.33364996101557, 209.3495842372402, 203.60238918078568, 196.9688056953933, 162.4562087184578, 140.1210341299403, 117.81949194216844, 106.24464435896247, 100.66370021869872, 97.97631385427377, 89.66154871727255, 81.25930508945216, 80.5383250542424, 64.39156700112603, 64.2491464560542, 56.92806806601112, 41.76045126026362, 215.34775509070474, 32.606581698875864, 527.1399195550416, 785.8956922813135, 105.00471158540468, 328.37097899595227, 257.37863893704514, 792.2800124137257, 423.6616942773682, 219.9934924476286, 570.1754737643474, 128.16902417405515, 345.8467032126247, 337.59118863905513, 168.89650522131663, 271.5740601684087, 150.99943757599576, 521.1074298058531, 485.20355065089285, 452.16698811340535, 433.59502433868147, 334.7435407568536, 292.99022120547, 244.64145611692973, 229.1124511565562, 223.97716717343343, 219.36123988849968, 179.55817638552003, 165.42161779096745, 152.12171336807165, 165.70782525059562, 136.6409384513528, 135.66615529696602, 121.3593517181437, 115.50371228679238, 108.31256449610841, 105.75228547510137, 104.46099804782178, 102.8876760856778, 97.2720660457004, 95.43691891986235, 94.33615482198579, 91.23739607153732, 90.02316118271708, 89.41610409052461, 87.9351296431438, 85.61589737946551, 735.8030639125407, 390.3573161287467, 391.7920435339102, 689.7399752546604, 338.9577119505789, 565.763828974545, 3122.6524348284147, 138.45385871760172, 335.7320066211581, 802.4348780881273, 241.31059998085675, 398.7639662048217, 368.38935030986, 531.2704673637217, 336.064288688096, 173.27885576445945, 164.2016235762335, 335.4246586418586, 302.6819596738966, 310.37455679761575, 291.1045075429794, 353.4568321818433, 232.46584453505199, 285.6279059037607, 330.1178019256776, 252.94038610622943, 781.763778085618, 606.7016414366791, 391.44534196372456, 355.3634109826378, 347.26727803862025, 396.88907191541466, 290.35419392475364, 267.43173936393964, 232.8133703151019, 204.4902374454969, 196.64800253510825, 189.9277162250119, 184.17548586039666, 180.33445997510017, 176.8058978718808, 163.21383243507907, 157.23696267026918, 151.36332230933638, 131.43255695697417, 118.38121870053737, 117.21784259027065, 114.9777633140985, 101.66244285542582, 98.06205529167158, 92.58283076334018, 82.37825209261709, 82.27664682166468, 81.98758146205843, 73.37155629661275, 73.02747646428159, 314.1530500043587, 384.82925468910213, 2127.137266515995, 901.7377015267067, 213.93246145380803, 407.52680320103235, 132.3117098054213, 568.8367033937689, 197.99640859483927, 374.6887262455469, 653.8174951738466, 157.21388543826473, 448.4380813132704, 479.5455113131611, 303.66633883994666, 172.90782128750607, 214.15521007238888, 196.18910014112043, 1959.3102277970572, 640.6498507400921, 587.7151754410933, 2014.4565501978054, 527.8211722971424, 326.05810960633477, 290.71155733877237, 241.02598158832484, 237.19109325017692, 184.29846563495846, 150.4371692846668, 139.87997292901792, 114.93956812817119, 91.21167633310246, 86.02939744750637, 83.93171854260132, 76.69391748070367, 75.13442930011986, 69.61557811584001, 66.86068606221971, 66.39683664603464, 65.87309436077177, 59.76025421619963, 57.6805568026208, 38.94643810748631, 33.821154224794604, 33.81807643466312, 31.62072238309938, 29.77924156390492, 28.72457366565866, 768.7373021306944, 788.9105071650067, 905.6620949272962, 1051.9038518797413, 74.56435386952946, 1778.0007898075996, 646.2694975065879, 428.5444099951072, 566.3078883349973, 613.2162669132631, 124.03471330490115, 212.88190531222267, 321.1704371192593, 176.95241032494616, 263.54803392078816, 158.35068294951554, 251.40973124837947, 180.76436653679994, 194.34456890482195, 173.73563166195626, 690.8477679834793, 639.8559364445902, 527.1397870428127, 513.0296631514321, 473.3565839042776, 449.9988709028878, 384.03743242803404, 355.00961056658207, 349.2698871802671, 336.8960019839135, 305.0507628915531, 293.23737147662035, 286.81622328079163, 233.58077594653327, 222.08795315854803, 216.21178074336174, 206.52337559068124, 310.9458941564423, 184.68397705459054, 184.4254120695199, 181.25542175925082, 178.23734151093407, 153.233820337409, 245.8860720351877, 150.54979866678994, 148.60689255904606, 145.0361117103162, 140.97310716875904, 140.30103645516243, 138.33382792769723, 2264.593215675565, 1118.1721736608085, 567.516121121134, 784.5278650588106, 678.2747692910873, 1635.8178609733166, 568.2699176934559, 756.0931908629432, 622.4552216274926, 2367.2202035668192, 4951.88492327511, 2542.085513131798, 1086.9031970085161, 390.0340801297338, 3514.283112021997, 2036.4633824635616, 4352.379115768025, 2170.6552144404527, 3493.0265955724153, 795.6279069665928, 2444.1382620744193, 1911.759725576443, 2567.53084158593, 1371.9714690356902, 1118.8415668926264, 2111.870201893883, 1410.6598274522428, 1319.6748493549887, 1088.362481545744, 2823.665812555219, 2422.271781094973, 1403.913849445699, 2800.674561357822, 2007.8123677399549, 1174.9659600986793, 1605.3870842338504, 1486.096634678488, 1246.6675093401907, 1604.092778148364, 1968.3777780612986, 2021.3426060457114, 2015.3612712610793, 1704.996880572868, 1312.8658878621727, 1372.7437687161357, 1363.6286636832572, 707.0555392685413, 437.5275440977985, 423.5940374037188, 418.83586587838806, 370.66599680974747, 336.956444512205, 331.8672504186313, 269.3370872775153, 263.6070809691634, 242.39697476086533, 241.32498539772004, 230.21997894228005, 277.2638060724601, 198.4693567845009, 193.3978159718858, 182.3511953410892, 181.003592480458, 174.61789018789648, 165.51173628668232, 163.8091649452163, 144.07826991573165, 132.83119882307838, 131.58184768090976, 124.1375419975841, 124.12758494423655, 121.35730096831001, 117.64429034071888, 114.3182294580259, 109.25790799312786, 108.08385176808977, 874.3760115052298, 942.8068356173695, 454.4266889516473, 186.42851919589242, 441.3336819055163, 920.9485695106499, 197.3480248996579, 1733.1964868917744, 752.7088455295268, 260.7097762672782, 527.2562509774535, 1254.2551746343827, 498.7877072216864, 516.7660949212516, 729.9017862220832, 673.3749632208542, 325.62458303432817, 466.5837141428941, 615.3554300271437, 547.8224551758003, 1178.608699101836, 1127.8618278175197, 953.4049084220574, 852.8392348182313, 441.0536786380588, 738.2758093889817, 578.1567076901317, 655.7876230128345, 505.20231040324535, 527.5388541945078, 519.4157450039247, 505.7349232663206, 515.3444123373231, 487.4467647604349, 715.3221251547521, 605.697630964374, 525.3307999981594, 491.71626178352534, 479.19667335081544, 332.9157097555545, 329.0587561432313, 283.18337796379507, 274.84750247272314, 271.72129421726544, 216.48801431438022, 211.6005740293124, 200.47987912356646, 189.23927761951603, 2016.3564074546778, 177.42640034625848, 155.9526695987119, 142.07022005024632, 137.59780717100656, 136.50954525890674, 135.46981756945115, 132.79543711743216, 132.1882998015319, 131.88447934476133, 129.19836506770878, 117.26176644401673, 116.62387190219692, 111.73232729635556, 107.41414119464936, 105.82401905348485, 432.90139545837525, 725.7462438035694, 150.24063853639205, 359.74162587109726, 172.82306467031898, 393.3333007528333, 498.775441715204, 250.04949851754273, 539.1794329650368, 348.85739523764806, 432.54187710251824, 345.89885981673666, 265.28116168569494, 739.587373165597, 457.7030756763344, 463.4801967337439, 366.10770436956966, 549.3155441230085, 743.1494703591582, 381.96113436509097, 535.6346091705594, 508.57882591181607, 483.91618900251655, 557.3316099322105, 421.44057070594795, 356.22399634922994, 473.2253650430649, 449.7192904759158, 449.13325390117035, 412.2609929071508, 399.391177495059, 372.33013164096786, 367.21227025529447, 371.10302760528765, 352.1740462190455, 311.7810584744576, 195.61378302613187, 188.53264506783714, 187.05750464919456, 182.35659752718868, 171.36316786528158, 164.73382716095207, 142.00999382371785, 137.4942976684158, 133.80550354829288, 132.6794737695698, 128.51112134633448, 128.06803939245293, 126.48919340676976, 121.68016886805405, 121.17190644227797, 120.55795474309895, 877.1580976765941, 93.68463045295977, 92.43037519576659, 92.34161788595908, 86.26804016084684, 78.67539321909558, 74.82721425471827, 72.73175368067882, 72.08813852543138, 71.55647812051271, 61.39056294957416, 1321.12727777859, 487.9843506246491, 1927.6759148924925, 401.6756932630391, 362.1877308965447, 946.545761447363, 249.36912522664298, 1413.2387206869657, 325.30941554913903, 313.50185328426505, 285.97351565723915, 225.54882258746207, 173.65925920001627, 330.82279676551383, 145.72897713629484, 246.88220784380727, 238.23511257122178, 270.42810860945775, 184.73448693957448, 156.0064730801332, 220.86426207307585, 164.5055925300291, 179.2077292551608, 170.6218795615216, 2594.174245642471, 676.7625216096467, 624.4148548559517, 547.1419128505729, 400.7026349931127, 374.86273051069463, 352.0844404495329, 172.37837811460827, 170.9202965326549, 143.36619162330672, 129.81609694847958, 128.65751465953235, 112.72511392110681, 104.22531492144421, 96.94831203748865, 94.13191823895676, 93.77775788190574, 81.09093837168332, 79.45288387319923, 78.22082533078267, 78.06744294352886, 73.25757380631637, 70.1351446139734, 68.09726202919545, 64.2212905195647, 61.678379075406895, 60.14336553338311, 59.41732724158443, 57.953980664020584, 57.73968790723936, 3082.783513385439, 315.2915605930673, 193.09416368145918, 519.3388837640896, 439.05871222822583, 559.1114041491593, 302.0710185284903, 854.6923790770691, 481.6029642365545, 393.1056889125498, 348.8309776609875, 115.0927049061866, 104.15736213344385, 752.612052400939, 454.9099534310874, 571.6413430773308, 165.3308044072287, 253.76125996179078, 309.2053814573926, 424.09808026267444, 401.1040068171915, 246.33324694818668, 233.24341273697675, 209.75576808227908, 262.85030650650316, 244.28268163692803, 222.96088693173886, 210.34303992621875, 479.5865193760383, 385.7609502840633, 358.078100274596, 280.773742068355, 276.3481331162261, 248.42079605177221, 244.16658773606798, 212.3718290388337, 209.11705709212717, 198.72753702581124, 192.02413975459606, 174.76252263915373, 169.63937987624553, 168.81294685092212, 159.13574111779928, 144.34128405489906, 142.77370280577014, 139.71444630364317, 134.70015407477607, 127.4080290071768, 121.61191985804345, 100.33874133652272, 100.2084620129581, 99.91545596396368, 99.9041331340423, 95.30928274831138, 95.1621156390973, 89.10816354122997, 87.31917125353353, 81.39379189280757, 393.94478901764677, 1042.8207044942217, 293.05178551054917, 221.3147722533719, 389.784368755813, 141.28365993978568, 356.6878110950225, 573.9001425053061, 245.35269498000812, 220.7126440435946, 805.8115309615558, 372.6028476410101, 327.2013747004873, 326.1273182694005, 310.7392881435873, 297.3604338728312, 245.8681551262722, 244.94131231279184, 12286.38717305104, 183.7737561476362, 168.06121869530043, 143.58564220327196, 269.2768856969066, 124.3390264620791, 123.11304493566932, 120.65892675435651, 110.78758996012346, 107.48085480040282, 100.32906089563235, 137.64860710791731, 84.47006068162797, 84.31671985030012, 80.89811288757208, 74.82971812517552, 73.33205857895078, 666.3809535663247, 111.4559205254616, 71.4881913964286, 66.30652883794966, 62.85969313099518, 745.3599016037801, 504.4620688822576, 1697.7353935655497, 1446.4944572518673, 278.7307465897232, 1352.6636475454347, 7787.996089654384, 154.5267366350548, 254.6313661177064, 1770.7405088349735, 1039.7963075761797, 1212.0551777941612, 452.1548535155739, 1665.084004694435, 520.0204000128333, 544.9600513282196, 3021.7194164914918, 479.5216380523099, 1041.5856144711302, 1844.9341545415127, 599.5229525665472, 980.6159289066212, 472.88856523433986, 509.05863698172453, 537.4751983636503, 681.9623950109554, 587.1906195213422, 531.7713986104402, 572.4801709645587, 591.9146611784936, 557.8805968906822, 541.6948482457913, 559.7648670007277, 511.63231894061363, 509.8842804636187, 741.0406427655151, 657.9046024254734, 648.7851963756677, 593.578531726744, 511.33595148687914, 444.4409491996302, 315.43010079335403, 280.7287218052869, 268.9035709507027, 197.05738093854237, 137.68412886585187, 132.1560697428419, 109.56458782229123, 95.16052608907182, 77.53950047703287, 76.04378166470345, 75.68542820787863, 74.07741101826566, 73.14117682942772, 69.00553090747067, 65.00737109104887, 64.9435492539211, 64.26820792341344, 57.88028754443294, 57.73464123080889, 93.41851839668982, 52.77583503070068, 43.19484404466308, 43.084895077395345, 40.39388540727894, 609.4109549152384, 89.47572938428075, 180.64597644489635, 122.7438770234965, 478.8203449665052, 236.18573269052362, 243.73326939074076, 525.1205559142381, 314.9101056464628, 196.999489650969, 144.52126556377243, 132.05895699388853, 130.13753877258284, 94.32063481823546, 80.56345348011408, 3121.529260254661, 1832.7836214180338, 1630.2387568642355, 1477.9667068909853, 1308.5111983747179, 1133.4558364014758, 806.7593842480841, 739.4552742091367, 311.0412117166895, 182.08165905530544, 178.78956426759228, 177.96945038128956, 159.5046910106105, 141.82760786280048, 134.94539069356136, 131.38550147407065, 116.32398050327427, 115.03329742263722, 107.66240921573578, 101.59273452025843, 96.03243086813374, 95.90451073788171, 95.8434736054134, 92.22481080399737, 88.21916615293651, 86.69559324599662, 82.29837403404423, 79.11428669282074, 77.16534999224594, 74.93860018835974, 8770.86957457317, 1086.7959896603975, 293.55691746498275, 664.4478489622376, 993.681944665835, 966.4285336874219, 3818.261927220269, 582.9365672206325, 1741.1808146202059, 432.2687119197496, 251.03261037105347, 985.9573717658271, 2312.5519901353964, 405.50288941134784, 1976.2250686468196, 1489.0685759333483, 653.480139576406, 735.8240405009369, 1367.693767797559, 809.947355710819, 564.0450924870087, 1866.8200732468222, 1414.0494983219146, 745.5460491322846, 940.8465546740275, 779.0470695619033, 893.2770886654648, 940.4300088859861, 963.2009336876442, 846.0212917130181, 769.1042958619441, 773.4050681377163, 750.874696429127, 392.8547398560508, 326.89049084628215, 216.69999036076777, 210.09064053638255, 167.9306247555367, 158.8645332383848, 244.23807982927457, 137.6329848235525, 137.35191094877302, 137.27093780706846, 133.66314633152123, 183.87004564193788, 132.26228730367498, 131.4939658354133, 121.2303241698938, 121.08323480064244, 107.76022896012365, 100.25830414476752, 94.26848764076186, 93.93812550933225, 93.11512349406276, 89.4385462816918, 88.57375549592273, 87.48061808291104, 87.9729383359284, 86.90393305001682, 85.04529757434236, 84.8235388934972, 84.3949080414112, 82.82399357780672, 243.76429224487396, 294.6258665088792, 207.66930683584368, 124.62320535092277, 139.6475231924548, 200.08716652221415, 150.24429120410974, 120.98727452481536, 153.49362263129225, 492.09703232146234, 140.02133735847895, 249.8216514769491, 337.39796347666845, 245.7481342166359, 182.4189453840632, 223.96811114854125, 161.4309201414538, 237.95264624806487, 313.1938365742398, 203.86359285208616, 183.64996206336627, 163.40110085509775, 165.20336884331223, 151.28803973595308, 1136.8190182532617, 886.0525994972846, 366.4719172232522, 185.7456328125937, 151.1419623525966, 141.42590551411425, 110.91466127072184, 115.89298334545938, 109.45115364390085, 106.88843424285274, 96.9425021523042, 94.87109710488791, 91.14593330394577, 84.79279525323216, 82.41385968062336, 74.79534893807745, 70.22997053474526, 67.68136853994048, 60.1337786861913, 56.26572384097548, 55.850333470447566, 53.50786468437138, 52.36861384750482, 50.96034534200485, 47.878320013932324, 47.44645615652452, 44.50590059331335, 41.02917452588978, 40.464401965852645, 39.452296018965065, 1202.1391885968835, 306.6528994635591, 60.83872373592148, 105.72418701202629, 295.13704502723647, 423.1115233987638, 79.02564402707266, 572.6623968350093, 551.170362811383, 238.97462626032532, 235.70797292107528, 538.0448378996809, 372.1911837336404, 286.39209404011103, 3110.869211250699, 417.7030687566409, 360.33733188324607, 630.7981809862497, 204.98698980708946, 806.7081214812367, 261.22854235926724, 764.7351649622225, 358.7625161392269, 290.9349990589099, 325.9876984253249, 360.09367308251666, 267.96391872280697, 337.25567791662104, 476.71386500890344, 316.6207612277248, 418.37276798470276, 481.6092854771135, 410.670158517735, 310.43214819247396, 417.34372353502016, 365.6977873441417, 322.47421940772864, 2031.6098198842878, 1611.9292136807599, 1007.5967929647489, 696.1218075174049, 615.5441751276899, 409.1779797607517, 346.9003886204256, 343.50753235401857, 313.37805366625815, 275.75121574052446, 251.2662842681902, 197.87895796402483, 192.854879201269, 186.1449509975874, 166.63661270152423, 159.78191642747052, 159.51341914251682, 144.61349404639213, 134.37220871438166, 133.51801635319435, 124.55688613164934, 107.64959491144369, 97.63015838730509, 92.26850970178218, 89.2722210509292, 88.15174641108447, 73.2439613762296, 67.42319892746296, 59.428274454069744, 57.34087918945919, 3520.2512201043346, 1944.7767133920602, 9374.616395960025, 527.5617544654793, 1124.5395245765967, 318.0140988845411, 899.037660166903, 194.17921590889438, 548.6914625542993, 539.6736755830764, 837.4422317014383, 372.054977982912, 1078.2117270685294, 699.4396647077477, 378.95124822129526, 390.2872800430941, 674.9617560647407, 409.2087379752735, 474.2653841723685, 579.7845801016441, 546.3614833559868, 622.2612002411769, 452.44717211975876, 368.72467527224245, 382.22193096316147, 370.85383711042573, 1408.8432944847073, 1105.2521343827673, 402.9555957421018, 364.77257306073454, 246.6653195679517, 244.9479261783207, 214.23287406961748, 196.64413908899206, 171.29849648189128, 163.6627344133794, 161.00100976673366, 156.74119234933008, 151.75518731371903, 122.84418358775883, 118.03308857404305, 93.66114114390275, 85.59794097465182, 76.73224128181317, 75.62954731058457, 72.98608429555367, 69.02355513855144, 65.51297483281289, 64.60703626846002, 64.08854715823675, 61.58094431493139, 60.47681357697762, 58.435066017415096, 52.58939691848325, 46.204230077696145, 45.34540684944746, 150.4759914483713, 670.9613094404201, 1071.5716447339903, 395.57822310759667, 455.0647084837357, 353.51282815828665, 935.0971572720625, 1645.3110642593738, 204.5957612605176, 847.4528196546096, 608.7708617412941, 249.38252300246268, 329.9688910245532, 249.10343106609776, 138.16631293460844, 123.20347323041979, 1877.6072684918956, 941.4572825265273, 781.3666577179171, 717.2913645075761, 490.16197368334764, 430.9908371894398, 301.9182100395559, 222.46193219479522, 207.29301668278362, 200.61535373815704, 182.46067915161333, 166.70898444457023, 139.35150344458853, 129.33767062392178, 116.3626831624737, 115.3915684853971, 114.27369805067957, 109.02541042809246, 104.3412217649424, 100.13009810943576, 97.82465285465398, 95.17641975727544, 91.52277173407455, 83.60576776168429, 81.88738846643841, 79.92975374399462, 77.30641945046156, 68.83322771492996, 66.81657178849385, 63.518952129442035, 1041.7635979339177, 129.62392101020046, 133.35517234962316, 181.46626017537102, 215.61642923449304, 104.72898077221868, 297.2210936230792, 127.35610606586098, 113.76053738305052, 101.18001811395432, 108.58438269909159, 105.43355559394256], \"Total\": [12929.0, 12486.0, 12349.0, 7771.0, 9775.0, 7076.0, 6982.0, 5512.0, 9034.0, 11422.0, 4854.0, 6130.0, 6967.0, 10308.0, 3738.0, 5896.0, 5147.0, 3143.0, 3693.0, 3004.0, 3528.0, 14710.0, 3672.0, 3064.0, 6312.0, 2623.0, 2896.0, 2595.0, 8439.0, 4566.0, 5512.496326392839, 1440.2893439432748, 1317.5695688170258, 780.8672065049392, 690.0152725162334, 655.6356961488458, 575.2669194092447, 1019.7132046397085, 485.44774519605653, 342.7856817384251, 265.30019832014204, 261.2914508354144, 216.82643394512218, 207.06352516925327, 153.24912743632973, 153.05709352349905, 145.63324134477352, 137.17310641311144, 136.07729167050027, 127.01919988294613, 119.03486731170804, 105.84425972496103, 92.22056851595133, 89.17055608960185, 88.0186584998787, 78.34675118412989, 69.79480788284911, 69.71780340187469, 63.09294457039552, 58.88265957207596, 1460.515479834329, 901.2634011881843, 217.86419635134888, 695.7579424451699, 1383.0155272501802, 7143.118318838404, 1756.7426111635364, 586.4132378751134, 891.153138107299, 896.7117515256042, 4854.778213879654, 1720.8631325156164, 1242.2912432914327, 1233.650223811962, 1178.4350947600706, 942.731862721999, 546.7613510037347, 462.18792419206983, 226.69395836997458, 152.85337410924552, 94.5556139177353, 76.9952711522474, 76.21352520713111, 76.18930794404784, 75.17361804504284, 69.67389779190404, 68.8608525847921, 68.12779357924553, 67.67057194070581, 66.54495961051701, 56.89413867614724, 55.7411070385604, 50.55763565912673, 44.57549557715081, 43.88114236403067, 42.08812564248552, 41.65862792531313, 41.24144987023675, 39.88196570427444, 38.17746486930223, 475.99435629723564, 198.45939823042693, 1044.8024009864835, 115.39285856144123, 153.12353057490293, 1236.516874235469, 99.77397485852649, 3458.742932098261, 2644.699467905966, 1058.1231686656163, 528.6342452586725, 1326.0799238702139, 7725.441380132751, 1094.315591358884, 2798.3766503784964, 6530.06777414076, 2361.9948697891446, 426.57672078500246, 3064.4487875769123, 5867.7488623666595, 982.0784199613314, 3663.8335006139987, 1601.46710843963, 9785.715981131554, 2542.7825391551223, 3469.5704581370055, 6469.984623285656, 1389.724029696034, 1134.8057460645573, 842.274020027011, 526.492620098665, 436.1059812708138, 430.02955350912384, 273.08908368432276, 256.320016104228, 239.41919794088903, 238.33297296442404, 204.0192987161454, 163.8748940090642, 159.770640170028, 159.1490890594512, 134.60367842994245, 116.14835482177494, 103.35744947594186, 95.79370667576012, 86.19495337333291, 78.1802009484093, 71.93415825897887, 67.90777017034681, 55.95469252772991, 52.93822785463007, 45.96036367838677, 44.07437460300715, 42.98073218029098, 42.360568416108364, 39.12488557749385, 37.92163033589899, 615.6535069736724, 539.7359968566443, 12929.591403691724, 450.0910852432634, 732.7793778147235, 1697.7449606335315, 443.1897511666995, 230.491979786416, 1501.7942654907358, 452.310338771201, 3520.4731458926885, 4959.929613725652, 4680.659859858671, 1479.8652414854073, 1136.0980904644325, 1564.593592815526, 1156.917983134496, 891.7021999733681, 1320.4077200477986, 10308.452793180732, 2769.451242652184, 9340.96478934574, 7620.610431597654, 11132.713769468759, 5007.400850221529, 2276.1411618950374, 1471.9796780238657, 2347.869064382979, 867.5413910002127, 749.4878535187238, 569.7010430608352, 467.10502880273305, 277.8455861022484, 228.4922490336047, 227.9869903334491, 227.19185847416117, 186.31003035166705, 183.44515806135894, 180.25750609257824, 175.79397991670606, 168.3417056121623, 166.74791517932698, 129.43577655068432, 128.0495924639646, 120.15291266499585, 116.54833022556626, 113.79500449433557, 111.75094650968137, 111.1939958454963, 105.43500655570843, 99.74521481264043, 93.15279752848106, 89.88353340996231, 89.44713807190927, 88.76213070949736, 88.72369947850466, 88.32021777010289, 324.3255477715831, 402.4282382456271, 170.6926071077383, 414.7361275626837, 428.73515194722904, 234.39676397601045, 942.0291808576274, 361.90656928494735, 505.49385859008834, 194.6170257751491, 277.7259350671155, 1635.772381025174, 2826.708778209007, 1835.269289781277, 509.25057090711454, 1088.2815985742152, 807.8785387553771, 966.9907649326594, 447.8225414125867, 3777.969103326149, 944.490994624266, 2329.2345217467137, 599.7416836717736, 1096.2242742820363, 1133.6294159135634, 2542.7825391551223, 3194.966760098359, 945.0608782878194, 3597.167744082687, 17422.08024721183, 10308.452793180732, 6312.777945231084, 5297.898990902841, 12929.591403691724, 692.8271111193717, 466.5017918264255, 399.49684357533357, 387.84494097881463, 410.2318300647512, 360.48792003474426, 293.2994822017814, 265.6989831236416, 230.95520211084266, 195.72522970881224, 190.91940713009893, 189.46841799220235, 166.47075657525903, 155.46860706041215, 154.03123102663432, 151.9873313682799, 132.32214889604424, 122.36384394014524, 120.5347940006259, 1697.8939782423363, 107.04674582384153, 106.20623678709948, 101.90330092277051, 101.34814727691186, 99.35512588954668, 98.03103425297277, 95.12376106777269, 94.76501120826462, 92.93412418755116, 92.58212908986249, 289.0015607660684, 476.1656981358323, 1301.0036931717375, 536.6744290069756, 482.2820296042654, 198.1487477881101, 1167.1600592750601, 178.5324730434362, 266.1810255417866, 453.2756234394778, 1170.1213948274062, 1719.5739581962416, 1959.7298542132924, 3517.2313727543014, 1650.7072549733857, 906.5654436207076, 1213.3814645302791, 566.6029329528972, 996.3247976711696, 1471.9796780238657, 1080.1499216761524, 588.9163056056603, 4256.63181110468, 1659.688296274005, 7143.118318838404, 17422.08024721183, 11132.713769468759, 9785.715981131554, 1145.6820555579238, 553.9461670909409, 527.1835113379599, 504.4830065526833, 503.1887071891977, 322.4895580286207, 318.22561570386694, 259.5415489470096, 188.30667351134804, 163.11620091468222, 165.59460579375732, 131.18107342244653, 109.50602710967037, 100.73809775192977, 97.8023502308062, 93.81577551185626, 83.3759873842781, 82.83355141380486, 76.58363903861778, 73.6427992888765, 72.31459462385432, 70.53048371185942, 68.59020447956831, 66.56387369294734, 65.76799078504897, 65.54246977989901, 58.27203464972084, 56.92242708910558, 54.36687770286977, 54.241321922631634, 73.80300269082157, 335.82071486184964, 180.6630820357712, 123.60318261139014, 1405.0367287042313, 228.08736226359193, 271.45101758984663, 208.8010960818697, 2197.014252441213, 514.5760537130839, 1062.1691575639143, 548.5049647124487, 450.65289644218376, 996.3247976711696, 143.26897758744812, 449.79472784527866, 1374.3989573506817, 768.1826044660621, 751.5024857098891, 623.0955647824687, 493.44271655543025, 444.035125668602, 390.014769440056, 372.18905061380553, 365.243999152082, 344.4607542273848, 335.77624682349295, 242.47884564514186, 221.47726187618605, 210.66585458404708, 201.49918612738767, 186.47012461396073, 184.37909793092012, 184.201682078703, 178.09448714701057, 170.20550515858665, 168.39326020701722, 166.44709791458703, 160.2493649369375, 153.22332575375668, 147.04255811993391, 145.2538275605119, 139.51542832501679, 134.20297820812613, 130.0171154090187, 124.94704045728326, 1014.2858089600222, 809.2577618892183, 764.307853489424, 1964.587337668337, 625.1819630909001, 383.8785673726387, 408.0163988910772, 771.3221906639327, 1489.0358309910507, 786.1302190152323, 710.3030373029683, 549.4265804652272, 502.94871486599015, 14710.177695443741, 2996.9298667403004, 704.0309871450982, 11795.464933537312, 810.053945864466, 3861.980372232686, 5398.710021103604, 2139.4780516303217, 15220.673634975674, 6807.269236981376, 6624.0898368002245, 1956.285201412604, 2556.17209596604, 17422.08024721183, 4200.879154691094, 2230.81572687879, 6315.928392622642, 2731.38647514795, 3402.765844103396, 12664.330235837046, 2146.831507339872, 1839.5738328313082, 2109.3592802754415, 6504.1480141155225, 3691.1283656756773, 5896.780119119082, 4987.115139040325, 3431.7753916002657, 3339.955403332737, 9785.715981131554, 6177.393279362236, 11132.713769468759, 3965.6355250634006, 5867.7488623666595, 4195.82889945922, 9340.96478934574, 6469.984623285656, 8439.001269184782, 12929.591403691724, 4680.659859858671, 9775.112901989449, 815.0449152950629, 559.48947359842, 402.30792714414537, 344.84310867483384, 322.13500552599004, 269.9379384854978, 238.01392916604757, 225.8781941637682, 225.5952393882457, 214.71674138007126, 200.61403225357864, 183.74861688351203, 144.77287161180507, 833.8202479243373, 133.12301977375262, 124.16423633020375, 118.74142347325007, 111.64313324715653, 106.42529764223283, 101.76031006154442, 97.84265483811208, 96.41347859299388, 95.68794559511508, 94.94094467248671, 88.93076520080916, 87.26064017491917, 84.18902456695994, 83.08133237811796, 80.39488900319202, 608.5023698481808, 779.4219041892167, 100.0995616835747, 379.4059375815547, 2344.144650317235, 280.97711095258205, 977.1334823076061, 262.75456185529, 764.25714690994, 449.38661918521, 3517.2313727543014, 148.18078520393038, 255.6700920156297, 2511.2075928566646, 360.4532262516923, 1270.7730324074485, 285.33465407052375, 565.0677999302429, 4791.907351787909, 2211.4860910696298, 653.3028477069096, 8439.001269184782, 3528.616755821021, 4256.63181110468, 6918.298406896916, 3480.1933917725955, 17422.08024721183, 3965.6355250634006, 8836.941307766872, 12929.591403691724, 3524.569885800106, 12664.330235837046, 3004.7384531100943, 971.5320762033753, 402.60601056361315, 278.511252850767, 237.13537261421482, 236.43795341579087, 221.92619729040183, 208.18352603223818, 206.21932215766213, 154.74758597928115, 151.36498514098034, 139.77930736969398, 126.54736604205729, 94.71808332583275, 93.54777372352079, 88.68473568646314, 86.75455862327341, 78.79044529099085, 62.73342911857588, 55.93386283739448, 52.61686745733564, 48.36424847671333, 46.478225331738855, 35.65751536410245, 31.593986993088897, 28.771361985084816, 27.916750222422365, 25.311500151124935, 22.463839985205357, 1030.044439697392, 1948.8868437164356, 3064.4487875769123, 193.11735715533734, 2644.699467905966, 156.4631816487187, 1068.065779415464, 1171.7581674123394, 713.5246432308306, 181.08291551145163, 81.39791154110276, 1393.4421312470758, 351.02568318394424, 667.0473119352743, 324.53414809840433, 309.44910963459483, 1725.0192577580442, 247.64892901783674, 1446.7188785674095, 1121.213786868447, 2915.230941801528, 6530.06777414076, 677.9689252562079, 2361.9948697891446, 1098.4270791374788, 4577.300127397735, 3517.2313727543014, 2211.4860910696298, 1056.1751960867548, 818.929512003393, 639.0205758371147, 525.2608366601079, 443.38930555403556, 368.8194071826491, 354.30580342243513, 303.1253088640521, 298.18694665327195, 263.5559439760387, 253.15358240945778, 250.68309091113812, 246.81872674543686, 234.99927543203972, 232.9269429712743, 223.3618454047541, 222.36035062964916, 218.27904706036009, 168.100201440956, 167.16749458646, 164.4320262213931, 164.1243305451417, 154.60107305404344, 141.26190510286335, 140.34563211910364, 131.17679737272596, 131.04753096432034, 130.2911798735639, 116.63144902211343, 113.46374594385767, 2939.850668392773, 496.4560698582359, 1925.2434769401912, 1333.6801831184582, 362.0793634913956, 3630.321463762049, 1512.6002836731677, 4704.410332812264, 1670.9441729741113, 1068.8731725907637, 761.041231393714, 231.98123890940738, 1121.213786868447, 2965.582659552091, 644.2852873630065, 2382.6330675648614, 1082.216271862138, 966.9907649326594, 3157.9737373391977, 1544.8125279097176, 895.3055858264121, 3777.969103326149, 2887.89588196219, 2039.7081736587463, 12664.330235837046, 17422.08024721183, 2826.708778209007, 1457.2522365553318, 7620.610431597654, 9785.715981131554, 643.1515939686685, 487.64926500426725, 452.59964245087275, 313.846733161056, 303.249361364094, 301.05537891359035, 281.31994454279766, 241.76092116654866, 163.74130252101267, 144.4965438369935, 143.31532998446113, 119.99982716298115, 114.54781252079849, 91.53655969283074, 90.83544879610834, 84.78487583495138, 65.83217792155675, 65.81441592771124, 64.3296899398422, 62.30796339303027, 61.92741303896494, 63.21835398749942, 57.25428537040896, 55.69939974956666, 48.006937009138014, 43.89268351096736, 41.082089487837756, 40.88589688240999, 38.452462465370836, 36.3101519300428, 250.4531002242253, 1595.514184031465, 371.5978571333632, 6982.771881095692, 641.5501635110354, 444.5661642598724, 1168.47413277911, 459.85249571210755, 5297.898990902841, 10308.452793180732, 166.7828852865243, 882.1365220858072, 209.05039795789025, 213.02605875650872, 5114.744469213912, 935.1034117892935, 7620.610431597654, 5983.683246495002, 4577.300127397735, 2170.6329079362486, 2701.6371211508513, 4055.800097638101, 8836.941307766872, 1377.8573828758695, 1254.626940172157, 5867.7488623666595, 802.7833924970658, 2211.4860910696298, 9785.715981131554, 1998.6351588835337, 642.7518760515615, 559.2899214920723, 314.14566711061065, 310.7641364489189, 293.81384196716743, 288.73328682825377, 253.34278428699187, 213.65804305514405, 213.64118763084159, 200.23392828140243, 197.14454084882362, 177.0391508392559, 158.74346012840894, 149.32012516940256, 146.8986612724627, 146.53603591255418, 140.85709763287736, 137.35331914557705, 120.49684332635931, 111.08977304901305, 92.71890935059123, 89.30968948206717, 88.38016423927326, 78.06365536151155, 76.41341273853931, 74.28761971105602, 65.30207791566134, 65.2406148134568, 61.11138636494456, 1558.2409831509026, 320.4995109011018, 695.0847559546883, 432.74070095016083, 155.96464988863755, 235.34877505630828, 502.2224196349473, 159.21492425412885, 527.3840375337754, 512.3443810428396, 944.490994624266, 1512.6002836731677, 229.6563515775644, 611.0496385945831, 557.605029131104, 401.9650343952557, 313.2838693985559, 258.68827032777244, 200.82087968835995, 144.23629120859664, 131.28031076007636, 112.50908442990514, 110.03822123572692, 109.01518381314956, 106.0039103866715, 104.60947759657667, 97.84634021810878, 88.84466966691977, 86.2543296587393, 78.18453451554939, 76.29014744876588, 66.96453271544324, 63.46953322433556, 61.37432703379744, 58.69630534246633, 55.90664968703877, 47.44047012998903, 47.01857995391393, 46.76702599874463, 44.981195936375876, 43.597177471357554, 228.5936726689992, 36.198902942904645, 2107.3940110821877, 1518.1560111117744, 379.0962911526671, 1489.301533560608, 896.7117515256042, 103.4205027123726, 532.1362519984342, 408.5471335292509, 367.361316472846, 198.2873390676854, 94.90239218671553, 301.1485382198624, 1405.0367287042313, 3480.1933917725955, 1745.0561100142215, 1725.0192577580442, 4791.907351787909, 7076.40648406218, 313.1453179098203, 252.9658294240748, 200.4671190435205, 174.9327923015191, 172.11257831854292, 160.1402502000487, 154.29451852977485, 153.00572736356764, 120.22174018031363, 112.91475159991603, 86.10100159815416, 78.92267805185266, 73.30432644825525, 65.42202341082151, 61.41814858597997, 59.76701197087613, 59.590167592773035, 57.93495766251942, 55.674684425079356, 53.8336573290294, 52.177416498028, 51.813670173550754, 45.83979430753768, 43.789849966565974, 41.200802006322576, 38.36029328737088, 35.72448841063309, 34.40526591642862, 33.432821192232666, 1745.0561100142215, 1482.5680211710912, 1899.2567328278851, 1206.0828495886417, 1725.0192577580442, 260.3671903339914, 3458.742932098261, 1310.8413856784023, 574.505561841448, 1556.1668069862073, 340.78101736008387, 679.5177915602376, 3693.3365189633837, 1382.9968937091955, 3630.321463762049, 4117.82633872119, 993.653234062723, 2965.582659552091, 1005.4542059581246, 5867.7488623666595, 1749.6393782627733, 6177.393279362236, 3520.4731458926885, 9785.715981131554, 5896.780119119082, 563.8113008127534, 320.1648828190788, 309.470723002617, 258.65634646178825, 252.9572813496896, 167.0382257648124, 138.08243202555082, 134.75738831748097, 130.59496902296365, 130.3260978575228, 130.13159307267446, 130.05772534805104, 129.74438132094883, 128.94976196953252, 126.43531984569256, 123.99184533357815, 121.68548998851138, 113.01992273191944, 112.47283709752904, 106.28935129458898, 104.30472539280382, 103.35406391783141, 101.15785402990912, 100.0116182728395, 99.9786851895612, 93.84357140782566, 93.57990995408007, 92.40596559858402, 89.72512753439499, 89.00190883845497, 1637.0368288330799, 708.3541662538064, 294.3456667042046, 250.89072244440476, 3155.5165256287005, 1210.721854063891, 226.11432467971653, 190.31912402457644, 210.49176796957963, 870.4334505204024, 678.1503500003105, 1642.068857562102, 1544.8125279097176, 2839.8899201568206, 242.63756509553156, 730.7717325708053, 656.2681980071346, 3480.1933917725955, 1628.365007404652, 2556.8464535063904, 600.7298212645198, 2276.1411618950374, 804.6710924524306, 256.4355660531144, 2731.38647514795, 599.7416836717736, 763.2378173657244, 1224.925027787554, 17422.08024721183, 5983.683246495002, 7725.441380132751, 5007.400850221529, 9785.715981131554, 10308.452793180732, 12664.330235837046, 1719.5739581962416, 2623.150664087553, 456.3049859122544, 403.65114039536445, 328.04112461622026, 296.3541946948479, 290.4673800717571, 281.3851899687255, 226.73293846750852, 212.6869777105664, 355.4046214897514, 168.5643963145414, 167.60684164554178, 158.3544206252614, 155.3112839724306, 146.93210795834295, 143.51296219805914, 119.20924102819832, 118.12701765632163, 115.1448685203683, 104.73693090058593, 100.0145654775583, 99.55796490055843, 98.94528094467185, 93.48932133859218, 90.28043171189977, 89.8219144830388, 87.63134256571925, 79.17520693591162, 78.9085620987671, 76.154327618657, 288.3225219607908, 270.4168845399928, 1473.5161777752037, 3693.3365189633837, 177.56830106496486, 155.05831853450346, 297.55045953411553, 1590.1706200062858, 1743.8763741738398, 138.28453918447204, 1421.0144641393304, 2859.5151178737824, 3528.616755821021, 558.5198160232114, 1637.0368288330799, 2826.708778209007, 1479.3839318729551, 1635.772381025174, 3458.742932098261, 1704.306450000425, 2146.831507339872, 7771.982674883251, 1471.2113642437484, 542.6739086679538, 531.3424438308632, 432.89017129078235, 325.33897257336696, 249.06046128498556, 219.44734057182387, 189.27578403109231, 185.20775952404935, 171.84957056910298, 156.87627002057988, 147.73597061853863, 131.83194351899706, 120.32602520862841, 109.59379369682898, 102.40620619472978, 97.7080892314912, 93.80478060599647, 80.96198182321747, 73.8974538681517, 72.26899223266574, 69.82972048498517, 62.50926504954663, 60.20317243659058, 59.21017399088314, 50.94798802825661, 50.839223144137, 44.124390701140186, 30.6574167845655, 479.87129448208174, 1187.8890302574912, 191.1681780849581, 319.17357762620395, 554.8842123185268, 294.1585246009546, 264.9834546352071, 344.27570824319685, 273.25909841885584, 1501.7942654907358, 6312.777945231084, 683.8011483392361, 1471.9796780238657, 2915.230941801528, 9785.715981131554, 3182.0926250815246, 7620.610431597654, 815.9504460929955, 507.6477500883759, 483.982457940236, 482.2647396744416, 318.98529825241127, 312.03426102984804, 1678.5405622108467, 201.8255838718852, 178.5107857884081, 176.60740315744926, 175.94929929671275, 168.79954346914383, 163.88943080294496, 163.2207298409988, 147.3710936887267, 117.9493992293454, 112.20238187255487, 109.76809732709513, 105.49925621095923, 98.99450938595962, 87.2641795178496, 86.30712537125541, 85.78058534850027, 77.59929262917998, 72.88731789524554, 72.33276757971593, 72.30088895124081, 59.398780484556575, 58.5082933974673, 58.28054755075999, 1093.3027418945726, 1704.306450000425, 714.6505019709284, 435.58751271943356, 2235.20428841471, 833.9220221908291, 899.3886599346669, 437.3898003318733, 1504.8832442285184, 1170.1213948274062, 213.0024308309508, 4680.659859858671, 729.8554202827418, 540.793316260003, 5147.421968176167, 1421.0144641393304, 813.1659800862932, 751.362114921901, 703.6392136030693, 633.21673836737, 460.0112887502386, 436.9591900700126, 365.9959669334457, 319.96038879968444, 196.75709439388362, 175.67358969299212, 150.42971363481465, 143.024117222701, 640.0668545125997, 135.86790956786598, 132.5901518282905, 127.78646928161776, 124.53298711342528, 108.13700976923327, 105.70000738220531, 103.41871910922127, 91.2792495136339, 89.26870224346187, 87.00028686149996, 85.22358856382688, 83.27464482990752, 82.58683412480947, 80.89513451792261, 74.70582970946289, 70.85105702403823, 64.48078381433346, 203.4490050890696, 114.44144278489654, 280.86839082749486, 157.12452711007305, 835.4081168043903, 873.0324467663374, 161.36802017488316, 228.68901038191663, 150.3254818106072, 1759.717336303341, 642.378389031206, 800.3189801119186, 231.8517993039428, 410.75297140831015, 1037.867104589796, 924.0710601740931, 827.9675751002633, 3509.6742543986256, 923.7262761907958, 396.3520263008147, 1713.389958488308, 285.4773267777534, 546.8166343395076, 2211.4860910696298, 536.6112896660917, 2333.715799572043, 1168.47413277911, 4791.907351787909, 1756.7426111635364, 1270.2938512889407, 9785.715981131554, 492.5906889042889, 306.7873996277714, 212.97786940919002, 190.53551393631005, 175.51159037062607, 169.11968094503595, 158.286032035066, 157.9049426715698, 149.77466557217474, 140.4070325998293, 134.7629421523153, 124.3154431683104, 115.74358365736069, 118.64481282812238, 111.68734136991289, 91.90079192164679, 88.47811917531612, 87.12774675996883, 85.99376979770722, 84.71845164802015, 84.462441982997, 82.01383081326904, 81.81641246944146, 81.42470237133712, 81.05196938249087, 79.12482983544601, 78.1082450293083, 77.20300871392573, 75.0570626968935, 70.83247774730023, 633.8599470770769, 193.0989182920216, 166.65064709208053, 296.76459548896815, 771.5098336955355, 340.10696094377676, 245.29976814572316, 213.96393450035347, 201.39593732936115, 234.15584083589985, 243.7611880960929, 128.21182939344578, 295.7083482256988, 730.7717325708053, 214.36499387067067, 6530.06777414076, 1271.1384909857543, 2333.715799572043, 1391.8597573201068, 1014.5723791724962, 951.3750937747515, 713.0759374564635, 605.5786439647277, 370.85538922086977, 358.7048831098146, 309.53431080105275, 297.8513706709454, 270.2963992677101, 244.727628350931, 195.1417865546502, 171.46543245577936, 148.06702539505702, 147.1826630023633, 141.3627804918505, 140.46699880649197, 132.76114374131993, 99.28642556751586, 95.57852305177295, 92.92853435692442, 91.40851012828088, 89.58567826127974, 83.33861965045034, 83.24078467870869, 82.25333589433448, 78.6543967056053, 78.28844110615476, 75.84639332719101, 71.62122525960847, 282.0623192199125, 1641.833642936597, 136.26939901783084, 317.1704089933869, 119.85175605111108, 229.6759277244555, 219.74008348352098, 2757.93907820101, 298.431948140955, 4566.333153619091, 388.3837500785077, 647.017775631663, 2859.5151178737824, 1596.9947475888198, 10308.452793180732, 5147.421968176167, 5297.898990902841, 5114.744469213912, 11132.713769468759, 9340.96478934574, 7620.610431597654, 7725.441380132751, 6967.961150259002, 3738.2813954485873, 2380.218231421307, 1954.0054988783068, 1321.4371205578711, 503.36500675644857, 345.8818792851076, 334.56495142199896, 292.56273064118847, 153.20705396208072, 114.17883569707404, 109.27541913054444, 103.43423792282404, 102.7011088427162, 96.3520381382452, 72.56922480408271, 65.6637133765637, 63.27222822354082, 60.12689175403563, 57.95679379546013, 38.86516447236875, 36.34157158557347, 35.30991018003229, 34.980683911190646, 33.846656048713825, 29.993309256934197, 28.075779200156084, 25.948629467220734, 23.970041827729258, 23.060054023181994, 21.43083598750454, 148.99318940441154, 154.5586815732217, 187.2540332299173, 6130.980419311581, 80.13676405042423, 5896.780119119082, 1034.7038204789428, 572.616847894743, 359.28463223629495, 721.5365222295786, 436.4887099421486, 5114.744469213912, 11132.713769468759, 9340.96478934574, 1331.3529379533359, 4055.800097638101, 1073.2959641506727, 4987.115139040325, 3339.955403332737, 1956.285201412604, 2245.924910145922, 1659.7558362445225, 1650.7072549733857, 2308.6139184061813, 8836.941307766872, 7620.610431597654, 620.5158025929761, 540.1536811828296, 427.96158170958586, 406.0344071858365, 346.42478044399166, 344.65224942493126, 322.932046535525, 299.39511982553853, 291.6912631514209, 287.48292076845854, 223.31712238822362, 210.33305666444826, 204.58586160799373, 197.95227812260137, 163.43968114566584, 141.10450655714837, 118.80296436937647, 107.2281167861705, 101.64717264590675, 98.9597862814818, 90.64502114448058, 82.24277751666018, 81.52179748145043, 65.37503942833406, 65.23261888326223, 57.91154049321915, 42.74392368747165, 220.6867332703159, 33.59005412608389, 570.5391977978427, 951.2520543958678, 121.09878773130687, 449.79472784527866, 348.343631739534, 1382.9968937091955, 655.3596979579313, 320.8829350609699, 1596.9947475888198, 165.02843594730845, 1899.2567328278851, 2001.1608043729707, 387.42678524218275, 4959.929613725652, 3672.2120309622446, 522.0923964687407, 486.18851731378055, 453.15195477629305, 434.57999100156917, 335.7285074197413, 293.9751878683577, 245.6268292220272, 230.09741781944388, 224.9621338363211, 220.34620655138735, 180.5431430484077, 166.40658445385512, 153.14732180056293, 166.8916466895476, 137.62590511424048, 136.6511219598537, 122.34431838103136, 116.48867894968004, 109.29753115899607, 106.73725213798903, 105.44603305067263, 103.87264274856545, 98.25703270858806, 96.42188558275001, 95.32112148487344, 92.22236273442498, 91.00827541757353, 90.40512547252385, 88.92009630603145, 86.60086404235317, 765.4622024193891, 403.7647009995249, 408.32981012493156, 803.3729195770513, 396.8028702412871, 790.6911688212981, 6967.961150259002, 155.12600786369288, 500.0923361223795, 1650.7072549733857, 333.63111478302505, 853.9243285908061, 777.6180939133282, 1670.9441729741113, 738.7422794390246, 233.7129109798135, 211.81946533950196, 1479.3839318729551, 1632.9414146878792, 1822.4858642804063, 1489.301533560608, 4791.907351787909, 750.1880127552403, 2859.5151178737824, 11132.713769468759, 6177.393279362236, 782.7481630092484, 607.6858076798999, 392.42950820694523, 356.34757722585846, 348.2514442818409, 398.13003974212893, 291.3383601679743, 268.4159056071603, 233.79753655832263, 205.47440368871762, 197.63216877832897, 190.91188246823262, 185.1596521036174, 181.3186262183209, 177.79006411510153, 164.1979986782998, 158.2211289134899, 152.34748855464395, 132.4167232001949, 119.36538494375812, 118.20200883349139, 115.96192955731924, 102.64660909864656, 99.04622153489233, 93.56699700656092, 83.36241833583783, 83.26081306488543, 82.97174770527917, 74.3557225398335, 74.01164270750233, 333.3283297902636, 430.548444015206, 2896.3817501483954, 1270.2938512889407, 247.6269368126081, 605.4697527644166, 151.16052920266299, 1289.9466910949843, 274.64693026630454, 853.9243285908061, 2268.9132469426686, 197.68706844988444, 2001.1608043729707, 3509.6742543986256, 2769.451242652184, 340.78101736008387, 2511.2075928566646, 7620.610431597654, 1960.2953908618738, 641.6350138049086, 588.7003393147426, 2017.987006219926, 528.8063354761973, 327.0435236984388, 291.69672040358876, 242.01114465314123, 238.1762563149933, 185.28362870650062, 151.42233234948318, 140.86513599383431, 115.92473119298757, 92.19683939969458, 87.01456051232275, 84.9168816074177, 77.67908054552005, 76.11959236493624, 70.60074118065639, 67.84584912703609, 67.38199971281836, 66.85825742558815, 60.74541728101602, 58.66571986743719, 39.9316011723027, 34.806317289610995, 34.80323949947951, 32.60588544791577, 30.764404628721312, 29.70973673047505, 865.1811715786582, 1104.998835833012, 1532.3500279136351, 2107.560655783094, 92.13452427558796, 5147.421968176167, 1504.8832442285184, 907.4771478398403, 1564.593592815526, 1896.4319677841272, 199.8706756769662, 554.8842123185268, 2556.8464535063904, 951.6635532966316, 4680.659859858671, 1213.3814645302791, 10308.452793180732, 2344.144650317235, 3520.4731458926885, 2399.1528279491636, 691.832428488319, 640.8836796877964, 528.1244475476524, 514.0143236777877, 474.3412444091171, 450.9835314077273, 385.02209293287353, 355.9979275104214, 350.2545476851066, 337.880662488753, 306.0354233963926, 294.22203198145985, 287.81762139649436, 234.5654364513728, 223.07261366338756, 217.19644124820127, 207.50803609705764, 312.4832215297763, 185.66863755943007, 185.41710860771045, 182.24030339525822, 179.2220020157736, 154.21848084224854, 247.48156776001957, 151.5422456250518, 149.59342437808098, 146.02077221515572, 141.95776767359857, 141.28569696000196, 139.31848843253675, 2281.6606335920933, 1131.1995505727023, 574.3398986947842, 797.587005151939, 693.4679786030246, 1788.4139423531465, 600.6094497003777, 812.8836897655166, 670.2649075413381, 2828.596021362531, 6312.777945231084, 3131.6019640305576, 1267.266813891109, 419.9343514429179, 5564.114492566414, 3055.1742417169335, 7813.880072329612, 3352.9067489496083, 6530.06777414076, 1025.3711338985165, 4463.519356826509, 3263.5325461636376, 5016.942036214565, 2251.8654957493354, 1756.7426111635364, 4361.573107448992, 2581.2054633774587, 2325.8996841439357, 1749.6393782627733, 8836.941307766872, 6918.298406896916, 2798.3766503784964, 11422.991417927855, 6504.1480141155225, 2333.715799572043, 4704.410332812264, 4099.96640411294, 2986.6164199548202, 6469.984623285656, 12664.330235837046, 14710.177695443741, 15220.673634975674, 11795.464933537312, 4256.63181110468, 7143.118318838404, 8439.001269184782, 708.0361368401519, 438.50814166940916, 424.5746349776997, 419.8164634499987, 371.6465943813581, 337.93704208381564, 332.8478479902419, 270.31768484912595, 264.58767854077405, 243.37757233247595, 242.3062354497017, 231.20057651389067, 278.4785531490162, 199.44995441663013, 194.3784135434964, 183.33179291269983, 181.9841900520686, 175.5984877595071, 166.49233385829294, 164.78976251682693, 145.05886748734227, 133.811796394689, 132.56244525252038, 125.11813956919475, 125.1081825158472, 122.33789853992067, 118.62488791232954, 115.29882702963656, 110.24018150956502, 109.06444933970043, 894.9450652097466, 966.323946727658, 464.0781362582201, 188.78375033880303, 452.0264005244317, 966.3816136747166, 201.15196704361642, 2197.014252441213, 929.5369102118036, 285.769834397058, 670.4020314007066, 2109.006751466449, 682.5874180346318, 750.1880127552403, 1665.9543252894198, 1479.8652414854073, 480.0498025945022, 935.1034117892935, 1835.269289781277, 1474.752662665191, 9785.715981131554, 10308.452793180732, 7620.610431597654, 5983.683246495002, 1206.71837394162, 6982.771881095692, 4987.115139040325, 8836.941307766872, 4055.800097638101, 6967.961150259002, 9340.96478934574, 7725.441380132751, 11132.713769468759, 5007.400850221529, 716.3067992218193, 606.6823050314412, 526.3154740652266, 492.7009358505927, 480.1813474178828, 333.90038382262185, 330.04343021029865, 284.16805203086244, 275.8321765397905, 272.7059682843328, 217.4726883814476, 212.5852480963798, 201.46455319063384, 190.2239516865834, 2027.0103516511551, 178.41107441531886, 156.93734366577928, 143.0548941173137, 138.58248123807394, 137.49421932597411, 136.45449163651853, 133.78011118449953, 133.17297386859929, 132.8691534118287, 130.18303913477615, 118.2464405110841, 117.6085459692643, 112.71700136342294, 108.39881526171673, 106.80869312055222, 438.1580025532292, 746.3047121664861, 154.3222118884611, 403.04632225309666, 181.89780912807848, 455.3170915962636, 597.9981991533598, 288.0488774940277, 760.0205414843116, 481.02508777699063, 648.7340084949656, 484.00860844025516, 342.2199275586355, 1632.9414146878792, 841.870114092831, 1082.216271862138, 866.2533079561072, 2165.865240302958, 4873.840781670379, 1008.9864468821122, 2660.074709003879, 2325.8996841439357, 2764.883258317267, 4704.410332812264, 1828.0699580648684, 1094.315591358884, 6967.961150259002, 5016.942036214565, 17422.08024721183, 11422.991417927855, 8836.941307766872, 2382.6330675648614, 3055.1742417169335, 372.08614991929716, 353.157168533055, 312.88405549051055, 196.59690534014146, 189.51576738184673, 188.04062696320415, 183.33971984119827, 172.34629017929117, 165.71694947496167, 142.99311613772744, 138.4774199824254, 134.78862586230247, 133.6625960835794, 129.49424366034407, 129.05116170646252, 127.4723157207793, 122.6632911820636, 122.15502875628752, 121.5410770571085, 885.4020571599265, 94.66775276696931, 93.41349750977614, 93.32474019996863, 87.25116247485639, 79.65851553310513, 75.81033656872782, 73.71487599468837, 73.0712608615091, 72.53960043452226, 62.37368526358373, 1350.5221294371795, 503.62490711884305, 2391.6799848514306, 474.63246705545953, 426.2818714621009, 1263.3353863951297, 326.08806003191154, 3520.4731458926885, 517.5369537302411, 551.7405585874491, 484.8409273814368, 364.38520175281417, 242.97196505680628, 1785.7798727224438, 219.4478622562401, 2001.1608043729707, 2769.451242652184, 11132.713769468759, 1039.0854064332766, 368.98234278200465, 9340.96478934574, 696.6787463506972, 7620.610431597654, 10308.452793180732, 2595.158321196113, 677.7501274731478, 625.3989303951773, 548.1260062841255, 401.68671052753297, 375.8468060451149, 353.068652842789, 173.36245364902854, 171.90437206707517, 144.350267157727, 130.80017248289985, 129.64159019395262, 113.70918945552708, 105.20939045586448, 97.93238757190892, 95.11599377337703, 94.76225544654963, 82.07501390610359, 80.4369594076195, 79.20490086520294, 79.05151847794913, 74.24164934073664, 71.11922014839367, 69.08133756361572, 65.20536605398497, 62.66245460982716, 61.12744119259715, 60.401402776004694, 58.93805619844085, 58.72376344165962, 3143.333313885743, 329.3643501116175, 219.92366106587113, 690.1228329053087, 570.9725821134347, 800.6389611691127, 484.4366537133735, 2165.865240302958, 1046.6077696115265, 803.029334723529, 761.6778245953525, 145.3361721443098, 128.55378859285824, 6967.961150259002, 2770.182659230505, 4566.333153619091, 330.68917337813, 891.153138107299, 2235.20428841471, 6918.298406896916, 7143.118318838404, 2168.436029395343, 2308.6139184061813, 1382.9968937091955, 9034.679593967636, 5016.942036214565, 11422.991417927855, 4791.907351787909, 480.57006947698545, 386.74450038501044, 359.06165037554314, 281.7572921693021, 277.33168322085123, 249.4043461527193, 245.15013783701508, 213.3553791397808, 210.10060719307427, 199.71108712675834, 193.00768985554316, 175.74607274010083, 170.62292997719263, 169.79649695186922, 160.11929121874638, 145.32483415584616, 143.7737918855712, 140.69799640459027, 135.68370417572316, 128.3915791081239, 122.59546995899055, 101.32229143746981, 101.1920121139052, 100.89900606491078, 100.8876832349894, 96.29283284925847, 96.1456657400444, 90.09171364217707, 88.30272135448062, 82.37734199375467, 429.48235662378863, 1247.2014811492297, 348.70341079882405, 355.1762246625467, 1230.6982227475887, 208.07503836879886, 1703.0041851733667, 6918.298406896916, 1785.7798727224438, 6504.1480141155225, 806.843883232147, 373.5887223090704, 328.1872493685476, 327.1132058767764, 311.72516281164764, 298.3463085408915, 246.85402979433258, 245.97494783176913, 12349.26587896042, 184.75963081569657, 169.0470933633608, 144.57151687133234, 271.37327010954453, 125.32490113013948, 124.0989196037297, 121.64480142241689, 111.77346462818383, 108.4667294684632, 101.31493556369273, 139.01929720406665, 85.45593534968835, 85.3025945183605, 81.88398755563246, 75.8155927932359, 74.31793324701115, 675.4896297596188, 112.98179493417639, 72.47406606448898, 67.29240350601003, 63.845567799055544, 759.0956382970063, 521.3953518798979, 1787.9343807997345, 1705.9586278856652, 309.31381442977505, 1693.2838251998392, 11422.991417927855, 164.94689680877008, 294.8376673862904, 2887.89588196219, 1659.688296274005, 2354.020132846863, 672.4073404980389, 3663.8335006139987, 810.2987242836083, 865.259947098054, 8439.001269184782, 779.593148322859, 2839.8899201568206, 7725.441380132751, 1506.07846303413, 5007.400850221529, 1029.979510335471, 1326.0799238702139, 1531.3982679491742, 7813.880072329612, 4959.929613725652, 3693.3365189633837, 5983.683246495002, 8836.941307766872, 7620.610431597654, 5867.7488623666595, 10308.452793180732, 12664.330235837046, 3597.167744082687, 742.0218298364636, 658.8857914215884, 649.7663852651015, 594.5597187976924, 512.3171385578275, 445.42213627057856, 316.4112878643024, 281.70990887623526, 269.8847580287619, 198.03856800949075, 138.66531593680025, 133.1372568137903, 110.54577489323958, 96.14171316002017, 78.52068754798123, 77.0249687356518, 76.66661527882698, 75.0709958047149, 74.12236390037607, 69.98671797841902, 65.98855816199722, 65.92473632486946, 65.2493949943618, 58.8614746153813, 58.71582830175725, 95.11814110448064, 53.757022101649035, 44.17603111561144, 44.0660821483437, 41.375072478227295, 638.326317546095, 93.03481510043818, 212.4639641155973, 141.04975776581253, 715.0460812918876, 339.64979815055017, 363.66711368984573, 990.1150165230605, 540.793316260003, 391.3036335912641, 244.78616381249012, 215.3153636210699, 218.6015830621941, 149.3962064479461, 92.89330368658631, 3122.514388543765, 1833.7687497071383, 1631.22388515334, 1478.9518351800898, 1309.4963266638224, 1134.4409646905804, 807.7445125371886, 740.4404024982413, 312.0263400057938, 183.06910833299452, 179.77469255669666, 178.95457867039394, 160.48981929971487, 142.81273615190486, 135.93051898266575, 132.37062976317503, 117.30910879237867, 116.01842571174161, 108.64753750484017, 102.57786280936283, 97.01755915723814, 96.8896390269861, 96.82860189451779, 93.20993909310177, 89.2042944420409, 87.68072153510101, 83.28350232314862, 80.09941498192514, 78.15047828135033, 75.92372847746414, 9034.679593967636, 1105.4819524329318, 298.9752524156928, 684.0229019037897, 1029.5932845245645, 1050.7465298737645, 4873.840781670379, 734.1308066986683, 2770.182659230505, 550.767207263334, 304.00931215881815, 1743.8763741738398, 5147.421968176167, 572.6179507132052, 4566.333153619091, 3509.6742543986256, 1165.3163233964556, 1457.2522365553318, 3663.8335006139987, 1759.717336303341, 1046.6077696115265, 7143.118318838404, 4959.929613725652, 1896.4319677841272, 3458.742932098261, 2308.6139184061813, 4117.82633872119, 5114.744469213912, 8836.941307766872, 6918.298406896916, 2839.8899201568206, 6967.961150259002, 2859.5151178737824, 393.83849298868324, 327.87424375205745, 217.6837432665431, 211.07439344215788, 168.91437766131205, 159.84828614416014, 245.83306144469478, 138.61673772932784, 138.33566385454836, 138.2546907128438, 134.64689923729657, 185.23153435754756, 133.2460402094503, 132.47771874118862, 122.21407707755101, 122.06698770641778, 108.74398186589899, 101.24205705054285, 95.2522405465372, 94.92187841510759, 94.09887639983809, 90.42229918746713, 89.55750840169806, 88.46437098868637, 88.96575240760318, 87.88768595579215, 86.02905048011769, 85.80729181168907, 85.37866094718653, 83.80774648358205, 249.43309644275672, 315.6217700543583, 223.79778522568492, 130.2903835535314, 151.78443752308996, 231.00202053125807, 169.98375342474753, 137.8979143790913, 196.50002645219047, 1096.2242742820363, 181.45321865207518, 515.195694253823, 1080.1499216761524, 730.873471412167, 388.5502602964801, 790.6911688212981, 311.06321763643206, 1153.0176491639618, 4791.907351787909, 1156.917983134496, 883.6093678267546, 500.0923361223795, 951.6635532966316, 4195.82889945922, 1137.805327289048, 887.0388948763609, 367.45821260413715, 186.7319359063872, 152.128257731673, 142.41220089319063, 111.90095664979822, 116.93438473254936, 110.43744902297723, 107.93739247839048, 97.92879753138058, 95.85739248396429, 92.13222868302215, 85.77909063230854, 83.40015505969974, 75.78164431715383, 71.21626591382164, 68.66766391901686, 61.120074065267666, 57.25201922005185, 56.83662884952393, 54.494160063447744, 53.354909226581185, 51.946640721081216, 48.86461539300869, 48.432751603508805, 45.492195972389716, 42.015469904966146, 41.45069734492901, 40.43859139804143, 1232.5773727859355, 318.22860170139444, 62.368665429884544, 110.27379932585576, 332.61439190018575, 516.4462483578038, 84.26187414783281, 768.3087413437469, 822.540888325437, 324.04533414680304, 323.4269979529361, 969.5405419037943, 647.7994885281927, 474.95813770902515, 12486.449803444508, 808.9545452994179, 694.4415118452971, 1565.4902626350838, 340.3456482289958, 2915.230941801528, 497.1623988039386, 3480.1933917725955, 1082.2590922504473, 738.7422794390246, 929.1465560966511, 1270.7730324074485, 907.4771478398403, 1622.1142465887929, 4256.63181110468, 1506.07846303413, 4577.300127397735, 7725.441380132751, 5147.421968176167, 1601.46710843963, 17422.08024721183, 7813.880072329612, 3194.966760098359, 2032.5972116745552, 1612.9166054710272, 1008.5841847550164, 697.1091993076724, 616.5315669179573, 410.16537155101923, 347.8877804106931, 344.4949241442861, 314.37258194279764, 276.73860753079197, 252.2536760584577, 198.86634975429234, 193.8422709915365, 187.13234619534657, 167.62400449179174, 160.76930821773803, 160.50081093278433, 145.60088583665964, 135.35960050464917, 134.50540814346186, 125.54427792191683, 108.63698670171118, 98.61755017757258, 93.25590149204967, 90.2596128411967, 89.13913820135197, 74.2313531664971, 68.41059071773046, 60.41566624433725, 58.328270979726696, 3672.2120309622446, 2211.7859522163694, 12486.449803444508, 633.118650592316, 1446.7188785674095, 393.2839922866449, 1320.4077200477986, 229.03614812639643, 832.4589389988005, 891.7021999733681, 1622.1142465887929, 548.5060418654977, 2757.93907820101, 2235.20428841471, 756.5607187758733, 808.9545452994179, 2859.5151178737824, 1460.515479834329, 2399.1528279491636, 5114.744469213912, 4959.929613725652, 10308.452793180732, 2511.2075928566646, 2354.020132846863, 9340.96478934574, 4566.333153619091, 1409.827569591844, 1106.236409489904, 403.93987084923856, 365.7568481678713, 247.64959467508845, 245.93220128545744, 215.21714917675422, 197.6284141961288, 172.28277158902802, 164.64700952051615, 161.9852848738704, 157.72546745646682, 152.73946242085577, 123.82845869489556, 119.01736368117977, 94.64541625103948, 86.58221608178854, 77.7165163889499, 76.6138224177213, 73.97035940269039, 70.00783024568817, 66.49724993994961, 65.59131235992436, 65.07282226537347, 62.565219422068125, 61.46108868411435, 59.41934112455183, 53.573672025619985, 47.18850518483288, 46.329681979582496, 155.612798020029, 774.254748626526, 1465.5659719195014, 516.8645648392373, 612.3963527192013, 509.5006230681141, 1665.9543252894198, 3528.616755821021, 320.026163864458, 2382.6330675648614, 3155.5165256287005, 853.5660749357387, 1628.365007404652, 982.0784199613314, 300.6092641236544, 4117.82633872119, 1878.5910886926579, 942.4411027272893, 782.3504779186791, 718.2751847083381, 491.14579388410965, 431.9746573902018, 302.9020302403179, 223.44575239555726, 208.27683688354566, 201.59917393891908, 183.44449935237537, 167.69280464533227, 140.33532364535057, 130.32154526815773, 117.34650336323575, 116.37538878484717, 115.25751924616598, 110.00923062885451, 105.32504196570446, 101.11391831019782, 98.80847305541603, 96.1602399580375, 92.5065919348366, 84.58958796244634, 82.87120866720046, 80.91357394475668, 78.29023965122362, 69.81704791569202, 67.8003919892559, 64.50277233020408, 1161.5231405108846, 146.88659248965752, 165.56029421205153, 267.23063857690363, 685.6498238908556, 161.10561321034078, 2965.582659552091, 295.88212844844236, 220.47938953485757, 172.53611215832535, 1532.3500279136351, 1247.2014811492297], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.4134000539779663, -2.7560999393463135, -2.8452000617980957, -3.368799924850464, -3.4927000999450684, -3.5439000129699707, -3.6749000549316406, -3.10260009765625, -3.844899892807007, -4.19379997253418, -4.450799942016602, -4.466100215911865, -4.65339994430542, -4.699699878692627, -5.002399921417236, -5.003600120544434, -5.053699970245361, -5.113900184631348, -5.122000217437744, -5.191400051116943, -5.256899833679199, -5.375400066375732, -5.514500141143799, -5.548500061035156, -5.561699867248535, -5.679500102996826, -5.7967000007629395, -5.797800064086914, -5.899099826812744, -5.969299793243408, -3.0710999965667725, -3.506200075149536, -4.826300144195557, -4.1875, -4.522299766540527, -4.0345001220703125, -4.53380012512207, -4.9720001220703125, -5.0630998611450195, -5.115200042724609, -1.8533999919891357, -2.890899896621704, -3.2170000076293945, -3.2239999771118164, -3.2697999477386475, -3.4932000637054443, -4.038700103759766, -4.207099914550781, -4.9217000007629395, -5.31790018081665, -5.802199840545654, -6.0100998878479, -6.020400047302246, -6.020699977874756, -6.034299850463867, -6.111299991607666, -6.123199939727783, -6.134099960327148, -6.140900135040283, -6.1579999923706055, -6.317200183868408, -6.3379998207092285, -6.4375, -6.566100120544434, -6.582099914550781, -6.624899864196777, -6.635300159454346, -6.645699977874756, -6.679999828338623, -6.724800109863281, -4.226600170135498, -5.199100017547607, -3.7802999019622803, -5.749199867248535, -5.514500141143799, -3.9895999431610107, -6.077899932861328, -3.9677000045776367, -4.159900188446045, -4.773399829864502, -5.236000061035156, -4.969900131225586, -4.412199974060059, -5.174200057983398, -4.898799896240234, -4.636499881744385, -5.011499881744385, -5.535299777984619, -5.0030999183654785, -4.833700180053711, -5.3480000495910645, -5.093999862670898, -5.259799957275391, -4.940000057220459, -5.197000026702881, -5.210700035095215, -5.2403998374938965, -3.4279000759124756, -3.63070011138916, -3.9291000366210938, -4.399700164794922, -4.588399887084961, -4.602499961853027, -5.0578999519348145, -5.121500015258789, -5.190000057220459, -5.194499969482422, -5.3506999015808105, -5.571000099182129, -5.596499919891357, -5.600399971008301, -5.769100189208984, -5.917699813842773, -6.035399913787842, -6.112199783325195, -6.218900203704834, -6.317699909210205, -6.402100086212158, -6.4604997634887695, -6.657299995422363, -6.713699817657471, -6.857900142669678, -6.9008002281188965, -6.926499843597412, -6.941400051116943, -7.022799968719482, -7.054800033569336, -4.3277997970581055, -4.479899883270264, -1.5456000566482544, -4.715000152587891, -4.408699989318848, -3.8341000080108643, -4.9583001136779785, -5.50439977645874, -4.205599784851074, -5.131700038909912, -3.7191998958587646, -3.4886999130249023, -3.589400053024292, -4.395699977874756, -4.5929999351501465, -4.427499771118164, -4.657599925994873, -4.803100109100342, -4.622799873352051, -3.773099899291992, -4.524400234222412, -4.16349983215332, -4.243800163269043, -4.147600173950195, -4.491799831390381, -4.683499813079834, -4.938199996948242, -3.259000062942505, -4.25540018081665, -4.401800155639648, -4.676499843597412, -4.875400066375732, -5.396399974822998, -5.592700004577637, -5.594900131225586, -5.598400115966797, -5.797800064086914, -5.813399791717529, -5.830999851226807, -5.856200218200684, -5.899799823760986, -5.909299850463867, -6.164299964904785, -6.17519998550415, -6.2393999099731445, -6.270100116729736, -6.2941999435424805, -6.3125, -6.317500114440918, -6.371200084686279, -6.427199840545654, -6.496300220489502, -6.532400131225586, -6.537300109863281, -6.545100212097168, -6.545499801635742, -6.550099849700928, -5.257400035858154, -5.045100212097168, -5.90339994430542, -5.096799850463867, -5.06689977645874, -5.639100074768066, -4.387700080871582, -5.254700183868408, -4.964799880981445, -5.825799942016602, -5.52269983291626, -4.059199810028076, -3.6249001026153564, -4.033100128173828, -5.104700088500977, -4.662700176239014, -4.864200115203857, -4.7778000831604, -5.302499771118164, -4.0254998207092285, -4.857800006866455, -4.453400135040283, -5.191299915313721, -4.89769983291626, -4.903900146484375, -4.55079984664917, -4.497099876403809, -5.077199935913086, -4.823400020599365, -4.562399864196777, -4.71150016784668, -4.968900203704834, -5.1219000816345215, -5.101399898529053, -4.440199851989746, -4.836400032043457, -4.991799831390381, -5.021500110626221, -4.96560001373291, -5.094799995422363, -5.301700115203857, -5.400899887084961, -5.541600227355957, -5.707900047302246, -5.732900142669678, -5.740600109100342, -5.870699882507324, -5.939499855041504, -5.948800086975098, -5.962299823760986, -6.101799964904785, -6.180600166320801, -6.195799827575684, -3.5510001182556152, -6.315499782562256, -6.323500156402588, -6.365300178527832, -6.370800018310547, -6.3907999992370605, -6.404399871826172, -6.434800148010254, -6.438600063323975, -6.4583001136779785, -6.462299823760986, -5.363900184631348, -4.894700050354004, -3.9758999347686768, -4.843100070953369, -4.973599910736084, -5.774899959564209, -4.228600025177002, -5.880899906158447, -5.547399997711182, -5.127099990844727, -4.449999809265137, -4.190499782562256, -4.342700004577637, -4.097499847412109, -4.5690999031066895, -4.967299938201904, -4.847599983215332, -5.252900123596191, -5.0229997634887695, -5.064300060272217, -5.181399822235107, -5.396299839019775, -4.8566999435424805, -5.188199996948242, -4.9593000411987305, -5.039999961853027, -5.106900215148926, -5.202400207519531, -2.4475998878479004, -3.1751999855041504, -3.2248001098632812, -3.268899917602539, -3.272200107574463, -3.717400074005127, -3.730799913406372, -3.93530011177063, -4.257599830627441, -4.4019999504089355, -4.3871002197265625, -4.621399879455566, -4.803500175476074, -4.887700080871582, -4.917600154876709, -4.959700107574463, -5.078999996185303, -5.085599899291992, -5.164999961853027, -5.204699993133545, -5.223100185394287, -5.248499870300293, -5.276700019836426, -5.307199954986572, -5.319399833679199, -5.32289981842041, -5.442399978637695, -5.46619987487793, -5.513000011444092, -5.5152997970581055, -5.232800006866455, -3.9551000595092773, -4.4969000816345215, -4.844399929046631, -2.9886999130249023, -4.456399917602539, -4.378200054168701, -4.649400234222412, -3.3529999256134033, -4.191800117492676, -4.085899829864502, -4.3805999755859375, -4.46560001373291, -4.207499980926514, -4.942399978637695, -4.773900032043457, -5.538700103759766, -6.120999813079834, -6.142899990081787, -6.330599784851074, -6.564300060272217, -6.670000076293945, -6.800000190734863, -6.8470001220703125, -6.865799903869629, -6.924600124359131, -6.950200080871582, -7.277200222015381, -7.3678998947143555, -7.418099880218506, -7.462800025939941, -7.5406999588012695, -7.55210018157959, -7.552999973297119, -7.586900234222412, -7.632500171661377, -7.6433000564575195, -7.65500020980835, -7.6930999755859375, -7.73829984664917, -7.779699802398682, -7.791999816894531, -7.832600116729736, -7.871699810028076, -7.90369987487793, -7.943699836730957, -5.855999946594238, -6.081399917602539, -6.143799781799316, -5.214399814605713, -6.351500034332275, -6.837600231170654, -6.783299922943115, -6.1682000160217285, -5.534599781036377, -6.150300025939941, -6.251800060272217, -6.500500202178955, -6.588099956512451, -3.363300085067749, -4.879000186920166, -6.272900104522705, -3.6240999698638916, -6.141900062561035, -4.6869001388549805, -4.378399848937988, -5.256899833679199, -3.490000009536743, -4.218699932098389, -4.2754998207092285, -5.365600109100342, -5.134399890899658, -3.5532000064849854, -4.79010009765625, -5.32889986038208, -4.46750020980835, -5.186100006103516, -5.010700225830078, -3.9953999519348145, -5.424900054931641, -5.543700218200684, -5.450900077819824, -4.679900169372559, -5.07289981842041, -4.7621002197265625, -4.891200065612793, -5.155399799346924, -5.1732001304626465, -4.521299839019775, -4.82480001449585, -4.4994001388549805, -5.083899974822998, -4.911099910736084, -5.069799900054932, -4.754700183868408, -4.91480016708374, -4.973800182342529, -4.933499813079834, -5.122000217437744, -1.8906999826431274, -4.3755998611450195, -4.752299785614014, -5.082799911499023, -5.237500190734863, -5.305699825286865, -5.482999801635742, -5.609399795532227, -5.6620001792907715, -5.6631999015808105, -5.712900161743164, -5.781099796295166, -5.8694000244140625, -6.1092000007629395, -4.358500003814697, -6.193699836730957, -6.263899803161621, -6.309000015258789, -6.371099948883057, -6.419400215148926, -6.464700222015381, -6.504300117492676, -6.519199848175049, -6.526800155639648, -6.534800052642822, -6.600900173187256, -6.619999885559082, -6.656300067901611, -6.6697001457214355, -6.703000068664551, -4.678899765014648, -4.49560022354126, -6.491700172424316, -5.2505998611450195, -3.6112000942230225, -5.552999973297119, -4.482600212097168, -5.648099899291992, -4.758500099182129, -5.215099811553955, -3.5848000049591064, -6.156700134277344, -5.740699768066406, -4.022799968719482, -5.502099990844727, -4.672500133514404, -5.720799922943115, -5.362800121307373, -4.311299800872803, -4.738699913024902, -5.322999954223633, -4.282400131225586, -4.769999980926514, -4.771200180053711, -4.628200054168701, -4.867599964141846, -4.689799785614014, -5.0991997718811035, -5.081900119781494, -5.118299961090088, -5.270699977874756, -5.300899982452393, -2.2581000328063965, -3.3877999782562256, -4.270100116729736, -4.639699935913086, -4.801199913024902, -4.804100036621094, -4.867800235748291, -4.932000160217285, -4.941500186920166, -5.230199813842773, -5.252500057220459, -5.332699775695801, -5.4328999519348145, -5.725200176239014, -5.737800121307373, -5.7916998863220215, -5.814000129699707, -5.911399841308594, -6.142600059509277, -6.259200096130371, -6.321499824523926, -6.40749979019165, -6.4481000900268555, -6.719699859619141, -6.844299793243408, -6.941100120544434, -6.972300052642822, -7.074100017547607, -7.198599815368652, -3.374500036239624, -2.743799924850464, -2.3452000617980957, -5.162899971008301, -2.75570011138916, -5.372099876403809, -3.6145999431610107, -3.592900037765503, -4.163300037384033, -5.382400035858154, -6.005300045013428, -4.113699913024902, -5.030099868774414, -4.637700080871582, -5.10290002822876, -5.15749979019165, -4.303100109100342, -5.309599876403809, -4.566100120544434, -4.713699817657471, -4.472700119018555, -4.245200157165527, -4.926000118255615, -4.570799827575684, -4.905099868774414, -4.702199935913086, -4.765600204467773, -5.047399997711182, -4.139200210571289, -4.393799781799316, -4.642600059509277, -4.838600158691406, -5.008399963378906, -5.192999839782715, -5.23330020904541, -5.389699935913086, -5.406199932098389, -5.530099868774414, -5.570499897003174, -5.580399990081787, -5.5960001945495605, -5.645199775695801, -5.654099941253662, -5.696300029754639, -5.700799942016602, -5.719399929046631, -5.981900215148926, -5.987500190734863, -6.0040998458862305, -6.00600004196167, -6.066199779510498, -6.1570000648498535, -6.163599967956543, -6.231599807739258, -6.232600212097168, -6.238399982452393, -6.350100040435791, -6.377900123596191, -3.16759991645813, -4.925899982452393, -3.704699993133545, -4.140600204467773, -5.318600177764893, -3.347899913787842, -4.156599998474121, -3.34689998626709, -4.218900203704834, -4.611800193786621, -4.92140007019043, -5.764699935913086, -4.810200214385986, -4.412199974060059, -5.17710018157959, -4.544899940490723, -4.982900142669678, -5.103600025177002, -4.661799907684326, -5.0055999755859375, -5.187399864196777, -4.816299915313721, -4.983799934387207, -5.058800220489502, -4.790800094604492, -4.823200225830078, -5.0991997718811035, -5.149400234222412, -5.109000205993652, -5.120200157165527, -4.173399925231934, -4.4506001472473145, -4.525400161743164, -4.892499923706055, -4.9268999099731445, -4.934199810028076, -5.002200126647949, -5.154399871826172, -5.546000003814697, -5.671800136566162, -5.680099964141846, -5.859000205993652, -5.905900001525879, -6.132299900054932, -6.140100002288818, -6.209799766540527, -6.46619987487793, -6.4664998054504395, -6.48960018157959, -6.52209997177124, -6.528299808502197, -6.508200168609619, -6.608099937438965, -6.636099815368652, -6.787600040435791, -6.879199981689453, -6.946899890899658, -6.9517998695373535, -7.014699935913086, -7.073599815368652, -5.151100158691406, -3.3454999923706055, -4.779300212860107, -1.9169000387191772, -4.316699981689453, -4.785900115966797, -3.9655001163482666, -4.824100017547607, -2.890399932861328, -2.452199935913086, -5.738500118255615, -4.43120002746582, -5.6082000732421875, -5.618000030517578, -3.394399881362915, -4.719299793243408, -3.5072999000549316, -3.6828999519348145, -3.9375998973846436, -4.343400001525879, -4.274600028991699, -4.090099811553955, -3.921999931335449, -4.7153000831604, -4.80810022354126, -4.459799766540527, -5.008500099182129, -4.887700080871582, -4.872300148010254, -2.125, -3.260499954223633, -3.399899959564209, -3.9779999256134033, -3.9888999462127686, -4.045199871063232, -4.062699794769287, -4.193900108337402, -4.364999771118164, -4.365099906921387, -4.430200099945068, -4.445799827575684, -4.553999900817871, -4.663700103759766, -4.725299835205078, -4.741700172424316, -4.744200229644775, -4.783999919891357, -4.8094000816345215, -4.941299915313721, -5.0233001708984375, -5.2058000564575195, -5.24370002746582, -5.254300117492676, -5.379899978637695, -5.401599884033203, -5.430099964141846, -5.5609002113342285, -5.5619001388549805, -5.628300189971924, -2.4200000762939453, -3.996500015258789, -3.2711000442504883, -3.7416999340057373, -4.719299793243408, -4.362199783325195, -3.7021000385284424, -4.775599956512451, -3.8231000900268555, -3.8592000007629395, -4.101500034332275, -4.229000091552734, -4.756400108337402, -3.1556999683380127, -3.2474000453948975, -3.5754001140594482, -3.8252999782562256, -4.017499923706055, -4.2718000411987305, -4.604700088500977, -4.69950008392334, -4.855000019073486, -4.877399921417236, -4.886899948120117, -4.91510009765625, -4.928500175476074, -4.995999813079834, -5.093500137329102, -5.1234002113342285, -5.222899913787842, -5.247700214385986, -5.379899978637695, -5.434299945831299, -5.468400001525879, -5.513800144195557, -5.563300132751465, -5.730800151824951, -5.7399001121521, -5.7453999519348145, -5.785099983215332, -5.8171000480651855, -4.16480016708374, -6.007800102233887, -1.9536000490188599, -2.3742001056671143, -3.726099967956543, -2.4814999103546143, -3.061300039291382, -5.051000118255615, -3.675800085067749, -4.108099937438965, -4.394499778747559, -4.781899929046631, -5.2204999923706055, -4.867099761962891, -4.466800212860107, -4.378600120544434, -4.53439998626709, -4.689700126647949, -4.648900032043457, -1.4726999998092651, -4.593500137329102, -4.807700157165527, -5.041299819946289, -5.178299903869629, -5.1946001052856445, -5.267199993133545, -5.304599761962891, -5.313000202178955, -5.5559000968933105, -5.619200229644775, -5.89300012588501, -5.981100082397461, -6.0559000968933105, -6.171299934387207, -6.235499858856201, -6.263199806213379, -6.266200065612793, -6.294899940490723, -6.335400104522705, -6.36959981918335, -6.401400089263916, -6.408599853515625, -6.533599853515625, -6.580399990081787, -6.6427998542785645, -6.716000080108643, -6.789100170135498, -6.827899932861328, -6.857399940490723, -3.0197999477386475, -3.2792000770568848, -3.1705000400543213, -3.5952000617980957, -3.4066998958587646, -5.05810022354126, -3.0051000118255615, -3.904099941253662, -4.761000156402588, -4.162099838256836, -5.304900169372559, -5.0071001052856445, -4.138899803161621, -4.675000190734863, -4.304699897766113, -4.473899841308594, -5.060400009155273, -4.647299766540527, -5.061999797821045, -4.665200233459473, -5.001299858093262, -4.829800128936768, -5.093599796295166, -5.099400043487549, -5.205999851226807, -4.606599807739258, -5.173900127410889, -5.207900047302246, -5.387800216674805, -5.410200119018555, -5.827199935913086, -6.018799781799316, -6.043399810791016, -6.074999809265137, -6.077099800109863, -6.07859992980957, -6.079100131988525, -6.081600189208984, -6.087800025939941, -6.107600212097168, -6.127299785614014, -6.146200180053711, -6.220699787139893, -6.225599765777588, -6.282700061798096, -6.301700115203857, -6.3109002113342285, -6.332600116729736, -6.344099998474121, -6.3445000648498535, -6.408400058746338, -6.411300182342529, -6.423999786376953, -6.453800201416016, -6.461999893188477, -3.6856000423431396, -4.491099834442139, -5.338699817657471, -5.490300178527832, -3.2764999866485596, -4.135799884796143, -5.653200149536133, -5.838600158691406, -5.767899990081787, -4.700699806213379, -4.906199932098389, -4.3333001136779785, -4.383900165557861, -4.003900051116943, -5.689599990844727, -5.007999897003174, -5.110499858856201, -4.121799945831299, -4.572999954223633, -4.420000076293945, -5.261199951171875, -4.606500148773193, -5.191400051116943, -5.743800163269043, -4.73769998550415, -5.3831000328063965, -5.3572001457214355, -5.286600112915039, -4.605800151824951, -5.012599945068359, -5.036799907684326, -5.168499946594238, -5.151899814605713, -5.17609977722168, -5.255199909210205, -5.320400238037109, -2.2435998916625977, -3.9944000244140625, -4.117300033569336, -4.325300216674805, -4.427199840545654, -4.447299957275391, -4.4791998863220215, -4.696000099182129, -4.760200023651123, -4.247700214385986, -4.993899822235107, -4.99970006942749, -5.05679988861084, -5.076300144195557, -5.1321001052856445, -5.155799865722656, -5.342800140380859, -5.3520002365112305, -5.377799987792969, -5.473400115966797, -5.519999980926514, -5.524600028991699, -5.530799865722656, -5.588099956512451, -5.6234002113342285, -5.628600120544434, -5.653500080108643, -5.756199836730957, -5.7596001625061035, -5.795599937438965, -4.473999977111816, -4.581099987030029, -3.1600000858306885, -2.453000068664551, -5.064300060272217, -5.175600051879883, -4.644400119781494, -3.508699893951416, -3.6294000148773193, -5.308899879455566, -3.8024001121520996, -4.038099765777588, -4.170599937438965, -4.951200008392334, -4.7108001708984375, -4.795400142669678, -4.8815999031066895, -4.892199993133545, -4.825699806213379, -4.964900016784668, -4.99970006942749, -1.020400047302246, -2.6856000423431396, -3.683799982070923, -3.7049999237060547, -3.9103000164031982, -4.196700096130371, -4.464799880981445, -4.591899871826172, -4.740499973297119, -4.762400150299072, -4.837600231170654, -4.929299831390381, -4.989799976348877, -5.104499816894531, -5.196499824523926, -5.2906999588012695, -5.3592000007629395, -5.406599998474121, -5.447800159454346, -5.596799850463867, -5.689199924468994, -5.7118000984191895, -5.746600151062012, -5.859099864959717, -5.897299766540527, -5.9141998291015625, -6.067200183868408, -6.069399833679199, -6.214000225067139, -6.588200092315674, -3.8399999141693115, -3.0276999473571777, -4.8094000816345215, -4.513199806213379, -4.146500110626221, -4.685100078582764, -4.850900173187256, -4.736999988555908, -4.92519998550415, -4.6057000160217285, -4.1528000831604, -4.909599781036377, -4.8403000831604, -4.833600044250488, -4.862299919128418, -5.127500057220459, -5.113900184631348, -3.186300039291382, -3.661600112915039, -3.709399938583374, -3.7132999897003174, -4.127299785614014, -4.149400234222412, -2.4677000045776367, -4.586900234222412, -4.710299968719482, -4.721099853515625, -4.724800109863281, -4.766499996185303, -4.796199798583984, -4.800300121307373, -4.90310001373291, -5.127500057220459, -5.1778998374938965, -5.199999809265137, -5.240099906921387, -5.304299831390381, -5.43179988861084, -5.442999839782715, -5.449100017547607, -5.550600051879883, -5.614099979400635, -5.621799945831299, -5.622300148010254, -5.821800231933594, -5.837200164794922, -5.84119987487793, -2.9203999042510986, -2.814300060272217, -3.6068999767303467, -4.111400127410889, -2.7788000106811523, -3.809299945831299, -3.8942999839782715, -4.762800216674805, -4.257999897003174, -4.506100177764893, -5.145299911499023, -4.617499828338623, -4.979400157928467, -5.027699947357178, -4.916900157928467, -5.01039981842041, -3.811199903488159, -3.8903000354766846, -3.9560000896453857, -4.061600208282471, -4.381800174713135, -4.4334001541137695, -4.610899925231934, -4.745800018310547, -5.23390007019043, -5.347899913787842, -5.503900051116943, -5.554800033569336, -4.056399822235107, -5.606500148773193, -5.631100177764893, -5.6682000160217285, -5.694200038909912, -5.836599826812744, -5.859600067138672, -5.881700038909912, -6.007800102233887, -6.030300140380859, -6.056300163269043, -6.077199935913086, -6.100599765777588, -6.109000205993652, -6.130000114440918, -6.210599899291992, -6.2642998695373535, -6.359899997711182, -5.253300189971924, -5.811299800872803, -4.979800224304199, -5.521699905395508, -4.019999980926514, -4.056700229644775, -5.549200057983398, -5.263400077819824, -5.611299991607666, -3.6556999683380127, -4.504799842834473, -4.338099956512451, -5.2881999015808105, -4.879700183868408, -4.217700004577637, -4.309800148010254, -4.392000198364258, -3.499000072479248, -4.469399929046631, -4.976500034332275, -4.157199859619141, -5.2316999435424805, -4.998799800872803, -4.530300140380859, -5.024899959564209, -4.588200092315674, -4.84089994430542, -4.504799842834473, -4.76200008392334, -4.898900032043457, -4.955999851226807, -3.5611000061035156, -4.035799980163574, -4.402200222015381, -4.514100074768066, -4.596700191497803, -4.633999824523926, -4.7006001472473145, -4.703000068664551, -4.756199836730957, -4.821199893951416, -4.862599849700928, -4.943900108337402, -5.015900135040283, -4.991300106048584, -5.0518999099731445, -5.248799800872803, -5.287199974060059, -5.302700042724609, -5.315999984741211, -5.331099987030029, -5.334199905395508, -5.363900184631348, -5.366399765014648, -5.371200084686279, -5.375899791717529, -5.400199890136719, -5.413300037384033, -5.425099849700928, -5.453700065612793, -5.512400150299072, -3.3252999782562256, -4.532299995422363, -4.730599880218506, -4.248899936676025, -3.4444000720977783, -4.168600082397461, -4.497300148010254, -4.63070011138916, -4.731400012969971, -4.685800075531006, -4.728000164031982, -5.100900173187256, -4.88539981842041, -4.5894999504089355, -5.003900051116943, -4.149400234222412, -4.625199794769287, -4.970399856567383, -3.1117000579833984, -3.4282000064849854, -3.4925999641418457, -3.7811999320983887, -3.9449000358581543, -4.436299800872803, -4.469699859619141, -4.617499828338623, -4.656099796295166, -4.753600120544434, -4.853300094604492, -5.0808000564575195, -5.2108001708984375, -5.358399868011475, -5.364500045776367, -5.405099868774414, -5.411499977111816, -5.468299865722656, -5.76140022277832, -5.799799919128418, -5.8282999992370605, -5.844900131225586, -5.865300178527832, -5.938399791717529, -5.939599990844727, -5.951700210571289, -5.997000217437744, -6.001699924468994, -6.03380012512207, -6.091899871826172, -4.745500087738037, -3.077899932861328, -5.467800140380859, -4.69890022277832, -5.611100196838379, -5.035399913787842, -5.091000080108643, -2.9240000247955322, -4.963699817657471, -3.23799991607666, -4.962200164794922, -4.669000148773193, -3.848400115966797, -4.392199993133545, -3.5940001010894775, -4.221399784088135, -4.347799777984619, -4.634500026702881, -4.696199893951416, -4.757999897003174, -4.876200199127197, -4.939700126647949, -4.9857001304626465, -2.078399896621704, -2.5299999713897705, -2.727400064468384, -3.118799924850464, -4.085100173950195, -4.46120023727417, -4.49459981918335, -4.629199981689453, -5.279099941253662, -5.575399875640869, -5.619699954986572, -5.675099849700928, -5.682300090789795, -5.746799945831299, -6.033599853515625, -6.1350998878479, -6.172699928283691, -6.224599838256836, -6.261899948120117, -6.670100212097168, -6.738999843597412, -6.768599987030029, -6.778299808502197, -6.81220006942749, -6.9369001388549805, -7.005300045013428, -7.087100028991699, -7.1697001457214355, -7.210100173950195, -7.2866997718811035, -5.507800102233887, -5.510499954223633, -5.374199867248535, -2.3831000328063965, -6.153299808502197, -2.8666999340057373, -4.300300121307373, -4.78249979019165, -5.1230998039245605, -4.796800136566162, -5.18209981918335, -4.008399963378906, -3.7360999584198, -3.9005000591278076, -4.827099800109863, -4.366700172424316, -4.990099906921387, -4.577600002288818, -4.821599960327148, -5.000899791717529, -5.060800075531006, -5.17519998550415, -5.200699806213379, -5.1616997718811035, -5.099699974060059, -5.214900016784668, -3.16729998588562, -3.3062000274658203, -3.5394999980926514, -3.5922999382019043, -3.7513999938964844, -3.7565999031066895, -3.821899890899658, -3.8977999687194824, -3.9238998889923096, -3.938499927520752, -4.1921000480651855, -4.252299785614014, -4.280099868774414, -4.313199996948242, -4.505899906158447, -4.653800010681152, -4.827099800109863, -4.930500030517578, -4.984499931335449, -5.011499881744385, -5.100200176239014, -5.198599815368652, -5.207499980926514, -5.431300163269043, -5.433499813079834, -5.554500102996826, -5.864299774169922, -4.223999977111816, -6.111800193786621, -3.3287999629974365, -2.9293999671936035, -4.942299842834473, -3.8020999431610107, -4.0457000732421875, -2.9214000701904297, -3.547300100326538, -4.202700138092041, -3.2502999305725098, -4.7428998947143555, -3.7502999305725098, -3.774399995803833, -4.4670000076293945, -3.992000102996826, -4.578999996185303, -4.263500213623047, -4.33489990234375, -4.405399799346924, -4.447400093078613, -4.706099987030029, -4.839300155639648, -5.019700050354004, -5.085299968719482, -5.107900142669678, -5.128799915313721, -5.328999996185303, -5.410999774932861, -5.494800090789795, -5.409299850463867, -5.602099895477295, -5.609300136566162, -5.720699787139893, -5.770199775695801, -5.834499835968018, -5.858399868011475, -5.870699882507324, -5.885799884796143, -5.941999912261963, -5.960999965667725, -5.972599983215332, -6.00600004196167, -6.019400119781494, -6.026199817657471, -6.042900085449219, -6.0696001052856445, -3.9184999465942383, -4.5524001121521, -4.548799991607666, -3.9832000732421875, -4.693600177764893, -4.181300163269043, -2.4730000495910645, -5.588900089263916, -4.703199863433838, -3.8317999839782715, -5.033400058746338, -4.531099796295166, -4.610300064086914, -4.244200229644775, -4.702199935913086, -5.36460018157959, -5.418399810791016, -4.704100131988525, -4.80679988861084, -4.781700134277344, -4.845799922943115, -4.651700019836426, -5.070700168609619, -4.864799976348877, -4.71999979019165, -4.986299991607666, -3.5053999423980713, -3.7588999271392822, -4.1971001625061035, -4.293799877166748, -4.316800117492676, -4.183300018310547, -4.495800018310547, -4.578100204467773, -4.716700077056885, -4.846399784088135, -4.885499954223633, -4.920300006866455, -4.951099872589111, -4.972099781036377, -4.9918999671936035, -5.071899890899658, -5.1092000007629395, -5.147299766540527, -5.288400173187256, -5.39300012588501, -5.402900218963623, -5.4222002029418945, -5.545300006866455, -5.581299781799316, -5.638800144195557, -5.7555999755859375, -5.756899833679199, -5.76039981842041, -5.871399879455566, -5.876100063323975, -4.417099952697754, -4.214099884033203, -2.5044000148773193, -3.362600088119507, -4.801300048828125, -4.156799793243408, -5.281799793243408, -3.8232998847961426, -4.878699779510498, -4.240799903869629, -3.6840999126434326, -5.109300136566162, -4.061200141906738, -3.9941000938415527, -4.451000213623047, -5.014200210571289, -4.80019998550415, -4.887899875640869, -2.4809000492095947, -3.5987000465393066, -3.684999942779541, -2.4530999660491943, -3.7925000190734863, -4.274199962615967, -4.388899803161621, -4.576300144195557, -4.592400074005127, -4.844699859619141, -5.047699928283691, -5.1203999519348145, -5.316800117492676, -5.547999858856201, -5.606500148773193, -5.631199836730957, -5.721399784088135, -5.7418999671936035, -5.81820011138916, -5.85860013961792, -5.865600109100342, -5.873499870300293, -5.970900058746338, -6.00629997253418, -6.39900016784668, -6.54010009765625, -6.540200233459473, -6.607399940490723, -6.667399883270264, -6.703499794006348, -3.4165000915527344, -3.3905999660491943, -3.2525999546051025, -3.1029000282287598, -5.749599933624268, -2.578000068664551, -3.5899999141693115, -4.000800132751465, -3.722100019454956, -3.6424999237060547, -5.240699768066406, -4.700500011444092, -4.289299964904785, -4.885300159454346, -4.486999988555908, -4.996399879455566, -4.53410005569458, -4.863999843597412, -4.791600227355957, -4.90369987487793, -5.646599769592285, -5.723299980163574, -5.917099952697754, -5.944200038909912, -6.024700164794922, -6.075300216674805, -6.233799934387207, -6.312399864196777, -6.328700065612793, -6.364799976348877, -6.464099884033203, -6.503600120544434, -6.525700092315674, -6.730999946594238, -6.781499862670898, -6.808300018310547, -6.854100227355957, -6.444900035858154, -6.96589994430542, -6.967299938201904, -6.984600067138672, -7.001399993896484, -7.152599811553955, -6.679699897766113, -7.170199871063232, -7.183199882507324, -7.207600116729736, -7.236000061035156, -7.240799903869629, -7.254899978637695, -4.459400177001953, -5.16510009765625, -5.843299865722656, -5.519499778747559, -5.664999961853027, -4.784599781036377, -5.841899871826172, -5.556399822235107, -5.750899791717529, -4.41510009765625, -3.677000045776367, -4.343800067901611, -5.19350004196167, -6.218299865722656, -4.019899845123291, -4.5655999183654785, -3.8060998916625977, -4.501800060272217, -4.026000022888184, -5.50540018081665, -4.3831000328063965, -4.628799915313721, -4.333799839019775, -4.9604997634887695, -5.1645002365112305, -4.529200077056885, -4.932700157165527, -4.9994001388549805, -5.1921000480651855, -4.238699913024902, -4.392099857330322, -4.9375, -4.2469000816345215, -4.579699993133545, -5.115499973297119, -4.803400039672852, -4.8805999755859375, -5.056300163269043, -4.804200172424316, -4.599599838256836, -4.572999954223633, -4.576000213623047, -4.743199825286865, -5.0046000480651855, -4.960000038146973, -4.966599941253662, -4.740099906921387, -5.220099925994873, -5.252399921417236, -5.263700008392334, -5.385900020599365, -5.481200218200684, -5.496500015258789, -5.7052001953125, -5.7266998291015625, -5.8105998039245605, -5.815000057220459, -5.862199783325195, -5.676199913024902, -6.0106000900268555, -6.036399841308594, -6.095300197601318, -6.102700233459473, -6.138599872589111, -6.192200183868408, -6.202499866485596, -6.3308000564575195, -6.412099838256836, -6.421599864959717, -6.479800224304199, -6.479899883270264, -6.502500057220459, -6.5335001945495605, -6.56220006942749, -6.607500076293945, -6.618299961090088, -4.527699947357178, -4.452300071716309, -5.182199954986572, -6.0731000900268555, -5.211400032043457, -4.475800037384033, -6.016200065612793, -3.8434998989105225, -4.677499771118164, -5.737800121307373, -5.0335001945495605, -4.166900157928467, -5.089000225067139, -5.053599834442139, -4.7083001136779785, -4.788899898529053, -5.515399932861328, -5.155799865722656, -4.879000186920166, -4.995200157165527, -4.229100227355957, -4.273099899291992, -4.441199779510498, -4.552599906921387, -5.211999893188477, -4.696899890899658, -4.941299915313721, -4.815400123596191, -5.076200008392334, -5.0329999923706055, -5.048500061035156, -5.075200080871582, -5.056399822235107, -5.111999988555908, -4.325399875640869, -4.491700172424316, -4.634099960327148, -4.700200080871582, -4.72599983215332, -5.090199947357178, -5.101900100708008, -5.251999855041504, -5.281899929046631, -5.293300151824951, -5.520599842071533, -5.543399810791016, -5.597400188446045, -5.655099868774414, -3.2890000343322754, -5.7195000648498535, -5.848499774932861, -5.941800117492676, -5.973800182342529, -5.9816999435424805, -5.989299774169922, -6.009300231933594, -6.013899803161621, -6.016200065612793, -6.0366997718811035, -6.133699893951416, -6.139100074768066, -6.182000160217285, -6.221399784088135, -6.236299991607666, -4.827600002288818, -4.3109002113342285, -5.885900020599365, -5.012700080871582, -5.745800018310547, -4.923399925231934, -4.6859002113342285, -5.376399993896484, -4.607999801635742, -5.043399810791016, -4.828400135040283, -5.052000045776367, -5.317299842834473, -4.291999816894531, -4.771900177001953, -4.759300231933594, -4.995200157165527, -4.589399814605713, -4.287199974060059, -4.9527997970581055, -4.61460018157959, -4.666500091552734, -4.71619987487793, -4.574900150299072, -4.854400157928467, -5.022500038146973, -4.738500118255615, -4.7895002365112305, -4.790800094604492, -4.876399993896484, -4.908199787139893, -4.978300094604492, -4.992199897766113, -4.138800144195557, -4.191199779510498, -4.313000202178955, -4.779099941253662, -4.815999984741211, -4.82390022277832, -4.849299907684326, -4.911499977111816, -4.951000213623047, -5.099400043487549, -5.131700038909912, -5.158899784088135, -5.167399883270264, -5.1992998123168945, -5.202700138092041, -5.215099811553955, -5.253900051116943, -5.2581000328063965, -5.263199806213379, -3.278599977493286, -5.515399932861328, -5.528800010681152, -5.529799938201904, -5.597799777984619, -5.690000057220459, -5.740099906921387, -5.768499851226807, -5.777400016784668, -5.784800052642822, -5.938000202178955, -2.86899995803833, -3.865000009536743, -2.4911999702453613, -4.059599876403809, -4.163099765777588, -3.202500104904175, -4.536399841308594, -2.8015999794006348, -4.270500183105469, -4.307499885559082, -4.399400234222412, -4.6367998123168945, -4.898200035095215, -4.253699779510498, -5.073500156402588, -4.54640007019043, -4.581999778747559, -4.4552998542785645, -4.836400032043457, -5.00540018081665, -4.657700061798096, -4.952300071716309, -4.866700172424316, -4.915800094604492, -2.4616000652313232, -3.805299997329712, -3.8859000205993652, -4.01800012588501, -4.329400062561035, -4.396100044250488, -4.458799839019775, -5.172999858856201, -5.18149995803833, -5.3572998046875, -5.456500053405762, -5.4654998779296875, -5.597700119018555, -5.67609977722168, -5.748499870300293, -5.7779998779296875, -5.781700134277344, -5.92710018157959, -5.947500228881836, -5.963099956512451, -5.965099811553955, -6.02869987487793, -6.072199821472168, -6.1016998291015625, -6.160299777984619, -6.200699806213379, -6.225900173187256, -6.238100051879883, -6.263000011444092, -6.26669979095459, -2.289099931716919, -4.569200038909912, -5.059500217437744, -4.070099830627441, -4.23799991607666, -3.996299982070923, -4.611999988555908, -3.571899890899658, -4.145500183105469, -4.348599910736084, -4.468100070953369, -5.576900005340576, -5.676799774169922, -3.6991000175476074, -4.202600002288818, -3.9742000102996826, -5.214700222015381, -4.786300182342529, -4.588699817657471, -4.27269983291626, -4.328400135040283, -4.815999984741211, -4.87060022354126, -4.9766998291015625, -4.751100063323975, -4.8242998123168945, -4.9156999588012695, -4.973899841308594, -3.5032999515533447, -3.7209999561309814, -3.7955000400543213, -4.038700103759766, -4.054599761962891, -4.161099910736084, -4.178400039672852, -4.31790018081665, -4.3333001136779785, -4.384300231933594, -4.418600082397461, -4.512800216674805, -4.542500019073486, -4.547399997711182, -4.606500148773193, -4.703999996185303, -4.715000152587891, -4.736599922180176, -4.773200035095215, -4.828800201416016, -4.875400066375732, -5.067699909210205, -5.068999767303467, -5.071899890899658, -5.072000026702881, -5.119100093841553, -5.12060022354126, -5.186399936676025, -5.206699848175049, -5.276899814605713, -3.700000047683716, -2.7265000343322754, -3.9958999156951904, -4.276599884033203, -3.710599899291992, -4.725500106811523, -3.7994000911712646, -3.3238000869750977, -4.173500061035156, -4.279399871826172, -4.805699825286865, -5.577000141143799, -5.706900119781494, -5.71019983291626, -5.758600234985352, -5.802599906921387, -5.992700099945068, -5.996500015258789, -2.0813000202178955, -6.28380012512207, -6.373199939727783, -6.530600070953369, -5.901800155639648, -6.674499988555908, -6.6844000816345215, -6.704500198364258, -6.789899826049805, -6.820199966430664, -6.889100074768066, -6.572800159454346, -7.061100006103516, -7.062900066375732, -7.104300022125244, -7.182300090789795, -7.202499866485596, -4.99560022354126, -6.783899784088135, -7.228000164031982, -7.303199768066406, -7.356599807739258, -4.883600234985352, -5.27400016784668, -4.060500144958496, -4.220600128173828, -5.867300033569336, -4.287700176239014, -2.5371999740600586, -6.457099914550781, -5.957699775695801, -4.018400192260742, -4.5507001876831055, -4.39739990234375, -5.383500099182129, -4.079899787902832, -5.243599891662598, -5.196800231933594, -3.4839000701904297, -5.324699878692627, -4.548999786376953, -3.977299928665161, -5.101399898529053, -4.609300136566162, -5.338699817657471, -5.264900207519531, -5.210599899291992, -4.972499847412109, -5.122200012207031, -5.22130012512207, -5.147500038146973, -5.114099979400635, -5.173399925231934, -5.2027997970581055, -5.170000076293945, -5.259900093078613, -5.263299942016602, -2.8048999309539795, -2.9238998889923096, -2.9377999305725098, -3.0267999172210693, -3.1758999824523926, -3.3160998821258545, -3.6589999198913574, -3.775599956512451, -3.8185999393463135, -4.129499912261963, -4.48799991607666, -4.5289998054504395, -4.716400146484375, -4.857399940490723, -5.06220006942749, -5.081600189208984, -5.086400032043457, -5.107800006866455, -5.12060022354126, -5.178800106048584, -5.238399982452393, -5.2393999099731445, -5.249899864196777, -5.354599952697754, -5.357100009918213, -4.875899791717529, -5.446899890899658, -5.647200107574463, -5.649799823760986, -5.714300155639648, -3.000499963760376, -4.919000148773193, -4.216400146484375, -4.602799892425537, -3.2416000366210938, -3.9482998847961426, -3.9168999195098877, -3.1493000984191895, -3.6607000827789307, -4.129700183868408, -4.439499855041504, -4.529699802398682, -4.544400215148926, -4.866199970245361, -5.023900032043457, -3.5866000652313232, -4.11899995803833, -4.236199855804443, -4.334199905395508, -4.455999851226807, -4.599599838256836, -4.939599990844727, -5.026700019836426, -5.8927001953125, -6.428199768066406, -6.446400165557861, -6.451000213623047, -6.5605998039245605, -6.677999973297119, -6.727799892425537, -6.754499912261963, -6.876299858093262, -6.887400150299072, -6.95359992980957, -7.01170015335083, -7.067999839782715, -7.069300174713135, -7.069900035858154, -7.108399868011475, -7.1528000831604, -7.170199871063232, -7.222300052642822, -7.26170015335083, -7.2866997718811035, -7.315999984741211, -2.5534000396728516, -4.641600131988525, -5.9506001472473145, -5.133699893951416, -4.731200218200684, -4.758999824523926, -3.3850998878479004, -5.264599800109863, -4.170300006866455, -5.563600063323975, -6.107100009918213, -4.738999843597412, -3.8864998817443848, -5.627500057220459, -4.043700218200684, -4.326700210571289, -5.150300025939941, -5.031599998474121, -4.411799907684326, -4.935699939727783, -5.297500133514404, -4.100599765777588, -4.378399848937988, -5.018499851226807, -4.785900115966797, -4.974599838256836, -4.837699890136719, -4.786300182342529, -4.762400150299072, -4.892099857330322, -4.987400054931641, -4.981800079345703, -5.01140022277832, -4.169899940490723, -4.353799819946289, -4.764900207519531, -4.79580020904541, -5.019800186157227, -5.075300216674805, -4.645199775695801, -5.218800067901611, -5.220799922943115, -5.221399784088135, -5.2480998039245605, -4.929200172424316, -5.258600234985352, -5.264400005340576, -5.345699787139893, -5.34689998626709, -5.463500022888184, -5.535600185394287, -5.5971999168396, -5.6006999015808105, -5.609499931335449, -5.649799823760986, -5.6595001220703125, -5.671999931335449, -5.666399955749512, -5.678599834442139, -5.700200080871582, -5.7027997970581055, -5.707900047302246, -5.7266998291015625, -4.647200107574463, -4.457699775695801, -4.807400226593018, -5.3180999755859375, -5.2042999267578125, -4.844600200653076, -5.131100177764893, -5.347700119018555, -5.1097002029418945, -3.944700002670288, -5.201600074768066, -4.622600078582764, -4.3221001625061035, -4.639100074768066, -4.937099933624268, -4.731900215148926, -5.059299945831299, -4.671299934387207, -4.396599769592285, -4.825900077819824, -4.9303998947143555, -5.0472002029418945, -5.036200046539307, -5.124199867248535, -3.5845999717712402, -3.8338000774383545, -4.716599941253662, -5.396200180053711, -5.60230016708374, -5.668799877166748, -5.911799907684326, -5.8678998947143555, -5.925099849700928, -5.948800086975098, -6.04640007019043, -6.067999839782715, -6.108099937438965, -6.1803998947143555, -6.208799839019775, -6.305799961090088, -6.368800163269043, -6.405799865722656, -6.52400016784668, -6.5904998779296875, -6.597899913787842, -6.640699863433838, -6.662300109863281, -6.689499855041504, -6.7519001960754395, -6.761000156402588, -6.824900150299072, -6.906300067901611, -6.920100212097168, -6.945499897003174, -3.528700113296509, -4.894800186157227, -6.51230001449585, -5.959700107574463, -4.93310022354126, -4.57289981842041, -6.250800132751465, -4.270299911499023, -4.308499813079834, -5.144199848175049, -5.1579999923706055, -4.332600116729736, -4.701200008392334, -4.963200092315674, -2.577899932861328, -4.5858001708984375, -4.733500003814697, -4.173600196838379, -5.297599792480469, -3.9275999069213867, -5.055200099945068, -3.9809999465942383, -4.7378997802734375, -4.947500228881836, -4.833700180053711, -4.7342000007629395, -5.029699802398682, -4.799699783325195, -4.45359992980957, -4.8628997802734375, -4.584199905395508, -4.443399906158447, -4.602799892425537, -4.882599830627441, -4.586699962615967, -4.718800067901611, -4.8445000648498535, -3.4442999362945557, -3.6756999492645264, -4.145599842071533, -4.515399932861328, -4.638400077819824, -5.046800136566162, -5.211900234222412, -5.221700191497803, -5.313499927520752, -5.441400051116943, -5.53439998626709, -5.7733001708984375, -5.798999786376953, -5.834400177001953, -5.945099830627441, -5.987100124359131, -5.988800048828125, -6.0868000984191895, -6.160299777984619, -6.1666998863220215, -6.236199855804443, -6.381999969482422, -6.479700088500977, -6.536200046539307, -6.569200038909912, -6.581900119781494, -6.767099857330322, -6.849899768829346, -6.976099967956543, -7.011899948120117, -2.8945999145507812, -3.48799991607666, -1.9151999950408936, -4.792600154876709, -4.035799980163574, -5.298799991607666, -4.2596001625061035, -5.792099952697754, -4.753399848937988, -4.769899845123291, -4.330599784851074, -5.141900062561035, -4.077899932861328, -4.5106000900268555, -5.123499870300293, -5.093999862670898, -4.546299934387207, -5.0467000007629395, -4.899099826812744, -4.698299884796143, -4.757599830627441, -4.627600193023682, -4.946199893951416, -5.150899887084961, -5.1149001121521, -5.145100116729736, -2.521199941635132, -2.7639000415802, -3.772900104522705, -3.8724000453948975, -4.263700008392334, -4.270699977874756, -4.404699802398682, -4.490300178527832, -4.628300189971924, -4.673900127410889, -4.690299987792969, -4.717100143432617, -4.749499797821045, -4.9608001708984375, -5.000800132751465, -5.23199987411499, -5.3221001625061035, -5.431399822235107, -5.445899963378906, -5.481400012969971, -5.537300109863281, -5.5894999504089355, -5.603400230407715, -5.611499786376953, -5.651400089263916, -5.66949987411499, -5.703800201416016, -5.809199810028076, -5.938600063323975, -5.957399845123291, -4.757900238037109, -3.263000011444092, -2.794800043106079, -3.7913999557495117, -3.6512999534606934, -3.9038000106811523, -2.9310998916625977, -2.365999937057495, -4.450699806213379, -3.0295000076293945, -3.360300064086914, -4.252699851989746, -3.9727001190185547, -4.253900051116943, -4.843299865722656, -4.957900047302246, -1.9479000568389893, -2.6382999420166016, -2.8245999813079834, -2.9102001190185547, -3.2909998893737793, -3.419600009918213, -3.7755000591278076, -4.080900192260742, -4.151599884033203, -4.184299945831299, -4.279200077056885, -4.3694000244140625, -4.548699855804443, -4.623300075531006, -4.729000091552734, -4.737400054931641, -4.747099876403809, -4.794099807739258, -4.8379998207092285, -4.879199981689453, -4.902500152587891, -4.929999828338623, -4.969099998474121, -5.059599876403809, -5.0802998542785645, -5.104499816894531, -5.137899875640869, -5.254000186920166, -5.283699989318848, -5.334400177001953, -2.5369999408721924, -4.621099948883057, -4.592700004577637, -4.284599781036377, -4.112199783325195, -4.8343000411987305, -3.7911999225616455, -4.638700008392334, -4.7515997886657715, -4.868800163269043, -4.7982001304626465, -4.827600002288818], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 4.4629, 4.4624, 4.4623, 4.4618, 4.4617, 4.4616, 4.4614, 4.4612, 4.4611, 4.4602, 4.4594, 4.4593, 4.4585, 4.4583, 4.4566, 4.4566, 4.4563, 4.4559, 4.4558, 4.4553, 4.4548, 4.4537, 4.4523, 4.452, 4.4518, 4.4504, 4.4489, 4.4488, 4.4473, 4.4462, 4.1334, 4.1811, 4.2809, 3.7586, 2.7367, 1.5826, 2.4861, 3.145, 2.6355, 2.5771, 4.1499, 4.1496, 4.1494, 4.1493, 4.1493, 4.1491, 4.1483, 4.148, 4.1458, 4.1437, 4.1397, 4.1373, 4.1371, 4.1371, 4.137, 4.1359, 4.1357, 4.1356, 4.1355, 4.1352, 4.1327, 4.1323, 4.1305, 4.1278, 4.1274, 4.1264, 4.1262, 4.126, 4.1251, 4.124, 4.0991, 4.0014, 3.7592, 3.9935, 3.9453, 3.3814, 3.8103, 2.3747, 2.4509, 2.7534, 2.9848, 2.3312, 1.1266, 2.319, 1.6554, 1.0704, 1.7123, 2.8999, 1.4603, 0.9801, 2.2534, 1.1908, 1.8526, 0.3624, 1.4531, 1.1286, 0.4758, 3.8263, 3.8261, 3.8258, 3.8251, 3.8247, 3.8247, 3.8234, 3.8231, 3.8229, 3.8229, 3.8222, 3.821, 3.8208, 3.8208, 3.8197, 3.8185, 3.8174, 3.8167, 3.8155, 3.8143, 3.8132, 3.8124, 3.8093, 3.8082, 3.8054, 3.8044, 3.8038, 3.8035, 3.8015, 3.8007, 3.7406, 3.7201, 3.4782, 3.6666, 3.4855, 3.2199, 3.4388, 3.5465, 2.9711, 3.245, 2.6055, 2.4932, 2.4505, 2.7957, 2.8627, 2.7082, 2.78, 2.8948, 2.6825, 1.4773, 2.0403, 1.1854, 1.3087, 1.0259, 1.4806, 2.0773, 2.2585, 3.4708, 3.47, 3.4699, 3.4694, 3.4691, 3.4676, 3.4669, 3.4669, 3.4668, 3.4659, 3.4658, 3.4657, 3.4656, 3.4653, 3.4653, 3.4635, 3.4635, 3.4629, 3.4627, 3.4625, 3.4623, 3.4623, 3.4618, 3.4613, 3.4606, 3.4602, 3.4601, 3.46, 3.46, 3.46, 3.4519, 3.4485, 3.4478, 3.3666, 3.3634, 3.3949, 3.2554, 3.345, 3.3008, 3.3942, 3.3417, 3.032, 2.9193, 2.943, 3.1534, 2.836, 2.9325, 2.8391, 3.0841, 2.2286, 2.7826, 2.2843, 2.9033, 2.5938, 2.5539, 2.0993, 1.9246, 2.5626, 1.4798, 0.1632, 0.5388, 0.7719, 0.7941, -0.0776, 3.5101, 3.5094, 3.509, 3.509, 3.5088, 3.5088, 3.5081, 3.5078, 3.5072, 3.5065, 3.5063, 3.5063, 3.5056, 3.5051, 3.5051, 3.505, 3.504, 3.5034, 3.5033, 3.5029, 3.5023, 3.5022, 3.5018, 3.5017, 3.5015, 3.5014, 3.5011, 3.5011, 3.5008, 3.5007, 3.4608, 3.4306, 3.3443, 3.3625, 3.339, 3.4271, 3.2002, 3.4254, 3.3595, 3.2474, 2.9762, 2.8508, 2.5678, 2.2281, 2.513, 2.7141, 2.5423, 2.8985, 2.564, 2.1324, 2.3248, 2.7165, 1.2781, 1.8885, 0.6579, -0.3144, 0.0666, 0.1, 4.9998, 4.9988, 4.9988, 4.9987, 4.9979, 4.9976, 4.9975, 4.9968, 4.9954, 4.9946, 4.9944, 4.9931, 4.9916, 4.9908, 4.9905, 4.9901, 4.9888, 4.9887, 4.9877, 4.9872, 4.9869, 4.9866, 4.9862, 4.9857, 4.9855, 4.9855, 4.9836, 4.9832, 4.9824, 4.9823, 4.9569, 4.7194, 4.7975, 4.8296, 4.2546, 4.605, 4.5091, 4.5002, 3.4432, 4.056, 3.4371, 3.8032, 3.9148, 3.3795, 4.5839, 3.6084, 1.7266, 1.7261, 1.726, 1.7258, 1.7254, 1.7251, 1.7248, 1.7247, 1.7247, 1.7245, 1.7244, 1.7229, 1.7229, 1.7227, 1.7225, 1.7221, 1.722, 1.722, 1.7218, 1.7216, 1.7215, 1.7214, 1.7212, 1.7209, 1.7206, 1.7206, 1.7203, 1.72, 1.7197, 1.7195, 1.7131, 1.7135, 1.7083, 1.6937, 1.7015, 1.7032, 1.6964, 1.6747, 1.6506, 1.6737, 1.6735, 1.6817, 1.6825, 1.5315, 1.6067, 1.6614, 1.4915, 1.6521, 1.5452, 1.5187, 1.5659, 1.3706, 1.4466, 1.4171, 1.5467, 1.5104, 1.1724, 1.3579, 1.4521, 1.2727, 1.3924, 1.348, 1.0491, 1.3945, 1.4301, 1.386, 1.031, 1.2045, 1.0469, 1.0853, 1.1948, 1.2042, 0.7811, 0.9376, 0.674, 1.1217, 0.9027, 1.0794, 0.5942, 0.8013, 0.4767, 0.0903, 0.9179, 3.4128, 3.4123, 3.4117, 3.411, 3.4105, 3.4104, 3.4098, 3.4093, 3.4091, 3.4091, 3.4089, 3.4086, 3.4081, 3.4067, 3.4065, 3.4061, 3.4055, 3.4052, 3.4046, 3.4042, 3.4038, 3.4034, 3.4032, 3.4031, 3.4031, 3.4023, 3.4021, 3.4017, 3.4016, 3.4012, 3.4012, 3.3369, 3.3932, 3.3019, 3.1202, 3.2998, 3.1239, 3.2717, 3.0936, 3.168, 2.7408, 3.336, 3.2064, 2.6398, 3.1016, 2.6711, 3.1166, 2.7913, 1.7051, 2.051, 2.686, 1.168, 1.5524, 1.3636, 1.021, 1.4686, 0.0358, 1.1064, 0.3225, -0.0945, 1.0529, -0.2564, 4.225, 4.2244, 4.223, 4.2219, 4.2212, 4.2212, 4.221, 4.2207, 4.2206, 4.219, 4.2189, 4.2183, 4.2176, 4.215, 4.2148, 4.2142, 4.214, 4.2128, 4.2096, 4.2077, 4.2065, 4.2048, 4.204, 4.1974, 4.1938, 4.1906, 4.1895, 4.1857, 4.1806, 4.1792, 4.1722, 4.1183, 4.0649, 3.855, 4.0662, 3.9029, 3.8319, 3.7575, 3.9097, 4.0865, 3.1379, 3.6001, 3.3505, 3.6057, 3.5987, 2.735, 3.6695, 2.6479, 2.7552, 2.0407, 1.4617, 3.0459, 2.153, 2.5844, 1.36, 1.56, 1.7423, 3.3895, 3.3892, 3.3886, 3.3886, 3.3882, 3.3878, 3.3877, 3.3872, 3.3871, 3.3867, 3.3865, 3.3865, 3.3864, 3.3862, 3.3862, 3.386, 3.386, 3.3859, 3.3846, 3.3845, 3.3844, 3.3844, 3.384, 3.3834, 3.3834, 3.3829, 3.3829, 3.3828, 3.382, 3.3817, 3.3374, 3.3576, 3.2235, 3.1548, 3.2806, 2.9461, 3.0129, 2.6879, 2.851, 2.9049, 2.935, 3.2797, 2.6587, 2.0841, 2.8458, 2.1702, 2.5214, 2.5133, 1.7716, 2.1428, 2.5065, 1.4378, 1.539, 1.8117, 0.2537, -0.0976, 1.445, 2.0574, 0.4435, 0.1821, 3.8513, 3.8508, 3.8507, 3.8497, 3.8496, 3.8496, 3.8493, 3.8488, 3.8468, 3.846, 3.846, 3.8446, 3.8442, 3.842, 3.8419, 3.8412, 3.8378, 3.8378, 3.8374, 3.8369, 3.8368, 3.8363, 3.8355, 3.835, 3.8321, 3.8302, 3.8286, 3.8285, 3.8269, 3.8253, 3.8167, 3.7706, 3.7939, 3.723, 3.7105, 3.608, 3.4621, 3.5361, 3.0256, 2.7982, 3.6358, 3.2776, 3.5403, 3.5116, 2.5568, 2.9311, 2.0451, 2.1114, 2.1246, 2.4649, 2.3149, 2.0931, 1.4824, 2.5475, 2.5484, 1.354, 2.7945, 1.902, 0.43, 4.7658, 4.7648, 4.7645, 4.7632, 4.7631, 4.763, 4.7629, 4.7624, 4.7617, 4.7617, 4.7614, 4.7613, 4.7607, 4.7601, 4.7597, 4.7596, 4.7596, 4.7593, 4.7591, 4.7581, 4.7574, 4.7556, 4.7552, 4.7551, 4.7536, 4.7534, 4.753, 4.7511, 4.7511, 4.7501, 4.7198, 4.7247, 4.6759, 4.6792, 4.7222, 4.6678, 4.5699, 4.6452, 4.4, 4.3929, 3.5389, 2.9405, 4.2981, 4.9202, 4.92, 4.9193, 4.9187, 4.918, 4.9169, 4.915, 4.9143, 4.913, 4.9128, 4.9127, 4.9125, 4.9123, 4.9117, 4.9107, 4.9103, 4.9091, 4.9088, 4.907, 4.9062, 4.9056, 4.9049, 4.904, 4.9008, 4.9006, 4.9005, 4.8997, 4.899, 4.8943, 4.8942, 4.8843, 4.7916, 4.8272, 4.7035, 4.631, 4.8012, 4.5384, 4.3704, 4.1902, 4.4195, 4.7177, 3.9163, 2.7764, 1.9576, 2.4921, 2.3484, 1.3675, 4.1539, 4.1509, 4.1501, 4.1491, 4.1484, 4.1483, 4.1479, 4.1476, 4.1476, 4.1458, 4.1453, 4.1425, 4.1415, 4.1405, 4.1389, 4.1379, 4.1374, 4.1374, 4.1369, 4.1362, 4.1356, 4.135, 4.1348, 4.1323, 4.1313, 4.1298, 4.128, 4.1261, 4.125, 4.1242, 4.0068, 3.9104, 3.7713, 3.8007, 3.6314, 3.8709, 3.3373, 3.4085, 3.3765, 2.979, 3.3549, 2.9626, 2.1379, 2.5841, 1.9893, 1.6941, 2.5293, 1.8489, 2.5158, 1.1487, 2.0226, 0.9326, 1.2311, 0.203, 0.6029, 3.5498, 3.5484, 3.5483, 3.5477, 3.5476, 3.5456, 3.5444, 3.5442, 3.544, 3.544, 3.544, 3.5439, 3.5439, 3.5439, 3.5437, 3.5436, 3.5434, 3.5428, 3.5428, 3.5422, 3.5421, 3.542, 3.5418, 3.5417, 3.5417, 3.541, 3.541, 3.5408, 3.5405, 3.5404, 3.4048, 3.437, 3.4676, 3.4757, 3.1576, 3.2563, 3.4168, 3.4037, 3.3737, 3.0214, 3.0655, 2.7541, 2.7645, 2.5356, 3.3099, 2.8889, 2.894, 2.2144, 2.5227, 2.2246, 2.8317, 2.1544, 2.6092, 3.2004, 1.8408, 2.7115, 2.4963, 2.0938, 0.1198, 0.7817, 0.502, 0.8039, 0.1505, 0.0742, -0.2107, 1.7209, 4.3753, 4.3735, 4.3732, 4.3726, 4.3723, 4.3723, 4.3722, 4.3713, 4.371, 4.3701, 4.3698, 4.3698, 4.3694, 4.3693, 4.369, 4.3688, 4.3674, 4.3673, 4.3671, 4.3662, 4.3658, 4.3658, 4.3657, 4.3651, 4.3647, 4.3647, 4.3644, 4.3632, 4.3631, 4.3627, 4.353, 4.31, 4.0356, 3.8237, 4.2475, 4.2717, 4.1511, 3.6107, 3.3978, 4.2529, 3.4296, 2.4946, 2.1519, 3.2146, 2.3796, 1.7488, 2.3101, 2.1989, 1.5167, 2.0853, 1.8196, 4.5124, 4.5116, 4.5107, 4.5107, 4.5103, 4.5095, 4.5086, 4.5081, 4.5074, 4.5072, 4.5068, 4.5063, 4.5059, 4.5051, 4.5044, 4.5035, 4.5029, 4.5024, 4.502, 4.5003, 4.4992, 4.4989, 4.4984, 4.4967, 4.4961, 4.4958, 4.4931, 4.493, 4.49, 4.4799, 4.4775, 4.3834, 4.4285, 4.2121, 4.0258, 4.1219, 4.0605, 3.9127, 3.9555, 2.5709, 1.5879, 3.0538, 2.3564, 1.6798, 0.4401, 1.2983, 0.4385, 4.6005, 4.5997, 4.5996, 4.5993, 4.5986, 4.5985, 4.5977, 4.5968, 4.5961, 4.5961, 4.5961, 4.5958, 4.5956, 4.5956, 4.595, 4.5933, 4.5929, 4.5927, 4.5923, 4.5917, 4.5903, 4.5902, 4.5901, 4.5889, 4.5881, 4.588, 4.588, 4.585, 4.5847, 4.5846, 4.5737, 4.2358, 4.3124, 4.303, 4.0002, 3.9556, 3.795, 3.6474, 2.9166, 2.9201, 3.9844, 1.4224, 2.9188, 3.1704, 1.0279, 2.2215, 3.9789, 3.9789, 3.9788, 3.9786, 3.9781, 3.9778, 3.9775, 3.9771, 3.9752, 3.9746, 3.9736, 3.9733, 3.9731, 3.9729, 3.9727, 3.9725, 3.9723, 3.9711, 3.9708, 3.9706, 3.9694, 3.9691, 3.9688, 3.9686, 3.9683, 3.9682, 3.9679, 3.9669, 3.9662, 3.9648, 3.9224, 3.9397, 3.8734, 3.9123, 3.7432, 3.6624, 3.8582, 3.7953, 3.8669, 3.3625, 3.5211, 3.4679, 3.7568, 3.5934, 3.3284, 3.3525, 3.3801, 2.8288, 3.1932, 3.5322, 2.8876, 3.6052, 3.1881, 2.2594, 3.1809, 2.1477, 2.5867, 1.5116, 2.2579, 2.4452, 0.3464, 4.7303, 4.7291, 4.7277, 4.7271, 4.7267, 4.7265, 4.7261, 4.7261, 4.7257, 4.7253, 4.725, 4.7244, 4.7238, 4.7236, 4.7235, 4.7216, 4.7211, 4.721, 4.7208, 4.7206, 4.7206, 4.7203, 4.7202, 4.7202, 4.7201, 4.7198, 4.7196, 4.7195, 4.7191, 4.7183, 4.7139, 4.6956, 4.6446, 4.5492, 4.3984, 4.4932, 4.4913, 4.4946, 4.4544, 4.3493, 4.2669, 4.5365, 3.9163, 3.3075, 4.1195, 1.5575, 2.7182, 1.7654, 4.1409, 4.1407, 4.1406, 4.1403, 4.14, 4.139, 4.1389, 4.1385, 4.1383, 4.138, 4.1376, 4.1366, 4.1359, 4.135, 4.1349, 4.1346, 4.1346, 4.1342, 4.1317, 4.1313, 4.131, 4.1308, 4.1306, 4.1298, 4.1297, 4.1296, 4.129, 4.129, 4.1286, 4.1278, 4.1034, 4.0096, 4.1086, 4.0327, 4.0937, 4.019, 4.0076, 3.6448, 3.8288, 2.8266, 3.5669, 3.3497, 2.6842, 2.7229, 1.6564, 1.7234, 1.5682, 1.3167, 0.4772, 0.5909, 0.6763, 0.5991, 0.6562, 4.1863, 4.1862, 4.1861, 4.1858, 4.1846, 4.1837, 4.1836, 4.1832, 4.1801, 4.1779, 4.1775, 4.177, 4.1769, 4.1763, 4.1729, 4.1715, 4.1709, 4.1701, 4.1694, 4.1609, 4.1591, 4.1583, 4.158, 4.157, 4.1532, 4.1509, 4.1479, 4.1446, 4.1429, 4.1395, 3.9793, 3.94, 3.8844, 3.3868, 3.9541, 2.9422, 3.2489, 3.3583, 3.4839, 3.1129, 3.2302, 1.9428, 1.4373, 1.4484, 2.47, 1.8165, 2.5225, 1.3988, 1.5558, 1.9114, 1.7134, 1.9015, 1.8814, 1.585, 0.3047, 0.3376, 4.8932, 4.893, 4.8925, 4.8924, 4.892, 4.8919, 4.8917, 4.8915, 4.8914, 4.8914, 4.8904, 4.8901, 4.89, 4.8898, 4.8888, 4.8878, 4.8865, 4.8856, 4.8851, 4.8848, 4.8839, 4.8828, 4.8827, 4.8796, 4.8796, 4.8777, 4.8715, 4.8703, 4.8651, 4.8157, 4.7038, 4.7522, 4.5802, 4.5922, 4.3377, 4.4586, 4.5173, 3.8649, 4.642, 3.1916, 3.1152, 4.0646, 1.9899, 1.7035, 3.9697, 3.9696, 3.9694, 3.9693, 3.9686, 3.9682, 3.9676, 3.9673, 3.9672, 3.9671, 3.9661, 3.9656, 3.9649, 3.9645, 3.9644, 3.9643, 3.9635, 3.9631, 3.9625, 3.9623, 3.9622, 3.9621, 3.9615, 3.9613, 3.9612, 3.9608, 3.9607, 3.9606, 3.9604, 3.9601, 3.9321, 3.9378, 3.9302, 3.8191, 3.814, 3.6369, 3.1689, 3.8579, 3.5731, 3.2503, 3.6476, 3.2101, 3.2245, 2.8257, 3.1839, 3.6724, 3.7169, 2.4876, 2.2861, 2.2014, 2.3392, 1.3647, 2.8, 1.6679, 0.4534, 0.7761, 4.3229, 4.3225, 4.3216, 4.3214, 4.3213, 4.321, 4.3207, 4.3205, 4.3199, 4.3193, 4.3191, 4.319, 4.3188, 4.3187, 4.3186, 4.3181, 4.3179, 4.3176, 4.3167, 4.3158, 4.3158, 4.3156, 4.3145, 4.3141, 4.3136, 4.3123, 4.3122, 4.3122, 4.3108, 4.3107, 4.2649, 4.2119, 4.0154, 3.9814, 4.1779, 3.9282, 4.1909, 3.5054, 3.9969, 3.5004, 3.0799, 4.095, 2.8284, 2.3337, 2.1137, 3.6456, 1.8623, 0.6646, 4.4293, 4.4283, 4.4282, 4.4281, 4.428, 4.4268, 4.4265, 4.4258, 4.4257, 4.4245, 4.4233, 4.4228, 4.4213, 4.4191, 4.4185, 4.4182, 4.4171, 4.4168, 4.4158, 4.4152, 4.4151, 4.415, 4.4135, 4.4129, 4.4049, 4.4011, 4.4011, 4.3992, 4.3973, 4.3961, 4.3117, 4.0929, 3.904, 3.7349, 4.2183, 3.3668, 3.5846, 3.6796, 3.4136, 3.3008, 3.9527, 3.4718, 2.3553, 2.7475, 1.5529, 2.3935, 0.7162, 1.8674, 1.5331, 1.8045, 2.3051, 2.3049, 2.3047, 2.3046, 2.3044, 2.3043, 2.304, 2.3037, 2.3037, 2.3036, 2.3033, 2.3032, 2.303, 2.3023, 2.3021, 2.302, 2.3018, 2.3016, 2.3012, 2.3012, 2.3011, 2.301, 2.3001, 2.3001, 2.3, 2.2999, 2.2998, 2.2996, 2.2995, 2.2994, 2.299, 2.2949, 2.2946, 2.29, 2.2844, 2.2173, 2.2512, 2.2341, 2.2325, 2.1285, 2.0637, 2.098, 2.153, 2.2327, 1.847, 1.9009, 1.7213, 1.8717, 1.6809, 2.0528, 1.7043, 1.7717, 1.6367, 1.811, 1.8554, 1.5813, 1.7023, 1.7398, 1.8318, 1.1656, 1.2571, 1.6168, 0.9008, 1.1311, 1.6203, 1.2314, 1.2917, 1.4329, 0.9119, 0.4449, 0.3218, 0.2847, 0.3724, 1.1303, 0.6572, 0.4838, 3.1885, 3.1876, 3.1876, 3.1875, 3.1872, 3.187, 3.1869, 3.1862, 3.1862, 3.1858, 3.1858, 3.1856, 3.1855, 3.1849, 3.1848, 3.1845, 3.1845, 3.1843, 3.184, 3.1839, 3.1831, 3.1825, 3.1825, 3.182, 3.182, 3.1818, 3.1816, 3.1813, 3.1809, 3.1808, 3.1666, 3.1652, 3.1689, 3.1773, 3.1659, 3.1417, 3.1708, 2.9527, 2.9789, 3.0981, 2.9497, 2.6702, 2.8762, 2.8171, 2.3646, 2.4025, 2.8017, 2.4947, 2.0971, 2.1996, 1.0733, 0.9772, 1.1113, 1.2417, 2.1834, 0.943, 1.0351, 0.589, 1.1069, 0.609, 0.3004, 0.4636, 0.1171, 0.8604, 3.5916, 3.5913, 3.5911, 3.591, 3.5909, 3.59, 3.59, 3.5895, 3.5894, 3.5894, 3.5884, 3.5883, 3.5881, 3.5878, 3.5877, 3.5874, 3.5867, 3.5861, 3.5858, 3.5858, 3.5857, 3.5856, 3.5855, 3.5855, 3.5854, 3.5846, 3.5846, 3.5842, 3.5838, 3.5837, 3.5809, 3.565, 3.5662, 3.4793, 3.5418, 3.4466, 3.4115, 3.4515, 3.2497, 3.2717, 3.1876, 3.257, 3.3383, 2.8009, 2.9836, 2.745, 2.7317, 2.2211, 1.7122, 2.6216, 1.9903, 2.0727, 1.8501, 1.4599, 2.1256, 2.4706, 0.9035, 1.181, -0.0652, 0.2712, 0.4962, 1.7368, 1.4743, 4.4331, 4.433, 4.4322, 4.4308, 4.4306, 4.4305, 4.4304, 4.4301, 4.4298, 4.4289, 4.4287, 4.4285, 4.4284, 4.4282, 4.4281, 4.428, 4.4277, 4.4277, 4.4277, 4.4264, 4.4253, 4.4252, 4.4252, 4.4244, 4.4234, 4.4227, 4.4224, 4.4222, 4.4221, 4.4199, 4.4138, 4.4042, 4.2201, 4.2689, 4.2728, 4.1471, 4.1675, 3.5231, 3.9715, 3.8705, 3.9079, 3.9561, 4.0999, 2.7498, 4.0264, 2.3432, 1.9826, 0.7181, 2.7086, 3.5749, 0.6912, 2.9924, 0.6857, 0.3345, 4.168, 4.1669, 4.1668, 4.1666, 4.1659, 4.1658, 4.1656, 4.1627, 4.1627, 4.1616, 4.1608, 4.1608, 4.1597, 4.159, 4.1583, 4.158, 4.158, 4.1563, 4.1561, 4.1559, 4.1559, 4.1551, 4.1545, 4.1541, 4.1532, 4.1526, 4.1522, 4.152, 4.1516, 4.1515, 4.149, 4.1247, 4.0383, 3.8841, 3.9057, 3.8093, 3.6961, 3.2386, 3.3922, 3.4541, 3.3875, 3.9351, 3.958, 1.9429, 2.3618, 2.0904, 3.4752, 2.9123, 2.1903, 1.3764, 1.2887, 1.9933, 1.8761, 2.2823, 0.6312, 1.1462, 0.232, 1.0425, 4.8128, 4.8123, 4.8121, 4.8113, 4.8113, 4.8109, 4.8108, 4.8102, 4.8102, 4.8099, 4.8097, 4.8092, 4.8091, 4.809, 4.8087, 4.8081, 4.8079, 4.8078, 4.8076, 4.8072, 4.8068, 4.8051, 4.8051, 4.805, 4.805, 4.8046, 4.8046, 4.8039, 4.8036, 4.8028, 4.7285, 4.6359, 4.641, 4.3418, 3.6651, 4.4277, 3.2516, 2.3254, 2.8299, 1.4315, 2.9923, 2.9909, 2.9905, 2.9905, 2.9904, 2.9902, 2.9896, 2.9893, 2.9885, 2.9882, 2.9877, 2.9867, 2.9858, 2.9857, 2.9856, 2.9854, 2.9847, 2.9844, 2.9838, 2.9836, 2.982, 2.9819, 2.9814, 2.9805, 2.9802, 2.98, 2.98, 2.9799, 2.9788, 2.978, 2.9753, 2.9605, 2.9418, 2.8286, 2.8894, 2.769, 2.6105, 2.9283, 2.8469, 2.5044, 2.526, 2.3298, 2.5967, 2.2049, 2.55, 2.5312, 1.9665, 2.5076, 1.9905, 1.5615, 2.0724, 1.3631, 2.2151, 2.0361, 1.9465, 0.5549, 0.8598, 1.0555, 0.6467, 0.2902, 0.3791, 0.611, 0.0804, -0.2154, 1.0398, 5.0768, 5.0766, 5.0766, 5.0765, 5.0762, 5.0759, 5.075, 5.0746, 5.0745, 5.0732, 5.071, 5.0707, 5.0692, 5.0679, 5.0655, 5.0653, 5.0652, 5.0648, 5.0648, 5.064, 5.0631, 5.0631, 5.063, 5.0613, 5.0613, 5.0601, 5.0597, 5.0557, 5.0556, 5.0541, 5.0318, 5.0391, 4.9159, 4.9391, 4.6771, 4.7148, 4.678, 4.4439, 4.5374, 4.3918, 4.5512, 4.5893, 4.5595, 4.6182, 4.9357, 2.8581, 2.8579, 2.8578, 2.8578, 2.8577, 2.8576, 2.8572, 2.8571, 2.8553, 2.853, 2.8529, 2.8529, 2.8523, 2.8515, 2.8512, 2.851, 2.85, 2.8499, 2.8493, 2.8488, 2.8482, 2.8482, 2.8482, 2.8478, 2.8473, 2.8471, 2.8465, 2.8461, 2.8457, 2.8454, 2.8288, 2.8414, 2.8401, 2.8294, 2.8229, 2.7748, 2.6143, 2.6278, 2.3941, 2.6162, 2.667, 2.2882, 2.0583, 2.5133, 2.0209, 2.0011, 2.28, 2.1751, 1.873, 2.0825, 2.2403, 1.5165, 1.6035, 1.9248, 1.5565, 1.7721, 1.3302, 1.1649, 0.642, 0.757, 1.5521, 0.6602, 1.5213, 4.3452, 4.3447, 4.3432, 4.343, 4.3418, 4.3415, 4.3412, 4.3406, 4.3405, 4.3405, 4.3404, 4.3403, 4.3403, 4.3402, 4.3396, 4.3396, 4.3386, 4.3379, 4.3373, 4.3373, 4.3372, 4.3367, 4.3366, 4.3365, 4.3365, 4.3364, 4.3362, 4.3362, 4.3361, 4.3359, 4.3247, 4.2788, 4.2729, 4.3032, 4.2643, 4.204, 4.2242, 4.2169, 4.1007, 3.5467, 4.0885, 3.6239, 3.1841, 3.2578, 3.5916, 3.0863, 3.6918, 2.7696, 1.6198, 2.6116, 2.7767, 3.2291, 2.5967, 1.025, 3.8696, 3.8694, 3.8678, 3.8652, 3.864, 3.8636, 3.8616, 3.8616, 3.8615, 3.8607, 3.8604, 3.8602, 3.8597, 3.8589, 3.8586, 3.8574, 3.8566, 3.856, 3.8542, 3.8531, 3.853, 3.8522, 3.8518, 3.8513, 3.8501, 3.8499, 3.8486, 3.8467, 3.8464, 3.8458, 3.8455, 3.8334, 3.8457, 3.8284, 3.751, 3.6712, 3.8063, 3.5766, 3.4701, 3.566, 3.5541, 3.2816, 3.3163, 3.3646, 2.4808, 3.2095, 3.2144, 2.9615, 3.3635, 2.5858, 3.227, 2.3552, 2.7664, 2.9387, 2.8231, 2.6095, 2.6507, 2.2999, 1.6812, 2.3109, 1.478, 1.0954, 1.342, 2.2298, 0.1389, 0.8087, 1.5772, 3.4297, 3.4295, 3.4292, 3.4287, 3.4286, 3.4277, 3.4273, 3.4273, 3.427, 3.4266, 3.4262, 3.4252, 3.425, 3.4249, 3.4242, 3.424, 3.424, 3.4233, 3.4228, 3.4228, 3.4223, 3.421, 3.4201, 3.4195, 3.4192, 3.419, 3.4168, 3.4156, 3.4137, 3.4131, 3.3879, 3.3015, 3.1435, 3.2478, 3.1782, 3.2177, 3.0458, 3.2651, 3.0133, 2.928, 2.769, 3.042, 2.491, 2.2683, 2.7388, 2.7013, 1.9864, 2.1578, 1.8091, 1.2529, 1.2243, 0.6228, 1.7163, 1.5763, 0.234, 0.9195, 4.7187, 4.7185, 4.7169, 4.7167, 4.7154, 4.7153, 4.7148, 4.7144, 4.7136, 4.7134, 4.7133, 4.7131, 4.7129, 4.7114, 4.711, 4.7089, 4.7079, 4.7066, 4.7064, 4.706, 4.7052, 4.7044, 4.7042, 4.7041, 4.7035, 4.7032, 4.7026, 4.7008, 4.6983, 4.6979, 4.6858, 4.5762, 4.4062, 4.4519, 4.4224, 4.3538, 4.1418, 3.9564, 4.272, 3.6856, 3.0739, 3.4889, 3.123, 3.3475, 3.942, 1.2101, 5.0049, 5.0043, 5.0041, 5.004, 5.0034, 5.0031, 5.0021, 5.001, 5.0006, 5.0005, 5.0, 4.9995, 4.9983, 4.9978, 4.997, 4.9969, 4.9968, 4.9964, 4.996, 4.9956, 4.9954, 4.9951, 4.9947, 4.9937, 4.9934, 4.9931, 4.9927, 4.9912, 4.9908, 4.99, 4.8966, 4.8804, 4.7891, 4.6183, 3.8485, 4.5747, 2.705, 4.1624, 4.3437, 4.4717, 2.3583, 2.5348]}, \"token.table\": {\"Topic\": [32, 32, 1, 2, 3, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 31, 33, 35, 36, 37, 38, 39, 31, 31, 7, 28, 37, 7, 28, 37, 15, 3, 15, 3, 17, 18, 21, 24, 26, 31, 35, 38, 30, 10, 11, 19, 28, 33, 29, 35, 29, 7, 6, 6, 8, 18, 21, 28, 35, 37, 5, 6, 37, 5, 1, 38, 38, 2, 4, 29, 16, 31, 7, 15, 22, 23, 25, 28, 35, 38, 16, 37, 15, 16, 29, 7, 16, 19, 20, 21, 25, 28, 37, 38, 39, 23, 35, 36, 9, 36, 15, 22, 19, 24, 1, 4, 10, 4, 10, 14, 27, 23, 33, 31, 1, 5, 7, 13, 14, 15, 21, 27, 28, 29, 31, 33, 35, 36, 38, 18, 35, 34, 9, 29, 31, 1, 2, 3, 4, 5, 7, 8, 9, 10, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 7, 19, 28, 33, 2, 2, 16, 26, 25, 31, 20, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 17, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 33, 35, 37, 38, 19, 24, 35, 37, 25, 8, 17, 28, 11, 4, 15, 27, 13, 7, 1, 2, 7, 10, 14, 16, 21, 27, 29, 38, 2, 3, 4, 5, 7, 9, 10, 12, 13, 15, 17, 18, 21, 22, 24, 26, 28, 30, 33, 3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 17, 18, 19, 22, 24, 26, 28, 30, 31, 33, 35, 37, 38, 11, 2, 10, 19, 27, 29, 33, 9, 27, 29, 33, 33, 3, 26, 3, 8, 15, 16, 17, 18, 19, 20, 26, 28, 36, 38, 12, 4, 20, 17, 31, 24, 36, 37, 8, 13, 28, 13, 8, 11, 12, 15, 8, 6, 7, 12, 38, 30, 4, 5, 10, 14, 18, 19, 24, 28, 29, 31, 32, 36, 37, 14, 19, 24, 28, 31, 36, 37, 5, 5, 10, 19, 29, 29, 31, 35, 29, 23, 1, 3, 5, 7, 8, 9, 15, 19, 21, 24, 25, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 1, 27, 3, 12, 15, 36, 21, 13, 17, 40, 24, 4, 11, 16, 22, 36, 37, 5, 3, 4, 12, 26, 28, 35, 39, 7, 18, 21, 26, 38, 2, 9, 19, 25, 27, 28, 31, 33, 38, 22, 36, 4, 31, 26, 19, 34, 35, 1, 34, 20, 36, 21, 35, 3, 4, 10, 19, 27, 28, 35, 3, 4, 5, 6, 7, 8, 10, 11, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 33, 35, 37, 38, 3, 4, 5, 6, 7, 8, 10, 11, 14, 15, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 33, 35, 37, 38, 38, 20, 1, 2, 4, 5, 7, 10, 14, 15, 16, 19, 25, 27, 28, 29, 31, 33, 35, 37, 30, 7, 5, 7, 22, 37, 38, 22, 9, 7, 8, 29, 36, 37, 38, 4, 6, 23, 23, 2, 5, 7, 14, 20, 26, 27, 28, 33, 35, 38, 36, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 19, 21, 24, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 19, 28, 2, 4, 9, 14, 27, 29, 9, 1, 1, 15, 2, 16, 25, 35, 4, 14, 10, 32, 14, 23, 25, 31, 23, 25, 2, 5, 7, 12, 30, 1, 37, 4, 12, 16, 27, 27, 25, 2, 4, 5, 7, 8, 10, 14, 15, 16, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 37, 38, 39, 35, 36, 2, 7, 10, 16, 9, 20, 22, 22, 19, 24, 31, 34, 38, 7, 7, 3, 35, 25, 40, 30, 5, 4, 5, 7, 15, 16, 19, 22, 27, 28, 29, 33, 35, 36, 37, 11, 15, 33, 28, 14, 14, 8, 9, 4, 30, 2, 4, 5, 7, 8, 10, 14, 15, 16, 21, 23, 25, 27, 28, 29, 31, 32, 33, 35, 37, 38, 2, 4, 7, 10, 14, 15, 16, 23, 25, 27, 28, 29, 33, 35, 37, 18, 35, 32, 16, 27, 31, 4, 32, 32, 8, 2, 4, 5, 7, 8, 9, 10, 11, 15, 27, 28, 29, 30, 31, 33, 35, 37, 40, 4, 28, 29, 38, 25, 3, 8, 8, 17, 19, 31, 37, 4, 6, 7, 18, 21, 24, 38, 5, 6, 7, 8, 15, 36, 37, 38, 31, 35, 20, 20, 36, 17, 17, 23, 4, 5, 29, 21, 3, 20, 25, 24, 27, 4, 8, 10, 12, 14, 16, 26, 27, 28, 29, 39, 4, 10, 16, 29, 27, 28, 28, 12, 21, 26, 14, 35, 19, 23, 29, 36, 36, 14, 15, 3, 9, 28, 37, 38, 28, 33, 20, 14, 14, 25, 38, 25, 31, 5, 19, 40, 32, 24, 6, 24, 9, 32, 27, 11, 39, 39, 25, 13, 18, 16, 21, 24, 25, 35, 38, 37, 32, 4, 34, 40, 20, 28, 20, 3, 4, 8, 11, 20, 22, 23, 24, 25, 28, 30, 33, 35, 38, 1, 2, 4, 5, 8, 10, 11, 20, 22, 24, 28, 32, 35, 36, 38, 6, 7, 33, 1, 4, 5, 7, 15, 16, 27, 1, 26, 7, 8, 10, 15, 17, 22, 28, 37, 2, 14, 16, 27, 33, 35, 39, 3, 9, 11, 19, 26, 28, 31, 6, 13, 15, 28, 31, 29, 22, 22, 29, 3, 4, 10, 12, 15, 16, 33, 4, 8, 10, 15, 16, 18, 22, 28, 32, 33, 16, 33, 5, 7, 8, 10, 12, 16, 21, 24, 26, 30, 8, 3, 16, 18, 27, 33, 34, 34, 7, 34, 38, 34, 38, 5, 20, 37, 4, 7, 8, 10, 12, 15, 25, 27, 29, 31, 34, 35, 36, 38, 2, 8, 32, 14, 27, 5, 8, 18, 30, 37, 6, 40, 38, 35, 9, 27, 1, 27, 28, 37, 38, 9, 37, 38, 11, 15, 2, 4, 8, 27, 29, 31, 33, 35, 38, 39, 1, 2, 10, 12, 16, 19, 29, 33, 35, 29, 10, 15, 33, 35, 33, 4, 10, 14, 15, 27, 28, 4, 8, 37, 19, 6, 24, 28, 37, 4, 7, 10, 15, 23, 24, 26, 28, 29, 34, 39, 39, 6, 22, 13, 21, 22, 26, 30, 35, 38, 14, 30, 31, 37, 11, 27, 34, 23, 14, 14, 39, 35, 38, 31, 38, 6, 6, 38, 15, 15, 7, 7, 12, 17, 18, 24, 37, 28, 14, 16, 40, 19, 19, 11, 14, 5, 18, 21, 30, 33, 38, 37, 11, 12, 28, 38, 27, 8, 8, 23, 26, 38, 5, 38, 16, 12, 4, 10, 11, 19, 20, 26, 28, 29, 31, 33, 36, 5, 8, 9, 19, 20, 24, 38, 35, 12, 22, 40, 21, 16, 32, 4, 33, 2, 14, 40, 8, 18, 18, 23, 5, 26, 27, 13, 12, 23, 25, 35, 17, 17, 12, 17, 17, 28, 28, 7, 25, 12, 18, 4, 30, 4, 30, 36, 36, 26, 31, 18, 4, 29, 39, 2, 4, 5, 6, 7, 13, 17, 23, 24, 28, 33, 35, 39, 24, 17, 13, 15, 35, 14, 23, 31, 35, 15, 4, 5, 7, 15, 16, 24, 28, 29, 36, 32, 1, 7, 8, 9, 14, 19, 27, 30, 35, 23, 27, 2, 29, 2, 2, 10, 23, 38, 2, 20, 29, 15, 28, 34, 19, 19, 12, 16, 17, 32, 10, 1, 2, 4, 8, 25, 31, 33, 34, 35, 4, 5, 10, 12, 15, 18, 21, 27, 28, 33, 37, 4, 8, 10, 15, 20, 24, 28, 29, 3, 5, 7, 14, 25, 26, 30, 33, 35, 38, 30, 31, 5, 6, 18, 20, 11, 35, 18, 10, 4, 9, 10, 13, 19, 31, 38, 40, 19, 29, 2, 8, 16, 27, 29, 31, 33, 35, 39, 2, 3, 4, 5, 7, 8, 10, 15, 18, 22, 24, 25, 26, 28, 30, 33, 35, 5, 3, 5, 15, 24, 26, 30, 28, 29, 21, 30, 1, 10, 24, 8, 28, 3, 4, 5, 7, 8, 9, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 38, 10, 19, 34, 34, 10, 32, 3, 20, 24, 25, 32, 14, 35, 40, 2, 8, 11, 17, 24, 25, 27, 2, 32, 40, 1, 27, 38, 5, 24, 28, 6, 8, 10, 29, 31, 9, 24, 26, 5, 2, 34, 38, 9, 2, 8, 19, 21, 32, 13, 14, 38, 14, 29, 3, 18, 31, 38, 9, 10, 31, 35, 30, 30, 25, 25, 8, 23, 7, 27, 38, 7, 27, 38, 24, 28, 4, 7, 10, 15, 22, 24, 25, 28, 30, 33, 36, 6, 6, 33, 33, 35, 36, 10, 12, 22, 29, 32, 14, 8, 26, 30, 33, 37, 38, 8, 39, 39, 1, 7, 8, 11, 12, 13, 15, 17, 21, 26, 27, 28, 29, 31, 32, 33, 35, 36, 37, 38, 7, 10, 15, 20, 27, 1, 38, 31, 13, 13, 13, 5, 3, 10, 11, 12, 15, 16, 17, 20, 21, 23, 28, 32, 33, 35, 38, 5, 7, 28, 21, 20, 2, 4, 7, 8, 10, 11, 15, 24, 27, 30, 33, 2, 27, 28, 31, 33, 36, 19, 34, 14, 15, 18, 19, 40, 40, 5, 29, 37, 40, 35, 20, 12, 14, 15, 20, 29, 20, 4, 5, 24, 26, 30, 31, 35, 36, 38, 9, 18, 38, 5, 24, 28, 20, 1, 5, 9, 9, 19, 28, 33, 32, 6, 3, 5, 7, 18, 21, 24, 26, 38, 26, 29, 18, 1, 10, 40, 20, 20, 4, 16, 2, 10, 14, 16, 28, 31, 35, 14, 16, 2, 2, 8, 10, 19, 23, 28, 35, 2, 8, 11, 28, 34, 8, 10, 19, 28, 11, 19, 35, 34, 1, 1, 4, 7, 15, 23, 28, 30, 37, 2, 7, 15, 29, 32, 33, 34, 36, 39, 8, 23, 24, 37, 8, 22, 8, 38, 8, 21, 38, 3, 15, 26, 21, 4, 8, 27, 29, 31, 27, 9, 10, 36, 5, 24, 5, 7, 8, 11, 17, 18, 28, 33, 37, 3, 8, 11, 17, 22, 28, 30, 37, 11, 28, 26, 28, 22, 19, 31, 2, 9, 10, 16, 27, 29, 33, 3, 4, 4, 10, 14, 15, 27, 28, 29, 31, 33, 5, 10, 21, 7, 28, 17, 28, 18, 6, 30, 6, 18, 6, 1, 2, 5, 10, 19, 24, 27, 28, 30, 35, 37, 38, 30, 3, 3, 37, 7, 28, 38, 40, 8, 10, 14, 27, 2, 7, 22, 27, 3, 7, 10, 11, 14, 19, 20, 28, 33, 35, 33, 33, 18, 18, 18, 5, 7, 8, 15, 18, 21, 22, 28, 30, 37, 38, 37, 35, 25, 8, 10, 14, 20, 22, 24, 17, 21, 3, 10, 11, 5, 5, 7, 8, 14, 19, 27, 28, 33, 35, 37, 40, 1, 5, 19, 24, 27, 28, 33, 35, 28, 36, 8, 4, 9, 10, 14, 23, 27, 29, 35, 39, 39, 39, 39, 18, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 19, 21, 24, 27, 28, 29, 30, 31, 32, 33, 35, 36, 38, 4, 25, 24, 12, 17, 32, 35, 40, 2, 20, 35, 8, 26, 38, 8, 35, 33, 24, 24, 13, 20, 19, 20, 33, 37, 2, 4, 5, 7, 10, 12, 13, 15, 19, 24, 27, 28, 35, 37, 4, 15, 17, 20, 5, 25, 34, 25, 2, 4, 12, 27, 29, 19, 31, 38, 31, 10, 8, 5, 40, 4, 28, 4, 11, 35, 2, 4, 7, 8, 10, 15, 17, 18, 21, 26, 28, 33, 35, 38, 1, 4, 12, 19, 37, 39, 19, 25, 35, 1, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 26, 26, 2, 4, 10, 16, 19, 26, 27, 29, 33, 37, 4, 5, 8, 10, 15, 34, 25, 28, 35, 26, 1, 1, 2, 16, 19, 24, 27, 29, 33, 16, 34, 40, 40, 25, 2, 4, 7, 8, 10, 12, 15, 16, 24, 27, 28, 29, 31, 33, 35, 2, 4, 8, 9, 10, 14, 27, 29, 31, 11, 20, 21, 25, 28, 34, 37, 34, 3, 21, 28, 30, 10, 20, 25, 33, 33, 29, 1, 10, 11, 14, 27, 34, 40, 1, 10, 14, 19, 35, 20, 12, 1, 2, 4, 5, 7, 10, 13, 14, 15, 16, 27, 29, 33, 1, 2, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 27, 28, 29, 31, 33, 35, 9, 7, 8, 15, 18, 24, 33, 37, 38, 37, 34, 33, 28, 34, 1, 30, 13, 23, 26, 23, 33, 38, 3, 18, 18, 5, 10, 24, 37, 16, 25, 1, 4, 5, 7, 14, 15, 16, 17, 18, 21, 22, 24, 25, 27, 28, 29, 31, 33, 35, 36, 37, 38, 22, 14, 31, 12, 14, 14, 18, 32, 27, 33, 15, 20, 7, 18, 30, 28, 40, 22, 3, 4, 5, 7, 14, 15, 18, 20, 21, 22, 24, 27, 28, 35, 36, 37, 38, 22, 7, 36, 39, 32, 30, 24, 18, 28, 33, 8, 22, 28, 8, 26, 1, 2, 3, 5, 7, 9, 10, 17, 19, 20, 22, 24, 25, 28, 30, 33, 35, 37, 38, 40, 19, 40, 15, 21, 24, 38, 27, 4, 15, 36, 36, 11, 38, 38, 38, 18, 2, 36, 4, 12, 36, 25, 4, 5, 7, 15, 24, 27, 28, 37, 5, 7, 15, 19, 22, 27, 28, 33, 35, 36, 37, 38, 13, 21, 17, 34, 5, 6, 28, 37, 3, 28, 35, 37, 15, 17, 36, 5, 11, 16, 28, 29, 1, 7, 27, 29, 35, 22, 22, 7, 24, 37, 38, 29, 3, 4, 7, 15, 21, 23, 26, 33, 9, 34, 5, 40, 17, 34, 27, 10, 24, 24, 5, 7, 8, 21, 24, 28, 29, 35, 37, 38, 10, 4, 4, 5, 16, 26, 4, 6, 28, 6, 4, 14, 29, 7, 16, 21, 24, 28, 29, 31, 35, 37, 38, 28, 2, 29, 18, 21, 24, 38, 39, 19, 2, 10, 16, 23, 25, 27, 28, 29, 31, 33, 34, 35, 13, 27, 32, 32, 21, 18, 5, 3, 12, 27, 33, 10, 5, 17, 26, 30, 3, 15, 24, 28, 7, 13, 3, 24, 19, 26, 6, 8, 21, 26, 37, 26, 31, 1, 22, 26, 28, 6, 8, 15, 37, 24, 5, 21, 26, 37, 38, 37, 37, 11, 4, 28, 16, 27, 39, 4, 5, 7, 8, 10, 11, 12, 13, 15, 17, 18, 19, 21, 22, 24, 26, 27, 28, 30, 32, 33, 35, 37, 38, 40, 40, 20, 2, 16, 10, 22, 26, 24, 31, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 36, 23, 33, 11, 23, 35, 23, 12, 21, 21, 5, 10, 31, 35, 31, 24, 6, 7, 16, 20, 21, 24, 36, 38, 16, 35, 4, 5, 7, 15, 21, 24, 28, 31, 35, 37, 37, 4, 2, 4, 7, 8, 10, 14, 22, 27, 29, 31, 33, 35, 37, 1, 1, 21, 34, 16, 5, 7, 15, 28, 39, 2, 3, 5, 7, 9, 10, 11, 14, 17, 18, 19, 20, 22, 23, 25, 27, 28, 29, 33, 35, 37, 38, 17, 37, 40, 19, 22, 28, 30, 35, 38, 3, 3, 23, 23, 13, 6, 28, 1, 19, 19, 34, 16, 24, 16, 31, 2, 9, 12, 13, 14, 23, 29, 2, 3, 5, 7, 8, 15, 19, 20, 22, 24, 26, 28, 32, 35, 37, 38, 15, 28, 28, 33, 36, 32, 3, 28, 7, 10, 16, 27, 29, 38, 1, 5, 17, 27, 28, 30, 32, 34, 36, 2, 4, 10, 14, 27, 29, 33, 35, 19, 28, 4, 13, 28, 35, 37, 38, 13, 14, 3, 18, 34, 35, 36, 37, 34, 1, 6, 8, 11, 12, 18, 27, 33, 37, 38, 16, 35, 7, 3, 5, 7, 19, 21, 24, 30, 33, 37, 38, 1, 27, 1, 40, 20, 40, 4, 4, 12, 4, 15, 4, 7, 17, 21, 29, 13, 7, 36, 36, 13, 4, 37, 29, 10, 11, 14, 19, 20, 23, 25, 28, 30, 35, 38, 30, 23, 2, 2, 10, 29, 3, 16, 28, 29, 28, 3, 4, 5, 7, 15, 20, 21, 28, 33, 38, 20, 21, 3, 5, 18, 21, 38, 19, 4, 5, 7, 10, 14, 15, 16, 17, 21, 22, 27, 28, 33, 36, 37, 8, 11, 20, 27, 28, 11, 5, 8, 31, 4, 6, 4, 11, 36, 32, 8, 1, 2, 13, 2, 4, 2, 10, 2, 27, 36, 10, 27, 4, 38, 8, 11, 14, 25, 27, 33, 35, 11, 3, 7, 15, 27, 29, 37, 7, 27, 29, 2, 4, 7, 8, 9, 10, 15, 19, 20, 22, 25, 28, 29, 32, 33, 36, 37, 39, 19, 15, 24, 28, 13, 19, 20, 24, 28, 29, 37, 2, 5, 32, 34, 37, 7, 8, 18, 20, 21, 26, 34, 36, 38, 40, 1, 28, 35, 3, 29, 39, 15, 38, 21, 23, 35, 23, 21, 17, 10, 11, 22, 28, 33, 3, 4, 5, 7, 10, 12, 22, 24, 28, 36, 14, 38, 40, 40, 36, 22, 2, 33, 13, 28, 32, 35, 36, 2, 14, 27, 33, 35, 25, 17, 12, 20, 25, 3, 38, 38, 38, 4, 11, 30, 35, 38, 8, 28, 29, 8, 15, 8, 12, 3, 4, 3, 4, 21, 22, 30, 37, 38, 4, 4, 2, 4, 5, 7, 8, 15, 16, 19, 22, 24, 27, 28, 29, 32, 33, 35, 37, 38, 14, 2, 28, 7, 12, 30, 5, 34, 36, 39, 10, 15, 18, 30, 38, 29, 7, 8, 10, 17, 21, 26, 28, 32, 33, 35, 38, 39, 7, 38, 18, 20, 21, 38, 3, 5, 6, 7, 25, 26, 28, 31, 33, 35, 38, 29, 5, 2, 20, 6, 19, 7, 32, 23, 24, 24, 20, 7, 3, 24, 4, 8, 15, 27, 28, 29, 17, 33, 30, 30, 5, 7, 11, 15, 31, 31, 30, 38, 36, 21, 37, 38, 28, 28, 6, 37, 2, 3, 4, 5, 7, 8, 10, 11, 14, 15, 16, 20, 21, 22, 24, 26, 27, 28, 29, 30, 32, 33, 35, 36, 38, 31, 3, 9, 23, 38, 19, 35, 15, 4, 5, 7, 8, 9, 10, 12, 14, 16, 19, 23, 25, 27, 28, 29, 31, 32, 33, 35, 38, 39, 31, 31, 10, 17, 10, 4, 7, 13, 19, 27, 34, 13, 17, 10, 7, 15, 16, 22, 27, 29, 31, 35, 37, 15, 4, 15, 27, 29, 4, 6, 7, 8, 11, 15, 17, 19, 21, 22, 25, 26, 28, 33, 35, 36, 38, 11, 27, 33, 27, 33, 11, 1, 2, 7, 9, 10, 11, 13, 18, 19, 20, 22, 27, 28, 33, 35, 36, 37, 38, 8, 9, 10, 33, 35, 9, 38, 9, 17, 17, 5, 21, 26, 28, 33, 32, 6, 8, 19, 25, 28, 31, 34, 13, 18, 4, 13, 27, 28, 9, 7, 9, 21, 25, 28, 39, 13, 1, 16, 27, 39, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 19, 21, 22, 27, 28, 29, 31, 32, 33, 35, 36, 37, 38, 32, 15, 21, 33, 37, 22, 2, 7, 16, 19, 21, 22, 27, 28, 29, 31, 33, 35, 37, 38, 24, 5, 15, 16, 27, 36, 16, 5, 3, 4, 5, 10, 15, 16, 17, 20, 27, 30, 38, 4, 5, 10, 13, 18, 31, 25, 2, 14, 29, 31, 33, 35, 38, 29, 33, 8, 31, 24, 33, 35, 33, 2, 4, 5, 7, 8, 9, 10, 14, 15, 17, 19, 21, 22, 24, 25, 27, 28, 29, 31, 32, 33, 35, 36, 37, 38, 39, 16, 1, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 18, 19, 22, 25, 26, 27, 28, 29, 31, 32, 33, 35, 37, 38, 38, 5, 8, 18, 39, 17, 26, 25, 31, 35, 39, 2, 4, 5, 7, 8, 14, 15, 19, 21, 22, 24, 27, 28, 29, 33, 35, 36, 37, 38, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 24, 27, 28, 29, 31, 33, 35, 37, 38, 4, 5, 6, 7, 8, 19, 20, 24, 27, 28, 29, 32, 35, 36, 8, 9, 2, 23, 25, 27, 28, 29, 31, 33, 35, 2, 3, 4, 5, 7, 9, 10, 14, 15, 18, 20, 21, 22, 23, 24, 26, 27, 28, 29, 31, 33, 35, 37, 38, 39, 32, 35, 38, 28, 32, 26, 12, 10, 12, 26, 35, 35, 27, 13, 1, 8, 11, 12, 27, 28, 29, 31, 35, 36, 40, 27, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 18, 23, 30, 33, 2, 8, 9, 10, 11, 14, 18, 19, 27, 33, 35, 38, 2, 40, 10, 11, 23, 23, 28, 28, 34, 32, 12, 12, 20, 6, 16, 21, 23, 21, 21, 6, 1, 38, 28, 13, 28, 37, 13, 36, 3, 25, 18, 20, 19, 20, 5, 39, 14, 14, 27, 29, 5, 3, 7, 11, 14, 18, 19, 23, 25, 28, 33, 35, 37, 38, 24, 37, 20, 13, 23, 23, 7, 13, 40, 21, 21, 5, 8, 9, 25, 35, 9, 3, 7, 9, 37, 19, 40, 35, 38, 24, 8, 5, 34, 12, 27, 12, 34, 5, 34, 3, 21, 26, 26, 6, 7, 9, 10, 15, 17, 19, 20, 27, 30, 32, 33, 35, 36, 38, 40, 6, 9, 24, 26, 35, 38, 10, 27, 1, 10, 27, 27, 17, 5, 8, 37, 34, 18, 34, 6, 12, 16, 19, 28, 29, 33, 40, 19, 17, 21, 17, 39, 24, 4, 16, 18, 19, 25, 35, 38, 39, 38, 19, 8, 11, 20, 28, 8, 28, 29, 28, 4, 26, 37, 36, 12, 1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 17, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 30, 6, 10, 7, 29, 13, 13, 5, 19, 9, 9, 28, 9, 11, 22, 5, 2, 4, 8, 16, 28, 33, 39, 2, 9, 31, 35, 2, 9, 1, 22, 5, 7, 10, 15, 16, 18, 27, 28, 29, 31, 33, 36, 4, 5, 6, 7, 8, 9, 15, 28, 30, 3, 7, 15, 17, 22, 25, 28, 3, 1, 2, 5, 10, 16, 27, 3, 12, 30, 12, 15, 33, 12, 9, 5, 11, 36, 12, 15, 4, 16, 27, 11, 23, 38, 4, 8, 9, 10, 14, 15, 25, 27, 29, 36, 37, 39, 4, 5, 6, 7, 8, 11, 15, 20, 21, 23, 28, 33, 35, 37, 38, 3, 4, 6, 8, 10, 11, 14, 20, 21, 22, 23, 25, 28, 33, 35, 36, 38, 4, 7, 21, 16, 21, 38, 2, 4, 16, 27, 29, 27, 29, 26, 20, 36, 21, 24, 26, 10, 14, 15, 20, 24, 28, 24, 25, 11, 26, 31, 38, 8, 5, 11, 22, 24, 26, 28, 33, 35, 38, 4, 16, 16, 16, 25, 16, 16, 15, 16, 35, 33, 11, 18, 19, 35, 2, 9, 31, 2, 10, 39, 11, 1, 4, 8, 13, 14, 15, 19, 24, 25, 27, 33, 37, 38, 17, 17, 10, 11, 14, 28, 29, 31, 35, 29, 4, 7, 28, 35, 27, 33, 35, 27, 24, 29, 19, 12, 15, 7, 8, 10, 27, 28, 33, 36, 37, 7, 7, 25, 34, 4, 15, 4, 35, 36, 35, 2, 4, 9, 10, 15, 20, 22, 24, 25, 27, 28, 31, 33, 36, 37, 1, 2, 10, 14, 16, 19, 20, 22, 29, 35, 36, 37, 1, 28, 39, 37, 11, 11, 11, 24, 2, 36, 8, 2, 4, 34, 14, 14, 19, 29, 38, 25, 29, 34, 39, 37, 25, 25, 9, 35, 29, 38, 15, 35, 17, 28, 23, 39, 23, 39, 6, 2, 4, 5, 8, 9, 10, 14, 15, 16, 17, 24, 27, 28, 36, 4, 8, 9, 10, 15, 16, 24, 28, 36, 8, 36, 4, 15, 4, 7, 8, 15, 19, 28, 29, 37, 16, 23, 38, 18, 34, 35, 38, 15, 38, 15, 38, 40, 6, 23, 14, 9, 15, 17, 1, 4, 10, 13, 15, 28, 11, 26, 5, 23, 3, 5, 7, 26, 30, 35, 38, 5, 15, 22, 24, 28, 37, 38, 10, 11, 18, 38, 18, 3, 16, 4, 14, 19, 27, 36, 39, 6, 7, 19, 25, 37, 38, 2, 3, 4, 5, 7, 8, 9, 10, 11, 15, 16, 19, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 24, 28, 40, 4, 3, 5, 15, 24, 9, 4, 39, 15, 15, 39, 23, 7, 5, 15, 34, 7, 11, 19, 21, 28, 35, 38, 7, 19, 28, 37, 38, 27, 32, 17, 16, 27, 31, 37, 11, 36, 37, 38, 11, 4, 5, 8, 15, 28, 16, 15, 30, 19, 28, 19, 12, 3, 15, 33, 4, 12, 16, 6, 10, 40, 19, 37, 38, 37, 23, 32, 38, 38, 3, 38, 15, 10, 16, 20, 21, 27, 31, 32, 33, 35, 38, 16, 24, 24, 14, 28, 15, 24, 28, 33, 7, 24, 7, 27, 21, 21, 23, 28, 17, 28, 34, 28, 6, 11, 28, 36, 6, 35, 8, 18, 23, 24, 25, 23, 31, 4, 13, 15, 28, 4, 15, 27, 14, 35, 35, 21, 20, 38, 21, 28, 2, 3, 27, 31, 9, 11, 13, 28, 29, 12, 12, 39, 8, 19, 21, 26, 38, 3, 26, 38, 35, 4, 14, 15, 17, 23, 27, 28, 33, 2, 4, 5, 7, 16, 21, 26, 27, 29, 33, 35, 38, 3, 4, 5, 7, 8, 10, 14, 15, 16, 17, 21, 22, 24, 26, 27, 28, 29, 30, 31, 33, 36, 37, 38, 39, 20, 9, 29, 17, 18, 21, 24, 25, 1, 1, 31, 7, 7, 2, 4, 7, 8, 9, 10, 11, 14, 15, 16, 19, 20, 23, 25, 27, 28, 29, 33, 35, 37, 38, 39, 4, 7, 15, 27, 37, 2, 4, 14, 16, 19, 23, 25, 27, 28, 29, 33, 35, 37, 38, 39, 2, 4, 8, 9, 10, 14, 16, 19, 22, 25, 27, 28, 29, 31, 35, 39, 4, 5, 29, 31, 32, 16, 8, 2, 19, 25, 31, 35, 25, 32, 30, 13, 28, 13, 13, 2, 8, 10, 33, 3, 8, 19, 23, 25, 33, 35, 38, 8, 23, 25, 35, 10, 3, 4, 7, 10, 13, 15, 17, 18, 21, 25, 27, 28, 29, 31, 32, 33, 35, 37, 38, 10, 5, 32, 2, 3, 5, 8, 9, 14, 17, 19, 27, 33, 35, 38, 26, 38, 2, 7, 9, 11, 14, 19, 22, 27, 28, 30, 33, 35, 37, 38, 37, 3, 4, 7, 9, 10, 15, 22, 25, 27, 28, 32, 22, 37, 23, 5, 26, 21, 5, 33, 5, 7, 10, 18, 21, 30, 33, 38, 10, 16, 27, 33, 12, 33, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 20, 21, 22, 24, 27, 28, 29, 31, 33, 36, 39, 33, 39, 5, 17, 31, 29, 35, 2, 4, 7, 8, 10, 11, 14, 16, 25, 27, 31, 32, 33, 36, 40, 19, 2, 39, 17, 22, 35, 38, 16, 3, 4, 5, 15, 2, 38, 18, 19, 31, 28, 3, 25, 30, 33, 38, 25, 33, 40, 9, 9, 30, 20, 35, 25, 21, 2, 27, 10, 19, 32, 34, 8, 23, 34, 17, 40, 7, 8, 16, 28, 29, 31, 33, 35, 39, 7, 19, 29, 10, 19, 24, 28, 31, 35, 8, 9, 11, 28, 37, 9, 9, 20, 37, 20, 37, 8, 13, 3, 24, 1, 24, 6, 7, 26, 28, 31, 32, 33, 35, 38, 12, 20, 35, 35, 13, 4, 7, 8, 21, 24, 27, 29, 37, 4, 8, 10, 12, 13, 14, 15, 17, 25, 27, 28, 31, 33, 36, 37, 40, 21, 2, 4, 5, 7, 8, 9, 10, 15, 16, 19, 20, 22, 24, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 4, 34, 12, 28, 33, 22, 8, 15, 27, 28, 10, 30, 2, 30, 22, 19, 30, 34, 1, 3, 4, 5, 7, 8, 10, 14, 15, 17, 18, 21, 22, 27, 28, 29, 33, 36, 37, 38, 3, 4, 5, 7, 10, 15, 18, 21, 22, 24, 28, 33, 37, 38, 31, 31, 31, 37, 10, 27, 35, 22, 33, 28, 5, 25, 2, 3, 5, 7, 11, 15, 17, 21, 28, 38, 11, 12, 24, 36, 29, 35, 39, 9, 9, 23, 26, 3, 4, 7, 8, 10, 15, 25, 28, 35, 14, 15, 16, 17, 36, 40, 36, 21, 28, 5, 6, 11, 38, 32, 7, 16, 26, 28, 30, 33, 18, 26, 21, 36, 2, 3, 7, 10, 11, 12, 15, 16, 17, 20, 21, 22, 25, 28, 30, 33, 35, 38, 3, 5, 18, 21, 24, 26, 9, 5, 28, 8, 17, 39, 39, 19, 40, 10, 1, 2, 4, 5, 7, 10, 13, 14, 15, 19, 24, 28, 29, 31, 33, 35, 38, 31, 12, 17, 28, 30, 34, 13, 28, 29, 1, 2, 4, 7, 9, 11, 14, 16, 19, 20, 24, 25, 27, 28, 29, 31, 32, 33, 35, 38, 40, 1, 2, 4, 5, 7, 8, 9, 10, 13, 14, 15, 18, 19, 21, 24, 27, 28, 36, 37, 38, 39, 40, 1, 2, 3, 4, 5, 7, 8, 10, 11, 14, 15, 17, 19, 20, 21, 24, 26, 27, 28, 29, 31, 33, 35, 36, 37, 38, 39, 4, 16, 27, 29, 2, 4, 29, 9, 38, 4, 7, 8, 9, 10, 14, 16, 24, 29, 5, 7, 22, 24, 27, 29, 35, 36, 37, 27, 27, 5, 7, 11, 25, 27, 28, 33, 36, 38, 16, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 17, 18, 19, 21, 22, 24, 26, 27, 28, 29, 33, 35, 36, 37, 38, 12, 2, 27, 4, 5, 7, 8, 14, 15, 21, 27, 28, 29, 36, 37, 39, 33, 24, 36, 36, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 12, 26, 7, 3, 4, 5, 8, 10, 18, 19, 20, 21, 22, 24, 27, 28, 32, 33, 34, 35, 37, 38, 9, 35, 32, 3, 35, 36, 15, 2, 35, 38, 1, 2, 7, 8, 14, 25, 27, 29, 31, 33, 35, 37, 38, 39, 37, 8, 29, 15, 18, 18, 11, 3, 4, 5, 7, 10, 12, 15, 16, 17, 21, 22, 24, 26, 28, 30, 37, 14, 15, 10, 27, 37, 37, 38, 30, 30, 6, 39, 39, 13, 31, 35, 31, 6, 21, 37, 38, 28, 5, 28, 35, 7, 5, 21, 32, 10, 13, 4, 10, 15, 17, 21, 22, 10, 4, 8, 9, 35, 35, 21, 36, 1, 30, 1, 13, 37, 30, 38, 27, 31, 24, 7, 18, 6, 9, 23, 23, 29, 29, 19, 1, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 19, 20, 21, 24, 25, 27, 28, 29, 30, 31, 32, 33, 35, 36, 38, 32, 4, 7, 8, 10, 14, 15, 17, 20, 24, 25, 27, 28, 29, 30, 36, 37, 15, 7, 24, 36, 4, 25, 29, 2, 2, 4, 5, 7, 8, 10, 15, 18, 19, 22, 23, 26, 27, 28, 29, 31, 33, 34, 35, 36, 38, 39, 22, 12, 12, 7, 15, 6, 10, 39, 29, 16, 21, 37, 14, 18, 6, 37, 37, 10, 2, 2, 19, 21, 2, 3, 4, 5, 7, 10, 11, 14, 15, 18, 21, 22, 24, 26, 27, 28, 31, 33, 35, 37, 38, 3, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 32, 33, 35, 37, 38, 7, 18, 21, 24, 26, 28, 35, 37, 26, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 30, 31, 32, 33, 35, 37, 38, 7, 8, 37, 26, 6, 28, 2, 4, 5, 7, 8, 10, 13, 15, 19, 20, 22, 24, 27, 28, 33, 35, 31, 2, 4, 10, 12, 29, 33, 11, 35, 40, 8, 10, 11, 31, 33, 25, 4, 21, 28, 39, 1, 2, 7, 9, 20, 22, 23, 25, 27, 28, 33, 35, 36, 37, 20, 28, 29, 22, 17, 30, 7, 15, 22, 28, 37, 30, 5, 7, 18, 14, 26, 6, 32, 14, 22, 22, 38, 7, 11, 2, 29, 20, 20, 19, 20, 1, 5, 11, 14, 16, 20, 22, 24, 27, 28, 29, 30, 32, 39, 17, 23, 4, 22, 33, 4, 5, 7, 8, 10, 15, 16, 17, 21, 24, 27, 28, 29, 31, 36, 37, 38, 15, 20, 2, 8, 27, 29, 31, 33, 35, 38, 10, 12, 15, 18, 21, 28, 33, 37, 4, 6, 7, 8, 11, 15, 21, 23, 28, 33, 35, 37, 38, 2, 16, 27, 29, 15, 19, 29, 32, 37, 36, 13, 24, 13, 37, 22, 14, 33, 28, 29, 33, 29, 29, 5, 7, 8, 11, 14, 20, 22, 24, 27, 28, 30, 32, 7, 15, 24, 27, 36, 1, 2, 4, 5, 7, 8, 9, 10, 17, 21, 26, 27, 28, 29, 31, 33, 35, 38, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 19, 25, 25, 7, 10, 11, 21, 22, 28, 36, 21, 22, 28, 27, 5, 7, 20, 5, 7, 12, 35, 16, 2, 35, 38, 4, 14, 19, 27, 39, 16, 28, 39, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 15, 16, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 28, 37, 34, 18, 4, 5, 7, 17, 24, 28, 36, 37, 39, 39, 27, 27, 15, 21, 37, 20, 37, 10, 19, 29, 35, 38], \"Freq\": [0.9947790170624944, 0.9980749554699051, 0.0024107339309679472, 0.0012627653924117818, 0.01228326336255097, 0.03656279795301386, 0.02181140223256714, 0.5740990661319383, 0.034152064022045917, 0.009642935723871789, 0.030535963125593996, 0.003042116627173838, 0.032315314360356055, 0.0013775622462673983, 0.0008035779769893157, 0.012398060216406586, 0.0035587024695241123, 0.0006313826962058909, 0.002353335504040139, 0.012685052351045627, 0.008552365612243432, 0.0018367496616898645, 0.06365485546293936, 0.01630115324749755, 0.025771893690585912, 0.0062564285351311005, 0.021237417963289058, 0.028986205598543174, 0.0006887811231336992, 0.023935144028896047, 0.010159521566222064, 0.0005165858423502743, 0.9950510465584944, 0.9767985061340095, 0.9476801450696613, 0.03816161657998636, 0.012720538859995453, 0.9490197596544121, 0.036301302281862756, 0.012964750814950984, 0.9985610419450907, 0.07813942110149454, 0.9206862225436965, 0.04323909393692215, 0.04007525779519614, 0.05062137826761618, 0.0284745252755341, 0.0870054938974653, 0.3232385924796741, 0.015819180708630056, 0.39337029362126735, 0.01740109877949306, 0.9858050480812475, 0.013902205719819665, 0.39781696367483965, 0.0802050329989596, 0.4994100054735218, 0.008555203519889025, 0.0789914576353362, 0.9193463623582503, 0.9985773604237577, 0.9929173550675495, 0.9976376512981824, 0.3372884124345149, 0.17973921978418228, 0.04438005426769932, 0.10429312752909342, 0.01109501356692483, 0.2684993283195809, 0.051037062407854224, 0.47036948906242876, 0.3026408340479193, 0.22424591920418116, 0.9911357049811851, 0.9920832369804299, 0.9984088585995222, 0.9974480839492407, 0.2499955535706881, 0.03305726328207446, 0.7148633184748602, 0.9908495105256969, 0.9882670229358483, 0.9270987492055911, 0.004937942738778115, 0.0037034570540835864, 0.014813828216334346, 0.007406914108167173, 0.01975177095511246, 0.0037034570540835864, 0.016048313901028875, 0.9949482104620678, 0.9891806503979619, 0.9874912149196471, 0.019439845782979696, 0.9719922891489848, 0.1962851791452158, 0.004636657775083837, 0.10664312882692827, 0.05100323552592221, 0.45284690936652144, 0.03554770960897609, 0.05873099848439527, 0.06027655107608989, 0.03400215701728147, 0.9994129994265302, 0.7756239054514752, 0.2181442234082274, 0.9946931796103519, 0.06336698509914407, 0.9346630302123751, 0.1996586733890623, 0.7986346935562492, 0.9957201129935169, 0.9844574578578412, 0.9985286231237592, 0.41906901382938955, 0.5804881895266358, 0.2246921624559597, 0.6346568097440265, 0.12220100063394299, 0.017081860303669448, 0.005020541351015049, 0.9948769522350146, 0.9982904325447284, 0.0033605768903094357, 0.007169230699326796, 0.3542048042386145, 0.001120192296769812, 0.004928846105787172, 0.0006721153780618871, 0.0020163461341856613, 0.547549994661084, 0.007169230699326796, 0.01590673061413133, 0.0069451922399728335, 0.006273076861910946, 0.025316345906997748, 0.001792307674831699, 0.015234615236069442, 0.4458584123454807, 0.5525975833808078, 0.9974800003341482, 0.002282281720687085, 0.9882279850575076, 0.006846845162061254, 0.0023183980910889, 0.025347819129238638, 0.0012364789819140798, 0.02751165734758828, 0.00896447261887708, 0.39613695383072334, 0.0092735923643556, 0.0037094369457422397, 0.01607422676488304, 0.01962910383788602, 0.0092735923643556, 0.007109754146005959, 0.00587327516409188, 0.00046367961821777996, 0.00479135605491706, 0.00015455987273925998, 0.00942815223709486, 0.0046367961821778, 0.0017001586001318598, 0.006027835036831139, 0.00432767643669928, 0.00200927834561038, 0.24791403587377303, 0.010664631219008939, 0.038639968184815, 0.00015455987273925998, 0.0108191910917482, 0.00108191910917482, 0.036630689839204617, 0.0448223630943854, 0.00649151465504892, 0.017001586001318597, 0.019783663710625277, 0.9555416841235749, 0.009100396991653095, 0.01638071458497557, 0.01820079398330619, 0.9842841688625105, 0.09028860968129698, 0.9028860968129698, 0.990606852190069, 0.8642032355387277, 0.13326498491952343, 0.9943386353835093, 0.000655408191838525, 0.029274899235454118, 0.017696021179640177, 0.05155877775796397, 0.003058571561913117, 0.2149738869230362, 0.009612653480298368, 0.05702051268995168, 0.051777247155243476, 0.1776156199882403, 0.016822143590522143, 0.02009918454971477, 0.007427959507503284, 0.018788368166037718, 0.0021846939727950833, 0.002403163370074592, 0.03495510356472133, 0.006335612521105742, 0.03582898115383937, 0.000655408191838525, 0.004806326740149184, 0.07209490110223776, 0.0032770409591926252, 0.0028401021646336086, 0.06401153340289595, 0.09132020806283449, 0.0028401021646336086, 0.4535341180678526, 0.0420616319175831, 0.42061631917583103, 0.0822944972300539, 0.9928243924194506, 0.9869846529037759, 0.5745462855891734, 0.4245055358493256, 0.9908026127774895, 0.06783156566973525, 0.6149078887886869, 0.31556424028963787, 0.9854577774390922, 0.9989821315396089, 0.005821912216666864, 0.014401572325439087, 0.3254142512684321, 0.0006128328649123015, 0.022674816001755155, 0.002757747892105357, 0.002757747892105357, 0.5858682188561602, 0.036463555462281944, 0.0030641643245615077, 0.0004272901812737977, 0.25252849713281444, 0.009186738897386651, 0.002136450906368989, 0.4450227237966603, 0.00021364509063689886, 0.0564023039281413, 0.002991031268916584, 0.006622997809743864, 0.009827674169297347, 0.02435554033260647, 0.04166079267419528, 0.022860024698148178, 0.003204676359553483, 0.005341127265922472, 0.0564023039281413, 0.02243273451687438, 0.015809736707130515, 0.02264637960751128, 0.05080482831006652, 0.03793650008679309, 0.03158589654803477, 0.011029995619948652, 0.2765854962275003, 0.01637787228416618, 0.00016712114575679775, 0.016879235721436573, 0.17531008189888084, 0.06267042965879915, 0.0036766652066495503, 0.0016712114575679776, 0.0003342422915135955, 0.008021814996326293, 0.002673938332108764, 0.010027268745407865, 0.14255433733054848, 0.01336969166054382, 0.0016712114575679776, 0.09559329537288831, 0.0061834823930015164, 0.0003342422915135955, 0.03425983488014354, 0.9850241921395714, 0.11897550068496511, 0.04759020027398604, 0.0535389753082343, 0.10261636934078242, 0.004461581275686192, 0.6722115788700529, 0.06082855010313706, 0.12283823710148067, 0.016535916532891628, 0.7990391096072277, 0.9963824025131021, 0.9994790118897258, 0.9914910074510523, 0.3509323961755732, 0.0293884272659347, 0.09335147484473375, 0.009508020586037697, 0.03543898582068596, 0.03543898582068596, 0.031117158281577916, 0.006914924062572871, 0.006050558554751262, 0.055319392500582966, 0.1763305635956082, 0.17028000504085694, 0.9863745124848974, 0.2087578560667479, 0.7879874003646259, 0.9914206866558672, 0.9979457163659156, 0.7158294190179834, 0.283296448515951, 0.974605725204642, 0.26676733669437347, 0.48181447545820516, 0.2504346426110445, 0.9904927367045603, 0.9913876535800636, 0.9908221263935705, 0.9963419515942545, 0.9919183449015772, 0.996525355084357, 0.60823435500647, 0.3879289980749927, 0.9174085061292233, 0.08087984310962926, 0.9884499178315366, 0.005472903527708218, 0.004104677645781163, 0.2462806587468698, 0.06157016468671745, 0.002736451763854109, 0.06430661645057156, 0.09303935997103971, 0.06841129409635273, 0.05062435763130101, 0.00957758117348938, 0.01915516234697876, 0.3365835669540554, 0.03967855057588458, 0.11324146319301437, 0.12096247204708353, 0.09522577586685299, 0.08235742777673773, 0.056620731596507184, 0.4684078704801958, 0.059194401214530235, 0.9881863263693701, 0.9866978596734646, 0.004019805785884892, 0.022778899453347722, 0.9727930001841439, 0.7255338835087971, 0.05820902217262555, 0.21412604584930114, 0.9957968623965299, 0.9976301547603705, 0.05613794733631258, 0.019599283359311128, 0.057677891028829885, 0.2476509447330099, 0.02155921169524224, 0.013019523945828105, 0.0026599027416207956, 0.0030798873850346057, 0.0016799385736552394, 0.020439252646138745, 0.00027998976227587325, 0.19221297180238697, 0.004339841315276035, 0.00013999488113793662, 0.002939892503896669, 0.05613794733631258, 0.0026599027416207956, 0.26137044308452767, 0.011339585372172866, 0.002239918098206986, 0.022679170744345733, 0.9977976842288346, 0.9978708663216146, 0.9756235995930876, 0.993915126413358, 0.9926346357292468, 0.9905918040921192, 0.9985546250014854, 0.9848495035125674, 0.9958843769537478, 0.9935297387394597, 0.9867454845204406, 0.0943038965733089, 0.151934055590331, 0.0034927369101225517, 0.436592113765319, 0.24274521525351733, 0.06985473820245103, 0.9899485340210878, 0.12915326631718535, 0.07082598475458551, 0.01458182039064996, 0.0416623439732856, 0.6790962067645553, 0.06457663315859268, 0.9988823279732095, 0.26290779772576567, 0.5398999417582687, 0.04694782102245815, 0.03755825681796652, 0.10797998835165375, 0.03660001291503724, 0.3372942366679903, 0.12343533767424326, 0.11769415917776682, 0.006458825808535984, 0.012917651617071969, 0.04090589678739457, 0.18587065382342444, 0.13850593122749388, 0.9974503455141093, 0.9868534268658448, 0.9954834635572716, 0.9938824814393539, 0.9912578709640866, 0.2945188572464245, 0.6291993768446341, 0.06693610391964193, 0.9914479853683673, 0.9947557285435382, 0.07247119783493379, 0.9223606997173391, 0.027776847744598498, 0.9707277317059685, 0.16888119784066657, 0.24682636607482036, 0.08877088604445293, 0.45576271981359373, 0.001082571781029914, 0.028146866306777762, 0.009743146029269225, 0.09535863623009228, 0.05325726479178093, 0.005917473865753437, 0.00029102330487311987, 0.009894792365686075, 0.028908314950729905, 0.006790543780372796, 0.34825788816483344, 0.006305504938917597, 0.030848470316550705, 0.001940155365820799, 9.700776829103995e-05, 0.0008730699146193596, 0.08332967296200332, 0.007857629231574237, 0.006402512707208637, 0.019110530353334872, 0.0017461398292387192, 0.02434894984105103, 0.10942476263229306, 0.01658832837776783, 0.0016491320609476792, 0.05432435024298237, 0.024445957609342067, 0.0015521242926566392, 0.06033883187702685, 0.04832104206584236, 0.0687064816873696, 0.0005662622117090901, 0.00018875407056969672, 0.006983900611078779, 0.052662385688945386, 0.016610358210133312, 0.4371544274394176, 0.012269014587030287, 0.002642556987975754, 0.00018875407056969672, 0.003397573270254541, 0.07625664451015747, 0.002831311058545451, 0.006040130258230295, 0.019819177409818155, 0.0009437703528484836, 0.003963835481963631, 0.08135300441553929, 0.01038147388133332, 0.00811642503449696, 0.06379887585255749, 0.03510825712596359, 0.0007550162822787869, 0.0403933711019151, 0.9937379282241283, 0.9935755950017762, 0.001223642594445017, 0.00020394043240750282, 0.0033310270626558795, 0.0008837418737658455, 0.8220838830346439, 0.007817716575620942, 0.000747781585494177, 0.005778312251545913, 0.0010197021620375142, 6.798014413583427e-05, 0.0025832454771617026, 0.13738787129852106, 0.000747781585494177, 0.013324108250623518, 0.00040788086481500564, 0.0005438411530866742, 0.0016315234592600225, 0.00013596028827166855, 0.9918546900890867, 0.9970762228177135, 0.00785023989038865, 0.9812799862985812, 0.0026167466301295498, 0.0052334932602590995, 0.0013083733150647749, 0.9996572233834116, 0.9481856016713949, 0.12354684589313902, 0.47608816411050386, 0.1141037748694596, 0.000786922585306618, 0.2832921307103825, 0.002360767755919854, 0.020009127370420935, 0.24900247394301606, 0.7292215308331185, 0.9951893550726828, 0.16213200737743955, 0.0723938265499265, 0.0678692123905561, 0.026393582596327368, 0.009049228318740813, 0.07541023598950677, 0.04072152743433365, 0.08822997610772292, 0.38383810118658945, 0.030918196755697777, 0.04298383451401886, 0.983446771029733, 0.020659831823138672, 0.005425814418198035, 0.029841979300089194, 0.007512666117504972, 0.000626055509792081, 0.032763571679118905, 0.1811387274998421, 0.0008347406797227746, 0.006469240267851503, 0.006886610607712891, 0.028589868280505033, 0.01606875808466341, 0.034433053038564455, 0.011477684346188151, 0.08472617899186163, 0.011895054686049539, 0.07366586498553486, 0.00459107373847526, 0.12082871338987163, 0.03338962718891099, 0.02045114665320798, 0.0016694813594455493, 0.04382388568544567, 0.0029215923790297115, 0.052171292482673416, 0.06531845818830712, 0.04653679289454469, 0.05363208867218827, 0.001252111019584162, 0.8913991053680745, 0.09978348194418744, 0.0006745053081679805, 0.0040470318490078835, 0.08633667944550151, 0.7837751680911934, 0.0006745053081679805, 0.12410897670290842, 0.9681953146621239, 0.9950574317250513, 0.9918490404661605, 0.9942627086169731, 0.056196644126466494, 0.37617345456083695, 0.0022937405765904692, 0.5654070521295507, 0.9929685463032005, 0.9845141985443397, 0.9931495193832187, 0.993009641685103, 0.20824341783413877, 0.5726693990438816, 0.06652220291923877, 0.15184415883739286, 0.9911913610233813, 0.98943703738841, 0.9870736067637272, 0.008231402332908673, 0.27163627698598625, 0.004115701166454337, 0.7161320029630546, 0.03627334892289437, 0.9612437464567009, 0.017904467689619827, 0.0734083175274413, 0.313328184568347, 0.5944283272953783, 0.9963987587337358, 0.9898308933549121, 0.009930289690384368, 0.031209481884065155, 0.008227954314889905, 0.22300593418977466, 0.09447961333994269, 0.005390728689065799, 0.0002837225625824105, 0.02780481113307623, 0.004823283563900978, 0.0190094116930215, 0.009362844565219547, 0.01702335375494463, 0.004823283563900978, 0.01163262506587883, 0.002269780500659284, 0.010214012252966777, 0.053623564328075586, 0.05702823507906451, 0.05277239664032835, 0.002837225625824105, 0.006809341501977852, 0.004539561001318568, 0.12114953422268929, 0.17165215036235834, 0.035181597760218904, 0.013618683003955703, 0.001702335375494463, 0.11177538804266593, 0.8824372740210469, 0.9998397014558916, 0.9910361288237545, 0.9972750818228092, 0.9963793742571372, 0.8500402369332055, 0.1086517596080037, 0.03195639988470697, 0.9595310748850223, 0.9889346309483632, 0.989012008086517, 0.0968853474128253, 0.8719681267154278, 0.021530077202850068, 0.9993313585524412, 0.997688202096377, 0.17104739203789446, 0.8256326038752213, 0.9885612990365553, 0.9890894819323659, 0.9848683804005068, 0.9969972343088133, 0.0006673496174187618, 0.0010010244261281426, 0.8862402919321155, 0.0030030732783844277, 0.0020020488522562853, 0.0003336748087093809, 0.0013346992348375235, 0.08608810064702026, 0.0006673496174187618, 0.009009219835153282, 0.0003336748087093809, 0.005338796939350094, 0.0016683740435469042, 0.002669398469675047, 0.9965636282792089, 0.0036849613066030143, 0.9912545914762109, 0.9957571297704341, 0.9934268645958731, 0.9769099424416109, 0.2650719415712328, 0.7289478393208904, 0.9993759479307511, 0.9972784988343079, 0.0002543350361264935, 0.004578030650276884, 8.477834537549785e-05, 0.7898798438635134, 8.477834537549785e-05, 0.0066127109392888316, 0.002797685397391429, 0.01059729317193723, 0.00237379367051394, 0.0007630051083794806, 0.000508670072252987, 0.007375716047668313, 0.14454707886522383, 0.002289015325138442, 0.004408473959525888, 0.0001695566907509957, 0.0002543350361264935, 0.0050867007225298706, 0.009749509718182251, 0.0025433503612649353, 0.005001922377154373, 0.00014690178472262708, 0.010870732069474405, 0.7552220752590258, 0.0211538570000583, 0.0019097232013941521, 0.011899044562532794, 0.0007345089236131354, 0.00029380356944525417, 0.00954861600697076, 0.17672284702132038, 0.0008814107083357626, 0.0064636785277955916, 0.0008814107083357626, 0.0030849374791751686, 0.00029380356944525417, 0.994572982537145, 0.9998352646362008, 0.9910900404278271, 0.9964256734011858, 0.9931941322959271, 0.9815558909288404, 0.9956708480075747, 0.9973122535233372, 0.9953084348752039, 0.99693923577189, 0.019135162277544453, 0.0041858167482128495, 0.05222304704913174, 0.0924866176747982, 0.0007972984282310189, 0.0007972984282310189, 0.03926694759037768, 0.0007972984282310189, 0.004385141355270604, 0.5118655909243142, 0.004983115176443868, 0.08969607317598963, 0.00039864921411550945, 0.04863520412209215, 0.020929083741064246, 0.10823326163236081, 0.0009966230352887737, 0.9918177760427226, 0.7285215200429673, 0.2709864414175997, 0.9969830331245982, 0.9793805213062593, 0.9947249563919935, 0.9984874043402524, 0.9991251424351706, 0.995740042737834, 0.7394095769305452, 0.21618330850935433, 0.04073018855973343, 0.9832116012410591, 0.9980148397178973, 0.010961074998800236, 0.40418964058075874, 0.18633827497960403, 0.13290303436045287, 0.19592921560355422, 0.0698768531173515, 0.05366376870773535, 0.20900625707223242, 0.5761794113883164, 0.09414696264514974, 0.002824408879354492, 0.022595271034835936, 0.03860025468451139, 0.0009414696264514973, 0.029110052798729292, 0.9708147266070518, 0.9900214095827485, 0.9899789022403211, 0.9888455299862497, 0.99794365557405, 0.9918529669298948, 0.9978641612137064, 0.9765716285548718, 0.0024849150853813533, 0.01739440559766947, 0.9888406911645781, 0.9950039103037818, 0.9894185055791547, 0.999044184266914, 0.9979076568129907, 0.9971968165168822, 0.0923024278450066, 0.017020305843760082, 0.01898418728727086, 0.0013092542956738525, 0.03534986598319401, 0.0009819407217553893, 0.002291195017429242, 0.6664104364979909, 0.03796837457454172, 0.12012408162807596, 0.007200898626206188, 0.15608922621213886, 0.07736596429645144, 0.7119026012892768, 0.054291904769439604, 0.04553066757208645, 0.9530396553157188, 0.9972271696584737, 0.9969201110066602, 0.8954177058992961, 0.10089213587597703, 0.9822066921360233, 0.9967107263900389, 0.14879868321195644, 0.022044249364734286, 0.049599561070652144, 0.7715487277657, 0.9842704819469547, 0.9842962258642294, 0.9937845019602027, 0.0873976996956251, 0.1939136461996682, 0.2822217386004561, 0.198465609725482, 0.23761249604748075, 0.8100807958539505, 0.18934159371885165, 0.9832939579966965, 0.9998012431782504, 0.4489686696320107, 0.5076573846165873, 0.0410821004892036, 0.9893010250823605, 0.9995536606816423, 0.9955694357452448, 0.9985661228889956, 0.9984708829834364, 0.9882202943789336, 0.9969916163015835, 0.753378183070331, 0.24120012975769492, 0.9939182631425415, 0.9988137640831318, 0.9964181830231809, 0.9953080307016879, 0.9909848981724957, 0.9712995660067667, 0.9937006287462925, 0.9668801304615326, 0.9815744976404072, 0.15247340266701986, 0.2329066196702643, 0.10001695679533872, 0.016086643400648883, 0.2626319389975503, 0.23605400642256516, 0.9988288057238959, 0.9949610442913785, 0.24697766824744732, 0.07109963176820452, 0.6773175447392116, 0.789852740344522, 0.20564213358082228, 0.9915195375500357, 0.07412562015226515, 0.006376397432452916, 0.03586723555754765, 0.27099689087924894, 0.009564596148679374, 0.04224363299000057, 0.01434689422301906, 0.01594099358113229, 0.027099689087924893, 0.09405186212868051, 0.01594099358113229, 0.06296692464547254, 0.21360931398717267, 0.11477515378415248, 0.007473996168925393, 0.01868499042231348, 0.01743932439415925, 0.027404652619393108, 0.05107230715432352, 0.031141650703855803, 0.34754082185503077, 0.032387316732010035, 0.07100296360479123, 0.07100296360479123, 0.08844228799895049, 0.016193658366005018, 0.07100296360479123, 0.09467061813972164, 0.056054971266940444, 0.991796207055222, 0.09698887860959421, 0.901996571069226, 0.0007890998083738394, 0.13256876780680502, 0.0007890998083738394, 0.004734598850243036, 0.0007890998083738394, 0.0015781996167476787, 0.8577514917023633, 0.8353827891320389, 0.16065053637154594, 0.4145666156412766, 0.07920841346612989, 0.07154308313069797, 0.008942885391337247, 0.007665330335431925, 0.0019163325838579813, 0.01213677303110055, 0.4030686201381287, 0.0013537894460273444, 0.1332128814890907, 0.5759020303400323, 0.11859195547199537, 0.14404319705730945, 0.026263515252930483, 0.0005415157784109378, 0.3266023856587847, 0.10673698318007249, 0.09267582371922461, 0.023009170026841975, 0.36175528431090437, 0.056244637843391496, 0.031318036980979354, 0.9926731021490265, 0.36526825150879944, 0.6309178889697444, 0.1995373868193308, 0.7912689477318291, 0.9926259487742614, 0.9972882366908175, 0.16388009217878494, 0.834450673440956, 0.055339973517843095, 0.055339973517843095, 0.12577266708600704, 0.006917496689730387, 0.06225747020757349, 0.4653588682182261, 0.2282773907611028, 0.005629780837484565, 0.0408159110717631, 0.028852626792108396, 0.13722590791368627, 0.388454877786435, 0.09218766121380975, 0.0014074452093711413, 0.04644569190924766, 0.0007037226046855707, 0.2575624733149189, 0.9932341299342932, 0.002813694419077318, 0.0074355275618504186, 0.5177831011252201, 0.0020278711532319325, 0.0020278711532319325, 0.004731699357541175, 0.12707992560253442, 0.0027038282043092433, 0.2264456121108991, 0.0013519141021546216, 0.1068012140702151, 0.9900891583106463, 0.004693991506045174, 0.1015075663182269, 0.6935372450181746, 0.12380402597194148, 0.07569061303497844, 0.9857335607016467, 0.9859177076398022, 0.10634545026943451, 0.872032692209363, 0.014179393369257933, 0.9777314707805947, 0.010513241621296717, 0.054005190841096305, 0.9180882442986372, 0.024002307040487246, 0.9838213038102364, 0.08264595445234707, 0.09788235946311497, 0.10065261491961823, 0.0060022201557570504, 0.0004617092427505423, 0.004617092427505423, 0.01615982349626898, 0.2534783742700477, 0.39476140255171366, 0.026317426836780913, 0.008310766369509762, 0.0004617092427505423, 0.007849057126759219, 0.11798082507535305, 0.867729939263887, 0.011417499200840618, 0.9645390279688315, 0.9909000204007491, 0.016312014514650783, 0.27186690857751306, 0.13955834640312337, 0.5691080619555939, 0.0018124460571834204, 0.9930609282879754, 0.9922054151776566, 0.9956339005955444, 0.9864995911959892, 0.010446775530718476, 0.9889614169080158, 0.494424826529536, 0.14372814724695815, 0.007186407362347907, 0.19834484320080223, 0.15666368049918436, 0.058952563977656726, 0.6021583320574937, 0.3347663454445507, 0.9941383017383298, 0.995247618293752, 0.005159293877464319, 0.0004299411564553599, 0.006879058503285758, 0.567522326521075, 0.2188400486357782, 0.013758117006571516, 0.021926998979223355, 0.13027217040597405, 0.027086292856687674, 0.008168881972651839, 0.09300317884550004, 0.46339844763888277, 0.06388914025038699, 0.004852339765852176, 0.054993184012991324, 0.06874148001623916, 0.09785551861135221, 0.1391004066210957, 0.012939572708939135, 0.9948256653948997, 0.995747751008157, 0.361985861741864, 0.36691563028698665, 0.2707851436570948, 0.9911448092599134, 0.08925565005673723, 0.07933835560598865, 0.47450439602812444, 0.09841007570358208, 0.2219948219359875, 0.035091964979571906, 0.9248933862958275, 0.06679785567692087, 0.987710829324258, 0.9972787488840641, 0.9964092683399489, 0.45482708835230984, 0.15025537740210235, 0.39391274616226835, 0.016581049013718117, 0.2591556549551498, 0.0006141129264340044, 0.3574137231845905, 0.00429879048503803, 0.03684677558604026, 0.04974314704115435, 0.05772661508479641, 0.0006141129264340044, 0.01289637145511409, 0.20265726572322143, 0.9968202234547854, 0.9924786605175342, 0.9332347096334085, 0.9906876842505759, 0.9835423839336932, 0.9540307224728817, 0.020069574652137972, 0.8061279151942086, 0.04641089138306906, 0.12710730613020715, 0.9708548875786859, 0.9967233610523561, 0.9919561280706369, 0.990276880253211, 0.01504538053213896, 0.9842186431440904, 0.9878085974010551, 0.9811042557486757, 0.9963425354162565, 0.991610080888745, 0.9835135125844335, 0.31904844549171396, 0.6782058384166719, 0.16584569085394488, 0.8339669025798372, 0.9979134402595395, 0.6751798892830723, 0.3244370896555023, 0.9878694217352456, 0.9885539878142653, 0.9963055818900829, 0.058780694510524804, 0.6939914255113574, 0.04171533158811438, 0.11566523758522622, 0.04171533158811438, 0.04550763445976114, 0.9974527460659238, 0.9961819767267631, 0.9954304858203803, 0.9976671002818768, 0.9981871392037878, 0.9859853195501698, 0.9790080862572924, 0.9797201179676668, 0.1970249478375632, 0.003615136657569967, 0.007832796091401596, 0.031933707141868044, 0.6266236873121276, 0.13255501077756546, 0.9817766710620607, 0.7318809315580415, 0.09088717450720776, 0.05740242600455227, 0.11480485200910454, 0.9971592378794347, 0.9966619213040191, 0.7429872151021718, 0.19626077380057366, 0.031541910075092196, 0.024532596725071708, 0.9186003935547316, 0.07841710676686733, 0.9950772463579926, 0.9974603315180285, 0.037346819988072114, 0.13693833995626442, 0.212763701750229, 0.03055648908114991, 0.01131721817820367, 0.007922052724742569, 0.06790330906922203, 0.15617761085921064, 0.1097770163285756, 0.02263443635640734, 0.20823681447894754, 0.3103228156327326, 0.08395206606489385, 0.41676204225072305, 0.02398630458996967, 0.002998288073746209, 0.01649058440560415, 0.14541697157669115, 0.9870192051955615, 0.18121633685358612, 0.8121176577512563, 0.9881948766699943, 0.997026782975694, 0.935592466536915, 0.05916790302209739, 0.9863896574526202, 0.9894282690974263, 0.9843900938840231, 0.9928796776661439, 0.9889835313593472, 0.985553163804527, 0.9922770864414474, 0.998724016627153, 0.9825958025540436, 0.9962946539396521, 0.9766700772082019, 0.9986211543283067, 0.9776074606400061, 0.9135576554658248, 0.0848817349172971, 0.9962183628794413, 0.9908180169116089, 0.9998735618793353, 0.9957421532124597, 0.991159094141497, 0.9889797306415693, 0.8788699730425987, 0.12038161508150538, 0.9927302344045598, 0.9925203130593642, 0.9955561964666932, 0.9988302234819131, 0.9919507921570727, 0.15169632293946506, 0.8469711364120133, 0.14778953602675335, 0.8492033657410271, 0.9909247873878666, 0.9906485760665633, 0.9875327800017315, 0.9947390979286885, 0.9965607151988541, 0.9901702416845087, 0.02570482666525383, 0.9639309999470186, 0.9741464932002787, 0.020640029835194917, 0.023486930502118353, 0.4740089610427522, 0.21494100035271946, 0.1167229273438609, 0.020640029835194917, 0.014946228501348044, 0.0348745331698121, 0.019928304668464056, 0.04768558617096756, 0.003558625833654296, 0.007828976834039452, 0.9946573318243648, 0.9927530132145601, 0.9954751704497043, 0.9902110616107264, 0.9980546678795736, 0.6818456807952544, 0.18217652938622242, 0.007371304657245994, 0.12794478797934117, 0.9887428387600821, 0.5830084318437592, 0.004951239336252731, 0.18567147510947743, 0.09531135722286509, 0.011140288506568646, 0.028469626183453205, 0.009902478672505462, 0.06931735070753824, 0.009902478672505462, 0.9957548255362488, 0.1383241907242489, 0.0005692353527746868, 0.0017077060583240602, 0.03244641510815714, 0.006830824233296241, 0.17873990077125165, 0.6369743597548745, 0.0034154121166481205, 0.9996209789567819, 0.9824336655169098, 0.9975894297986652, 0.7116084139243098, 0.2806343040828264, 0.9840772985656676, 0.11248333131638259, 0.03574236696034587, 0.826279424436231, 0.02417866000258691, 0.06930295991923219, 0.7867924273183419, 0.14268256453959569, 0.23648000262499755, 0.09074232658866185, 0.6709432632616209, 0.997804852050694, 0.9969984134495921, 0.028677666149268775, 0.1261817310567826, 0.0028677666149268775, 0.8402556181735751, 0.9934550855291959, 0.02173659993571917, 0.24760822535471402, 0.08694639974287668, 0.11813369530282158, 0.02173659993571917, 0.04725347812112863, 0.05670417374535436, 0.008505626061803153, 0.39125879884294507, 0.002477860616730626, 0.048731258795702316, 0.06194651541826565, 0.014041210161473548, 0.7441841385580981, 0.004955721233461252, 0.009911442466922504, 0.00660762831128167, 0.042123630484420645, 0.03716790925095939, 0.027256466784036888, 0.0369472419315069, 0.07936814933434815, 0.03010516009233895, 0.5158929706732629, 0.2408412807387116, 0.024631494621004595, 0.02052624551750383, 0.05199982197767637, 0.29484673138638406, 0.005397001826918398, 0.015906952753022646, 0.053685965541451434, 0.0073853709210462285, 0.055106229180114166, 0.4013665042860893, 0.02982553641191746, 0.11532540745941419, 0.02073584912447595, 0.989330968307953, 0.9977631391305812, 0.15178790474804488, 0.057433261256016985, 0.1599926563560473, 0.6276634980121856, 0.9213330816562708, 0.0783446498007033, 0.9969111483889405, 0.9945859454940519, 0.015138058094463353, 0.1362425228501702, 0.12615048412052796, 0.0050460193648211174, 0.6383214496498714, 0.01766106777687391, 0.01766106777687391, 0.04036815491856894, 0.9355636748998434, 0.05727940866733735, 0.149682547760074, 0.0315657073507639, 0.04378469084138219, 0.25048916155767487, 0.07127740369527333, 0.064149663325746, 0.13135407252414658, 0.0030547458726545716, 0.25354390743032945, 0.009013387553301513, 0.10365395686296741, 0.011266734441626892, 0.13294746641119734, 0.17275659477161234, 0.012768965700510478, 0.04281359087818219, 0.04281359087818219, 0.0210312376243702, 0.1795166354365885, 0.03380020332488068, 0.03680466584264785, 0.006760040664976135, 0.08562718175636438, 0.06159148161422701, 0.012768965700510478, 0.03229797206599709, 0.990008105921232, 0.051498700850463874, 0.8462696960650855, 0.001537274652252653, 0.07532645796038, 0.020753207805410814, 0.0038431866306316326, 0.04398073862652746, 0.9510834727986562, 0.007906012803328758, 0.9905104612170459, 0.020981256513057562, 0.254772400515699, 0.7223546885209818, 0.9826495313260941, 0.9986465631001958, 0.009077990090736957, 0.005043327828187199, 0.007817158133690157, 0.5456880710098548, 0.09960572460669717, 0.0007564991742280798, 0.003278163088321679, 0.008069324525099517, 0.009077990090736957, 0.03000780057771383, 0.0005043327828187198, 0.0005043327828187198, 0.015129983484561595, 0.0007564991742280798, 0.02395580718388919, 0.0007564991742280798, 0.017147314615836476, 0.008321490916508877, 0.07615425020562669, 0.009834489264965036, 0.0012608319570467997, 0.013364818744696076, 0.006051993393824638, 0.016390815441608395, 0.0015129983484561595, 0.04362478571381927, 0.046902948802140944, 0.9975995989571106, 0.9862817957770009, 0.9881246846712635, 0.9933687851668002, 0.9978906035369507, 0.9852470984529343, 0.06089532556803168, 0.012881703485545163, 0.46725451733932, 0.4391489824617669, 0.018737023251702054, 0.9782638424083611, 0.994366593400013, 0.9879343067514827, 0.1111792562041081, 0.0019505132667387386, 0.031208212267819817, 0.0058515398002162156, 0.0029257699001081078, 0.07021847760259459, 0.7763042801620179, 0.07857591694783607, 0.8362722589448267, 0.08418848244411006, 0.9970177115654912, 0.9905361560596085, 0.996242469723427, 0.10314195678947004, 0.889599377309179, 0.9929086079139752, 0.9953790022063025, 0.13675639874213008, 0.41950949343869637, 0.4278257609297718, 0.013860445818459131, 0.13690904163844958, 0.7742442354726113, 0.08497802584455491, 0.993503857463702, 0.2573674876830822, 0.5923537414928083, 0.14706713581890413, 0.9882762413053985, 0.9867044793701761, 0.17782046462690068, 0.10573108707545446, 0.03364170952400824, 0.6776401489835945, 0.08824931136383056, 0.8630094994410963, 0.04813598801663485, 0.9946677104432695, 0.998175643141683, 0.00044738640006334153, 0.5480483400775934, 0.13824239761957252, 0.3127230936442757, 0.999754237141895, 0.9954431519456362, 0.4605354689645633, 0.5388838267552152, 0.9955481959662554, 0.9917332688326151, 0.9964064921986996, 0.988288209756279, 0.01812523997580072, 0.9742316486992886, 0.9859154009315482, 0.011830984811178578, 0.0019718308018630963, 0.9741163948318399, 0.020793946030893132, 0.003199068620137405, 0.30925580795129737, 0.6891605720293997, 0.001826963580841519, 0.03653927161683038, 0.16808064943741974, 0.4506510166075747, 0.043238138079915944, 0.02801344157290329, 0.017660647948134683, 0.194876115289762, 0.042020162359354934, 0.014615708646732152, 0.001826963580841519, 0.9913952074637111, 0.9883227269697351, 0.982962735780369, 0.9822662823166687, 0.12986899392051224, 0.865793292803415, 0.9966828823880884, 0.9826671023238561, 0.7828741728925569, 0.006470034486715346, 0.20704110357489108, 0.9871666334724933, 0.745687771342712, 0.07721366511042956, 0.014077629550520306, 0.006825517357828027, 0.04137969898183241, 0.11432741574361946, 0.7324112555332151, 0.26633136564844184, 0.9951586681716984, 0.012195221880316333, 0.1600013110697503, 0.05024431414690329, 0.00048780887521265333, 0.00292685325127592, 0.0068293242529771465, 0.00292685325127592, 0.00048780887521265333, 0.00146342662563796, 0.012683030755528986, 0.3624419942830014, 0.05731754283748677, 0.00024390443760632666, 0.031219768013609813, 0.00073171331281898, 0.04219546770589451, 0.14609875812618967, 0.022195303822175726, 0.0009756177504253067, 0.08634217091263964, 0.970549225659224, 0.0024508818829778382, 0.022057936946800544, 0.0024508818829778382, 0.9958465653532811, 0.9991048021366701, 0.9765678948468115, 0.9894282052314741, 0.9830889375376775, 0.9837827934223544, 0.9914287091117124, 0.9894825729338717, 0.09473156023109804, 0.021240260141501804, 0.046728572311303965, 0.012319350882071046, 0.01189454567924101, 0.007221688448110612, 0.017841818518861515, 0.0016992208113201443, 0.08326181975468706, 0.011044935273580938, 0.014868182099051261, 0.0012744156084901082, 0.5148639058300037, 0.004248052028300361, 0.1567531198442833, 0.7677447936850406, 0.21399783042370382, 0.015443142401710585, 0.985094029525336, 0.9857841105278168, 0.006548952840712814, 0.022453552596729646, 0.11694558644130025, 0.001871129383060804, 0.6156015670270045, 0.002806694074591206, 0.026195811362851255, 0.002806694074591206, 0.12255897459048266, 0.07203848124784096, 0.010291211606834423, 0.9918106553267653, 0.36685089201635446, 0.0038820200213370844, 0.032997170181365214, 0.1106375706081069, 0.48525250266713554, 0.9857878269586383, 0.9859013537579618, 0.03886693285280339, 0.2857862709764955, 0.38409674819240996, 0.14174999040434177, 0.14632257073996568, 0.9970220396357123, 0.9919272820367914, 0.9975397890312996, 0.9704177120631526, 0.988525407024121, 0.9845887566283116, 0.9859165459063821, 0.09573449528270868, 0.45952557735700167, 0.10965951277837539, 0.0661438331044169, 0.2680565867915843, 0.9974334029731119, 0.04623482726387998, 0.07145382395326906, 0.03993007809153271, 0.18599010058424448, 0.01260949834469454, 0.12924735803311904, 0.07986015618306543, 0.17338060223954993, 0.25954550759496264, 0.20667456852161908, 0.015898043732432237, 0.7776217043037508, 0.9219479725622102, 0.07560393396865506, 0.995125421224803, 0.998800852477332, 0.9930934692462539, 0.9905536745444598, 0.9815602585489063, 0.9994523328499306, 0.6865694106509121, 0.16113363719358142, 0.1471220165680526, 0.9951428061804426, 0.9748582636961397, 0.07774689528155408, 0.13422968245191388, 0.04784424325018713, 0.18539644259447513, 0.0013290067569496425, 0.03787669257306481, 0.4292691824947345, 0.08638543920172675, 0.8140271042770453, 0.18451281030279693, 0.995267680276713, 0.9950991430523807, 0.9988873095191887, 0.9982246571571927, 0.9971894271821207, 0.9933793575130984, 0.9017782034034959, 0.09644686667417068, 0.16942571665610912, 0.0049150805173273976, 0.44177900179272134, 0.057246231907695566, 0.05319851854048477, 0.0011564895334887995, 0.2720641627532401, 0.9926814978410399, 0.996651746591325, 0.9969387875399749, 0.16150602480154774, 0.06987140080066959, 0.004581731200043907, 0.7273498280069703, 0.009163462400087814, 0.005727164000054884, 0.019472357600186608, 0.08449670479316435, 0.0751081820383683, 0.7088334679871009, 0.09388522754796039, 0.03285982964178614, 0.07332270032424502, 0.047444100209805606, 0.7979235035285488, 0.07763580034331827, 0.6769512287950081, 0.24818692332560346, 0.07445607699768104, 0.9968072168965696, 0.9988894315221397, 0.9990304125407146, 0.05866012658691103, 0.12966975350790858, 0.16671825451016817, 0.03704850100225959, 0.020067938042890615, 0.013893187875847349, 0.5742517655350238, 0.15991730482674651, 0.1827626340877103, 0.0027414395113156543, 0.3253174886761243, 0.06122548241938295, 0.0639669219306986, 0.0657945482715757, 0.06488073510113715, 0.07310505363508411, 0.7261953679386411, 0.11776141101707693, 0.10467680979295728, 0.049721484651654706, 0.9964766153739275, 0.9898922351107758, 0.9245463223282082, 0.06748513301665753, 0.1261638778401392, 0.5638752907549078, 0.30639798904033805, 0.6791673300377904, 0.3136354115456906, 0.006769109601705551, 0.9913262408265611, 0.001664975468665809, 0.008324877343329045, 0.9457060662021796, 0.039959411247979415, 0.001664975468665809, 0.9980266626219086, 0.5332056059066904, 0.3393126583042575, 0.12279886681487415, 0.9515519544982599, 0.04498245603082683, 0.12679014698370872, 0.09944325253624214, 0.008286937711353512, 0.051379013810391776, 0.05883725775060993, 0.018231262964977728, 0.3654539530706899, 0.19225695490340147, 0.07789721448672302, 0.009068891038638829, 0.03854278691421502, 0.5622712443956074, 0.04987890071251356, 0.05894779175115238, 0.18024420939294672, 0.030607507255406046, 0.06801668278979121, 0.7827856188276525, 0.21594086036624896, 0.08398367186178794, 0.9133224314969439, 0.9749855333568175, 0.37362986184586316, 0.6234045208698932, 0.008849963142090281, 0.30679872225912974, 0.09144961913493291, 0.12094949627523384, 0.36579847653973163, 0.042774821853436354, 0.06194974199463196, 0.9678812521412961, 0.9876246601383217, 0.40314863174768517, 0.14073167459958563, 0.02433705650970278, 0.14813860484166907, 0.09417382736363249, 0.005290664458631039, 0.08570876422982283, 0.0010581328917262077, 0.0952319602553587, 0.8603167695138747, 0.09767788649502507, 0.04132525967097214, 0.9921770652593009, 0.9948072079577253, 0.022122601663084735, 0.9756067333420368, 0.9848549541462223, 0.9818211713597759, 0.9903021474967972, 0.9977550296765614, 0.987826168929404, 0.9913044953537354, 0.1778721895401395, 0.08965915245112722, 0.07664411419209262, 0.0007230576810574776, 0.07881328723526505, 0.10556642143439172, 0.36875941733931356, 0.010845865215862165, 0.04338346086344866, 0.04410651854450613, 0.0014461153621149552, 0.0014461153621149552, 0.9930547975696193, 0.9712488468428665, 0.17814051373698941, 0.8190591012037666, 0.35831013577579857, 0.07256914142294654, 0.04989128472827575, 0.5170551326384941, 0.004972876905154884, 0.06763112591010642, 0.19394219930104048, 0.7320074804387989, 0.10996848006987817, 0.469657050298438, 0.3848896802445736, 0.034365150021836925, 0.07627765531197143, 0.06818760096070174, 0.045073159957074026, 0.045073159957074026, 0.03004877330471602, 0.0011557220501813854, 0.045073159957074026, 0.013868664602176624, 0.6298685173488551, 0.045073159957074026, 0.9965968788273386, 0.9946993096714607, 0.9979700546494656, 0.9909002095832176, 0.9892838115155114, 0.009008980168882243, 0.15030772176503532, 0.07586509615900837, 0.11284933053652495, 0.009957293870869847, 0.0056898822119256275, 0.016121332933789277, 0.594592691146228, 0.011853921274845058, 0.01232807812583886, 0.0004741568509938023, 0.9896855719587903, 0.9993564123202336, 0.9863313031505003, 0.046680195102609505, 0.5829039747428417, 0.039498626625284965, 0.011370816755763853, 0.0011969280795540898, 0.3177844051216109, 0.9648420426975306, 0.03334227361373756, 0.9870137041763151, 0.9984029061415154, 0.9973020806922929, 0.9933050523600893, 0.0013059959919364218, 0.1377825771492925, 0.021548933866950958, 0.0019589939879046327, 0.04897484969761582, 0.22593730660500097, 0.01436595591130064, 0.3506599238349293, 0.017630945891141694, 0.17500346291948052, 0.003917987975809265, 0.054370013614892625, 0.05728269291569044, 0.023301434406382552, 0.051457334314094806, 0.25825756467073996, 0.07184608941967954, 0.4592324364257895, 0.023301434406382552, 0.038375817643868465, 0.9593954410967116, 0.9942393837538333, 0.04868563337726033, 0.02812015031272795, 0.2950517264156381, 0.05582059688944504, 0.0050364448321303795, 0.043649188545129954, 0.15612978979604178, 0.010912297136282488, 0.35548906440120265, 0.9954004418679752, 0.9943445530181492, 0.9931806919277785, 0.9913124555861802, 0.010007873571984438, 0.005559929762213576, 0.1364962756623433, 0.031135606668396026, 0.10786263738694338, 0.01584579982230869, 0.017513778750972764, 0.013621827917423262, 0.011397856012537831, 0.0033359578573281456, 0.03224759262083874, 0.0002779964881106788, 0.002779964881106788, 0.017513778750972764, 0.26576464263380895, 0.03474956101383485, 0.003891950833549503, 0.0016679789286640728, 0.031135606668396026, 0.023629701489407698, 0.1417782089364462, 0.0811749745283182, 0.0005559929762213576, 0.010285870060095115, 0.9804963012780046, 0.015416608510660448, 0.9852534974382996, 0.9818137595781645, 0.005795888104180556, 0.06955065725016667, 0.3361615100424722, 0.5853846985222361, 0.9870894183070887, 0.987021050931822, 0.9922363602490599, 0.035294154831031606, 0.7140279015816394, 0.24977401880422367, 0.9790252659625859, 0.009990053734312101, 0.9926679444899623, 0.9954521271724176, 0.9881284495154009, 0.8895721601341016, 0.10636188871168606, 0.8972173737940294, 0.042724636847334733, 0.024922704827611927, 0.028483091231556488, 0.9944170410746994, 0.09118951544247661, 0.007353993180844887, 0.21277553603244542, 0.20640207527571316, 0.002941597272337955, 0.024513310602816293, 0.09658244377509619, 0.00931505802907019, 0.01764958363402773, 0.1559046554339116, 0.035789433480111786, 0.1382550717998839, 0.0014707986361689775, 0.5669107371667618, 0.43185259095938616, 0.9934046110194251, 0.9954085867611377, 0.09261677022332998, 0.8732438335342542, 0.026461934349522855, 0.9836566841145475, 0.1005832777027086, 0.5018576171693039, 0.2932796623542135, 0.002117542688478076, 0.1005832777027086, 0.992894338050026, 0.6981923527475287, 0.3010095832060007, 0.9906458977574901, 0.9939029631390881, 0.9915640452292832, 0.9888541917593535, 0.993869520477404, 0.8785639696956453, 0.11882217622932909, 0.0269108118037696, 0.941878413131936, 0.029601892984146557, 0.001955533932490602, 0.12828302597138347, 0.1400162295663271, 0.0711814351426579, 0.0402839990093064, 0.2651704012457256, 0.04341285330129136, 0.04380396008778948, 0.004693281437977444, 0.12554527846589664, 0.02346640718988722, 0.05279941617724625, 0.04536838723378196, 0.012906523954437971, 0.047080787437789894, 0.07096234628304564, 0.009552623538102297, 0.002046990758164778, 0.1391953715552049, 0.7314580309175474, 0.21569812348692227, 0.7100719247635179, 0.07399862630573245, 0.0028613133728782377, 0.02268612745639174, 0.01205839207141543, 0.033007293551416815, 0.3882189108415866, 0.030861308521758134, 0.01614598260409863, 0.040467146273563646, 0.03259853449814849, 0.019211675503611025, 0.03331386284136805, 0.0007153283432195594, 0.017065690473952348, 0.0006131385799024795, 0.026364958935806617, 0.003474451952780717, 0.0022481747929757583, 0.018087588107123145, 0.006948903905561434, 0.0006131385799024795, 0.11016056485581215, 0.12048173095083722, 0.001226277159804959, 0.0017372259763903585, 0.007664232248780994, 0.0028613133728782377, 0.0027591236095611577, 0.01144525349151295, 0.009299268461854273, 0.021562040059903864, 0.003372262189463637, 0.9852916663088704, 0.9761109720724307, 0.0029759308839097157, 0.007227260718066453, 0.4952799256792598, 0.013816821961009395, 0.0014879654419548579, 0.00042513298341567365, 0.3411692191910781, 0.11839953588126512, 0.018493284778581805, 0.00021256649170783683, 0.13982991525738192, 0.11985421307775594, 0.006658567393208663, 0.2280559332173967, 0.4860754197042324, 0.018311060331323823, 0.9988714436453301, 0.016282491053233216, 0.9832815430480281, 0.9993391858860086, 0.9886122204937765, 0.9913061833471187, 0.3121250684757697, 0.07377501618518192, 0.12106669322696521, 0.07377501618518192, 0.39725008715097965, 0.020808337898384648, 0.9870213058284375, 0.9983868733909022, 0.9808520064520176, 0.9996853553196245, 0.9921256872924783, 0.9922821375256551, 0.015552193703169294, 0.08390951021244829, 0.20109348137121227, 0.0065102206199313326, 0.060400380196029585, 0.002531752463306629, 0.04412482864620125, 0.00036167892332951846, 0.011935404469874109, 0.2940449646668985, 0.00687189954326085, 0.17505259889148694, 0.021700735399771107, 0.06691060081596091, 0.009041973083237962, 0.008051098437319645, 0.11070260351314512, 0.028178844530618755, 0.04428104140525805, 0.17813055292569713, 0.1972519117143313, 0.16907306718371254, 0.22140520702629024, 0.04327465410059309, 0.11467792376660398, 0.01818064645080307, 0.05034640555607004, 0.00839106759267834, 0.04754938302517726, 0.6698868961488208, 0.08950472098856896, 0.9967217191692329, 0.7109916241825928, 0.06550402679609299, 0.17331273756466273, 0.04912802009706975, 0.065033332228257, 0.03948452313858461, 0.8942083181385337, 0.9954874302032566, 0.9958885454991284, 0.9894589595619391, 0.26772929030199655, 0.4996476564234713, 0.1330120040990811, 0.020463385246012475, 0.05115846311503119, 0.025579231557515594, 0.9908259459403055, 0.01657095268325775, 0.8948314448959185, 0.06904563618024062, 0.0027618254472096247, 0.011047301788838499, 0.9884437000489108, 0.9800606970371908, 0.009769677647944614, 0.042631320645576495, 0.07016404856251132, 0.005772991337421818, 0.13055841947707802, 0.019095279039164473, 0.006217067594146573, 0.005328915080697062, 0.010657830161394124, 0.017763050268990208, 0.6092726242263641, 0.0031085337970732864, 0.06927589604906181, 0.002385989411279101, 0.009842206321526291, 0.030719613670218422, 0.003877232793328539, 0.04115831734456449, 0.0005964973528197752, 0.006561470881017528, 0.04294780940302382, 0.005964973528197752, 0.0017894920584593257, 0.015807179849724042, 0.015508931173314157, 0.647497876485866, 0.030719613670218422, 0.02863187293534921, 0.010736952350755955, 0.0793341479250301, 0.02624588352407011, 0.9717922118159203, 0.049507797987484566, 0.09901559597496913, 0.022601386037764692, 0.11623569962278985, 0.009686308301899154, 0.3400970470444592, 0.35085961182434716, 0.011838821257876744, 0.9920093244199601, 0.9866930327596826, 0.9960363937857772, 0.04386471187282366, 0.9540574832339147, 0.9999099634061499, 0.992188458609172, 0.009349203937660257, 0.6856082887617522, 0.30229092731768165, 0.9885466907096806, 0.8648827073574156, 0.1322761787723106, 0.9917144173317498, 0.9988351668933493, 0.9780278737147299, 0.0025970741155017705, 0.8461267468304768, 0.04311143031732939, 0.1075188683817733, 0.20233999276558842, 0.7941844716049345, 0.0006524242007690413, 0.0008155302509613017, 0.03164257373729851, 0.27401816432299736, 0.0013048484015380827, 0.00473007545557555, 0.00016310605019226033, 0.00032621210038452067, 0.00032621210038452067, 0.001467954451730343, 0.4495202743298695, 0.0006524242007690413, 0.0022834847026916446, 0.19018165452417554, 0.0004893181505767811, 0.00016310605019226033, 0.0030990149536529463, 0.0071766662084594545, 0.02332416517749323, 0.001467954451730343, 0.006034923857113633, 0.00032621210038452067, 0.9994854165564611, 0.039469966909273756, 0.9563876597247103, 0.9941303896097005, 0.993535752416167, 0.9838619427675135, 0.9946046997600604, 0.9891614456509489, 0.3835333861556372, 0.6157057704170764, 0.4524992548854382, 0.5411331295537198, 0.1918154898809409, 0.21656587567203006, 0.5898841946876248, 0.9890639077841925, 0.9970278948707368, 0.9896755323359437, 0.006274610762581295, 0.00033916814932871865, 0.034086399007536225, 0.5063780469477769, 0.028659708618276724, 0.0016958407466435932, 0.001526256671979234, 0.001187088522650515, 0.003730849642615905, 0.28812334285474644, 0.00237417704530103, 0.08852288697479556, 0.005935442613252576, 0.00016958407466435933, 0.0006783362986574373, 0.021876345631702352, 0.008309619658553606, 0.9989124563435418, 0.058013930348557875, 0.8774606965219378, 0.058013930348557875, 0.9908836355221613, 0.9941491660942658, 0.9915989164666025, 0.9973774991816831, 0.030686886529217774, 0.9666369256703599, 0.7832898999934995, 0.0022252553977088053, 0.21362451818004533, 0.993755982945437, 0.9814179115336831, 0.001083247090180591, 0.019137365259857105, 0.16754221661459806, 0.08052136703675726, 0.02563684780094065, 0.06535590777422898, 0.0018054118169676513, 0.016970871079495922, 0.05307910741884895, 0.0018054118169676513, 0.030330918525056545, 0.009388141448231788, 0.1097690384716332, 0.02347035362057947, 0.0859376024876602, 0.04369096597061717, 0.18379092296730692, 0.016970871079495922, 0.03538607161256597, 0.029247671434875954, 0.10892757282205033, 0.8850365291791589, 0.9286911757035209, 0.0039857990373541674, 0.04384378941089584, 0.023914794224125003, 0.997551728084304, 0.9257806990125899, 0.06826032803779465, 0.9945867387135898, 0.9877318074451729, 0.9783324581344547, 0.9991378107034898, 0.9950300979631029, 0.9941365577134408, 0.995263355263221, 0.9843878999632606, 0.9898997687089104, 0.9987694544895535, 0.06702479197849753, 0.9294104487684991, 0.9817670719411379, 0.006590059723798333, 0.006223945294698425, 0.7153875944612189, 0.18086052797535423, 0.006590059723798333, 0.04027258720098981, 0.002196686574599444, 0.042103159346489345, 0.005111729104109744, 0.8347453627011212, 0.0005111729104109744, 0.0010223458208219488, 0.10274575499260585, 0.007667593656164616, 0.0010223458208219488, 0.0020446916416438977, 0.0005111729104109744, 0.0005111729104109744, 0.0219804351476719, 0.02249160805808287, 0.04171820392742295, 0.9511750495452433, 0.9795613843142992, 0.9913050122742137, 0.38742386107647275, 0.19772668557529827, 0.3131508928908277, 0.10036887592654734, 0.9177240015692162, 0.07471735234014858, 0.006497161073056398, 0.9869840435858115, 0.9847849044217383, 0.9785560280833421, 0.9912589986329148, 0.0347937657165981, 0.7284944696912726, 0.006523831071862143, 0.228334087515175, 0.9958895730096514, 0.031293887641412825, 0.14305777207503007, 0.8117251263721572, 0.0022352776886723448, 0.011176388443361723, 0.9994881849885758, 0.9861338184374291, 0.18775283284016908, 0.473240017021796, 0.16974913654042684, 0.16717717992617795, 0.9920908810176544, 0.5448403744074884, 0.0005890166209810686, 0.10425594191364913, 0.15550038793900212, 0.04947739616240976, 0.0035340997258864115, 0.005301149588829617, 0.13547382282564577, 0.9811993657774554, 0.9859728475770845, 0.3413909602776631, 0.6517463787119022, 0.9813930232587234, 0.9667656780794548, 0.9931816294761284, 0.14065696919369733, 0.8588788384393908, 0.9974480425285277, 0.11549914516317461, 0.018479863226107937, 0.19496255703543874, 0.010163924774359366, 0.08408337767879112, 0.07207146658182095, 0.053591603355713016, 0.11642313832448001, 0.33171354490863747, 0.0027719794839161905, 0.9870994392818483, 0.9904046215824135, 0.9850519189893824, 0.9922498007437695, 0.9971401015712212, 0.9751529523178982, 0.49166687445791474, 0.38866946597463614, 0.11660083979239085, 0.9781707527913318, 0.2855185987160313, 0.003947261733401355, 0.7091913581011101, 0.15241187157889435, 0.000143514003369957, 0.030711996721170802, 0.44819423252437574, 0.0757753937793373, 0.06788212359398967, 0.10806604453757764, 0.11093632460497677, 0.005166504121318452, 0.000861084020219742, 0.9952074540022091, 0.12845042244876356, 0.867908259788943, 0.7415272260296581, 0.07346399762522929, 0.14922374517624698, 0.034436248886826226, 0.9761131460280471, 0.9856426068832986, 0.013662928735495771, 0.007051834186062334, 0.050685058212323025, 0.0008814792732577917, 0.2882437223552979, 0.09299606332869703, 0.08109609313971684, 0.06170354912804542, 0.002203698183144479, 0.09431828223858371, 0.007492573822691229, 0.29926221327102026, 0.8113599481086472, 0.17913141711489614, 0.9951982290469468, 0.9953091084553117, 0.983937583126947, 0.9906963186985456, 0.988642505152334, 0.9946612077807611, 0.03729868551779752, 0.9279912956828024, 0.032822843255661815, 0.9938822248400081, 0.3062477167552719, 0.048782822138007914, 0.2195226996210356, 0.4227844585294019, 0.5593504687231543, 0.2498284702202231, 0.11496531372966018, 0.07295875678997665, 0.021872871377502812, 0.9711554891611249, 0.7549069610197979, 0.23862001641430394, 0.9928395487237558, 0.992023003344747, 0.022039122470034672, 0.20826970734182765, 0.0011019561235017337, 0.4727391769822437, 0.2953242410984646, 0.9968092207219459, 0.9815143119818234, 0.9850098555586551, 0.9616830153675626, 0.9988103636638671, 0.9947322360144916, 0.02346774295224647, 0.8135484223445443, 0.08604839082490372, 0.07431451934878049, 0.9925083782917403, 0.17993026498816925, 0.13008471860631154, 0.018236175505557693, 0.6698755135708193, 0.9834119531172328, 0.981674203076528, 0.975831047296079, 0.9954727212401554, 0.8427402089279369, 0.15430454529666449, 0.9968262375108051, 0.23216913706847775, 0.7661581523259766, 0.029854567793282296, 0.025454947276377538, 0.4685595850503569, 0.04462472238574827, 0.021998102584523796, 0.0009427758250510199, 0.03142586083503399, 0.0028283274751530595, 0.078564652087585, 0.04022510186884352, 0.00031425860835034, 0.025454947276377538, 0.0015712930417516998, 0.00974201685886054, 0.004085361908554419, 0.004399620516904759, 0.034568446918537395, 0.06473727332017003, 0.025769205884727878, 0.005970913558656459, 0.04273917073564623, 0.02011255093442176, 0.0018855516501020398, 0.007856465208758498, 0.006285172167006799, 0.9898593493083707, 0.9948277930101507, 0.9941239457424633, 0.990459272749943, 0.9963144735146259, 0.9859677266369258, 0.9950614039653152, 0.995804923241587, 0.9832755690133174, 0.0008541014880003225, 0.0001971003433846898, 0.0016425028615390817, 0.012154521175389205, 0.013731323922466723, 6.570011446156327e-05, 0.699969019473495, 0.011234719572927319, 0.0030879053796934734, 0.016753529187698634, 0.00013140022892312654, 0.00013140022892312654, 6.570011446156327e-05, 0.0032193056086166003, 0.009526516596926674, 0.001445402518154392, 6.570011446156327e-05, 0.0022995040061547143, 0.001773903090462208, 0.002759404807385657, 0.005387409385848188, 0.002036703548308461, 0.004861808470155681, 0.000722701259077196, 0.13238573064005, 0.009198016024618857, 0.009132315910157294, 6.570011446156327e-05, 0.008409614651080099, 6.570011446156327e-05, 0.010840518886157939, 0.027856848531702824, 0.003679206409847543, 0.0009198016024618858, 0.0032850057230781634, 6.570011446156327e-05, 0.9977437162724077, 0.9900659800688867, 0.4362114506211983, 0.5626869599729067, 0.2929688850233888, 0.6469729544266503, 0.05950930477037585, 0.9936331466084038, 0.9050397647025729, 0.08922927257631, 0.9980349219009689, 0.9955631566382143, 0.3175475527152351, 0.48939681653759765, 0.19177381614959296, 0.986698313983147, 0.9974579061964728, 0.9982919511910161, 0.0012181502118709078, 0.004872600847483631, 0.048116933368900855, 0.8764590774411182, 0.0316719055086436, 0.0036544506356127235, 0.034108205932385414, 0.9900571468352861, 0.9914425915658636, 0.15912331924423684, 0.08395126842885599, 0.2748992515219402, 0.000548701100842196, 0.001646103302526588, 0.17009734126108078, 0.24197718547140845, 0.001646103302526588, 0.059808419991799366, 0.006584413210106352, 0.9909318711789962, 0.9889074212018799, 0.08254785858392434, 0.016795451746512743, 0.05574660579693592, 0.007504350780356758, 0.009291100966155987, 0.04359670453350117, 0.0003573500371598456, 0.5017194521724233, 0.06789650706037066, 0.006789650706037067, 0.1472282153098564, 0.05860540609421468, 0.0014294001486393824, 0.9956518076578795, 0.9868728407566997, 0.9895562742924912, 0.9955397044339719, 0.9898558227722745, 0.993712295282198, 0.9965940599846406, 0.6743745382781732, 0.132466427161784, 0.19299534483618772, 0.03190141771153427, 0.03224444370843249, 0.012691961885234065, 0.0782099272927937, 0.11251252698261549, 0.002744207975185744, 0.012691961885234065, 0.019552481823198426, 0.059000471466493495, 0.03087233972083962, 0.003087233972083962, 0.009261701916251886, 0.000686051993796436, 0.00857564992245545, 0.012691961885234065, 0.04459337959676834, 0.03190141771153427, 0.022639715795282386, 0.099134513103585, 0.09193096716872241, 0.27682197949686194, 0.005831441947269706, 0.982440709445895, 0.9960316233135655, 0.9982738197818044, 0.9980784804101877, 0.03603781351195657, 0.024025209007971046, 0.1681764630557973, 0.11171722188706536, 0.6594919872688052, 0.9868664573010982, 0.9927797949316446, 0.999168751882191, 0.9974524149492506, 0.9941737822367578, 0.21074055367894742, 0.7887977959516541, 0.754496409266724, 0.24410177946864603, 0.9904958029881953, 0.9733785248264225, 0.8822389019010404, 0.10847199613537381, 0.8785351837258728, 0.11263271586229139, 0.030144591004498605, 0.2255047288605761, 0.01855051754122991, 0.07652088485757338, 0.5930368576461937, 0.004637629385307478, 0.052173330584709125, 0.9889702979212386, 0.17397866469331977, 0.09665481371851099, 0.008786801247137363, 0.08347461184780494, 0.24734845510691675, 0.08391395191016181, 0.03470786492619258, 0.024163703429627748, 0.03426852486383571, 0.02328502330491401, 0.0593109084181772, 0.0013180201870706045, 0.048327406859255496, 0.03514720498854945, 0.04613070654747115, 0.014914097257371093, 0.979359053234035, 0.9911422059407918, 0.9807942139279568, 0.9978709724833643, 0.9865739452147769, 0.8509388711687051, 0.14663698563220506, 0.31833889279795513, 0.08671171333675644, 0.011878316895446087, 0.010690485205901478, 0.5440269138114308, 0.027320128859526, 0.08180867742461256, 0.045449265235895875, 0.0010099836719087972, 0.2151265221165738, 0.03332946117299031, 0.012119804062905566, 0.037369395860625494, 0.5302414277521185, 0.04342929789207828, 0.042145028071824894, 0.022862989084715468, 0.6412655251712964, 0.11486586082320903, 0.07905635984714866, 0.001101830799263396, 0.06307981325782942, 0.03525858557642867, 0.996152136743959, 0.9946091546208977, 0.0006164793892310229, 0.001849438167693069, 0.021576778623085803, 0.2509071114170263, 0.20775355417085473, 0.5159932487863662, 0.9989149503689974, 0.9883090884061683, 0.982276931195988, 0.2385384510521193, 0.5824776130342447, 0.06471973478158274, 0.08506022285579447, 0.027737029192106893, 0.995062905897881, 0.027886993294749887, 0.004647832215791648, 0.10358026080907101, 0.0006639760308273782, 0.005975784277446405, 0.08299700385342229, 0.1414268945662316, 0.39838561849642695, 0.21048040177227892, 0.024567113140612995, 0.9936561996469334, 0.9956907585507212, 0.9924204650721036, 0.9771820504086085, 0.06873209077539313, 0.10574167811598943, 0.04097490026994591, 0.015861251717398415, 0.011895938788048812, 0.08459334249279155, 0.15332543326818468, 0.017183022693848284, 0.5009512000745, 0.9983199152154613, 0.0009806678931389599, 0.9867646823740822, 0.9882973007297782, 0.9632368821388944, 0.03107215748835143, 0.9996298497236653, 0.03847025594764033, 0.9553446893664015, 0.9783669183434079, 0.017575453622935472, 0.8814429664271586, 0.0027631440953829424, 0.11328890791070063, 0.9686694221255633, 0.022015214139217348, 0.9905294966665896, 0.004067801109107412, 0.9925434706222086, 0.9931584088320726, 0.9835989998858337, 0.2689942415155457, 0.7296855287088366, 0.993458576430191, 0.07045910538117896, 0.016990138886241735, 0.02248694852590818, 0.05496809639666444, 0.011493329246575291, 0.16890196892793255, 0.223870065324597, 0.011493329246575291, 0.12342836190887378, 0.24335875404705073, 0.05246954656045242, 0.9905445664574978, 0.9936621628307578, 0.9994984304682298, 0.9987822029686219, 0.9960194546857475, 0.9988753503674286, 0.35678924350572805, 0.06487077154649601, 0.45477113802908137, 0.12298417105689868, 0.9977788892362065, 0.9849040942068171, 0.00604417960446923, 0.0020147265348230766, 0.9261026305070076, 0.0020147265348230766, 0.023505142906269226, 0.008058906139292306, 0.02149041637144615, 0.008058906139292306, 0.0020147265348230766, 0.9970851476558001, 0.9903596937814261, 0.1367379493335391, 0.5854093455842144, 0.18630545596694706, 0.001709224366669239, 0.08887966706680044, 0.993612107740331, 0.019560498324391485, 0.0019560498324391485, 0.8051101110319535, 0.005085729564341787, 0.0035208896983904676, 0.0003912099664878297, 0.0011736298994634892, 0.0007824199329756594, 0.0003912099664878297, 0.03247042721848987, 0.059855124872637944, 0.008606619262732254, 0.04185946641419778, 0.0035208896983904676, 0.01603960862600102, 0.13868909419789185, 0.09721198191441018, 0.715480186890059, 0.018146236624023235, 0.028515514694893652, 0.9780927250721181, 0.9987563266560832, 0.9263275719086327, 0.07184812192089117, 0.9969566329481397, 0.9899370315581334, 0.9892620475015993, 0.21374042924222622, 0.7786258493823955, 0.988084062539626, 0.9928105301996346, 0.1516652366478067, 0.09925151515922644, 0.7471743276031654, 0.9834459106923271, 0.9921221247994152, 0.05136315310959391, 0.9483474892023035, 0.9903278298866416, 0.9952534362564649, 0.0032001718207603372, 0.06905841329806524, 0.9287165926291532, 0.14844818286603684, 0.8470278669415043, 0.996112089672846, 0.04057979083123763, 0.11888164074503418, 0.005143917147621671, 0.6218424285124865, 0.0017146390492072238, 0.21204369575196003, 0.9793300738572555, 0.9960119838199696, 0.9666152090004921, 0.002036050993155328, 0.01679742069353146, 0.004072101986310656, 0.01018025496577664, 0.9860887810789273, 0.009885601815327591, 0.0024714004538318977, 0.0017509148954315363, 0.01867642555126972, 0.24862991515127816, 0.032100106416244834, 0.10038578733807475, 0.088129383070054, 0.013423680864975111, 0.3355920216243778, 0.00466910638781743, 0.005252744686294609, 0.01400731916345229, 0.025096446834518688, 0.09805123414416603, 0.004085468089340251, 0.0005836382984771788, 0.0011672765969543576, 0.0005836382984771788, 0.007003659581726145, 0.9885024877780878, 0.7058303291771557, 0.23787651977793645, 0.054594611096575575, 0.04470460442939604, 0.076548980187322, 0.0024495673659943036, 0.1855547279740685, 0.1555475277406383, 0.45316996270894616, 0.08206050676080917, 0.6766829778841085, 0.11294001611078472, 0.027756444637396245, 0.05455577049419262, 0.1272967978197828, 0.027408881283596104, 0.07243775767807542, 0.01761999511088321, 0.050249615686592856, 0.000652592411514193, 0.5912487248318588, 0.02936665851813868, 0.07570071973564638, 0.0646066487399051, 0.07113257285504702, 0.9919760171384692, 0.9943397728916695, 0.9862743693924203, 0.999289970052274, 0.9973034357962878, 0.9892919039533888, 0.9869049529154147, 0.9939489552802138, 0.24233016456960907, 0.3569203974281064, 0.4001265508009824, 0.9941020089541819, 0.9934623672818373, 0.9950183383541782, 0.01217281638973753, 0.8781612953161239, 0.0002864192091702948, 0.10568868818383878, 0.0034370305100435376, 0.24281888383602188, 0.020376409832393444, 0.45167708461805467, 0.011886239068896177, 0.006792136610797815, 0.010188204916196722, 0.08320367348227323, 0.0882977759403716, 0.08320367348227323, 0.0016980341526994537, 0.004304692541726546, 0.09814698995136525, 0.8970979256958123, 0.9874194973866147, 0.994909882602826, 0.9630843816862881, 0.2859977913832031, 0.7103059900746764, 0.048221709123872906, 0.1028729794642622, 0.003214780608258194, 0.32469284143407756, 0.5175796779295693, 0.06618827720627055, 0.30909282850695274, 0.2776052014864939, 0.19856483161881164, 0.1484416702393058, 0.9965887726189806, 0.997473486550059, 0.9901471682373929, 0.9918752651858688, 0.9954061656446386, 0.3180835689030931, 0.6808503058187635, 0.9956435578197983, 0.9985633340011745, 0.9930238350688576, 0.057966875184022285, 0.6279744811602415, 0.2724443133649047, 0.038644583456014854, 0.9992723457968449, 0.0006138036522093641, 0.9927304671345931, 0.9930197810153617, 0.005996496262170059, 0.9975550949007688, 0.9972300761539398, 0.9791045239522683, 0.9976343033480812, 0.9976058540611396, 0.9932801776348595, 0.9914534817030407, 0.9983113849206328, 0.7635974159116173, 0.10733297010805866, 0.12266625155206705, 0.997845664193483, 0.9947539560520972, 0.0005178690224268957, 0.0005178690224268957, 0.006732297291549645, 0.8335101915960886, 0.0002589345112134479, 0.0023304106009210306, 0.0031072141345613743, 0.00362508315698827, 0.009839511426111018, 0.006991231802763093, 0.08415371614437056, 0.004919755713055509, 0.0005178690224268957, 0.0033661486457748223, 0.01812541578494135, 0.0005178690224268957, 0.01942008834100859, 0.0015536070672806871, 0.9816797976469195, 0.979919794322562, 0.9887351236513212, 0.9938544580208869, 0.9922463566965303, 0.9970809181703411, 0.08749284640320519, 0.1906094153784113, 0.08124335737440483, 0.6405726254520381, 0.05923958368216157, 0.11392227631184917, 0.10025160315442727, 0.6653060936611992, 0.05923958368216157, 0.9940272745550257, 0.14671845657322946, 0.005418579362079497, 0.011253972521242033, 0.019590248462902797, 0.04001412451997167, 0.07252560069244865, 0.03042740718706179, 0.04584951767913421, 0.0937831043436836, 0.29176965795812676, 0.19756973981736012, 0.04459907628788509, 0.9929098205766825, 0.9973310282313617, 0.9966854247785659, 0.9870563993457137, 0.8838540547598924, 0.11320298238304041, 0.05089917068471097, 0.031405871273545063, 0.0018049351306635094, 0.008302701601052143, 0.0046928313397251245, 0.009385662679450249, 0.046928313397251245, 0.16424909689037936, 0.03537672856100479, 0.628478412497034, 0.017688364280502394, 0.9974112474003622, 0.9958641238555682, 0.3160288453016228, 0.6833056114629682, 0.9912713898020767, 0.9933774140651275, 0.9957593014314178, 0.9878821969519517, 0.029720280079694534, 0.9659091025900723, 0.9966827545024161, 0.9825028237150961, 0.9968392856765431, 0.9756964474434393, 0.9930928319474075, 0.12164009765489252, 0.05551360161431337, 0.23266730088351925, 0.3845133288285529, 0.19103209967278423, 0.01469477689790648, 0.9945818674872082, 0.9864776095338096, 0.9944659461095725, 0.9950427711043031, 0.053828533109496236, 0.4450700664419323, 0.005251564205804511, 0.03544805838918045, 0.45819897695644357, 0.9885049190892156, 0.6202227722554723, 0.3787200998728105, 0.99685900629839, 0.9976934696225749, 0.24915008260729168, 0.7508138940672965, 0.9985366045795685, 0.9980551895385685, 0.841402282714493, 0.1456273181621238, 0.041924084648159256, 0.0057943856830789215, 0.04328746951476606, 0.028119812873765353, 0.4383282346140878, 0.01942823434914697, 0.045332546814676265, 0.08214393821306, 0.04959312452282253, 0.004090154599820415, 0.0034084621665170125, 0.006987347441359875, 0.03476631409847353, 0.006476078116382324, 0.022836696515663984, 0.0015338079749326558, 0.008862001632944233, 0.02198458097403473, 0.003749308383168714, 0.0071577705496857266, 0.00545353946642722, 0.09236932471261104, 0.00034084621665170125, 0.020791619215753777, 0.00528311635810137, 0.9869020563635427, 0.9901130341145421, 0.9348357188188021, 0.0411196300014389, 0.9585503152653307, 0.539291157972891, 0.4603011991127942, 0.994379615641581, 0.040848978088301346, 0.0003166587448705531, 0.11653041811236353, 0.06428172520872227, 0.0018999524692233185, 0.19822837428896622, 0.012666349794822123, 0.02279942963067982, 0.007599809876893274, 0.008549786111504934, 0.007599809876893274, 0.012033032305081017, 0.31570876863594144, 0.00823312736663438, 0.054148645372864575, 0.039899001853689685, 0.0003166587448705531, 0.0025332699589644247, 0.07884802747276771, 0.0006333174897411062, 0.006649833642281615, 0.9904792725366336, 0.9937631297969594, 0.9977782970020248, 0.9862683498687979, 0.9968666236721709, 0.10035561783488686, 0.05874475190334841, 0.5752090290536198, 0.10769871182280542, 0.09301252384696831, 0.06119244989932126, 0.9905649035029127, 0.9936893631635099, 0.9930160191169451, 0.811675378538711, 0.0009261471685745218, 0.0009261471685745218, 0.00018522943371490438, 0.16466896657254998, 0.018522943371490436, 0.001481835469719235, 0.0011113766022894263, 0.00037045886742980875, 0.9920007212497743, 0.5301936564543056, 0.09005022241338294, 0.2885282636510433, 0.09096910223392765, 0.0014515290369352973, 0.014515290369352974, 0.09289785836385903, 0.024675993627900055, 0.2714359299069006, 0.02612752266483535, 0.007257645184676487, 0.031207874294108894, 0.008709174221611785, 0.06749610021749132, 0.03265940333104419, 0.0007257645184676486, 0.10160703258547081, 0.20829441680021515, 0.04644892918192951, 0.0029030580738705946, 0.060964219551282485, 0.9807278438831176, 0.930022350698259, 0.06889054449616733, 0.9951916389655697, 0.9965403449356524, 0.9916680949746436, 0.011179057021288802, 0.04594133022447453, 0.10597133505112125, 0.06309276017494501, 0.00030627553482983017, 0.00030627553482983017, 0.003215893115713217, 0.00015313776741491508, 0.02343007841448201, 0.04180661050427182, 0.0010719643719044056, 0.5349102215802984, 0.0018376532089789811, 0.05053546324692198, 0.07258730175466975, 0.0019907909763938964, 0.011944745858363377, 0.029555589111078612, 0.05262154344782644, 0.23010776626337665, 0.4807290155657365, 0.03210806040884325, 0.20335104925600728, 0.5733923444093415, 0.4239872969224004, 0.9909821605062161, 0.9991766212026434, 0.9944142602290014, 0.25299311705228483, 0.040725721281587315, 0.05059862341045697, 0.013575240427195772, 0.6417386383765274, 0.9963490840458789, 0.001769699140746383, 0.536218839646154, 0.2849215616601677, 0.05840007164463064, 0.0725576647706017, 0.044242478518659575, 0.9914580122723958, 0.9781865307057701, 0.9930025449488634, 0.12103647218649496, 0.6051823609324748, 0.01512955902331187, 0.25215931705519784, 0.9956746152908834, 0.07262012709070532, 0.03925412275173261, 0.017664355238279673, 0.14524025418141065, 0.029440592063799457, 0.6947979727056672, 0.9696362593784995, 0.27277935109235235, 0.1330630980938304, 0.12973652064148467, 0.4590676884237149, 0.00491993723552301, 0.0007687401930504703, 0.49845114117392497, 0.015989796015449784, 0.0015374803861009406, 0.01967974894209204, 0.00015374803861009406, 0.0016912284247110347, 0.005996173505793669, 0.02490718225483524, 0.0006149921544403763, 0.00030749607722018813, 0.007379905853284515, 0.3087260615290689, 0.009686126432435925, 0.015067307783789218, 0.011223606818536867, 0.03397831653283079, 0.011069858779926773, 0.019218504826261758, 0.006611165660234045, 0.00015374803861009406, 0.0015374803861009406, 0.9950390451717371, 0.9910108769820826, 0.004320018243189816, 0.47520200675087976, 0.5184021891827779, 0.9812581072933976, 0.0011655774471110612, 0.587159638982197, 0.007284859044444133, 0.008159042129777428, 0.0005827887235555306, 0.0005827887235555306, 0.2613807425146555, 0.006119281597333071, 0.04225218245777597, 0.005245098511999775, 0.006410675959110836, 0.06702070320888602, 0.0005827887235555306, 0.006410675959110836, 0.9978300698223685, 0.3055131453307087, 0.015738555971581965, 0.06110262906614174, 0.30458734792061565, 0.31199372720136015, 0.9858171733606033, 0.9911650754431808, 0.19510069261428678, 0.14003194873122196, 0.011800445117799603, 0.04248160242407857, 0.1250847182486758, 0.017307319506106086, 0.003933481705933202, 0.13373837800172883, 0.2525295255209115, 0.04484169144763849, 0.03304124632983889, 0.5315459243665006, 0.05170680198117711, 0.41572268792866396, 0.9973393833168378, 0.11822284093484843, 0.8775772423240672, 0.9963572866539093, 0.051858251737738385, 0.037938405218661236, 0.035481961715294684, 0.03875721971978342, 0.4544420481228127, 0.37337941251171636, 0.00818814501122185, 0.7743558415504465, 0.22207941116163746, 0.1789134357824578, 0.8089998835380701, 0.988921057860428, 0.949699283281578, 0.04977811319909332, 0.998424143251885, 0.0003019282723022523, 0.00015096413615112614, 0.015549306023565992, 0.7332328092860196, 0.007246278535254055, 0.002113497906115766, 0.004076031676080406, 0.0003019282723022523, 0.00588760130989392, 0.003472175131475901, 0.0031702468591736487, 0.0006038565446045046, 0.00015096413615112614, 0.002113497906115766, 0.006491457854498424, 0.16907983248926128, 0.011020381939032207, 0.003019282723022523, 0.0019625337699646396, 0.0015096413615112614, 0.0019625337699646396, 0.0028683185868713964, 0.004076031676080406, 0.012680987436694595, 0.0046798882206849106, 0.002566390314569144, 0.9967700057711043, 0.0037056946780412545, 0.2887794924102149, 0.0203813207292269, 0.22631206783751948, 0.018793165867209218, 0.010852391557120818, 0.14187516767357947, 0.020910705683232795, 0.002911617247032414, 0.0044997721090500944, 0.0526738029235864, 0.00026469247700294676, 0.002911617247032414, 0.008999544218100189, 0.02117539816023574, 0.00026469247700294676, 0.021440090637238687, 0.009528929172106083, 0.006352619448070722, 0.0044997721090500944, 0.016410933574182697, 0.04023325650444791, 0.062202732095692485, 0.002117539816023574, 0.011646468988129656, 0.9772276640912541, 0.8418310761716369, 0.1389228623238908, 0.01658780445658398, 0.9960703232788718, 0.614542624262396, 0.38386386794102745, 0.6738569484886382, 0.19654160997585282, 0.12882559309341612, 0.9925222480568948, 0.0011755143266562237, 0.011755143266562237, 0.002938785816640559, 0.6841493381139222, 0.02586131518643692, 0.0002938785816640559, 0.02174701504314014, 0.0014693929083202796, 0.0023510286533124473, 0.0020571500716483914, 0.006171450214945174, 0.12078409706392698, 0.03820421561632727, 0.007640843123265454, 0.011755143266562237, 0.008228600286593566, 0.0099918717765779, 0.014106171919874684, 0.028506222421413423, 0.0007766546537302052, 0.04867035830042619, 0.0014238668651720427, 0.04284544839744965, 0.02148744541986901, 0.00841375874874389, 0.2112500658146158, 0.0036243883840742907, 0.007637104095013684, 0.003883273268651026, 0.0007766546537302052, 0.00012944244228836754, 0.04737593387754251, 0.0060837947875532735, 0.005307140133823069, 0.0006472122114418377, 0.002459406403478983, 0.028995107072594326, 0.0011649819805953077, 0.12297032017394915, 0.06549787579791397, 0.026017930899961874, 0.0034949459417859233, 0.23882130602203808, 0.004271600595516128, 0.06239125718299315, 0.03391391987955229, 0.028439140768287564, 0.010563109428221095, 0.017063484460972538, 0.3591457205595172, 0.028439140768287564, 0.006500375032751443, 0.014625843823690747, 0.0008125468790939304, 0.06987903160207802, 0.01787603134006647, 0.034126968921945076, 0.3168932828466329, 0.004875281274563582, 0.0910052504585202, 0.9967489401627506, 0.9924187303984761, 0.007752258344499026, 0.009302710013398831, 0.4411034998019946, 0.027132904205746593, 0.07597213177609045, 0.024032000867946983, 0.015504516688998052, 0.007752258344499026, 0.39071382056275095, 0.005099777958708585, 0.0024284656946231362, 0.03691267855827167, 0.03739837169719629, 0.0009713862778492544, 0.0009713862778492544, 0.08111075420041275, 0.08548199245073439, 0.02914158833547763, 0.013599407889889562, 0.017970646140211207, 0.008013936792256348, 0.005099777958708585, 0.021856191251608224, 0.0019427725556985089, 0.0038855451113970177, 0.19379156243092627, 0.01554218044558807, 0.05051208644816123, 0.04516946191999033, 0.04249814965590488, 0.21686198652984606, 0.05099777958708586, 0.0026713122640854496, 0.029870128043864574, 0.09153690642400063, 0.0966222901142229, 0.8085760067453389, 0.9965917260043449, 0.9964394208804892, 0.9958219087199871, 0.9815030805733606, 0.07504852800039168, 0.3001941120015667, 0.6204011648032378, 0.9987298019593523, 0.9852786789453997, 0.9944914325422394, 0.9975992081084675, 0.1604662474776385, 0.008977132726021735, 0.02468711499655977, 0.02468711499655977, 0.35796316745011664, 0.04151923885785052, 0.026931398178065204, 0.28502396405119007, 0.02132069022430162, 0.04825208840236682, 0.9930300173266231, 0.9923571852762053, 0.0006287775060917633, 0.002245633950327726, 0.060721942016861714, 0.02928306671227355, 0.031888002094653714, 0.00035930143205243617, 0.3487918651649024, 0.00844358365323225, 0.029822018860352205, 0.021378435207119954, 8.982535801310904e-05, 0.0010779042961573085, 0.013563629059979466, 0.013743279776005684, 0.0004491267900655452, 0.0012575550121835266, 0.0002694760740393271, 0.004760743974694779, 0.02560022703373608, 0.06395565490533364, 0.0028744114564194893, 0.029642368144325984, 0.014551707998123666, 0.0029642368144325985, 0.07500417394094605, 0.04626005937675116, 0.0015270310862228538, 0.024252846663539444, 0.0026049353823801623, 0.002065983234301508, 0.041229839328017055, 0.00377266503655058, 0.06018298986878306, 0.004042141110589907, 8.982535801310904e-05, 0.030540621724457075, 0.0007404543607269456, 0.01999226773962753, 0.9781402105202951, 0.9847297198201853, 0.009495877041597366, 0.25593649550210046, 0.08365415489026251, 0.037983508166389464, 0.14198597100293206, 0.06647113929118156, 0.016730830978052502, 0.17906510992726463, 0.11078523215196928, 0.049288123692100616, 0.02351360029347919, 0.024417969535536085, 0.18724296273776153, 0.8033327111007188, 0.1858703903985329, 0.8034397520452712, 0.9977531120766855, 0.9948417082848101, 0.9970429037368653, 0.40255884137382125, 0.5946891974840541, 0.9943691993568573, 0.9976936443112885, 0.23833648601114144, 0.7596975491605134, 0.9837950147898362, 0.03545315810937301, 0.9607805847640085, 0.9953402051898792, 0.9845910394305278, 0.9927936325308752, 0.9915288329590247, 0.7189242527707442, 0.2800381136983185, 0.9927356139841277, 0.878038877587962, 0.021736896444413165, 0.09946276857898144, 0.98813715210176, 0.9880383373479693, 0.9861366202246522, 0.99680128603437, 0.017353991288965934, 0.9812893256124373, 0.9611902587313178, 0.03495237304477519, 0.9973692668469253, 0.9909659164102802, 0.989837610248519, 0.3031561536115256, 0.4385462804671584, 0.25606393557478374, 0.9863607853403238, 0.26351180395446105, 0.006854935986573586, 0.03709730063322176, 0.0006048472929329634, 0.0082662463367505, 0.005846857165018647, 0.05483948789258869, 0.01935511337385483, 0.08790447323959069, 0.11834845365054986, 0.2850846907357368, 0.002016157643109878, 0.11008220731379935, 0.03456634614610067, 0.9647152969866277, 0.9876382946240208, 0.910059022078546, 0.08704912385099137, 0.9935992888089942, 0.9934762697867788, 0.9959123787843485, 0.9945237206966455, 0.9870432885446049, 0.99394714384256, 0.0973722798767297, 0.03370578918809874, 0.7246744675441229, 0.07022039414187237, 0.07302920990754727, 0.9940872555253095, 0.995176584576441, 0.398831215796359, 0.5355733469265392, 0.06267347676799927, 0.9955490523228697, 0.9835197892231328, 0.9940400167393456, 0.9962773560166182, 0.9862865106553821, 0.990623416495652, 0.14590709595879298, 0.8519091731787589, 0.8223448094973522, 0.1772123197221897, 0.9969987639651698, 0.9848579586333137, 0.3032535292552864, 0.6948333291674524, 0.8985874628051121, 0.018527576552682726, 0.08152133683180399, 0.9990103192761521, 0.0035353227977684186, 0.019797807667503144, 0.0017676613988842093, 0.02085840450683367, 0.002828258238214735, 0.010605968393305257, 0.008131242434867363, 0.025100791864155772, 0.8368109062317847, 0.0152018880304042, 0.0035353227977684186, 0.0014141291191073675, 0.027575517822593664, 0.020151339947279988, 0.0014141291191073675, 0.0010605968393305257, 0.9771148290891173, 0.9981643368246922, 0.003467481839122818, 0.888831178095149, 0.02311654559415212, 0.08437539141865523, 0.9920064807279311, 0.9920989959465693, 0.982810376132065, 0.007012436365179634, 0.9926980229457419, 0.9987967772916659, 0.9881179463526053, 0.10279227639028923, 0.8935020947771295, 0.97813143995429, 0.9990585995317269, 0.0322459930377813, 0.9566311267875118, 0.00437541131852978, 0.06271422889892685, 0.051046465382847434, 0.24793997471668755, 0.010209293076569486, 0.25960773823276695, 0.04958799494333751, 0.31502961493414416, 0.9879880828912759, 0.9206553191179281, 0.07323394583892609, 0.9745177058929647, 0.9919880982523682, 0.9872066897001034, 0.002849267275294036, 0.020229797654587656, 0.007693021643293898, 0.316268667557638, 0.13676482921411373, 0.424255897291282, 0.05869490587105714, 0.033336427120940225, 0.9956548641984633, 0.996165674680131, 0.9925284223182439, 0.25024657046043103, 0.4430040639231954, 0.3043539370464701, 0.11933138065356357, 0.7860954700553501, 0.09248182000651177, 0.9945918925606284, 0.11726754251855627, 0.14195544620667339, 0.7375511226824987, 0.990361911428554, 0.9901901586518479, 0.005991132105725626, 0.09565840928808583, 0.043934968775321256, 0.034548861809684445, 0.008986698158588439, 0.14877978062551972, 0.04093940272245845, 0.045532604003514755, 0.015177534667838253, 0.00039940880704837504, 0.018772213931273627, 0.06410511353126419, 0.002995566052862813, 0.0023964528422902504, 0.04073969831893426, 0.00818788054449169, 0.014778125860789876, 0.005991132105725626, 0.01937132714184619, 0.020769257966515502, 0.09725604451627932, 0.0005991132105725626, 0.027159798879289505, 0.0057914277022014385, 0.19591001985722797, 0.0009985220176209376, 0.0013979308246693128, 0.009585811369161002, 0.005991132105725626, 0.017174578703080128, 0.9929463545140548, 0.6115283761832141, 0.38312621158466426, 0.9913680239511103, 0.9911921027628575, 0.9959018975313964, 0.9902475035847819, 0.006249347192092823, 0.9936462035427588, 0.9943149870943443, 0.8492245462332199, 0.14498955667396435, 0.9913023749385893, 0.9639177513614646, 0.9921561129305231, 0.998924351770524, 0.022259553838793824, 0.08552354895957627, 0.34443730676870443, 0.0702933279119805, 0.12067021291556652, 0.06326399512078244, 0.2917173108347191, 0.18262944650661284, 0.6904376176419773, 0.0011343443882398314, 0.12515599750246137, 0.9900906417446363, 0.995212133045786, 0.9920235662552238, 0.9834912573177041, 0.0005252564569542859, 0.003939423427157145, 0.0015757693708628579, 0.0019259403421657151, 0.007003419426057146, 0.00017508548565142864, 0.2452072226548258, 0.0013131411423857149, 0.0360676100441943, 0.019522031650134296, 0.6817828811266632, 0.0008754274282571432, 0.019902017405577684, 0.2772066710062606, 0.02644125169598178, 0.0187647592681161, 0.5103445891858849, 0.0696570609195219, 0.027578509833443364, 0.027578509833443364, 0.022460848214866245, 0.4248251672418812, 0.11586140924778579, 0.0013317403361814458, 0.14316208613950543, 0.07324571848997952, 0.02996415756408253, 0.21108084328475915, 0.9974639621598609, 0.007721450281672867, 0.017649029215252268, 0.4500502449889328, 0.3937939643653162, 0.06728692388314927, 0.0628746665793362, 0.9948501247608722, 0.9996822036874962, 0.9853395049041312, 0.9542811516824266, 0.04428069903570103, 0.0006417492613869715, 0.9938375664304587, 0.9731899384712933, 0.9988061796282259, 0.9958800857536054, 0.9883221092336021, 0.8855953715428377, 0.10677391004417193, 0.6443439272030228, 0.11370775185935697, 0.2414761397013226, 0.9622270623973983, 0.9236876309885551, 0.07361457400667802, 0.2128347651351045, 0.07011027557391677, 0.04726183755205997, 0.09139375208742723, 0.038498053105320375, 0.007511815240062512, 0.013458668971778668, 0.3834155695448574, 0.014710638178455752, 0.015649615083463568, 0.10078352113750537, 0.003755907620031256, 0.009623794326946634, 0.001110437806955381, 0.008513356519991253, 0.18692369750415577, 0.014065545554768159, 0.2146846426780403, 0.08624400300686792, 0.0025910215495625554, 0.054411452540813664, 0.012955107747812777, 0.027020653302580935, 0.1617537738798338, 0.07143816558079617, 0.022208756139107617, 0.12621976405726162, 0.04438149380997087, 0.007820527543607202, 0.011926304504000982, 0.017987213350296564, 0.006060908846295582, 0.27352295083766187, 0.018378239727476924, 0.012121817692591163, 0.059240496142824554, 0.10596814821587758, 0.010753225372459903, 0.026785306836854667, 0.029913517854297546, 0.07664116992735058, 0.18378239727476925, 0.0013685923201312603, 0.11339764938230443, 0.9920298680158712, 0.99752263948564, 0.9941489387034466, 0.00036258959014145265, 0.6084253322573576, 0.39087157817248597, 0.013419711975864744, 0.04585068258420454, 0.007269010653593403, 0.9147770330214468, 0.018452103966814022, 0.9940134217936738, 0.0040407049666409504, 0.9930720878284721, 0.9938458435711494, 0.9902880302149514, 0.9952037863944088, 0.9938904936351904, 0.97683416826601, 0.012836261323445324, 0.034230030195854196, 0.07273881416619017, 0.04278753774481774, 0.740224402985347, 0.08985382926411727, 0.9930616853654104, 0.993737015108624, 0.9952176081869829, 0.9794900757070371, 0.9821355827196598, 0.9971587763574179, 0.9959258638448059, 0.3791058405347192, 0.08076602689652712, 0.015658719500347095, 0.018131148895138744, 0.13021461479236007, 0.1054903208444436, 0.10301789144965195, 0.04367958597465243, 0.1219731834763879, 0.9925288164044739, 0.9995613427382168, 0.9929639822911613, 0.9943955774797552, 0.9971616315542006, 0.9884858870241496, 0.985156882041838, 0.8637557659640034, 0.13561087697596091, 0.9912218623421767, 0.9892424135565334, 0.9874433162168484, 0.5240298113868491, 0.003597458659406287, 0.4712670843822236, 0.06787517573901684, 0.8983671096611219, 0.03328494194894095, 0.09914594864708819, 0.8966242312432323, 0.9939174421019935, 0.9736603103365007, 0.9948637734802209, 0.0048175724790662745, 0.0028905434874397645, 0.006744601470692784, 0.014452717437198823, 0.14163663088454845, 0.5212613422349709, 0.005781086974879529, 0.017343260924638587, 0.0597378987404218, 0.07226358718599411, 0.03275949285765067, 0.11851228298503035, 0.9881181043058211, 0.9800148000861943, 0.998864967021253, 0.9964944027328166, 0.032007611037826136, 0.011900265642268692, 0.1524465064173386, 0.019902168401725227, 0.7833657624514114, 0.9935657330439649, 0.02982411438111881, 0.9563599344878765, 0.011929645752447524, 0.9931544513356533, 0.10812514805658753, 0.33038239683957304, 0.5603628704837433, 0.9973935694269502, 0.9957231298445047, 0.9941687058143968, 0.9894854705927261, 0.2101900420073875, 0.7871823141845297, 0.710642332966748, 0.018014949067869525, 0.052148536775411786, 0.07774872755606849, 0.0009481552140983961, 0.020385337103115516, 0.00047407760704919807, 0.11946755697639791, 0.9982417387566365, 0.2767189129924318, 0.7209255891118617, 0.9758071946411108, 0.8979902819990553, 0.1002950185089854, 0.9914137853225755, 0.9833589824726521, 0.013379033775138124, 0.999249713565085, 0.05031231068005703, 0.022361026968914236, 0.025156155340028515, 0.0018634189140761864, 0.019565898597799957, 0.05031231068005703, 0.18913701977873293, 0.1528003509542473, 0.04844889176598084, 0.1788882157513139, 0.015839060769647584, 0.016770770226685678, 0.0680147903637808, 0.10062462136011406, 0.05776598633636178, 0.10631061548519584, 0.03189318464555875, 0.02802734408246072, 0.11790813717448993, 0.05412176788337243, 0.008698141266970568, 0.09857893435899978, 0.3914163570136756, 0.034792565067882274, 0.03285964478633326, 0.08698141266970569, 0.007731681126196061, 0.9826772299527712, 0.9765962559893687, 0.022347740411656034, 0.996080285341474, 0.9907427376967647, 0.9876255693189503, 0.9982094517381782, 0.9915473028998731, 0.861637198967295, 0.13100916475526123, 0.9858767271259421, 0.9986075259300295, 0.9918032346392671, 0.9853643725202191, 0.9591554990494173, 0.752783020581729, 0.06529240484637445, 0.04992948605899223, 0.12674407999590334, 0.99522354262897, 0.24150622196897187, 0.6130542557673901, 0.13933051267440685, 0.9852801113215401, 0.9939402030128084, 0.9848570651850004, 0.21424659718998412, 0.7843604236107893, 0.9945681818338094, 0.004933373917826436, 0.9918672624388868, 0.9895115980439276, 0.6377756687965896, 0.3585129499152427, 0.1317395859449889, 0.8666398251871329, 0.9789669047948886, 0.9914519726390087, 0.9917233851314926, 0.012142051780518035, 0.03295699768997752, 0.023416814148141923, 0.06591399537995504, 0.013876630606306324, 0.0728523106831082, 0.026885971799718505, 0.03729344475444825, 0.03122241886418923, 0.03208970827708338, 0.1630504096240993, 0.2298316944169485, 0.05290465418654286, 0.20641488026880658, 0.4159732736247368, 0.01459555346051708, 0.0027366662738469525, 0.009122220912823175, 0.027366662738469525, 0.01915666391692867, 0.023717774373340255, 0.03831332783385734, 0.44881326891090023, 0.020045455359800127, 0.9782182215582462, 0.12383116390198222, 0.8756632304497314, 0.07194213245047043, 0.09184102014953671, 0.48369603945422673, 0.06888076511215253, 0.08571828547290093, 0.039797775398132576, 0.018368204029907343, 0.14082289756262298, 0.9780713559322618, 0.020810028849622592, 0.9997061829706717, 0.09711128836511872, 0.5034453633665364, 0.20955593805104564, 0.189111456289968, 0.8361372119095679, 0.15677572723304398, 0.8617105655594665, 0.13135831792065036, 0.9887092622384914, 0.9834965986317354, 0.9928840973686522, 0.9918987414225804, 0.9944247300665696, 0.9868987839809771, 0.9932596552822481, 0.027835092752764766, 0.04984423585960203, 0.28741351586575714, 0.0006473277384363899, 0.4550714001207821, 0.17930978354688, 0.9796621341061477, 0.9877288310068293, 0.2583655672146619, 0.7377772308240901, 0.38112906238843447, 0.2649419117296046, 0.15755681793886783, 0.030807199038326114, 0.0026406170604279526, 0.07129666063155472, 0.09154139142816903, 0.36832696904200785, 0.006663810295167905, 0.09995715442751858, 0.48585235061133275, 0.030290046796217752, 0.00605800935924355, 0.00242320374369742, 0.9900900438938625, 0.9986685820099379, 0.9961034232010265, 0.0035745338153146886, 0.9959094191328625, 0.9829381150248161, 0.9927954708071969, 0.18180420017519988, 0.03462937146194284, 0.5335087540855569, 0.24240560023359986, 0.0010821678581857137, 0.004328671432742855, 0.05833104046035562, 0.26148397447745625, 0.010057075941440625, 0.1367762328035925, 0.5249793641432007, 0.006034245564864375, 0.015503101721360239, 0.003508001119431879, 0.02523497579462287, 0.013918843151294229, 0.058843889745308936, 0.04549085322618114, 0.019463748146525264, 0.014371488457027374, 0.09347125563389458, 0.0018105812229325827, 0.008713422135363054, 0.007242324891730331, 0.0007921292850330049, 0.02059536141085813, 0.001923742549365869, 0.011089809990462069, 0.002376387855099015, 0.31956758584760087, 0.07423383014023589, 0.04515136924688128, 0.0021500652022324418, 0.001357935917199437, 0.0007921292850330049, 0.06699150524850556, 0.10897435735525482, 0.0012447745907661505, 0.007129163565297044, 0.02840349293475489, 0.019393285950864668, 0.9782835357436177, 0.9904847645577449, 0.9952952545565859, 0.9944070979862991, 0.13860786835174782, 0.00504028612188174, 0.8543284976589548, 0.9883087991700632, 0.9918432224675213, 0.9925542665862841, 0.9938030507363746, 0.9962156402670764, 0.9932755696477142, 0.9958871867013508, 0.9920160605590648, 0.9986465010125798, 0.9909757261616726, 0.9974290562257291, 0.061225587818931546, 0.052479075273369896, 0.5997608602670845, 0.019992028675569484, 0.1586867276123328, 0.047481068104477525, 0.05997608602670845, 0.1801569438348386, 0.6792403693232428, 0.038952852721046184, 0.08520936532728852, 0.012172766475326934, 0.37446200157784615, 0.6222263334489022, 0.9934788935023399, 0.7998643335071449, 0.06721549021068444, 0.09074091178442399, 0.04032929412641067, 0.9873591008556888, 0.9973336004010167, 0.9913154055618107, 0.9994207873136953, 0.9968525882393356, 0.5895592576111542, 0.004359033327993747, 0.06865477491590151, 0.001634637497997655, 0.3351006870895193, 0.9967673930728262, 0.9886477936114347, 0.9969638111082739, 0.7888360033187276, 0.20947845308160445, 0.9990915605743516, 0.9946997334430858, 0.9955151416589507, 0.05456301497101871, 0.9396963689453223, 0.15675595189380737, 0.6270238075752295, 0.21336226785546003, 0.9923790636493067, 0.9910669114794646, 0.9958686083949903, 0.008924389042723725, 0.9751923299412654, 0.014603545706275188, 0.9925835098061595, 0.9842597781814165, 0.9869496492952308, 0.9952148315728515, 0.9968796984272306, 0.39362917351833726, 0.6055833438743651, 0.9974624768703086, 0.02255639186518101, 0.0002189940957784564, 0.011387692980479732, 0.26848676142438754, 0.007007811064910605, 0.12526462278527706, 0.0006569822873353692, 0.05036864202904497, 0.43273233325822985, 0.08124680953380732, 0.03788560677240493, 0.9615105718789665, 0.9975554393584876, 0.990096225323511, 0.9887501862516671, 0.06799695965884492, 0.057445707297989666, 0.026378130902138114, 0.8476172729887047, 0.9921160313642506, 0.9986653987445845, 0.997398125610697, 0.9971724061001951, 0.9984911320100053, 0.9993822960139589, 0.8670607028120979, 0.12386581468744257, 0.6765059767346759, 0.1903735411916676, 0.13258157332991138, 0.9988412035692777, 0.6561085420088556, 0.13959756212954375, 0.13261768402306656, 0.0558390248518175, 0.9994046729154785, 0.9941600833546976, 0.7481065926363148, 0.069591310942913, 0.03991266362902364, 0.040936065260537065, 0.10029335988831581, 0.9848889160338921, 0.9847875465780389, 0.019530499419382702, 0.10109905581798104, 0.5882126883955261, 0.289510932569673, 0.8057075252265574, 0.19319995993574895, 0.9932531506426563, 0.9571432759444917, 0.9896455145251841, 0.988840518815162, 0.9847624916277613, 0.9882472310192204, 0.9994317093221571, 0.9919646582128744, 0.9946906031638817, 0.021167914052014566, 0.0997915948166401, 0.37497447749282947, 0.49895797408320053, 0.9482336062549177, 0.001539340269894347, 0.02155076377852086, 0.0025655671164905784, 0.0261687845882039, 0.9843837782928677, 0.980984011002899, 0.9762274194063862, 0.9876707631392584, 0.00986027384165649, 0.9990444775909875, 0.9980242656629419, 0.001486629988574392, 0.9866323077893787, 0.9871630302877191, 0.9860445574543637, 0.9969479727633057, 0.06732376834529105, 0.00015840886669480248, 0.008554078801519335, 0.053700605809538046, 0.00031681773338960496, 0.784440707872662, 0.05956173387724573, 0.025820645271252805, 0.036670462833603354, 0.08424295515827797, 0.044599211554382456, 0.03171499488311642, 0.03567936924350597, 0.029732807702921638, 0.042617024374187684, 0.25074667829463915, 0.3785977514172022, 0.040634837193992905, 0.02279515257223992, 0.001982187180194776, 0.03651057060259077, 0.03072230940949711, 0.02894130596546829, 0.31612811131511515, 0.03428431629755475, 0.04407983523971324, 0.020481539606331405, 0.040517828351655606, 0.007569264637122476, 0.005343010332086453, 0.0008905017220144089, 0.08415241273036164, 0.014248027552230542, 0.004452508610072045, 0.0792546532592824, 0.02805080424345388, 0.016474281857266564, 0.005788261193093658, 0.029386556826475493, 0.11487472213985875, 0.01870053616230259, 0.02805080424345388, 0.009350268081151294, 0.0017810034440288178, 0.9844175928637257, 0.539234472012902, 0.4591196361709851, 0.9987581701327636, 0.7486176788857325, 0.25047208321597403, 0.054000810586144705, 0.9420141402249688, 0.9977079505350382, 0.9961885000362486, 0.9876751182274164, 0.9984605164720124, 0.9968052509555437, 0.012797739288848113, 0.02060436025504546, 0.023419862898592044, 0.005503027894204688, 0.00012797739288848112, 0.0017916835004387357, 0.0005119095715539245, 0.002047638286215698, 0.025851433363473186, 0.003967299179542915, 0.006782801823089499, 0.0006398869644424056, 0.008446507930639753, 0.0028155026435465846, 0.5569576138506699, 0.03506580565144383, 0.04005692397409459, 0.08728058194994412, 0.08190553144862792, 0.04683972579718409, 0.03416996390122446, 0.0024315704648811415, 0.03930622843551364, 0.3930622843551364, 0.3485152254615543, 0.2030821802501538, 0.014412283759688335, 0.015096741828771304, 0.00826726338242238, 0.0007188924680367288, 0.001797231170091822, 0.0025161236381285508, 0.005211970393266284, 0.0001797231170091822, 0.6315470331702662, 0.02785708313642324, 0.04600911795435064, 0.07458509355881061, 0.11052971696064705, 0.040078255093047625, 0.03450683846576298, 0.0010783387020550932, 0.00043316034440716667, 0.03421966720816617, 0.03855127065223783, 0.05804348615056033, 0.090963672325505, 0.0506797602956385, 0.0142942913654365, 0.0034652827552573333, 0.07407041889362549, 0.03595230858579483, 0.12041857574519232, 0.00043316034440716667, 0.03162070514172317, 0.10092636024686982, 0.3374319082931828, 0.007363725854921833, 0.015939191511302023, 0.018837226331538753, 0.20865850705704467, 0.7520400358514318, 0.0028980348202367313, 0.9898561469080045, 0.9946614886946675, 0.02106075968641726, 0.010357750665451112, 0.7343645221804839, 0.005178875332725556, 0.22856103135095454, 0.9911551639779042, 0.9970432643685764, 0.9893110015677983, 0.6821561181685252, 0.3175878346294236, 0.9768466357056474, 0.9863023822643131, 0.9698980061529631, 0.8933111994391564, 0.024913061338940615, 0.08185720154223346, 0.04937863375084089, 0.46113272486672374, 0.011150014072770522, 0.019910739415661648, 0.08521796469903185, 0.012344658437710222, 0.1799930843175813, 0.1799930843175813, 0.06286914330080187, 0.0558836829340461, 0.17289014407720513, 0.7090242272257099, 0.9961121029617089, 0.0189076363997847, 0.0018446474536375319, 0.28868732649427375, 0.001383485590228149, 0.010145560995006426, 0.0009223237268187659, 0.002766971180456298, 0.0036892949072750638, 0.00046116186340938297, 0.03135900671183804, 0.09407702013551413, 0.011067884721825192, 0.002305809317046915, 0.11344581839870821, 0.008762075404778276, 0.04473270075071015, 0.3265025992938431, 0.023058093170469147, 0.014757179629100255, 0.9941403122398349, 0.9978214464350567, 0.9946179906961968, 0.08721441466059984, 0.0033869675596349454, 0.002540225669726209, 0.0055038222844067865, 0.12574117065144735, 0.03429304654130382, 0.01651146685322036, 0.07620677009178628, 0.4686716360644856, 0.16892500703679292, 0.0004233709449543682, 0.010584273623859206, 0.9892025991762561, 0.9956646536909046, 0.10053281715967828, 0.22292059457146055, 0.10240609946700148, 0.004995419486195194, 0.031221371788719966, 0.04370992050420795, 0.0012488548715487986, 0.21043204585597255, 0.014986258458585583, 0.010615266408164788, 0.04870533999040314, 0.010615266408164788, 0.19357250509006377, 0.0031221371788719963, 0.9919486242408291, 0.9940723302346063, 0.019882442513151855, 0.6838355228008289, 0.005422484321768687, 0.005422484321768687, 0.0006024982579742986, 0.10182220559765646, 0.0006024982579742986, 0.17653198958646948, 0.003012491289871493, 0.0018074947739228958, 0.9921214204511792, 0.9992922099503905, 0.997113799805488, 0.9912279692176336, 0.008245508953682038, 0.9942668184389629, 0.9902216007055078, 0.9892043904794955, 0.036358651520586475, 0.11981041358212306, 0.15686161084595882, 0.01004191327711436, 0.02562419250022285, 0.020430099425853353, 0.6132492556472252, 0.017313643581231657, 0.0013173570622055618, 0.011856213559850056, 0.003952071186616685, 0.9814310113431436, 0.9963530704683975, 0.9989540935369473, 0.008818281413898633, 0.02144987370948316, 0.011201600714952319, 0.5231385865812838, 0.01549157545684895, 0.007149957903161054, 0.03908643653728043, 0.015729907386954317, 0.01096326878484695, 0.0016683235107375792, 0.00309831509136979, 0.0019066554408429478, 0.005958298252634212, 0.02025821405895632, 0.2526318459116906, 0.006673294042950317, 0.006911625973055686, 0.00023833193010536848, 0.0023833193010536846, 0.03598812144591064, 0.009056613344004002, 0.9960468224744368, 0.9868817806142077, 0.01845174984904838, 0.00031813361808704103, 0.9808059445623475, 0.9972469957270297, 0.9878334679290899, 0.00944165218589085, 0.00033720186378181607, 0.0526034907499633, 0.10689299081883569, 0.27077309661679827, 0.02967376401279981, 0.09981175167941755, 0.0016860093189090802, 0.006744037275636321, 0.12779950637330828, 0.009104450322109034, 0.016522891325308986, 0.16792652816334439, 0.00033720186378181607, 0.10014895354319936, 0.9905518791209746, 0.9994729269290344, 0.9976732407047054, 0.1579961602605162, 0.3423250138977851, 0.3326234952852972, 0.1663117476426486, 0.9915570592239559, 0.13866282150550086, 0.2712305739338368, 0.07009329438739603, 0.5180808715590142, 0.9495910907770253, 0.04831990063688403, 0.9722833020229499, 0.02561047267793283, 0.000914659738497601, 0.9982601902152919, 0.3344439617549513, 0.007176909050535436, 0.23683799866766939, 0.3775054160581639, 0.043061454303212614, 0.00885098347554669, 0.9824591657856826, 0.9894871972882752, 0.9951689974706213, 0.9671612843501375, 0.9926926917835442, 0.8323095266570703, 0.16511403565261717, 0.9927275744041145, 0.9971416258081083, 0.8163343643572414, 0.17632822270116413, 0.2739818197580276, 0.63202624330545, 0.0918461782143388, 0.9986556222138632, 0.989533822196318, 0.993241209311243, 0.9988205218329527, 0.9834926048779763, 0.9881814462730606, 0.061780582898489596, 0.1555850459232605, 0.10825771865698636, 0.001983780184813886, 0.007935120739255544, 0.003967560369627772, 0.005667943385182532, 0.18845911755731917, 0.46618834343126325, 0.9976688203055032, 0.9978016001455398, 0.9968385063455625, 0.05963348259018575, 0.4491146657573364, 0.2403974766916863, 0.11553987251848488, 0.05217929726641253, 0.0819960385615054, 0.001706836833419915, 0.6750539676175764, 0.001706836833419915, 0.30467037476545483, 0.01621494991748919, 0.9899677519517557, 0.9941444493896805, 0.04747105426331036, 0.9375533217003796, 0.016033692449683883, 0.9780552394307168, 0.9987179659973897, 0.9855963645779193, 0.9990643361751723, 0.9861403069509301, 0.9884267890781351, 0.995235151014384, 0.025622038374945626, 0.0208772164536594, 0.4991552661193111, 0.005693786305543473, 0.08208541923825173, 0.004744821921286227, 0.11672261926364119, 0.18504805493016285, 0.05978475620820646, 0.6889896972842646, 0.1561449738887852, 0.15224134954156557, 0.994309077930974, 0.9982822367803091, 0.12582924532453618, 0.10043251691040961, 0.03232310889070654, 0.02539672841412657, 0.0046175869843866485, 0.2101002077895925, 0.4225092090713784, 0.07849897873457304, 0.08131732008601318, 0.14309549612309033, 0.10114380448861003, 0.01666574051232779, 0.05172126365894831, 0.012355635207415429, 0.2626290832459931, 0.0017240421219649436, 0.018389782634292733, 0.00028734035366082396, 0.04310105304912359, 0.0031607438902690635, 0.004597445658573183, 0.0336188213783164, 0.21981537055053033, 0.006321487780538127, 0.9982738236686266, 0.007752608509364085, 0.0030540578976282757, 0.10712695394757644, 0.06812898387016923, 0.12874028676156116, 0.008457391101124456, 0.025842028364546948, 0.009162173692884827, 0.0007047825917603713, 0.002114347775281114, 0.005403333203496181, 0.014330579365794217, 0.035239129588018565, 0.007752608509364085, 0.30845984766045587, 0.0004698550611735809, 0.014330579365794217, 0.01620999961048854, 0.0004698550611735809, 0.022318115405745093, 0.05755724499376366, 0.025842028364546948, 0.11206043208989905, 0.009397101223471618, 0.008692318631711247, 0.9930137135819952, 0.9850192489496379, 0.9953165936548973, 0.9902401804974504, 0.993080069310218, 0.9883283986399469, 0.986607529658745, 0.002884055301340596, 0.977694747154462, 0.017304331808043576, 0.9962876446435003, 0.9856602199974477, 0.9691581179281235, 0.996183276983026, 0.9629024777079993, 0.9437254309301869, 0.019660946477712226, 0.02949141971656834, 0.012666388063145947, 0.010766429853674056, 0.021216200005769462, 0.00617486418078365, 0.6347443718144012, 0.0017416283586825679, 0.010291440301306083, 0.0030082671649971624, 0.03689085523391257, 0.0003166597015786487, 0.001424968657103919, 0.0007916492539466217, 0.0007916492539466217, 0.21105369110216934, 0.005699874628415676, 0.004116576120522433, 0.004116576120522433, 0.000474989552367973, 0.022799498513662704, 0.011083089555252704, 0.03361998891093306, 0.03810265409905747, 0.01613759467724787, 0.7593634828682748, 0.007620530819811494, 0.05334371573868046, 0.0004482665188124408, 0.0013447995564373225, 0.004482665188124409, 0.0008965330376248816, 0.0219650594218096, 0.008517063857436375, 0.04258531928718188, 0.01031012993268614, 0.9988932093957038, 0.9977469382964153, 0.9842627612330624, 0.9644252841578674, 0.26350963159796087, 0.23057092764821574, 0.505060127229425, 0.9634420203803412, 0.9938059073212805, 0.993933297238649, 0.9185018933080581, 0.07570070549242237, 0.09850679473691012, 0.2085626619602166, 0.2520415230854735, 0.1976929466789024, 0.0006793572050821388, 0.07948479299461024, 0.1154907248639636, 0.002717428820328555, 0.03600593186935336, 0.008831643666067805, 0.9986751150583646, 0.017143005057255793, 0.9600082832063245, 0.01959200577972091, 0.16345860833661388, 0.794136405502049, 0.04086465208415347, 0.9984947801381187, 0.9922789905031237, 0.9981075143829191, 0.9976114904458824, 0.7055907425964871, 0.02877121081287879, 0.19451504084513485, 0.029931340281140032, 0.0010827875037104921, 0.004795201802146465, 0.0007734196455074944, 0.003171020546580727, 0.031246153678502774, 0.06680692087561627, 0.08713946201167339, 0.002904648733722446, 0.5489786106735423, 0.18008822149079165, 0.11037665188145294, 0.9891446721747655, 0.7304847934613037, 0.2647169664378119, 0.03740938957704306, 0.049879186102724085, 0.8666508585348309, 0.04364428783988357, 0.983280086970285, 0.5180529720958699, 0.007458877870490273, 0.061027182576738594, 0.3715877339116972, 0.004746558644857446, 0.036616309546043155, 0.9855131908093971, 0.9769205536314652, 0.9916801001213622, 0.9955507701347573, 0.009369298063316616, 0.05572266742919882, 0.31387148512110663, 0.012328023767521862, 0.17209921179460522, 0.015779870422427986, 0.004191528080957433, 0.0024656047535043728, 0.028354454665300285, 0.0002465604753504373, 0.013560826144274049, 0.09344642015781572, 0.009862419014017491, 0.12451304005197082, 0.006410572359111369, 0.10503476249928627, 0.030326938468103783, 0.0022190442781539355, 0.211624787909051, 0.7325473427620997, 0.005997463624952863, 0.000856780517850409, 0.04883648951747331, 0.9883403363029217, 0.9958265526931467, 0.005297066078014331, 0.9852542905106656, 0.1861539499603646, 0.0685830341959238, 0.7429828704558412, 0.9933096260453599, 0.8861731081847769, 0.11154626536591598, 0.9960204562020607, 0.004325969614237997, 0.06724916400315432, 0.2536591273803189, 0.03224813712431962, 0.09831749123268176, 0.1380377576906852, 0.009438479158337449, 0.0019663498246536354, 0.036180836773626886, 0.028315437475012348, 0.056630874950024696, 0.12270022905838683, 0.012191368912852538, 0.0007865399298614541, 0.011404828982991083, 0.08337323256531413, 0.04286642617744925, 0.9969732434919255, 0.9975411047823901, 0.9878554155633976, 0.25013151962891794, 0.7496030034448901, 0.9986229113546582, 0.02380154199622889, 0.9758632218453844, 0.9936389244324272, 0.004323301740369985, 0.04870919960816849, 0.0005764402320493313, 0.47931005294901896, 0.0005764402320493313, 0.02651625067426924, 0.028245571370417232, 0.016140326497381275, 0.01585210638135661, 0.0017293206961479938, 0.0005764402320493313, 0.031415992646688554, 0.15218022126102346, 0.03199243287873788, 0.0008646603480739969, 0.0011528804640986625, 0.005187962088443981, 0.10491212223297829, 0.040639036359477854, 0.00461152185639465, 0.004899741972419316, 0.0023804540982410942, 0.00023804540982410945, 0.0028565449178893135, 0.049989536063062986, 0.6910458247193897, 0.0126164067206778, 0.0038087265571857512, 0.02142408688416985, 0.0007141362294723284, 0.0252328134413556, 0.014044679179622457, 0.0061891806554268455, 0.00904572557331616, 0.0126164067206778, 0.028565449178893133, 0.027851312949420805, 0.005713089835778627, 0.006427226065250955, 0.05451239884972106, 0.015710997048391222, 0.007855498524195611, 0.00023804540982410945, 0.010714465970988024, 0.012053774217361528, 0.001339308246373503, 0.05055888630059974, 0.01975479663400917, 0.08437641952153069, 0.02946478142021707, 0.06060369814840101, 0.035826495590491204, 0.007701022416647643, 0.03281305203615083, 0.0006696541231867515, 0.0010044811847801272, 0.00033482706159337574, 0.002678616492747006, 0.01674135307966879, 0.010714465970988024, 0.41752934580693957, 0.0539071569165335, 0.020759277818789296, 0.028125473173843563, 0.008370676539834395, 0.06696541231867516, 0.010044811847801273, 0.009709984786207897, 0.007031368293460891, 0.0006696541231867515, 0.3996014867300693, 0.09262286116259884, 0.33785291262167005, 0.1684853950672036, 0.07090733551447069, 0.26050738482490315, 0.6674538321253436, 0.8722582515418443, 0.12285327486504849, 0.5759348159775749, 0.04245219773790478, 0.02228740381240001, 0.04386727099583494, 0.14292239905094611, 0.00035376831448253985, 0.07216873615443813, 0.056956698631688917, 0.04209842942342224, 0.010460228905865947, 0.421121067432455, 0.008910565364256178, 0.0011622476562073276, 0.5466438143028464, 0.006586070051841522, 0.000774831770804885, 0.000774831770804885, 0.00309932708321954, 0.9964218187290597, 0.9966166550757378, 0.011262798523818967, 0.9474829258162706, 0.0028156996309547416, 0.0014078498154773708, 0.004223549446432113, 0.011262798523818967, 0.0028156996309547416, 0.004223549446432113, 0.01407849815477371, 0.9914469035981851, 0.016349938466347313, 0.022987042200211073, 0.02913850419745066, 0.45391314316473136, 0.03464244387919134, 0.003723253314118695, 0.05261118813428591, 0.039822622403182566, 0.04451715919054962, 0.0019425669464967104, 0.00032376115774945173, 0.006151461997239583, 0.00016188057887472587, 0.0032376115774945175, 0.015054893835349506, 0.04095578645530565, 0.0032376115774945175, 0.11865846431517407, 0.025415250883331963, 0.02007319178046601, 0.008094028943736294, 0.0016188057887472588, 0.007608387207112116, 0.035937488510189144, 0.0137598492043517, 0.9938824406929353, 0.8579387081158683, 0.13865676090761508, 0.00794586323848739, 0.015424322757063758, 0.8506747702380618, 0.002337018599555115, 0.006076248358843298, 0.03505527899332672, 0.009815478118131483, 0.052816620349945595, 0.00046740371991102296, 0.0009348074398220459, 0.004206633479199207, 0.01261990043759762, 0.00046740371991102296, 0.986756045435811, 0.6718759231650696, 0.32593980796400696, 0.9838523943583591, 0.0010705532271576435, 0.0029975490360414016, 0.07129884492869906, 0.027620273260667204, 0.029868435037698253, 0.0007493872590103504, 0.32202241072901916, 0.007172706621956211, 0.03136720955571896, 0.025907388097214972, 0.0002141106454315287, 0.01595124308464889, 0.01306074937132325, 0.0038539916177675166, 0.0031046043587571663, 0.005780987426651275, 0.028690826487824846, 0.06466141492032167, 0.0020340511315995228, 0.0017128851634522295, 0.01841351550711147, 0.0040681022631990455, 0.06969301508796259, 0.0555617124894817, 0.00032116596814729307, 0.023659226320183923, 0.0007493872590103504, 0.0012846638725891723, 0.05309944006701912, 0.0020340511315995228, 0.06808718524722612, 0.0019269958088837583, 0.0006423319362945861, 0.04089513327742198, 0.0002141106454315287, 0.9853354155673092, 0.993858409408896, 0.991726152190982, 0.0005871976174258195, 0.005871976174258195, 0.015854335670497124, 0.17557208761032, 0.04169103083723318, 0.003523185704554917, 0.006459173791684014, 0.05402218080317539, 0.015854335670497124, 0.02055191660990368, 0.045214216541788095, 0.1990599923073528, 0.07692288788278234, 0.20962954942101755, 0.007046371409109834, 0.02994707848871679, 0.05460937842060121, 0.02231350946218114, 0.014679940435645486, 0.955298589145417, 0.044658267378749165, 0.9912012724792005, 0.03399400571669614, 0.9654297623541703, 0.9937748558256285, 0.9898849924607787, 0.9974297809832519, 0.9946658047115289, 0.9899556403861988, 0.007346840937966736, 0.00011849743448333444, 0.286289801711736, 0.10593670642810099, 0.00047398973793333777, 0.015167671613866809, 0.16163050063526818, 0.002962435862083361, 0.0009479794758666755, 0.3580992470086367, 0.037919179034667023, 0.0014219692138000134, 0.018485599779400175, 0.00319943073105003, 0.9829215152154503, 0.9973614718561445, 0.9964055265130645, 0.9913042402236442, 0.9925209877312277, 0.997138627864125, 0.9790251769458579, 0.01860926065289254, 0.03082158795635327, 0.516406983117768, 0.13840637610588827, 0.05873547893569208, 0.0011630787908057838, 0.16050487313119816, 0.0011630787908057838, 0.0017446181862086755, 0.005233854558626027, 0.004070775767820243, 0.005233854558626027, 0.02500619400232435, 0.004070775767820243, 0.026169272793130135, 0.0017446181862086755, 0.9878816659304595, 0.9963616158998392, 0.12340395776631471, 0.2732516207682683, 0.6023288414784408, 0.7457939356486323, 0.2538042189380162, 0.9971744949127413, 0.977976525552744, 0.9961485950741078, 0.997930734115677, 0.9962095192065743, 0.9783366500027796, 0.7688635387273013, 0.22943308331042475, 0.9921410108108714, 0.9620204789964438, 0.027099168422435038, 0.5167163005991714, 0.48210372543941826, 0.992700429103828, 0.029300276377179395, 0.7310418956106259, 0.2387972524740121, 0.9933299614431215, 0.9951843181166364, 0.08149345243222357, 0.9173834359513168, 0.9668529184003432, 0.030214153700010724, 0.29540751794670445, 0.3012571717674313, 0.1403916916974437, 0.23252373937389115, 0.004387240365545116, 0.02486102873808899, 0.991028921300905, 0.17658815458632737, 0.19480756736110716, 0.6264675007943519, 0.9995807815422413, 0.9990782821478414, 0.9994358419524854, 0.9933513785229984, 0.9897041592412622, 0.9925612985005435, 0.03653801785289264, 0.963275016121715, 0.9650018591277938, 0.9956736502980946, 0.9872206729351721, 0.9930094040753897, 0.9840840323053331, 0.9952306382668535, 0.9939011354115975, 0.9764510235202514, 0.99315701991327, 0.9833041597697388, 0.9971363533951515, 0.9903012494514511, 0.9932281685925338, 0.9893408299054535, 0.977035269630139, 0.002457251624626735, 0.016767128732747133, 0.0027463400510534097, 0.07848750777484219, 0.09135194275082921, 0.0008672652792800241, 0.0131535234024137, 0.01532168660061376, 0.0034690611171200965, 0.009106285432440252, 0.005203591675680145, 0.010985360204213639, 0.002457251624626735, 0.00043363263964001206, 0.0039026937567601086, 0.006504489594600181, 0.011418992843853651, 0.35008608440270306, 0.032956080612640916, 0.023271618327347313, 0.002457251624626735, 0.06128674640245504, 0.08296837838445564, 0.018935291930947194, 0.1222844043784834, 0.017634394012027158, 0.013298067615627036, 0.9936473167667695, 0.30525049897801393, 0.3739425943879749, 0.03606335009022949, 0.04464986201647461, 0.0068692095409960945, 0.11634723660062135, 0.017173023852490237, 0.02103695421930054, 0.0025759535778735352, 0.010303814311494141, 0.02189560541192505, 0.0030052791741857912, 0.03219941972341919, 0.0012879767889367676, 0.0008586511926245118, 0.0068692095409960945, 0.9921609721839885, 0.9922036200428679, 0.9896525493757361, 0.9951955875630192, 0.0483179753320302, 0.08565459263405353, 0.8631347411585395, 0.9841898795492272, 0.005263010077353277, 0.024059474639329264, 0.0304502725904011, 0.009022302989748474, 0.057517181559646524, 0.043231868492544775, 0.018044605979496947, 0.005638939368592797, 0.04285593920130525, 0.004887080786113757, 0.009022302989748474, 0.01729274739701791, 0.24059474639329265, 0.0022555757474371184, 0.2014981001043826, 0.0022555757474371184, 0.008270444407269435, 0.004887080786113757, 0.24698554434436448, 0.006766727242311356, 0.01691681810577839, 0.0022555757474371184, 0.9668823053693366, 0.9941944075960932, 0.995876710852801, 0.9913059588739143, 0.9954441658249559, 0.9909966171823958, 0.9904120128372668, 0.9856040354035934, 0.9975005977782376, 0.9848422584145539, 0.8737595661030078, 0.12287243898323547, 0.9819627158538101, 0.9820072896736012, 0.9970603438898423, 0.9910555413437965, 0.9905155832115374, 0.99129107477235, 0.9989605953528163, 0.999223678809491, 0.9846934822416711, 0.9900080813352974, 0.008398515827375002, 0.00975311515437097, 0.018422550847145166, 0.06854272594599599, 0.5927726654934357, 0.023028188558931458, 0.007856676096576615, 0.004876557577185485, 0.04957833536805243, 0.0010836794615967744, 0.039283380482883073, 0.001354599326995968, 0.013004153539161293, 0.004334717846387098, 0.0062311569041814535, 0.05608041213763308, 0.0002709198653991936, 0.04876557577185485, 0.015442432327754036, 0.010294954885169358, 0.02031898990493952, 0.03950179502731691, 0.024663557301319698, 0.023059423493103783, 0.5261558890948203, 0.0006015501780809682, 0.028072341643778517, 0.024463040575292708, 0.0002005167260269894, 0.006216018506836672, 0.020051672602698942, 0.012432037013673344, 0.0014036170821889258, 0.0004010334520539788, 0.014437204273943238, 0.06155863489028575, 0.002005167260269894, 0.003809817794512799, 0.0170439217122941, 0.001002583630134947, 0.11589866764359988, 0.028072341643778517, 0.0018046505342429048, 0.026067174383508623, 0.019851155876671953, 0.0014036170821889258, 0.0002005167260269894, 0.000777088030616087, 0.02797516910217913, 0.08917085151319598, 0.005633888221966631, 0.3454156296088507, 0.0017484480688861957, 0.4493511537037523, 0.07984579514580294, 0.9870186504495452, 0.0068236003489156516, 0.08057097335065788, 0.01719022395592212, 0.01915857021041702, 0.10196033598283578, 0.01719022395592212, 0.005248923345319732, 0.052489233453197316, 0.1640288545412416, 0.0065611541816496645, 0.001312230836329933, 0.017846339374087087, 0.028869078399258525, 0.012466192945134362, 0.017059000872289128, 0.0024932385890268725, 0.005511369512585718, 0.007217269599814631, 0.031231093904652403, 0.021258139548544913, 0.009448062021575518, 0.0001312230836329933, 0.025719724392066684, 0.007479715767080617, 0.12505559870224262, 0.0234889319703058, 0.005511369512585718, 0.014959431534161235, 0.07322248066721025, 0.053932687373160246, 0.0018371231708619062, 0.042516279097089826, 0.9768714168300516, 0.007814971334640413, 0.01041996177952055, 0.9984751781094469, 0.8136692806496796, 0.17712528558360371, 0.0059990166765667525, 0.01756854883851692, 0.008570023823666789, 0.06127567033921755, 0.03170908814756712, 0.04370712150070063, 0.005570515485383413, 0.000857002382366679, 0.15983094431138561, 0.051420142942000736, 0.004713513103016734, 0.018425551220883598, 0.5034888996404239, 0.0059990166765667525, 0.061704171530400885, 0.018425551220883598, 0.984346893071954, 0.015866716580086107, 0.019833395725107634, 0.6855743788978872, 0.16131161856420875, 0.031733433160172214, 0.08528360161796283, 0.3717696657679937, 0.19264428135250583, 0.42922497775032004, 0.9853394088293315, 0.7902944149140012, 0.19344967651595096, 0.015745903902461125, 0.9976737110180425, 0.9917047813796186, 0.6788403259940403, 0.2902935604579778, 0.004466054776276582, 0.022330273881382905, 0.9995677125288062, 0.00217441666575761, 0.7431068955226632, 0.0038052291650758174, 0.01739533332606088, 0.05055518747886443, 0.01522091666030327, 0.009241270829469841, 0.01739533332606088, 0.0217441666575761, 0.08045341663303157, 0.023918583323333707, 0.0032616249986364147, 0.011959291661666854, 0.8189571937062433, 0.14819225409922498, 0.023398776963035522, 0.9719649874862247, 0.029784071017878333, 0.9689751104483084, 0.4424167878517961, 0.390221548835573, 0.02858310708031267, 0.019883900577608814, 0.11806065967955233, 0.9945925861301153, 0.8608571137902957, 0.0018633270861261812, 0.13602287728721124, 0.9872126737469015, 0.9886523191236471, 0.998481941456916, 0.9947615230256671, 0.9774343657265495, 0.9931733079553219, 0.9798927861518321, 0.9958730619445973, 0.03194210809464036, 0.9662487698628709, 0.9778856009552228, 0.9909124941110695, 0.9901982137170833, 0.9942690668432588, 0.9938454416493461, 0.9858114206907035, 0.008399691490044802, 0.16071409717619053, 0.024639095037464753, 0.01791934184542891, 0.028558951066152326, 0.006159773759366188, 0.06719753192035842, 0.04591831347891158, 0.2239917730678614, 0.09239660639049282, 0.0011199588653393068, 0.1853531922136553, 0.13719496100406509, 0.9973769604753817, 0.9950563125279306, 0.9921724218163008, 0.9955146954699695, 0.9980765470709302, 0.979660778751156, 0.03214038911022763, 0.005123830148007304, 0.7168704179802946, 0.0037264219258234936, 0.0018632109629117468, 0.04937509051716129, 0.07778905770156543, 0.0023290137036396834, 0.0023290137036396834, 0.006521238370191114, 0.0507724987393451, 0.006055435629463177, 0.0009316054814558734, 0.006521238370191114, 0.015371490444021911, 0.0013974082221838101, 0.020495320592029215, 0.98982477125209, 0.9945651831483227, 0.005470250170614725, 0.0005470250170614725, 0.5366315417373045, 0.23029753218287993, 0.01969290061421301, 0.0339155510578113, 0.15754320491370408, 0.014769675460659758, 0.019764124596090452, 0.004235169556305097, 0.8922090531949404, 0.004235169556305097, 0.0028234463708700645, 0.033881356450440776, 0.04093997237761594, 0.0014117231854350323, 0.016585024519059452, 0.002303475627647146, 0.10181362274200385, 0.013820853765882877, 0.24969675803695063, 0.09352111048247413, 0.060811756569884655, 0.01842780502117717, 0.03086657341047176, 0.15893981830765308, 0.08062164696765012, 0.03409143928917776, 0.13820853765882876, 0.0044200866217358435, 0.002652051973041506, 0.9883313686201346, 0.0035360692973886746, 0.03225435708562682, 0.009924417564808254, 0.8931975808327428, 0.039697670259233014, 0.02232993952081857, 0.9903447613049898, 0.8037324699036761, 0.19539360797157038, 0.9913503129884857, 0.9909174761988547, 0.977739333304923, 0.702273480042342, 0.29682869640514575, 0.0029608152544277023, 0.008882445763283106, 0.9859514797244249, 0.9870956591329947, 0.9924285833210273, 0.1607182614307303, 0.10489994308951858, 0.015398156783782544, 0.001924769597972818, 0.025984389572633042, 0.020210080778714588, 0.045232085552361224, 0.07602839911992632, 0.12607240866721958, 0.20498796218410512, 0.17804118781248565, 0.040420161557429175, 0.9360383449488461, 0.005681568102876153, 0.015624312282909418, 0.02130588038578557, 0.019885488360066532, 0.008483177763731364, 0.012839404182944767, 0.00022927507469544227, 0.0018342005975635382, 0.19327888796825785, 0.014902879855203749, 0.01398577955642198, 0.00022927507469544227, 0.00022927507469544227, 0.0020634756722589806, 0.00022927507469544227, 0.4842289577567741, 0.0011463753734772114, 0.05915296927142411, 0.020634756722589805, 0.02522025821649865, 0.14169199616178332, 0.019488381349112596, 0.0005527335334474825, 0.006080068867922308, 0.0024478199338388512, 0.02834733407252089, 0.004658754067628781, 0.5074883456381386, 0.025504704471933835, 0.000868581266846044, 0.0434290633423022, 0.004106020534181299, 7.896193334964036e-05, 0.004737716000978422, 0.023214808404794268, 0.0027636676672374125, 0.003237439267335255, 0.0013423528669438863, 0.0048956398676777025, 0.0016582006003424475, 0.000868581266846044, 0.0004737716000978422, 0.0007896193334964036, 0.001105467066894965, 0.15539708483209222, 0.03158477333985615, 0.027636676672374128, 0.00031584773339856146, 0.01516069120313095, 0.04042850987501587, 0.00015792386669928073, 0.03387466940699572, 0.0086068507351108, 0.003237439267335255, 0.01500276733643167, 0.8308226078843711, 0.16616452157687422, 0.9927039386110489, 0.2894640924457941, 0.0027833085812095583, 0.03896632013693382, 0.07514933169265807, 0.49542892745530137, 0.07514933169265807, 0.01948316006846691, 0.0961261004076689, 0.7369667697921283, 0.16021016734611485, 0.9960330851402855, 0.3893393767307261, 0.6087573741019086, 0.001530823237473366, 0.5418256456952042, 0.45711023529334815, 0.9578797769046599, 0.03744155479757628, 0.9904464272005006, 0.9874986650255008, 0.12026480217647136, 0.8793798505008902, 0.163049863376174, 0.03502552620673367, 0.5483306516502444, 0.25000979051013345, 0.002415553531498874, 0.9947660189250921, 0.4381872833597523, 0.5612398766320116, 0.004491077930271889, 0.0080839402744894, 0.015269664962924421, 0.05089888320974807, 0.0017964311721087554, 0.5925228816005378, 0.0011976207814058369, 0.002095836367460215, 0.011077992228003992, 0.003592862344217511, 0.03922208059104116, 0.002095836367460215, 0.009281561055895237, 0.07215665207970168, 0.002694646758163133, 0.009281561055895237, 0.0227547948467109, 0.002694646758163133, 0.0011976207814058369, 0.006586914297732103, 0.0002994051953514592, 0.0910191793868436, 0.01137739742335545, 0.007485129883786481, 0.0032934571488660515, 0.026347657190928412, 0.9910633296415314, 0.9900837085282477, 0.9952020017961558, 0.9899538934822918, 0.015471245761983228, 0.10623588756561816, 0.1753407853024766, 0.006188498304793291, 0.08251331073057722, 0.057759317511404056, 0.0020628327682644305, 0.5549020146631318, 0.9907803846306757, 0.9748136716732684, 0.9978191411897963, 0.9973453654955542, 0.9943667072501734, 0.1112399249732505, 0.8869129153272675, 0.982685298356737, 0.9823058999635061, 0.413266716814317, 0.13738325991394862, 0.17200830916055354, 0.2770003939728395, 0.9865327397842297], \"Term\": [\"2d\", \"3d\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"abelian\", \"abelian_group\", \"able\", \"able\", \"able\", \"able_to\", \"able_to\", \"able_to\", \"abstract\", \"abstraction\", \"abstraction\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access_control\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"achievable\", \"achievable\", \"achievable_rate\", \"act\", \"action\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"activity\", \"activity\", \"activity\", \"actor\", \"acyclic\", \"ad\", \"ad_hoc\", \"additive\", \"additive\", \"additive\", \"adiabatic\", \"adjacency\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"adversary\", \"advertising\", \"affected\", \"affine\", \"affine\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"agent\", \"aggregate\", \"aggregate\", \"aggregation\", \"agreement\", \"agreement\", \"ai\", \"ai\", \"aided\", \"aimed\", \"al\", \"algebra\", \"algebra\", \"algebraic\", \"algebraic\", \"algebraic\", \"algebraic\", \"algorithm\", \"algorithm\", \"alignment\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"allocation\", \"allocation\", \"alpha\", \"alphabet\", \"alphabet\", \"alphabet\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"although\", \"although\", \"although\", \"although\", \"amplifier\", \"amplitude\", \"amplitude\", \"an_overview\", \"analog\", \"analog\", \"analyse\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analytical\", \"analytical\", \"analytical\", \"analytical\", \"angle\", \"ann\", \"annotation\", \"annotation\", \"anomaly\", \"answer\", \"answer\", \"answer\", \"answering\", \"antenna\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approaching\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation_algorithm\", \"architecture\", \"archive\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"arising_from\", \"arithmetic\", \"arithmetic\", \"arrangement\", \"array\", \"article\", \"article\", \"artifact\", \"artificial\", \"artificial\", \"artificial\", \"artificial_intelligence\", \"artificial_neural\", \"arxiv\", \"ary\", \"asp\", \"assembly\", \"assessment\", \"assessment\", \"assignment\", \"assignment\", \"assisted\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"associated_with\", \"association_rule\", \"astronomy\", \"asymptotic\", \"asymptotic\", \"asymptotic\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically_optimal\", \"asynchronous\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at\", \"at_least\", \"at_most\", \"atm\", \"atom\", \"atomic\", \"attachment\", \"attack\", \"attracted\", \"attribute\", \"auction\", \"audio\", \"author\", \"author\", \"author\", \"author\", \"author\", \"author\", \"authority\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automaton\", \"availability\", \"availability\", \"availability\", \"availability\", \"availability\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"aware\", \"away\", \"axiom\", \"axis\", \"backbone\", \"balance\", \"balance\", \"balance\", \"balanced\", \"balancing\", \"ball\", \"ball\", \"bandwidth\", \"bandwidth\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"battery\", \"bayes\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"beam\", \"beamforming\", \"because\", \"because\", \"because\", \"because\", \"because\", \"been\", \"beforehand\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"belief\", \"belief\", \"belief\", \"belief_propagation\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best_possible\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"bias\", \"bias\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binomial\", \"bipartite\", \"bipartite_graph\", \"bisimulation\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit_error\", \"bit_per\", \"black\", \"blind\", \"block\", \"block\", \"block\", \"block\", \"block_length\", \"board\", \"bob\", \"body\", \"body\", \"body\", \"body\", \"book\", \"book\", \"boolean\", \"boolean\", \"boolean\", \"boolean\", \"boolean_function\", \"border\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"bottleneck\", \"bottleneck\", \"bound\", \"bounding\", \"box\", \"branching\", \"breaking\", \"breaking\", \"breaking\", \"breakthrough\", \"bridge\", \"brief\", \"bring\", \"bring\", \"bring\", \"broadcast\", \"broadcast_channel\", \"buffer\", \"buffer\", \"bug\", \"burst\", \"bus\", \"business\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"by_applying\", \"c\", \"c\", \"ca\", \"cache\", \"caching\", \"calculated\", \"calculated\", \"calculus\", \"camera\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"cancellation\", \"capacity\", \"car\", \"card\", \"cardinality\", \"careful\", \"carlo\", \"carried\", \"carried_out\", \"carrier\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"categorical\", \"category\", \"category\", \"causal\", \"causally\", \"cdma\", \"cell\", \"cellular\", \"cellular_automaton\", \"center\", \"center\", \"center\", \"cf\", \"chain\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"channel\", \"channel\", \"chaos\", \"chaotic\", \"chapter\", \"character\", \"character_recognition\", \"check\", \"checking\", \"checking\", \"checking\", \"child\", \"chip\", \"cipher\", \"circuit\", \"citation\", \"cite\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classical\", \"classical\", \"classical\", \"classical\", \"classification\", \"classification\", \"classifier\", \"clause\", \"client\", \"client\", \"clinical\", \"closed_form\", \"closely\", \"closely\", \"closely\", \"closely\", \"closely_related\", \"closest\", \"closure\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"clustering\", \"clustering\", \"cnf\", \"code\", \"coded\", \"coded\", \"coded\", \"codewords\", \"coding\", \"coding_scheme\", \"coefficient\", \"cognitive\", \"coin\", \"collaboration\", \"collaborative\", \"collaborative\", \"collision\", \"color\", \"coloring\", \"column\", \"coming\", \"coming_from\", \"comment\", \"committee\", \"commodity\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"community\", \"commutative\", \"compact\", \"compact\", \"compact\", \"comparative\", \"comparative\", \"comparative_study\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"compatibility\", \"competitive\", \"competitive\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"completion\", \"completion\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"composite\", \"composition\", \"composition\", \"compound\", \"compound\", \"compress\", \"compressed\", \"compression\", \"compression\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational_complexity\", \"computational_complexity\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer_simulation\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"concentrate\", \"concentrate_on\", \"concerned\", \"concerned\", \"concerned\", \"concerned_with\", \"concerned_with\", \"conclude\", \"conclude\", \"conclude\", \"concurrency\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"conditional\", \"conditional\", \"conditional\", \"conducting\", \"confidential\", \"configuration\", \"configuration\", \"configuration\", \"configuration\", \"configuration\", \"conflict\", \"conflicting\", \"congestion\", \"congestion_control\", \"conjecture\", \"conjecture\", \"connected\", \"connected\", \"connected\", \"connected\", \"connected\", \"connectivity\", \"connectivity\", \"connectivity\", \"consecutive\", \"consensus\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant_factor\", \"constellation\", \"constraint\", \"constraint\", \"constraint\", \"constraint_satisfaction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"constructive\", \"constructive\", \"consuming\", \"consumption\", \"contact\", \"content\", \"content\", \"content\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context_free\", \"continuation\", \"continuing\", \"contract\", \"contraction\", \"contributed\", \"control\", \"control\", \"control\", \"control\", \"controllable\", \"controller\", \"converse\", \"converter\", \"convex\", \"convex\", \"convex_hull\", \"convolution\", \"convolutional\", \"convolutional_code\", \"cooperating\", \"cooperation\", \"cooperation\", \"cooperative\", \"cooperative\", \"coordinate\", \"coordination\", \"coordination\", \"cope\", \"cope_with\", \"copy\", \"core\", \"core\", \"core\", \"core\", \"core\", \"core\", \"corpus\", \"correcting\", \"correction\", \"correlated\", \"correlation\", \"correlation_between\", \"correspondence_between\", \"coset\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"costly\", \"count\", \"count\", \"count\", \"count\", \"counting\", \"country\", \"coupled\", \"coupled\", \"coupled\", \"coupled\", \"coupling\", \"coupling\", \"covering\", \"cr\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"cross_layer\", \"crossing\", \"crossing\", \"crossover\", \"cryptographic\", \"cryptography\", \"cryptography\", \"csma\", \"csp\", \"ct\", \"cube\", \"cubic\", \"cumulative\", \"curvature\", \"curve\", \"custom\", \"customer\", \"customized\", \"cut\", \"cutting\", \"cycle\", \"cycle\", \"cyclic\", \"d\", \"data\", \"data_mining\", \"data_stream\", \"data_warehouse\", \"database\", \"database\", \"dataset\", \"day\", \"db\", \"de\", \"deadline\", \"deal\", \"deal\", \"deal_with\", \"deal_with\", \"dealing\", \"dealing_with\", \"debugging\", \"decay\", \"decentralized\", \"decidability\", \"decidable\", \"decidable\", \"deciding_whether\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision_making\", \"decision_support\", \"decision_tree\", \"declarative\", \"decoder\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"deduction\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"degraded\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"delay\", \"delivering\", \"delta\", \"denote\", \"denote\", \"denotes\", \"density\", \"density\", \"density\", \"density\", \"dependence\", \"dependence\", \"dependence\", \"dependency\", \"dependency\", \"dependency\", \"depends\", \"depends_on\", \"depth\", \"depth\", \"depth\", \"depth\", \"derivative\", \"derive\", \"derive\", \"derive\", \"derive\", \"derive\", \"derive\", \"derive\", \"derive\", \"derive\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"designer\", \"destination\", \"detecting\", \"detecting\", \"detecting\", \"detecting\", \"detection\", \"detection\", \"detector\", \"determinant\", \"determined\", \"determined\", \"determined\", \"determined\", \"determined\", \"determined\", \"determined\", \"determined\", \"determined_by\", \"determined_by\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developer\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"deviation\", \"deviation\", \"device\", \"device\", \"diagram\", \"diagram\", \"diagram\", \"dichotomy\", \"dictionary\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"differential\", \"differential_equation\", \"differs\", \"differs_from\", \"diffusion\", \"digit\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital_library\", \"digraph\", \"dilemma\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimensional\", \"dimensional\", \"dimensional\", \"directed\", \"directed_graph\", \"directional\", \"discipline\", \"discipline\", \"discourse\", \"discrepancy\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discussion\", \"discussion\", \"discussion\", \"disease\", \"disjoint\", \"disjoint\", \"disjoint\", \"disorder\", \"displacement\", \"display\", \"display\", \"display\", \"display\", \"distance\", \"distance\", \"distance\", \"distance_between\", \"distortion\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distribution\", \"divergence\", \"diversity\", \"diversity\", \"divided\", \"divided_into\", \"division\", \"division_multiple\", \"dna\", \"dna\", \"do\", \"do\", \"do\", \"do_not\", \"do_not\", \"do_not\", \"document\", \"document\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"dominance\", \"dominated\", \"dominating\", \"dominating_set\", \"double\", \"double\", \"downlink\", \"download\", \"drawing\", \"drawing\", \"drawing\", \"driving\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamical\", \"dynamical\", \"dynamical_system\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"easily\", \"easily\", \"easily\", \"easily\", \"eavesdropper\", \"edge\", \"edge_coloring\", \"edge_weight\", \"edit\", \"edit_distance\", \"education\", \"educational\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"effort\", \"effort\", \"effort\", \"elastic\", \"election\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"elliptic\", \"em\", \"em\", \"em\", \"em\", \"em\", \"email\", \"embed\", \"embedded\", \"embedded\", \"embedded\", \"embedded\", \"embedded\", \"embedding\", \"emotion\", \"emph\", \"empirical_study\", \"empty\", \"en\", \"enables_u\", \"encoding\", \"encoding\", \"encoding\", \"encoding\", \"encoding\", \"encryption\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"energy\", \"energy\", \"energy\", \"engineering\", \"engineering\", \"english\", \"ensemble\", \"entanglement\", \"enterprise\", \"entropic\", \"entropy\", \"entry\", \"entry\", \"entry\", \"enumeration\", \"envelope\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environmental\", \"environmental\", \"epidemic\", \"epsilon\", \"equation\", \"equilibrium\", \"equipped\", \"equipped_with\", \"equivalence\", \"equivalence\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error_correcting\", \"error_correction\", \"error_probability\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimated\", \"estimated\", \"estimated\", \"estimated\", \"estimated\", \"estimating\", \"estimating\", \"estimating\", \"estimating\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"et\", \"et_al\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"evolution\", \"evolution\", \"evolution\", \"evolution\", \"evolutionary\", \"evolved\", \"evolving\", \"evolving\", \"exchange\", \"exchange\", \"exchange\", \"execution\", \"execution\", \"execution\", \"exercise\", \"existence\", \"existence\", \"existence\", \"existence\", \"existence\", \"exists\", \"expansion\", \"expansion\", \"expansion\", \"experience\", \"experience\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental_result\", \"experimental_result\", \"expert\", \"expert\", \"expertise\", \"exponent\", \"exponent\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exposure\", \"expressiveness\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"external\", \"external\", \"external\", \"extra\", \"extracted\", \"extraction\", \"extraction\", \"extremal\", \"eye\", \"fabrication\", \"face\", \"faced\", \"facial\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factorization\", \"failed\", \"failure\", \"failure\", \"false\", \"false\", \"false\", \"false\", \"family\", \"family\", \"family\", \"family\", \"far\", \"far\", \"far\", \"far\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"faster\", \"faster_than\", \"fault\", \"fault_tolerance\", \"fault_tolerant\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feed\", \"feedback\", \"fiber\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"file\", \"file\", \"film\", \"filter\", \"filtering\", \"financial\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"fine\", \"fine\", \"fingerprint\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite_automaton\", \"finite_state\", \"finitely\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first_order\", \"first_order\", \"fisher\", \"fix\", \"flat\", \"flat\", \"flat\", \"flat\", \"flipping\", \"floating\", \"floor\", \"flow\", \"flow\", \"flow\", \"fluctuation\", \"fluctuation\", \"fluid\", \"focusing\", \"focusing_on\", \"fold\", \"fold\", \"force\", \"force\", \"force\", \"force\", \"forest\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"formal\", \"formal\", \"format\", \"formation\", \"formed\", \"formed\", \"formed\", \"formed_by\", \"formula\", \"formula\", \"formula\", \"formula\", \"formula\", \"formulated_a\", \"forward\", \"forward\", \"forwarding\", \"fourier\", \"fpga\", \"fractal\", \"fractional\", \"fragment\", \"fragment\", \"frame\", \"frame\", \"frame\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"frequency\", \"frequency\", \"frequency\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"front\", \"front_end\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"fuzzy\", \"gain\", \"gain\", \"game\", \"game_theoretic\", \"gamma\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gas\", \"gate\", \"gateway\", \"gaussian\", \"gaussian_noise\", \"gene\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated_by\", \"generation\", \"generation\", \"generation\", \"generation\", \"generator\", \"generator\", \"generator\", \"genetic\", \"genetic_algorithm\", \"genome\", \"geometric\", \"geometric\", \"geometric\", \"geometric\", \"geometric\", \"geometric\", \"geometrical\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"gf\", \"girth\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"glass\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"government\", \"gps\", \"gradient\", \"grammar\", \"grammar\", \"graph\", \"graphic\", \"graphical\", \"graphical\", \"graphical\", \"graphical_model\", \"greedy\", \"greedy\", \"green\", \"grid\", \"grid_computing\", \"group\", \"group\", \"group\", \"group\", \"grows\", \"grows\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha_been\", \"half\", \"half\", \"hamiltonian\", \"hamming\", \"hamming_distance\", \"handoff\", \"handwritten\", \"hard\", \"hard\", \"hardness\", \"hardness\", \"hardware\", \"hardware\", \"hardware\", \"harmonic\", \"hash\", \"hashing\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have_been\", \"he\", \"he\", \"he\", \"head\", \"health\", \"heterogeneity\", \"heterogeneous\", \"heuristic\", \"heuristic\", \"hidden\", \"hidden\", \"hidden\", \"hidden_markov\", \"hierarchical_clustering\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high_dimensional\", \"high_dimensional\", \"high_level\", \"high_level\", \"high_level\", \"high_level\", \"high_probability\", \"higher_order\", \"higher_order\", \"hilbert\", \"hilbert_space\", \"hit\", \"hoc\", \"hoc_network\", \"hole\", \"home\", \"homology\", \"homomorphism\", \"hop\", \"host\", \"host\", \"house\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"http\", \"http\", \"hub\", \"hull\", \"human\", \"human\", \"human\", \"human\", \"hybrid\", \"hybrid\", \"hybrid\", \"hyperbolic\", \"hypergraph\", \"hyperplane\", \"id\", \"identification\", \"identification\", \"identification\", \"identification\", \"ieee\", \"if\", \"if\", \"if\", \"if\", \"if\", \"image\", \"image_processing\", \"impact\", \"impact\", \"impact\", \"impact\", \"imperfect\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implying\", \"impose\", \"incentive\", \"incentive\", \"inconsistent\", \"increasingly_important\", \"independent_set\", \"index\", \"index\", \"indicator\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"induced_by\", \"induction\", \"inductive\", \"industry\", \"inequality\", \"inexpensive\", \"inference\", \"inference\", \"inference\", \"inferring\", \"infinite\", \"infinite\", \"infinite\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information_retrieval\", \"information_theoretic\", \"information_theoretic\", \"infrastructure\", \"infrastructure\", \"infrastructure\", \"infrastructure\", \"infty\", \"innovative\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"insertion\", \"insertion\", \"inspired\", \"inspired_by\", \"instant\", \"instantaneous\", \"institution\", \"instruction\", \"integer\", \"integer\", \"integer\", \"integral\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"integration\", \"integration\", \"integration\", \"integration\", \"intelligence\", \"intelligence\", \"intelligent\", \"intelligent\", \"intensity\", \"interacting\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interactive\", \"interconnected\", \"interconnection\", \"interdependency\", \"interface\", \"interleaved\", \"internal\", \"internal\", \"internal\", \"internal\", \"international\", \"internet\", \"internet\", \"internet\", \"internet\", \"interoperability\", \"interplay\", \"interplay_between\", \"interpolation\", \"interpretation\", \"interpretation\", \"intersection\", \"interval\", \"interval\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"intrinsic\", \"intrusion\", \"intrusion_detection\", \"intuitionistic\", \"inverse\", \"inversion\", \"ip\", \"irregular\", \"ising\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"item\", \"iterated\", \"iteration\", \"iteration\", \"iterative\", \"iterative\", \"iterative\", \"iterative_decoding\", \"java\", \"java\", \"job\", \"join\", \"joint\", \"joint\", \"joint\", \"joint_source\", \"journal\", \"kernel\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key_distribution\", \"kinematic\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge_about\", \"knowledge_base\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"kolmogorov\", \"kolmogorov_complexity\", \"la\", \"label\", \"labeling\", \"laboratory\", \"lambda\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large_amount\", \"large_scale\", \"lattice\", \"law\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layered\", \"layout\", \"ldpc\", \"ldpc_code\", \"leaf\", \"learning\", \"learning\", \"least\", \"least\", \"least_square\", \"leg\", \"legitimate\", \"legitimate\", \"lemma\", \"lemma\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"leq\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"lexical\", \"lexical\", \"lexicon\", \"licensed\", \"light\", \"lightweight\", \"likelihood\", \"likelihood\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_programming\", \"linguistic\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"list\", \"live\", \"living\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load_balancing\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local_search\", \"localization\", \"located\", \"locating\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"log\", \"log\", \"log_log\", \"logarithm\", \"logarithmic\", \"logarithmic\", \"logic\", \"logic_program\", \"logic_program\", \"logic_programming\", \"logic_programming\", \"logical\", \"logical\", \"logical\", \"long_term\", \"long_term\", \"longest\", \"look\", \"look\", \"look_at\", \"lookup\", \"loop\", \"loop\", \"lossy\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low_cost\", \"low_density\", \"lower\", \"lower_bound\", \"lp\", \"mac\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine_learning\", \"main_contribution\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"malicious\", \"manage\", \"management\", \"management\", \"management\", \"management\", \"management\", \"manifold\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"map\", \"map\", \"map\", \"map\", \"map\", \"mark\", \"market\", \"markov\", \"markov\", \"markov_chain\", \"markov_decision\", \"markovian\", \"mass\", \"mass\", \"massive\", \"matched\", \"matching\", \"matching\", \"matching\", \"math\", \"mathematics\", \"matrix\", \"matrix\", \"matrix_multiplication\", \"max\", \"max\", \"maximal\", \"maximal\", \"maximization\", \"maximization\", \"maximizing\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum_entropy\", \"maximum_likelihood\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may_be\", \"may_be\", \"may_be\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean_field\", \"meaning\", \"meaning\", \"meaning\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"median\", \"medical\", \"medium_access\", \"memory\", \"memoryless\", \"merge\", \"merging\", \"mesh\", \"message\", \"message\", \"message\", \"message_passing\", \"met\", \"metadata\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"metric\", \"metric\", \"metric\", \"metric_space\", \"micro\", \"microsoft\", \"minimization\", \"minimization\", \"minimizing\", \"minimizing\", \"minimizing\", \"minimizing\", \"minimizing\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum_distance\", \"mining\", \"minor\", \"mixing\", \"ml\", \"mobile\", \"mobile\", \"mobile_ad\", \"mobility\", \"modal\", \"mode\", \"mode\", \"mode\", \"mode\", \"model\", \"model\", \"model_checking\", \"modeling\", \"modeling\", \"modelling\", \"modular\", \"modulated\", \"modulation\", \"module\", \"modulo\", \"molecular\", \"moment\", \"monitoring\", \"monitoring\", \"monitoring\", \"monte\", \"monte_carlo\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more_importantly\", \"morphisms\", \"morphological\", \"most_important\", \"motif\", \"motion\", \"move\", \"move\", \"move\", \"move\", \"moving\", \"moving\", \"moving\", \"moving\", \"moving\", \"mu\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi_agent\", \"multicast\", \"multidimensional\", \"multilevel\", \"multimedia\", \"multimedia\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple_input\", \"multiple_output\", \"multiplication\", \"multiplication\", \"multiplicity\", \"music\", \"must_be\", \"mutation\", \"mutual\", \"mutual\", \"mutual_information\", \"naive\", \"name\", \"nation\", \"national\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"navigation\", \"near_optimal\", \"nearest\", \"nearest_neighbor\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"necessary_condition\", \"neighbor\", \"neighbor\", \"neighborhood\", \"net\", \"network\", \"network\", \"neural\", \"neural_network\", \"neuron\", \"neuron\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newton\", \"next_generation\", \"nine\", \"node\", \"node\", \"noise\", \"noise\", \"nominal\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non_negative\", \"non_zero\", \"nonlinear\", \"nonzero\", \"norm\", \"normal\", \"normal\", \"normal\", \"normal\", \"normal\", \"normal\", \"normal_form\", \"normalization\", \"normalized\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"notation\", \"notion\", \"notion\", \"notion\", \"notion\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novelty\", \"np\", \"np\", \"np_complete\", \"np_hard\", \"null\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numerical\", \"numerical\", \"numerical\", \"numerical\", \"numerical\", \"numerical_result\", \"numerical_result\", \"numerical_simulation\", \"object\", \"object_oriented\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"oblivious\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observer\", \"observing\", \"obstacle\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"odd\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"offs\", \"omega\", \"omega\", \"omega\", \"omega\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one_dimensional\", \"one_hand\", \"online\", \"online\", \"online\", \"online_social\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"ontology\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open_question\", \"open_source\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operator\", \"operator\", \"operator\", \"opinion\", \"opportunistic\", \"opportunistic\", \"optical\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimality\", \"optimality\", \"optimally\", \"optimally\", \"optimisation\", \"optimization\", \"optimization\", \"optimization_problem\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"oracle\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"organisation\", \"organization\", \"organization\", \"organization\", \"orientation\", \"oriented\", \"oriented\", \"orthogonal\", \"orthogonal\", \"orthogonal\", \"oscillator\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"outer\", \"outlier\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"overhead\", \"overhead\", \"overhead\", \"overlap\", \"overlapping\", \"overview\", \"pac\", \"package\", \"package\", \"package\", \"packet\", \"packet_loss\", \"packing\", \"page\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pairing\", \"pairwise\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"parallel\", \"parallel\", \"parallel\", \"parallelism\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameterized\", \"parameterized\", \"parametric\", \"parametric\", \"parity\", \"parity_check\", \"parser\", \"parsing\", \"parsing\", \"particle\", \"partition\", \"partitioning\", \"partitioning\", \"partner\", \"party\", \"party\", \"passing\", \"passive\", \"password\", \"patch\", \"path\", \"path\", \"patient\", \"pattern\", \"pattern\", \"pattern\", \"pattern_matching\", \"payment\", \"pc\", \"peak\", \"peer\", \"peer\", \"penalty\", \"penalty\", \"people\", \"perceived\", \"percolation\", \"perfect\", \"perfect\", \"perfect\", \"perfectly\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"period\", \"period\", \"permanent\", \"permutation\", \"permutation\", \"persistent\", \"person\", \"perturbation\", \"pervasive\", \"petri\", \"petri_net\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase_transition\", \"phone\", \"physic\", \"physic\", \"physic\", \"pi\", \"piecewise\", \"pilot\", \"pixel\", \"placed\", \"placement\", \"plan\", \"plan\", \"planar\", \"planar\", \"planar_graph\", \"planner\", \"planning\", \"planning\", \"platform\", \"platform\", \"platform\", \"player\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointed\", \"poisson\", \"policy\", \"policy\", \"policy\", \"policy\", \"poly\", \"polygon\", \"polyhedron\", \"polynomial\", \"polynomial\", \"polynomial_time\", \"polytope\", \"population\", \"population\", \"portal\", \"position\", \"positioning\", \"positioning\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive_integer\", \"post\", \"post\", \"post_processing\", \"posterior\", \"posteriori\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power_constraint\", \"power_law\", \"pr\", \"precision\", \"precision\", \"precision\", \"prediction\", \"prediction\", \"prediction\", \"predictive\", \"preference\", \"preference\", \"preference\", \"preferential\", \"preprocessing\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"preservation\", \"preserving\", \"preserving\", \"pressure\", \"previously_known\", \"price\", \"pricing\", \"primary\", \"primary\", \"prime\", \"principal\", \"principal\", \"principal_component\", \"print\", \"prior_knowledge\", \"privacy\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability_density\", \"probability_distribution\", \"probe\", \"probing\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processor\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"profile\", \"program\", \"programmable\", \"programming\", \"programming\", \"programming\", \"programming_language\", \"progression\", \"project\", \"projection\", \"projective\", \"prolog\", \"prolog\", \"proof\", \"proof\", \"proof\", \"propagate\", \"propagation\", \"propagation\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"propositional\", \"protection\", \"protein\", \"protocol\", \"protocol\", \"protocol\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove_that\", \"prove_that\", \"provider\", \"proximity\", \"proxy\", \"pseudo\", \"publication\", \"publish\", \"published\", \"published\", \"published\", \"published\", \"published\", \"published\", \"publishing\", \"pulse\", \"pursuit\", \"push\", \"puzzle\", \"qos\", \"qualitative\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quantifier\", \"quantum\", \"quantum_computation\", \"quantum_computer\", \"quasi\", \"qubit\", \"qubits\", \"query\", \"query\", \"queueing\", \"quotient\", \"radical\", \"radio\", \"radio\", \"radio\", \"random\", \"random\", \"random\", \"random_variable\", \"random_variable\", \"random_walk\", \"randomly_generated\", \"randomness\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"ranging\", \"ranging_from\", \"rank\", \"ranking\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate_distortion\", \"rather\", \"rather\", \"rather\", \"rating\", \"ratio\", \"ratio\", \"ratio\", \"rational\", \"rayleigh\", \"reachability\", \"reaction\", \"read\", \"read\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real_time\", \"realization\", \"realization\", \"really\", \"reasoning\", \"reasoning\", \"reasoning_about\", \"receive\", \"receive\", \"receiver\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently_introduced\", \"recognition\", \"recognition\", \"recommendation\", \"reconstruct\", \"reconstructing\", \"reconstruction\", \"record\", \"recover\", \"recover\", \"recovering\", \"recovery\", \"recursion\", \"red\", \"reducibility\", \"redundancy\", \"redundancy\", \"redundancy\", \"redundancy\", \"reed\", \"refinement\", \"refinement\", \"refinement\", \"reflection\", \"regarded\", \"regarded_a\", \"regime\", \"regime\", \"region\", \"region\", \"register\", \"registration\", \"regression\", \"regression\", \"regular\", \"regular\", \"regular_expression\", \"regular_language\", \"reinforcement\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation_between\", \"relation_between\", \"relational\", \"relational\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relaxation\", \"relaxation\", \"relay\", \"reliability\", \"reliability\", \"reliability\", \"reliability\", \"rely\", \"rely\", \"rely_on\", \"rely_on\", \"removal\", \"removing\", \"repeat\", \"replacement\", \"replica\", \"replication\", \"repository\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"reproduce\", \"reproducing\", \"request\", \"request\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"residual\", \"resolution\", \"resource\", \"resource\", \"resource_allocation\", \"resource_management\", \"resp\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retrieval\", \"retrieval\", \"revenue\", \"reverse\", \"reversible\", \"review\", \"review\", \"review\", \"reviewed\", \"revision\", \"reward\", \"rewrite\", \"rewriting\", \"rigid\", \"ring\", \"rise\", \"risk\", \"road\", \"robot\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"root\", \"root\", \"rotation\", \"round\", \"round\", \"round\", \"round\", \"rounding\", \"route\", \"router\", \"routing\", \"row\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"running_time\", \"safe\", \"safety\", \"sample\", \"sample\", \"sampling\", \"sat\", \"satellite\", \"satisfaction\", \"satisfaction\", \"satisfiability\", \"satisfiability\", \"satisfiability\", \"saturation\", \"sc\", \"scalar\", \"scale\", \"scale\", \"scale\", \"scale_free\", \"scan\", \"scene\", \"schedule\", \"scheduler\", \"scheduling\", \"scheduling\", \"schema\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"science\", \"science\", \"scientific\", \"screen\", \"script\", \"search\", \"search\", \"search\", \"search\", \"search_engine\", \"secondary\", \"secrecy\", \"secret\", \"secure\", \"security\", \"seed\", \"seed\", \"segment\", \"segment\", \"segment\", \"segmentation\", \"selecting\", \"selecting\", \"selecting\", \"selecting\", \"selection\", \"selective\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self_stabilizing\", \"selfish\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantics\", \"semantics\", \"semidefinite\", \"seminal\", \"send\", \"sender\", \"sending\", \"sends\", \"sensor\", \"sent\", \"sentence\", \"separation\", \"separation\", \"separation\", \"separation\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequent\", \"sequent_calculus\", \"sequentially\", \"series\", \"series\", \"server\", \"service\", \"service\", \"service_oriented\", \"service_provider\", \"service_qos\", \"session\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"shall\", \"shannon\", \"shannon\", \"shape\", \"sharing\", \"sharing\", \"shift\", \"shift\", \"shortest\", \"shortest_path\", \"shot\", \"should\", \"should_be\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"side\", \"side\", \"side\", \"side\", \"side\", \"sided\", \"sigma\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal_processing\", \"signature\", \"silicon\", \"similarity\", \"similarity\", \"similarity_between\", \"similarity_measure\", \"simplicial\", \"simulated\", \"simulated\", \"simulated\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation_result\", \"simulation_result\", \"simulation_result\", \"simulation_result\", \"simulator\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"singular\", \"site\", \"six\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"slice\", \"slot\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small_world\", \"smart\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so_far\", \"social\", \"soft\", \"software\", \"software\", \"software_development\", \"software_engineering\", \"solid\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solve\", \"solve\", \"solve\", \"solve\", \"solver\", \"solving\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"sorting\", \"soundness\", \"source\", \"source\", \"source\", \"source_coding\", \"source_destination\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spam\", \"sparse\", \"sparsity\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatially\", \"specification\", \"specification\", \"specification\", \"specification\", \"spectral\", \"spectral\", \"spectrum\", \"spectrum\", \"spectrum\", \"speech\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed_up\", \"speed_up\", \"spike\", \"spin\", \"spin_glass\", \"splitting\", \"spread\", \"spread\", \"spreading\", \"spreadsheet\", \"sqrt\", \"sqrt\", \"square\", \"square\", \"square\", \"stability\", \"stabilization\", \"stabilizing\", \"stable\", \"standardization\", \"starting_from\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state_information\", \"station\", \"stationary\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical_mechanic\", \"statistical_physic\", \"steady\", \"steady\", \"steady_state\", \"steady_state\", \"stochastic\", \"stock\", \"storage\", \"storing\", \"straight\", \"strategic\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"stream\", \"stream\", \"stream\", \"streaming\", \"string\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"student\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"su\", \"subdivision\", \"subgraph\", \"subgraphs\", \"subgroup\", \"suboptimal\", \"subsequence\", \"subset\", \"subset\", \"subset\", \"subspace\", \"substrate\", \"substring\", \"subsystem\", \"succeed\", \"successive\", \"successive\", \"successive\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"sufficient\", \"sufficient_condition\", \"sufficiently_large\", \"suggestion\", \"sum\", \"sum\", \"sum\", \"summarizes\", \"superposition\", \"supervised\", \"supply\", \"supply\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"surface\", \"survey\", \"survey\", \"survey\", \"symbol\", \"symbol\", \"symbol\", \"symmetry\", \"symmetry_breaking\", \"synchronization\", \"synthesis\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tackle\", \"tag\", \"tag\", \"target\", \"target\", \"target\", \"target\", \"targeted\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"taxonomy\", \"tcp_ip\", \"teaching\", \"team\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"telecommunication\", \"temperature\", \"template\", \"template\", \"temporal\", \"temporal\", \"temporal\", \"temporal_logic\", \"tends\", \"tends\", \"tensor\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"terminal\", \"termination\", \"ternary\", \"test\", \"test\", \"testing\", \"text\", \"text\", \"th\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theoretic\", \"theoretic\", \"theoretic\", \"theoretical_analysis\", \"theoretical_analysis\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there_exist\", \"there_exists\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"thermal\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"thesis\", \"theta\", \"theta\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"thin\", \"this_article\", \"this_article\", \"this_chapter\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_thesis\", \"thread\", \"threat\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"threshold\", \"threshold\", \"thresholding\", \"throughput\", \"throughput\", \"tie\", \"tier\", \"tight\", \"tile\", \"tiling\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time_consuming\", \"time_series\", \"time_varying\", \"token\", \"tolerance\", \"tolerant\", \"tomography\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"toolkit\", \"top\", \"topological\", \"topological\", \"topological\", \"topology\", \"topology\", \"trace\", \"tracing\", \"tracking\", \"trade\", \"trade_off\", \"trade_offs\", \"tradeoff\", \"tradeoff\", \"tradeoff_between\", \"trading\", \"trading\", \"traffic\", \"traffic\", \"trained\", \"training\", \"training\", \"training\", \"trajectory\", \"transaction\", \"transfer\", \"transfer\", \"transform\", \"transform\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transforms\", \"transition\", \"transition\", \"transition\", \"transmission\", \"transmit\", \"transmitter\", \"transport\", \"travel\", \"traveling\", \"tree\", \"tree\", \"tremendous\", \"triangle\", \"triangular\", \"triangulation\", \"trip\", \"trust\", \"try\", \"tsp\", \"tuning\", \"tuples\", \"turbo\", \"turbo_code\", \"turing\", \"turing_machine\", \"tv\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two_dimensional\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"typed\", \"ubiquitous\", \"un\", \"uncertain\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"unconditional\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"undertaken\", \"undirected\", \"undirected_graph\", \"unfortunately\", \"unification\", \"uniqueness\", \"unitary\", \"univariate\", \"universal\", \"universe\", \"university\", \"university\", \"unnecessary\", \"unreliable\", \"update\", \"updated\", \"updating\", \"uplink\", \"upper\", \"upper_bound\", \"urban\", \"usability\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user_interface\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"usually\", \"usually\", \"usually\", \"utility\", \"validation\", \"validation\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"vanishing\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variance\", \"variance\", \"variance\", \"variational\", \"vector\", \"vector\", \"vector\", \"vehicle\", \"velocity\", \"verification\", \"verification\", \"verification\", \"verification\", \"vertex\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very_large\", \"very_large\", \"very_large\", \"very_little\", \"video\", \"video\", \"view\", \"view\", \"view\", \"view\", \"view\", \"viewpoint\", \"virtual\", \"virtual\", \"virtual\", \"visibility\", \"visible\", \"visual\", \"visualization\", \"viterbi\", \"voice\", \"voip\", \"voltage\", \"volume\", \"volume\", \"von\", \"voronoi\", \"voter\", \"voting\", \"vulnerability\", \"vulnerable\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"walk\", \"warehouse\", \"water\", \"wave\", \"wavelet\", \"wavelet_transform\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"we_believe\", \"we_conclude\", \"we_consider\", \"we_consider\", \"we_consider\", \"we_consider\", \"we_consider\", \"we_consider\", \"we_consider\", \"we_consider\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_prove\", \"we_prove\", \"we_prove\", \"we_prove\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weakly\", \"web\", \"web\", \"web_page\", \"website\", \"week\", \"weight\", \"weight\", \"weighted\", \"weighted\", \"weighted\", \"welfare\", \"well_studied\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"what\", \"what\", \"what\", \"what\", \"what\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"white\", \"white\", \"wideband\", \"widely\", \"widely\", \"widely\", \"widely\", \"widely\", \"widely\", \"widely\", \"widely_used\", \"widely_used\", \"widely_used\", \"width\", \"will\", \"will\", \"will\", \"will_be\", \"will_be\", \"window\", \"window\", \"wire\", \"wired\", \"wireless\", \"wireless\", \"with_respect\", \"with_respect\", \"with_respect\", \"with_respect\", \"with_respect\", \"withdrawn\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"workflow\", \"workload\", \"workshop\", \"workspace\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worse\", \"worse_than\", \"worst\", \"worst_case\", \"write\", \"xml\", \"xml\", \"xor\", \"your\", \"zero\", \"zero\", \"zero\", \"zero\", \"zone\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el201221403475790047365016217701\", ldavis_el201221403475790047365016217701_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el201221403475790047365016217701\", ldavis_el201221403475790047365016217701_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el201221403475790047365016217701\", ldavis_el201221403475790047365016217701_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df_train_20000['Abstract'].tolist()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "#!pip3 install gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "#only ones that appear 20 times or more.\n",
    "bigram = Phrases(docs, min_count=25)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "#training parameters.\n",
    "NUM_TOPICS = 40\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None \n",
    "\n",
    "temp = dictionary[0]  \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "outputfile = f'model{NUM_TOPICS}.gensim'\n",
    "print(\"Saving model in \" + outputfile)\n",
    "print(\"\")\n",
    "model.save(outputfile)\n",
    "\n",
    "top_topics = model.top_topics(corpus)\n",
    "model.num_topics\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / NUM_TOPICS\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "model.print_topics( num_words=20)\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-dealing",
   "metadata": {},
   "source": [
    "## LDA with K=40 and first 1000 training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "standard-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/chaitanyamuvva/opt/anaconda3/lib/python3.8/asyncio/events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 715\n",
      "Number of documents: 1000\n",
      "Saving model in model40.gensim\n",
      "\n",
      "Average topic coherence: -1.9661.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el201221403474645047049529299960\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el201221403474645047049529299960_data = {\"mdsDat\": {\"x\": [0.028177198670769624, 0.107531326345538, -0.060980349151798285, 0.009006153306123739, 0.08283379585844432, -0.039066647843456925, 0.00790806647922433, 0.026290236036048707, 0.021434682405161394, -0.11944161464673618, -0.00203650487615392, -0.0440834917482231, -0.011833995032617816, 0.01631544499898514, -0.0402638913113692, -0.0007691628173944339, -0.21943151087748686, 0.17276541868829717, -0.019672730258250365, -0.05486656151120062, 0.026517027120575014, -0.005833565386455222, 0.01786496982429103, 0.0895506747869405, 0.02420693001411955, 0.027552621039452368, -0.05047917128578513, -0.0827382740144265, 0.21335050919254606, -0.05375545210769019, 0.07402992909240261, -0.07794977092091354, -0.08318572620186955, -0.042078300771380445, -0.12070702852975167, -0.08330954970211442, 0.06617825077141884, 0.06903438176751285, 0.11897617002167216, 0.012959512575549316], \"y\": [0.008022268026794345, -0.094627841229184, -0.17553944819807354, -0.013442666845850921, 0.012088764538281739, 0.08101051477406125, -0.06574237006734106, 0.06364941598182668, 0.10424536727587641, 0.04354488140318593, -0.09937874430324839, -0.019762344232365465, -0.07386822538769508, -0.051763276246338724, -0.008554566565916875, 0.050324189874568756, -0.061506672650230056, 0.0007157631046605925, 0.11471989416556695, 0.08689202428785966, 0.03134888833271878, -0.02220961110137101, 0.054056480758581146, 0.0869595730687453, -0.04038504071660953, 0.0022881888971750925, 0.04290523697922701, 0.10474507771461843, -0.04691629077277525, 0.007818515434166225, -0.0172938306489747, 0.12146108871827345, -0.10867614760362797, -0.010357728968163801, -0.11223738810389995, -0.06286484490777047, -0.02173834836168929, 0.12078286274035346, -0.15293753494936982, 0.12222392578395591], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [3.1706145649890374, 2.965624816402354, 2.9768847781079892, 5.144770910687585, 2.2515281915265, 2.685613405407516, 2.8276311593648087, 2.653148866696515, 4.086412819229956, 2.8369898659154007, 1.5699456870435904, 1.2998184228103737, 2.4259328577490016, 0.7305552634209689, 1.308549689060154, 3.107751051020788, 2.6872978866857182, 3.5440029480590467, 2.5784989139313965, 2.67194611414008, 1.8239955060867725, 1.230522505194725, 2.349431995249275, 1.6687437604450153, 2.821368056963438, 2.51404920786114, 2.035561836057245, 4.871489787850471, 2.1096236366425836, 1.1310678745068958, 2.770862937895114, 2.21030975921998, 2.4861418412420417, 2.222523265023394, 1.8334369604117167, 2.1161391293356395, 1.2272763602516845, 3.6402406356556347, 2.5341263078291862, 2.879570424029266]}, \"tinfo\": {\"Term\": [\"model\", \"grammar\", \"word\", \"discourse\", \"lexical\", \"algorithm\", \"language\", \"system\", \"dialogue\", \"learning\", \"knowledge\", \"tree\", \"speech\", \"structure\", \"text\", \"context\", \"feature\", \"natural\", \"natural_language\", \"theory\", \"problem\", \"network\", \"lexicon\", \"parsing\", \"tagging\", \"translation\", \"sentence\", \"can\", \"based\", \"planning\", \"linguistics\", \"mapping\", \"research\", \"discussed\", \"map\", \"project\", \"issue\", \"le\", \"recent\", \"nlp\", \"computation\", \"have_been\", \"done\", \"area\", \"literature\", \"field\", \"current\", \"been\", \"question\", \"attempt\", \"employ\", \"important\", \"classification\", \"reasoning\", \"purpose\", \"scale\", \"work\", \"developed\", \"difficulty\", \"ha_been\", \"have\", \"ha\", \"some\", \"automatic\", \"system\", \"it\", \"computational\", \"information\", \"so\", \"which\", \"corpus\", \"be\", \"by\", \"a\", \"one\", \"not\", \"from\", \"based\", \"clause\", \"formula\", \"artificial\", \"logic\", \"program\", \"containing\", \"special\", \"em\", \"construct\", \"fact\", \"class\", \"non\", \"reasoning\", \"scheme\", \"least\", \"instead\", \"yield\", \"generalization\", \"programming\", \"property\", \"relative\", \"underlying\", \"operation\", \"main\", \"represented\", \"world\", \"although\", \"induction\", \"linear\", \"degree\", \"complete\", \"order\", \"definition\", \"there\", \"we_show\", \"called\", \"result\", \"set\", \"by\", \"show\", \"one\", \"it\", \"function\", \"a\", \"these\", \"not\", \"general\", \"some\", \"learning\", \"can\", \"be\", \"method\", \"this_paper\", \"language\", \"interpretation\", \"computationally\", \"meaning\", \"inference\", \"query\", \"higher\", \"functional\", \"subject\", \"subset\", \"object\", \"argument\", \"focus\", \"account\", \"notion\", \"treatment\", \"semantic\", \"previous\", \"semantics\", \"resource\", \"unification\", \"extended\", \"mechanism\", \"fact\", \"position\", \"syntactic\", \"be_used\", \"linear\", \"such_a\", \"providing\", \"naturally\", \"effect\", \"em\", \"structure\", \"a\", \"non\", \"such\", \"it\", \"which\", \"by\", \"analysis\", \"be\", \"function\", \"theory\", \"order\", \"approach\", \"used\", \"can\", \"show\", \"this_paper\", \"one\", \"use\", \"lexicalized\", \"parsing\", \"down\", \"line\", \"efficiently\", \"context_free\", \"parser\", \"include\", \"practical\", \"computation\", \"up\", \"top\", \"fast\", \"improving\", \"methodology\", \"reduce\", \"free\", \"grammar\", \"technique\", \"required\", \"surface\", \"efficient\", \"generation\", \"off\", \"extended\", \"treatment\", \"generalized\", \"be_used\", \"traditional\", \"others\", \"can_be\", \"define\", \"general\", \"we_show\", \"how\", \"can\", \"be\", \"used\", \"input\", \"such\", \"provide\", \"algorithm\", \"show\", \"context\", \"a\", \"which\", \"representation\", \"some\", \"application\", \"it\", \"this_paper\", \"based\", \"using\", \"by\", \"approach\", \"also\", \"from\", \"clustering\", \"goal\", \"utility\", \"against\", \"might\", \"significant\", \"improvement\", \"achieve\", \"variant\", \"domain\", \"perform\", \"evaluated\", \"report\", \"criterion\", \"reported\", \"improved\", \"language_processing\", \"situation\", \"agent\", \"offer\", \"control\", \"distinction\", \"processor\", \"off\", \"strategy\", \"training\", \"increase\", \"useful\", \"detail\", \"initial\", \"often\", \"test\", \"performance\", \"one\", \"used\", \"can\", \"model\", \"be\", \"system\", \"but\", \"it\", \"by\", \"method\", \"data\", \"which\", \"language\", \"can_be\", \"a\", \"approach\", \"their\", \"from\", \"these\", \"representation\", \"not\", \"coverage\", \"wide\", \"french\", \"suggest\", \"driven\", \"development\", \"english\", \"compare\", \"produced\", \"initial\", \"limited\", \"being\", \"amount\", \"written\", \"combined\", \"would\", \"various\", \"subset\", \"quality\", \"grammar\", \"significant\", \"combine\", \"experiment\", \"preliminary\", \"parser\", \"down\", \"implement\", \"unification\", \"top\", \"either\", \"very\", \"we_describe\", \"domain\", \"parsing\", \"ha_been\", \"been\", \"based\", \"using\", \"system\", \"data\", \"describe\", \"performance\", \"algorithm\", \"have\", \"our\", \"ha\", \"be\", \"it\", \"statistical\", \"can\", \"a\", \"which\", \"learning\", \"by\", \"corpus\", \"from\", \"model\", \"this_paper\", \"condition\", \"identify\", \"control\", \"role\", \"principle\", \"determine\", \"generate\", \"incremental\", \"apply\", \"choice\", \"according\", \"determining\", \"consider\", \"represented\", \"factor\", \"directly\", \"element\", \"far\", \"contextual\", \"source\", \"suggests\", \"stochastic\", \"u\", \"ambiguous\", \"theory\", \"described\", \"partial\", \"french\", \"degree\", \"indicate\", \"sentence\", \"there\", \"semantics\", \"different\", \"relation\", \"be\", \"rule\", \"general\", \"between\", \"set\", \"representation\", \"problem\", \"can\", \"these\", \"both\", \"linguistic\", \"by\", \"information\", \"can_be\", \"domain\", \"semantic\", \"which\", \"based\", \"order\", \"method\", \"this_paper\", \"a\", \"or\", \"transducer\", \"correction\", \"finite_state\", \"finite\", \"hidden\", \"indicate\", \"perspective\", \"corresponding\", \"string\", \"generalized\", \"error\", \"state\", \"unknown\", \"improving\", \"markov\", \"whose\", \"extension\", \"stochastic\", \"underlying\", \"traditional\", \"relies\", \"single\", \"criterion\", \"characteristic\", \"morphology\", \"programming\", \"efficiently\", \"represented\", \"any\", \"via\", \"constraint\", \"rule\", \"model\", \"each\", \"morphological\", \"word\", \"algorithm\", \"our\", \"these\", \"language\", \"from\", \"into\", \"present\", \"which\", \"based\", \"by\", \"can\", \"be\", \"recognition\", \"different\", \"based_on\", \"system\", \"problem\", \"similarity\", \"parallel\", \"symbolic\", \"word\", \"word_sense\", \"content\", \"bilingual\", \"frequency\", \"spoken_language\", \"sense\", \"combination\", \"occurrence\", \"distribution\", \"similar\", \"pair\", \"dictionary\", \"analyze\", \"accurate\", \"detailed\", \"evaluate\", \"matching\", \"various\", \"interest\", \"even\", \"estimation\", \"statistical\", \"measure\", \"clustering\", \"translation\", \"related\", \"method\", \"spoken\", \"so\", \"analysis\", \"semantic\", \"between\", \"approach\", \"using\", \"model\", \"based_on\", \"based\", \"language\", \"text\", \"which\", \"information\", \"by\", \"not\", \"new\", \"more\", \"corpus\", \"also\", \"or\", \"from\", \"this_paper\", \"it\", \"be\", \"interface\", \"natural_language\", \"natural\", \"hypothesis\", \"understanding\", \"spoken_language\", \"generated\", \"module\", \"built\", \"french\", \"consists\", \"language\", \"knowledge_base\", \"user\", \"pattern\", \"describe\", \"database\", \"meaning\", \"via\", \"construction\", \"sample\", \"define\", \"we_describe\", \"basis\", \"generate\", \"language_processing\", \"spoken\", \"generation\", \"tool\", \"produce\", \"utterance\", \"semantic\", \"linguistic\", \"system\", \"from\", \"domain\", \"into\", \"knowledge\", \"class\", \"how\", \"used\", \"sentence\", \"it\", \"this_paper\", \"application\", \"be\", \"knowledge_base\", \"program\", \"conceptual\", \"knowledge\", \"relation\", \"situation\", \"base\", \"reasoning\", \"answer\", \"action\", \"able\", \"lead\", \"inference\", \"might\", \"environment\", \"concept\", \"finding\", \"acquired\", \"definition\", \"world\", \"addition\", \"question\", \"about\", \"basis\", \"constructed\", \"multiple\", \"choice\", \"distinguish\", \"construction\", \"planning\", \"over\", \"describes\", \"type\", \"first\", \"from\", \"information\", \"it\", \"other\", \"representation\", \"system\", \"based\", \"order\", \"this_paper\", \"semantic\", \"model\", \"text\", \"used\", \"morphology\", \"furthermore\", \"multi\", \"level\", \"phonological\", \"providing\", \"automaton\", \"environment\", \"aspect\", \"high\", \"including\", \"flexible\", \"individual\", \"utterance\", \"nlp\", \"help\", \"detailed\", \"two\", \"morphological\", \"relevant\", \"tool\", \"after\", \"non\", \"complex\", \"project\", \"test\", \"application\", \"pattern\", \"extension\", \"addition\", \"rate\", \"standard\", \"agent\", \"domain\", \"present\", \"based_on\", \"their\", \"implementation\", \"such\", \"model\", \"based\", \"this_paper\", \"can\", \"by\", \"a\", \"used\", \"our\", \"rule\", \"one\", \"database\", \"help\", \"theory\", \"designed\", \"aspect\", \"top\", \"structural\", \"programming\", \"formal\", \"dependent\", \"limited\", \"logical\", \"logic\", \"choice\", \"variety\", \"induction\", \"hpsg\", \"query\", \"may\", \"may_be\", \"thesis\", \"phonological\", \"be_used\", \"ability\", \"research\", \"do\", \"appropriate\", \"available\", \"aim\", \"recently\", \"constraint\", \"description\", \"german\", \"framework\", \"semantics\", \"time\", \"example\", \"be\", \"system\", \"domain\", \"well\", \"can\", \"data\", \"linguistic\", \"can_be\", \"grammar\", \"a\", \"by\", \"which\", \"used\", \"not\", \"language\", \"representation\", \"these\", \"present\", \"planning\", \"topic\", \"common\", \"difficult\", \"reference\", \"noun\", \"perspective\", \"additional\", \"handle\", \"focus\", \"scheme\", \"recently\", \"strategy\", \"environment\", \"original\", \"efficiently\", \"heuristic\", \"occurrence\", \"variety\", \"proposes\", \"parse\", \"choice\", \"sequence\", \"pair\", \"preliminary\", \"evidence\", \"flexible\", \"those\", \"best\", \"reported\", \"derivation\", \"partial\", \"state\", \"possible\", \"problem\", \"mechanism\", \"which\", \"can\", \"it\", \"from\", \"use\", \"this_paper\", \"term\", \"domain\", \"their\", \"form\", \"based\", \"model\", \"constituent\", \"traditional\", \"change\", \"order\", \"regular\", \"occurrence\", \"phonological\", \"query\", \"appropriate\", \"main\", \"free\", \"focus\", \"thesis\", \"element\", \"nature\", \"aim\", \"research\", \"dynamic\", \"database\", \"according\", \"field\", \"where\", \"full\", \"architecture\", \"category\", \"implemented\", \"size\", \"area\", \"probabilistic\", \"environment\", \"discus\", \"linear\", \"sentence\", \"time\", \"which\", \"support\", \"information\", \"speech\", \"language\", \"analysis\", \"it\", \"structure\", \"system\", \"search\", \"machine\", \"how\", \"used\", \"syntactic\", \"this_paper\", \"machine_learning\", \"sample\", \"distance\", \"modeling\", \"boundary\", \"ability\", \"classification\", \"applying\", \"comparison\", \"prolog\", \"required\", \"upon\", \"induction\", \"phrase\", \"finding\", \"whether\", \"feature\", \"speaker\", \"achieve\", \"needed\", \"linguistically\", \"learning\", \"complexity\", \"due\", \"rather_than\", \"gram\", \"string\", \"grammatical\", \"machine\", \"improving\", \"test\", \"dependency\", \"rather\", \"discourse\", \"make\", \"model\", \"data\", \"training\", \"than\", \"a\", \"example\", \"result\", \"task\", \"using\", \"from\", \"also\", \"or\", \"structure\", \"used\", \"speech\", \"corpus\", \"be\", \"language\", \"technique\", \"it\", \"our\", \"feature_structure\", \"categorial_grammar\", \"typed\", \"categorial\", \"abstract\", \"formalism\", \"unification\", \"feature\", \"structure\", \"hpsg\", \"grammar\", \"restriction\", \"style\", \"syntax\", \"phenomenon\", \"argument\", \"implementation\", \"article\", \"reference\", \"specification\", \"high\", \"sequence\", \"dynamic\", \"produce\", \"dependency\", \"detail\", \"including\", \"machine\", \"written\", \"grammatical\", \"resolution\", \"linguistic\", \"a\", \"we_present\", \"based\", \"based_on\", \"phrase\", \"such\", \"present\", \"such_a\", \"language\", \"process\", \"by\", \"approach\", \"this_paper\", \"semantic\", \"method\", \"rule\", \"search\", \"solving\", \"heuristic\", \"cost\", \"average\", \"default\", \"solution\", \"improve\", \"finding\", \"np\", \"minimum\", \"difficulty\", \"local\", \"pattern\", \"doe\", \"optimal\", \"doe_not\", \"case\", \"due\", \"variant\", \"problem\", \"offer\", \"where\", \"length\", \"non\", \"u\", \"effective\", \"environment\", \"algorithm\", \"significantly\", \"empirical\", \"strategy\", \"our\", \"we_show\", \"show\", \"number\", \"previous\", \"than\", \"two\", \"experiment\", \"result\", \"by\", \"more\", \"technique\", \"can\", \"be\", \"it\", \"approach\", \"from\", \"can_be\", \"extraction\", \"retrieval\", \"document\", \"evaluating\", \"noun_phrase\", \"evaluated\", \"evaluation\", \"comparing\", \"noun\", \"against\", \"phrase\", \"relationship\", \"may_be\", \"nlp\", \"hand\", \"conceptual\", \"report\", \"relies\", \"automatically\", \"information\", \"easily\", \"text\", \"allowing\", \"kind\", \"fully\", \"automatic\", \"user\", \"among\", \"useful\", \"purpose\", \"large\", \"application\", \"corpus\", \"technique\", \"may\", \"probabilistic\", \"performance\", \"or\", \"we_describe\", \"using\", \"result\", \"method\", \"system\", \"from\", \"by\", \"analysis\", \"this_paper\", \"processing\", \"natural\", \"present\", \"natural_language\", \"speech_recognition\", \"hybrid\", \"recognition\", \"speech\", \"rather_than\", \"statistic\", \"rather\", \"contribution\", \"language_processing\", \"sequence\", \"source\", \"dependent\", \"local\", \"performs\", \"lexicalized\", \"reduce\", \"automaton\", \"special\", \"hypothesis\", \"currently\", \"introduced\", \"efficient\", \"determining\", \"recently\", \"than\", \"since\", \"thus\", \"preference\", \"least\", \"along\", \"amount\", \"such_a\", \"processing\", \"utterance\", \"ha_been\", \"such\", \"information\", \"method\", \"syntactic\", \"been\", \"language\", \"ha\", \"using\", \"a\", \"text\", \"used\", \"algorithm\", \"by\", \"one\", \"this_paper\", \"based\", \"word\", \"transfer\", \"description\", \"variation\", \"learning\", \"accuracy\", \"being\", \"capture\", \"full\", \"object\", \"cannot\", \"module\", \"measure\", \"individual\", \"nature\", \"flexible\", \"indicate\", \"version\", \"size\", \"computation\", \"learned\", \"doe_not\", \"reasoning\", \"advantage\", \"introduce\", \"attempt\", \"known\", \"doe\", \"choice\", \"via\", \"without\", \"were\", \"based\", \"two\", \"called\", \"approach\", \"translation\", \"function\", \"system\", \"logic\", \"machine\", \"time\", \"english\", \"between\", \"speech\", \"result\", \"algorithm\", \"which\", \"word\", \"or\", \"using\", \"a\", \"from\", \"by\", \"can\", \"be\", \"network\", \"kind\", \"scheme\", \"category\", \"paradigm\", \"same\", \"project\", \"detail\", \"map\", \"morphology\", \"wa\", \"artificial\", \"transfer\", \"mapping\", \"acquisition\", \"output\", \"generalization\", \"form\", \"like\", \"phonological\", \"line\", \"generate\", \"after\", \"treatment\", \"respect\", \"simple\", \"according\", \"regular\", \"evaluation\", \"take\", \"example\", \"morphological\", \"implementation\", \"shown\", \"one\", \"type\", \"different\", \"model\", \"language\", \"a\", \"which\", \"it\", \"task\", \"rule\", \"set\", \"this_paper\", \"by\", \"class\", \"entry\", \"dictionary\", \"bilingual\", \"position\", \"restriction\", \"highly\", \"recall\", \"matching\", \"additional\", \"others\", \"argument\", \"degree\", \"attempt\", \"potential\", \"same\", \"measure\", \"achieved\", \"difference\", \"fast\", \"verb\", \"along\", \"procedure\", \"traditional\", \"rate\", \"precision\", \"include\", \"variation\", \"acquisition\", \"we_propose\", \"improved\", \"search\", \"algorithm\", \"machine_learning\", \"approach\", \"task\", \"many\", \"we_present\", \"constraint\", \"from\", \"application\", \"learning\", \"a\", \"english\", \"other\", \"lexical\", \"problem\", \"present\", \"by\", \"based\", \"which\", \"use\", \"language\", \"technique\", \"used\", \"machine\", \"result\", \"dialogue\", \"selection\", \"computer\", \"strategy\", \"spoken\", \"effective\", \"towards\", \"during\", \"experimental_result\", \"combined\", \"cost\", \"interaction\", \"human\", \"size\", \"prediction\", \"obtained\", \"improvement\", \"factor\", \"oriented\", \"modeling\", \"applied\", \"compare\", \"example\", \"experimental\", \"specific\", \"show_that\", \"increase\", \"out\", \"system\", \"reduce\", \"about\", \"may\", \"recognition\", \"our\", \"use\", \"model\", \"new\", \"result\", \"it\", \"by\", \"show\", \"performance\", \"present\", \"language\", \"information\", \"be\", \"method\", \"this_paper\", \"agent\", \"distinguish\", \"we_argue\", \"must\", \"speaker\", \"requirement\", \"naturally\", \"task\", \"whether\", \"argue\", \"resource\", \"design\", \"world\", \"situation\", \"cannot\", \"what\", \"contribution\", \"dialogue\", \"show_how\", \"behavior\", \"answer\", \"effect\", \"solving\", \"requires\", \"whose\", \"if\", \"relative\", \"would\", \"various\", \"review\", \"within\", \"natural\", \"natural_language\", \"support\", \"language\", \"or\", \"system\", \"how\", \"a\", \"it\", \"problem\", \"such\", \"by\", \"be\", \"model\", \"these\", \"can\", \"show\", \"processing\", \"this_paper\", \"view\", \"principle\", \"verb\", \"low\", \"point\", \"frequency\", \"np\", \"surface\", \"experimental_result\", \"support\", \"event\", \"article\", \"done\", \"found\", \"mean\", \"noun\", \"perspective\", \"corresponding\", \"study\", \"here\", \"address\", \"document\", \"pattern\", \"range\", \"field\", \"indicate\", \"construction\", \"experimental\", \"considered\", \"length\", \"selection\", \"various\", \"automatically\", \"result\", \"from\", \"method\", \"a\", \"between\", \"corpus\", \"linguistic\", \"text\", \"based_on\", \"based\", \"using\", \"present\", \"it\", \"be\", \"syntactic\", \"other\", \"our\", \"by\", \"problem\", \"which\", \"word\", \"context_free\", \"regular\", \"free\", \"tree\", \"probabilistic\", \"contextual\", \"tree_adjoining\", \"adjoining\", \"context\", \"automaton\", \"assumption\", \"complexity\", \"procedure\", \"second\", \"along\", \"further\", \"structural\", \"instead\", \"build\", \"principle\", \"against\", \"produced\", \"utility\", \"formal\", \"programming\", \"operation\", \"combination\", \"feature\", \"linear\", \"full\", \"three\", \"parameter\", \"theory\", \"model\", \"grammar\", \"result\", \"description\", \"language\", \"first\", \"by\", \"approach\", \"order\", \"this_paper\", \"based\", \"from\", \"disambiguation\", \"these\", \"been\", \"present\", \"system\", \"processor\", \"parse\", \"par\", \"robust\", \"error\", \"parser\", \"precision\", \"rate\", \"aim\", \"morphological\", \"recall\", \"processing\", \"architecture\", \"integrated\", \"combining\", \"component\", \"sentence\", \"partial\", \"module\", \"novel\", \"learned\", \"oriented\", \"built\", \"real\", \"suggests\", \"preliminary\", \"syntactic\", \"input\", \"associated\", \"nlp\", \"parsing\", \"natural\", \"natural_language\", \"human\", \"language\", \"system\", \"approach\", \"corpus\", \"analysis\", \"disambiguation\", \"information\", \"about\", \"by\", \"based\", \"which\", \"it\", \"using\", \"present\", \"model\", \"text\", \"graph\", \"value\", \"minimum\", \"bound\", \"difference\", \"computing\", \"known\", \"time\", \"distance\", \"optimal\", \"degree\", \"size\", \"find\", \"function\", \"factor\", \"instance\", \"apply\", \"set\", \"handle\", \"yield\", \"literature\", \"algorithm\", \"standard\", \"complexity\", \"point\", \"complete\", \"give\", \"independent\", \"when\", \"no\", \"problem\", \"new\", \"given\", \"solution\", \"it\", \"simple\", \"well\", \"present\", \"also\", \"linear\", \"or\", \"application\", \"this_paper\", \"called\", \"which\", \"generalization\", \"under\", \"oriented\", \"employ\", \"capture\", \"criterion\", \"applying\", \"computing\", \"segmentation\", \"language_processing\", \"clause\", \"thesis\", \"idea\", \"efficiently\", \"suggests\", \"least\", \"recently\", \"therefore\", \"nature\", \"recall\", \"acquisition\", \"extraction\", \"main\", \"precision\", \"u\", \"evaluate\", \"utterance\", \"performs\", \"human\", \"linguistically\", \"linguistic\", \"unification\", \"corpus\", \"representation\", \"processing\", \"approach\", \"language\", \"data\", \"learning\", \"based\", \"tree\", \"theory\", \"from\", \"ha\", \"or\", \"grammar\", \"which\", \"it\", \"structure\", \"use\", \"by\", \"model\", \"a\", \"expression\", \"event\", \"type\", \"characteristic\", \"prolog\", \"finite_state\", \"considered\", \"estimation\", \"multiple\", \"applicable\", \"generation\", \"finite\", \"regular\", \"implement\", \"german\", \"instance\", \"generating\", \"variation\", \"include\", \"alternative\", \"field\", \"relies\", \"representation\", \"consists\", \"whether\", \"common\", \"previously\", \"line\", \"least\", \"treatment\", \"problem\", \"number\", \"phrase\", \"ha\", \"state\", \"feature\", \"noun\", \"algorithm\", \"other\", \"constraint\", \"model\", \"form\", \"our\", \"these\", \"which\", \"it\", \"have\", \"by\", \"be\", \"text\", \"been\", \"this_paper\", \"a\", \"language\", \"from\", \"decision_tree\", \"adjoining\", \"tree_adjoining\", \"decision\", \"tree\", \"tag\", \"style\", \"derivation\", \"symbolic\", \"user\", \"integrated\", \"induction\", \"interface\", \"phonological\", \"speech_recognition\", \"constructed\", \"defined\", \"recognition\", \"ability\", \"real\", \"component\", \"basic\", \"incremental\", \"computer\", \"language_processing\", \"construct\", \"artificial\", \"le\", \"definition\", \"operation\", \"morphological\", \"spoken\", \"speech\", \"level\", \"based\", \"language\", \"system\", \"processing\", \"grammar\", \"which\", \"analysis\", \"word\", \"natural\", \"it\", \"present\", \"using\", \"be\", \"information\", \"pronoun\", \"centering\", \"anaphora\", \"discourse\", \"resolution\", \"preference\", \"specification\", \"additional\", \"ambiguity\", \"distinction\", \"element\", \"boundary\", \"conceptual\", \"functional\", \"dynamic\", \"generating\", \"phenomenon\", \"interpretation\", \"utterance\", \"after\", \"allow\", \"propose\", \"we_propose\", \"criterion\", \"evidence\", \"japanese\", \"interaction\", \"prolog\", \"will\", \"framework\", \"sentence\", \"within\", \"account\", \"structure\", \"theory\", \"algorithm\", \"processing\", \"level\", \"model\", \"into\", \"which\", \"ha\", \"natural\", \"present\", \"from\", \"state\", \"text\", \"by\", \"plan\", \"action\", \"machine_translation\", \"japanese\", \"planning\", \"noun_phrase\", \"translation\", \"machine\", \"making\", \"english\", \"reference\", \"noun\", \"building\", \"article\", \"decision\", \"up\", \"evaluate\", \"learning\", \"along\", \"correct\", \"acquired\", \"agent\", \"us\", \"issue\", \"aim\", \"while\", \"phrase\", \"process\", \"even\", \"through\", \"machine_learning\", \"we_propose\", \"knowledge\", \"domain\", \"system\", \"method\", \"from\", \"this_paper\", \"based\", \"number\", \"a\", \"approach\", \"not\", \"use\", \"it\", \"lexicon\", \"hierarchy\", \"term\", \"lexical\", \"represent\", \"hpsg\", \"transfer\", \"containing\", \"acquisition\", \"style\", \"entry\", \"discussed\", \"required\", \"thesis\", \"associated\", \"distinction\", \"help\", \"selection\", \"acquired\", \"dictionary\", \"off\", \"provided\", \"representation\", \"specific\", \"concept\", \"change\", \"related\", \"advantage\", \"bilingual\", \"far\", \"rule\", \"constraint\", \"used\", \"description\", \"implemented\", \"a\", \"processing\", \"theory\", \"more\", \"based\", \"computational\", \"how\", \"use\", \"or\", \"two\", \"from\", \"this_paper\", \"such\", \"based_on\", \"segmentation\", \"syntax\", \"boundary\", \"direct\", \"text\", \"deal\", \"allowing\", \"lexical\", \"scale\", \"either\", \"individual\", \"gram\", \"do\", \"found\", \"via\", \"during\", \"categorial\", \"while\", \"functional\", \"them\", \"french\", \"analysis\", \"control\", \"we_discus\", \"because\", \"independent\", \"proposes\", \"verb\", \"applied\", \"current\", \"rule\", \"account\", \"between\", \"structure\", \"ambiguity\", \"type\", \"into\", \"based_on\", \"our\", \"from\", \"we_present\", \"not\", \"present\", \"which\", \"based\", \"linguistic\", \"a\", \"implemented\", \"this_paper\", \"show\", \"it\", \"review\", \"context\", \"theoretical\", \"related\", \"nature\", \"finally\", \"real\", \"among\", \"evaluated\", \"data\", \"original\", \"described\", \"practical\", \"similar\", \"show_how\", \"version\", \"preliminary\", \"fully\", \"study\", \"although\", \"basis\", \"situation\", \"account\", \"difficult\", \"doe_not\", \"determining\", \"address\", \"basic\", \"provided\", \"resulting\", \"give\", \"then\", \"function\", \"show\", \"system\", \"support\", \"method\", \"first\", \"theory\", \"approach\", \"how\", \"very\", \"result\", \"this_paper\", \"based_on\", \"by\", \"we_show\", \"can_be\", \"used\", \"using\", \"based\", \"a\", \"be\", \"algorithm\", \"tagging\", \"tagger\", \"part\", \"unknown\", \"word_sense\", \"disambiguation\", \"tested\", \"obtained\", \"tag\", \"sense\", \"estimation\", \"accuracy\", \"combining\", \"good\", \"word\", \"category\", \"ambiguous\", \"compared\", \"rule\", \"best\", \"performed\", \"test\", \"experiment\", \"speech\", \"wa\", \"applying\", \"produced\", \"distance\", \"statistical\", \"dependent\", \"higher\", \"method\", \"corpus\", \"set\", \"performance\", \"based\", \"two\", \"semantic\", \"by\", \"this_paper\", \"problem\", \"data\", \"which\", \"learning\", \"be\", \"can\", \"expressed\", \"formula\", \"base\", \"knowledge_base\", \"logical\", \"can_be\", \"goal\", \"default\", \"respect\", \"can\", \"logic\", \"property\", \"change\", \"subset\", \"derived\", \"hierarchy\", \"introduce\", \"expression\", \"be\", \"they\", \"behavior\", \"computational\", \"content\", \"constraint\", \"represented\", \"whose\", \"out\", \"instead\", \"towards\", \"how\", \"under\", \"may_be\", \"account\", \"we_show\", \"all\", \"particular\", \"need\", \"relation\", \"show\", \"which\", \"will\", \"by\", \"a\", \"knowledge\", \"class\", \"between\", \"using\", \"have\", \"these\", \"from\", \"model\", \"this_paper\", \"markov\", \"head\", \"hidden\", \"gram\", \"dependency\", \"driven\", \"model\", \"statistical\", \"probability\", \"precision\", \"lexicalized\", \"length\", \"comparing\", \"potential\", \"best\", \"evaluate\", \"recall\", \"compare\", \"small\", \"training\", \"object\", \"experimental_result\", \"pair\", \"automaton\", \"determine\", \"upon\", \"translation\", \"standard\", \"improved\", \"performs\", \"accuracy\", \"over\", \"lexicon\", \"when\", \"based_on\", \"language\", \"lexical\", \"corpus\", \"parsing\", \"result\", \"by\", \"based\", \"this_paper\", \"it\", \"which\", \"word\", \"from\", \"technique\", \"using\", \"algorithm\", \"grammar\", \"sentence\"], \"Freq\": [730.0, 539.0, 537.0, 252.0, 264.0, 546.0, 822.0, 653.0, 146.0, 353.0, 242.0, 195.0, 302.0, 340.0, 362.0, 236.0, 236.0, 284.0, 248.0, 266.0, 391.0, 105.0, 132.0, 291.0, 104.0, 203.0, 284.0, 587.0, 675.0, 79.0, 26.20927142813671, 16.479653705189637, 33.079344130998955, 19.21265407908707, 10.2234603752387, 16.920667696583543, 18.133742796206462, 8.276984918489537, 15.439347518404096, 16.49954109460981, 12.051362838070844, 20.848085756354013, 6.46999853513666, 6.785286008956438, 9.062326239140598, 12.460379837951272, 16.22976643092484, 52.00399467621051, 13.052959111830141, 5.905398888798377, 5.308603284294072, 11.689121089416682, 15.67605104892687, 15.176963839742443, 10.427358061469343, 7.013686276121253, 31.72007445713467, 17.769645880628772, 4.715582944479136, 23.613410558726923, 38.51914992183366, 38.26255807430725, 29.44757103766199, 16.4018201769132, 53.87304937932372, 53.41793982964488, 17.760028695297226, 27.394533865242163, 14.320584467255813, 31.138082620888355, 22.96137318494912, 28.750840837529577, 23.468967262061366, 22.927839187639506, 16.75567842623996, 16.75968373162785, 17.8107990484933, 16.314864792873795, 44.79604668634498, 16.776913524646044, 8.342348368580733, 43.72556220579668, 35.267507311432055, 7.200822916558467, 6.932398387332776, 18.802267926873874, 13.400658841571051, 8.417176468209993, 39.67427383625333, 24.05148783383704, 16.743395922149954, 11.055583194183104, 7.007367907784329, 5.687458742104913, 6.265849082589653, 9.433606092982938, 10.35634572440471, 21.44341023476535, 6.448193880689644, 7.389722092584963, 8.234216099107439, 7.373551532548669, 5.85596989418609, 10.378748344358414, 6.498603029132001, 4.7222655342041255, 15.831066771996317, 4.984261489302286, 8.934025325461857, 33.916586207887136, 12.0628412882047, 12.492793557249945, 16.63081627147986, 14.822767543086668, 28.266374934014763, 24.40626495829034, 38.24849166380769, 22.78219344074037, 18.973624847649067, 32.87878295147485, 13.124563751057622, 28.85950947973916, 19.310265402352467, 16.965220121398755, 13.474634078204513, 14.915147441703704, 17.167436280255867, 18.788760936926188, 18.69864203624104, 16.59008707465918, 16.074097153689962, 15.169777801275085, 50.446107090797234, 9.586325645066095, 31.889944441550284, 24.848345989292444, 13.572214241281763, 16.671355337569047, 13.951737039869542, 9.382507378115422, 9.234093575073652, 13.342081532012093, 8.735512621540957, 11.812416860117471, 26.030693987932946, 14.044261284037265, 10.765225724555814, 65.47819155064103, 17.83392678958768, 28.692006831905093, 12.592618519937842, 21.697085488716553, 9.427535861028513, 13.122963178261482, 6.380201513567567, 5.198577673748453, 33.23966849220199, 3.858303394944579, 16.61215285006414, 22.54085113619518, 5.168402067106396, 4.536506217911858, 7.453089618682898, 10.475240831472453, 42.70329820553792, 82.34425895595878, 14.076089435915142, 27.760009638792262, 53.392401469549824, 43.1407998678855, 38.754503262383665, 22.149643384021633, 28.406682159932075, 14.469119124654345, 18.062276753087797, 16.75276420105442, 17.966389938614146, 16.799880247040107, 17.758199720631158, 15.78397643037006, 16.536086127541363, 14.86761026855762, 14.296955699172099, 17.46015833718228, 133.2601661683651, 12.900737281355616, 15.282410953293839, 9.828195144826617, 19.57948388819633, 55.962598067068555, 6.2276168708638515, 14.121018329758199, 12.15728164150059, 23.041445959415743, 10.450552100987847, 6.665603011437503, 6.7341325814925685, 8.217378103790136, 6.166446284355897, 26.374064214892257, 131.9384525361872, 51.66897038723368, 7.299528438743201, 6.321851896870101, 26.31221217830485, 30.44692163574608, 7.688341051275563, 9.207140130838166, 9.541778402763857, 5.193274989198017, 4.207889896067255, 4.52436118299246, 4.026039429807209, 69.32697276007245, 7.294310886671604, 24.93172300641952, 20.19346928092983, 32.764941623951444, 78.68005308280163, 86.27485824349205, 40.759543875760265, 17.821484165946714, 31.69816783596432, 14.625569838611803, 48.18146315698955, 30.742355644348848, 27.494300414413537, 56.42410270177022, 49.050552453678925, 24.019233128745142, 23.73389105340072, 22.978365732293092, 40.047151221525795, 29.1784627064089, 29.78364155156677, 23.843197000766526, 27.264815421119906, 23.82156519393745, 20.53245588919905, 20.694041211627567, 40.84926210732862, 13.218733878673582, 6.41470428934616, 7.004590229234394, 7.898345826507689, 6.5648079351073285, 7.9730119811845634, 7.211126585527452, 4.552978478216021, 44.666245679351086, 6.0623767889651825, 6.701380588143297, 11.994734320212428, 7.808588221420579, 4.618632643944895, 5.746109759655234, 4.870746352960901, 5.695201556765553, 19.052297773376008, 4.737393090779982, 9.082582367402422, 4.758723737134774, 5.570103438500931, 5.705898168140518, 17.580635334340293, 15.834284916417804, 3.773168897545678, 8.05403392151062, 3.755270731138846, 4.613096609072003, 6.151406531772021, 10.384683853982892, 14.48445542521838, 17.426420748768106, 21.379376899132122, 24.432056333150726, 25.773612566214208, 25.574140096883717, 21.879657393378263, 11.174021843535835, 21.783540050448597, 21.96584023955584, 16.333578679717476, 13.458372489294955, 18.120740763849096, 18.359865589331452, 13.348816371485551, 16.018520362882008, 13.660163578739978, 9.953361229779386, 12.623446607836724, 10.845592597806936, 10.041997729345209, 10.06298782837068, 31.784609086934527, 17.534513510466635, 9.829973709900873, 7.06997697435788, 11.866258930420603, 14.152712036310914, 37.52462653474005, 5.934085653940644, 5.048014942461113, 6.798971717321385, 6.347928379176662, 7.905471430409928, 6.26410747654078, 4.76172323683173, 4.125645201119768, 4.76819617097711, 10.683612966724105, 4.860079465278078, 5.8145640268810395, 82.70485989212825, 4.294839143480103, 7.029922330583957, 18.796164076949385, 3.8352152489474816, 26.111432138445114, 4.863335378259623, 3.489944691032235, 13.01252493784672, 4.863303950528141, 3.5917226137436953, 11.352057244549542, 11.838188466354088, 25.50384137708845, 28.263260628065456, 13.017152671307459, 20.29191667246907, 42.70003535815039, 27.65443947102127, 36.90643048539475, 18.649520710467378, 11.69529914239762, 12.978481633412034, 24.184115505599944, 16.1754134055908, 17.335585398504453, 16.00613421550882, 24.456417942253363, 22.33223642801781, 13.382713758360829, 19.19110380739221, 20.750642126727946, 18.802619332995803, 15.989325093325622, 18.998027539031995, 13.633998899585599, 14.17027542404968, 13.440157365963792, 13.067761533006355, 10.448416489798726, 9.189867012930797, 15.466316912785114, 13.282422556574552, 15.510304651479744, 7.424479914398504, 7.419374179956423, 9.283104573402701, 7.231124626028448, 7.188080769867585, 5.684880828281999, 5.000020011920546, 6.306201226222147, 5.317768015929465, 6.793674779218291, 6.972582708683011, 4.89649198100271, 3.6923890569174285, 5.758602934894029, 10.720336556430837, 3.594595877590607, 5.322361274271924, 6.333719827823423, 4.748974556907277, 39.74573039543817, 11.623582506757627, 7.51109457954583, 5.293262641435332, 3.934141281094283, 3.3958837869879352, 38.034766686926964, 9.698154788917417, 14.194209775228384, 16.360196007783372, 11.06666037665817, 42.05318153976755, 19.90685655031551, 12.84000553827096, 16.720878258456466, 17.75115292941235, 14.390181628413785, 19.929460096636873, 23.694282417933543, 16.327415736720194, 12.658761065233385, 13.398156949216323, 23.614749767134672, 17.003420932990515, 16.895733945509562, 13.559381351088502, 14.484645988007143, 18.59450742350578, 17.01240642999471, 12.513623201267997, 13.976359039262007, 14.320924971334865, 15.128147211870093, 11.861282074902116, 30.159914436458934, 37.51831062784958, 34.72663008191754, 39.61071347122969, 7.98541086166625, 7.812918470376266, 6.301884951912969, 8.912036249518138, 17.06986809248104, 7.141321454588295, 26.07508274036665, 35.23660299692276, 8.159780093867997, 5.086621535297505, 8.352791215307878, 6.255475442005939, 10.407637059367655, 5.85640224664877, 5.83351806046968, 3.8101527686343912, 3.4377038578571932, 9.08176538454432, 6.539324344042644, 4.413267212717049, 6.932521738861333, 5.89225472047603, 4.238149339878661, 3.73695325639112, 11.164936933894815, 2.800700520715072, 34.090925902126976, 24.443039942927236, 58.00209645274509, 14.776261383317115, 12.21394740738787, 36.096720842721936, 31.441466119875844, 19.63376783597965, 19.193852781608783, 33.60254985293188, 26.6253836729118, 13.925678324019223, 19.446231221963536, 24.994343720161382, 25.084629237603238, 23.030658942157384, 17.04124665667445, 16.52771566113179, 10.759486515654666, 11.034699077573618, 11.976714871380192, 13.294665470312484, 11.57266160518354, 52.39981995733803, 29.482403814215715, 13.878411155973582, 205.50544623728325, 16.658707562385082, 10.477017069012074, 11.24990695940248, 11.544942315197067, 12.082162094569524, 24.172993999138047, 8.60265662994538, 8.270111673540821, 12.884759115222627, 10.770945386188476, 10.111965704888581, 20.905310092751243, 5.304649309918266, 7.140679433459885, 5.206493479780702, 5.438470672557216, 5.874827996056074, 11.885365058831134, 4.396473561956644, 9.498843843065588, 5.8652256913246985, 35.78625231303653, 9.023006724958089, 10.119675523515653, 36.02184112964191, 8.294719970427108, 68.10684730597778, 14.622715300792285, 13.81673858448734, 34.750915366736066, 29.774200544074386, 25.625713196639506, 39.237881564939826, 32.64181073610044, 48.02198672598045, 24.464283834424243, 42.333214143649386, 46.27239707087132, 28.44828628410375, 34.888092514905516, 26.36587491293552, 34.43729108710331, 20.809975656932163, 19.705105686753924, 18.756255235869546, 20.71950453979426, 17.976374025998425, 19.0970641279633, 21.40807538726834, 20.265972576134676, 18.5547359511659, 18.35740452421549, 29.601558450821127, 94.86766515514803, 93.79328001777877, 11.93414811884053, 19.6090940982008, 12.070092110228513, 12.048563273700399, 14.178042336463792, 6.235858749094008, 8.424498745884309, 6.011092848660601, 175.0303257316462, 8.281208606529542, 13.300990619288633, 15.697636741809685, 23.997743150062835, 17.570020545242137, 14.333495824417945, 4.025239236943149, 12.75746862744338, 5.356592646790009, 6.722384808264894, 16.992009588013126, 7.272427533530216, 5.096484977104824, 4.283093097476397, 14.647007275788882, 20.280658677226178, 9.950386055295747, 5.740234455372012, 13.060271684162029, 32.460605205326935, 22.362356653976732, 62.16460967759601, 44.140504064314335, 21.686643837742228, 17.74597120528844, 20.71368550707645, 14.844444443412186, 15.906371589483134, 19.37884286369074, 16.66450136347398, 17.56912250749498, 15.026231518835557, 13.151398446094555, 13.141293446490492, 18.457101803496112, 48.49588859216952, 14.355761643163529, 97.27880511391481, 34.56882797859524, 11.208195201297068, 19.762089372319846, 16.960432812144617, 6.688111124011052, 6.075082945039696, 7.626286177440162, 5.231000516214943, 9.938022390432403, 5.386769459523456, 5.245108301247536, 9.43086781946448, 4.294952010666446, 4.018204079938171, 10.545685973454393, 8.188448525281007, 5.432905443985596, 7.650536631338324, 16.810857956248185, 4.830422737873609, 2.5794364403295442, 7.416457906799431, 3.506446198790238, 2.221996581376765, 7.250314808635754, 7.6406093730464075, 9.960626262440284, 8.990959928474165, 12.642561392020184, 11.912641974254868, 27.379365508772104, 19.663731710216705, 28.344134811744034, 11.692537379147433, 12.145364392985671, 16.536596662717674, 13.983352127234749, 9.217273308516832, 10.611038510136181, 9.437846387174638, 10.273480495974338, 8.615369317378834, 8.23029696666204, 34.52639086818524, 9.768791794214694, 19.043354081487646, 47.44226342869256, 8.053633483887687, 5.304419106949711, 5.685163832856398, 4.500829946967001, 6.359994281008919, 5.2944712398581215, 7.018604305966037, 3.8408060775967887, 4.1527298922508535, 11.164695061689672, 6.968893110499103, 3.0169061185635297, 2.4903179035831178, 26.3461046085512, 11.6737832444241, 3.1879101820300275, 6.793520883833787, 1.9757751258348604, 9.299775719216404, 7.030091506619811, 4.550903920155854, 7.77778627851673, 15.509947744585112, 6.290242088161979, 4.278572692580342, 2.8605027175937856, 3.1324460696175613, 4.96472644495252, 7.533388935528906, 13.455080420644519, 15.20156538582646, 12.080340708028688, 8.113059060574871, 6.355637906184712, 10.965625159893865, 16.582752198544384, 15.715164333065491, 13.028964902748855, 13.70791595823335, 12.206642114157582, 11.865653871763563, 9.47594042887174, 8.425122434373531, 7.419323862246844, 6.915815763675335, 45.49072754311308, 8.449962568647159, 78.10275285987441, 11.605360014915302, 12.83955537526727, 9.655173588532847, 8.182702520913688, 9.348409699503224, 9.166397210984396, 6.923023169447276, 6.507045115918702, 6.331843120583215, 26.091249207551876, 6.169469352038981, 6.56824871014114, 4.803942402811341, 13.0902746399525, 6.030846433263451, 14.657322563669899, 7.028697949879395, 4.051222153998693, 6.349564881190552, 2.990565910123461, 3.450284136108189, 9.996405840624382, 5.87545349120435, 6.462427726958317, 7.872711525537185, 3.2035559936005877, 2.914500203028052, 35.207825347206125, 14.368106553335366, 7.0984075633601, 16.431517074029806, 14.026994244277176, 18.45301140804145, 12.79826800235693, 41.673467405513684, 38.20662545302707, 18.334425363333455, 11.792456242770546, 22.766507812085727, 15.670293289986207, 12.681134049046717, 16.310830251933034, 18.31805065525446, 20.088519124637653, 19.541096281559923, 16.108400478306002, 13.043401754187544, 11.634065434599897, 13.263229655148507, 10.962346832593667, 11.066333893893857, 11.070164973740008, 29.157114135044502, 16.365752117032848, 6.613860283113695, 5.600351615338076, 5.56649743414706, 15.840186675029116, 2.889615238162902, 2.8883924968997934, 3.873549853489857, 5.088136088519846, 4.799655069319483, 2.2343327854319095, 11.116488181835045, 2.8861372328186334, 2.946418312971288, 2.6660741549189395, 4.497586041284412, 2.5923595917021114, 2.706465011305009, 1.9340321477500908, 4.10428696462887, 2.2000291330162267, 4.026889857807191, 2.8914558687795573, 1.779776423289113, 2.9050246420411696, 1.9355746269837653, 3.699513654893234, 3.593040910694657, 1.4854786736489458, 2.473632291341529, 3.0273641693005096, 5.6455543571982965, 4.038080259319832, 8.51312263639342, 3.029015436386469, 7.708222476467676, 6.771633018386607, 7.145106631038319, 5.862616258237902, 4.580995760798586, 5.387990033448667, 3.603490528914528, 4.184707048626813, 3.546903106253402, 3.2585922150106397, 3.3116580316951474, 3.2826039809058347, 16.38766486369005, 5.455672662658846, 8.465070506618996, 37.59909345210429, 6.407869451898888, 5.416643701188275, 6.978157804005312, 6.088724714658734, 6.788715550944417, 4.772239370390463, 13.392160870623979, 6.029077972139407, 3.590299240685, 3.5171621443328, 2.419938213315188, 2.5282090314816275, 7.495811610622878, 4.913266570001037, 9.372317187286304, 2.5181830482895378, 4.712697521794069, 7.805518191746353, 3.646465725345288, 7.008022711727158, 5.295574675950499, 9.485231284252908, 3.4684729885145242, 2.040294374725749, 6.814543956756055, 2.654502271753323, 7.018088700083285, 6.573667695951981, 17.520258910673032, 11.054840977537998, 23.316364800208298, 5.2311349702373375, 13.491906569192693, 11.668060038788441, 19.19621419753177, 10.37325328235887, 14.973809629925025, 9.261474936709883, 10.523949864106019, 6.4640167953323635, 7.186337786817443, 6.883708865720137, 6.686390380650458, 6.360074574197291, 6.422994849326081, 34.73698443438751, 10.279366791458907, 18.07727856387033, 14.289570543295767, 10.034991346514628, 6.9648092839928335, 19.604521674143303, 8.568229529138094, 7.26069729398747, 6.808197266516164, 6.556370990338815, 5.250827158837577, 5.489818821335653, 23.951635739796703, 5.711901324679045, 6.953135708932796, 44.3379646342777, 7.465642885494799, 6.123345408211835, 4.624853232736639, 4.895101564123807, 62.856961568245644, 14.42031240190657, 4.5056840288360425, 8.098568950829865, 5.720270894772519, 9.667992221370785, 10.7097002550647, 32.798684556518936, 4.241199522293542, 15.199753903773429, 12.192963004234887, 8.718878154051286, 31.19915338161156, 12.39026319734821, 60.03806890382394, 26.12602195603824, 13.5698566138748, 17.603098390435854, 43.919583413184405, 13.683743864871365, 24.188058479335776, 15.482794571396022, 22.222866510410253, 26.020253792645253, 16.014803014476406, 17.47403853765558, 17.874701418410854, 17.81636945620813, 17.103075333905736, 17.141150490754853, 19.215409172885884, 18.127564173856744, 14.006089826580215, 15.40898088201371, 13.713090775248553, 49.46042625732809, 31.43326405829048, 29.078470039003175, 36.62528733083032, 46.16892924544196, 74.9379187013998, 47.432100730178625, 90.75530991256618, 116.82158026625507, 21.954007639932044, 148.00909198617987, 9.626336112593192, 6.487809294241874, 13.663411133182233, 11.205780042030607, 6.721988580228996, 18.704782281881787, 7.036927653316417, 6.344024112828857, 7.931562614277396, 6.503285760571028, 8.9507388865657, 7.181663726808636, 5.964283257952111, 10.962330442730629, 3.71149334636384, 7.7271015556898925, 25.932529092704023, 3.3697531893740096, 7.53504494127406, 7.763642699095587, 18.366088596001426, 51.14327645736098, 15.472730372841921, 45.415086848531885, 20.36987253807909, 12.169302200811972, 18.399691761978453, 19.54960370581899, 11.657568955913705, 25.20249712779947, 11.661852097747097, 20.082526238289496, 16.971175907699884, 16.84929819485485, 13.805500118100106, 12.42294313043248, 11.804653367265017, 74.37620194277099, 16.804877591511957, 28.161267031869915, 16.121852315974625, 10.628815093728925, 15.761206709104307, 21.03176296514668, 16.847518912683753, 9.177606318783706, 10.634191008584756, 11.76505954100017, 7.013930191887734, 16.35977879878496, 22.316423098442606, 12.353886553780155, 7.983114844949624, 8.818439218101316, 35.698691577935314, 5.9467908750033756, 5.014917248364233, 89.66572266828078, 6.168652859303341, 16.665868978869526, 7.190498149488682, 19.065105045536598, 7.872552960936319, 7.806980278363091, 6.048640355442225, 97.1143912229636, 5.707804761531007, 10.724040845709437, 17.69653294674171, 37.471555283942095, 16.976859173150558, 30.044458818842756, 18.423955917561884, 11.878152822529485, 19.12669633697262, 24.43114314824982, 16.07243122484289, 27.127870004873344, 36.991536373115885, 19.600411378498258, 20.134778351988487, 27.595546603174224, 26.822571955733782, 25.861970103670266, 22.73582740284613, 17.086810888235675, 16.222050659594206, 46.311406391699144, 35.26696991265172, 39.41799628502953, 14.639260892967892, 15.420786049200128, 10.830292841062509, 34.18623990642939, 5.215999385017628, 25.43860098792366, 6.953159648688779, 27.24366085865924, 9.87499417651606, 10.164282259097314, 12.28712903760343, 10.39534158188184, 6.732183051690534, 13.041283885613037, 4.343528283587137, 16.61173504873635, 70.42266322451142, 5.127780437703364, 65.82460231436214, 3.7286981336927885, 5.477939790064088, 7.368344510759023, 17.438295273758126, 11.837197017432953, 8.331806064407175, 8.68969379666319, 7.196445577753722, 16.7876931727828, 26.349990083222178, 39.12728246911474, 25.094721651436853, 13.02928358621096, 11.440974408698766, 17.033376446045615, 21.14670914476722, 11.770606972837847, 21.97736401557442, 20.25851197836093, 20.58307225100016, 23.065669267371025, 19.889456756015125, 21.35240399262747, 16.17472439696279, 17.56052211003671, 15.329551185553859, 14.768432332986453, 14.335353386045911, 13.485794347334764, 28.167766749568393, 14.325189443274107, 43.83720240437451, 112.32770910580962, 16.723738745274026, 10.436722752333662, 17.236125638482566, 6.881642654554122, 7.2450392940887935, 15.040773218423265, 15.814749597675478, 7.5778200737533155, 12.029254003833922, 5.938241346123893, 6.523192970776695, 4.209112049153857, 6.678306630933044, 3.9608180196950147, 6.183190112540446, 4.139540818076564, 4.816898152451324, 18.489610826223444, 3.782138214529338, 3.095730808213439, 23.663475396504598, 5.380557191519614, 6.635506493684657, 5.020964669894815, 3.9020955403719944, 3.136790501913828, 4.3517764373832275, 16.08165929875965, 33.780620194920736, 11.322329844056298, 13.830873771238988, 25.932884021751512, 31.643394602720914, 32.38649872619741, 16.55261167559488, 18.82432942738156, 39.16031908551257, 17.126993222701742, 20.388822848574346, 29.10207968301062, 19.97468717905674, 19.05384910116376, 21.659730450776387, 18.776058207989703, 13.052628556498064, 15.88361378577538, 16.300990362074927, 12.792305416085542, 13.507842507670114, 35.94709998342281, 8.556537333992287, 70.07413659847828, 19.290397623355556, 7.209960581943133, 4.592350804824561, 5.85572529428132, 7.161054706081599, 4.490877876064254, 7.872470871129323, 6.505296863219448, 4.447669476842657, 2.8025026860388573, 3.732327992120551, 2.9852018770171385, 6.949742730853677, 4.602675847018636, 4.744166652116754, 3.0486011793354497, 3.6566988857025495, 7.300908360450924, 5.512924181526913, 3.974094546168009, 2.586915335439653, 4.137547877706907, 4.618602847606087, 3.1586099174366233, 2.055448076214994, 5.675716622851467, 5.282339398974404, 51.75343381076823, 20.56436343212949, 8.892266361150638, 31.6186698292011, 15.152608015114053, 8.884310598368504, 34.07077796869457, 9.85126315318114, 10.756793206482119, 8.911493277027153, 8.720057101406587, 9.51235160746134, 10.514012504794481, 10.512658173454952, 11.537465436502611, 11.596470099046808, 10.619788023622018, 8.479203306916366, 9.31838446511761, 11.003745760972137, 9.979418757264739, 9.325124104525909, 8.863308323797497, 7.361889556991032, 69.98876387343473, 8.546038026831239, 12.465457003046986, 13.785228291214686, 6.113164075721955, 13.735889956712262, 8.444580019557952, 4.5194212481892215, 4.424889586797985, 8.230184414705413, 10.987002554726029, 3.4077261742496567, 5.007380875590863, 4.786979077666801, 8.910689977678663, 5.608084062087848, 4.9916446785432464, 18.456733231121106, 5.522419491414946, 4.8693359941835554, 5.367141184658387, 3.317666037920839, 2.0997775537073395, 4.480766910209014, 3.462043629848476, 11.266496450942967, 2.560967070273172, 3.3654045449232726, 10.312836542219667, 4.870763743028057, 10.06943164402683, 9.461864391161185, 8.134069485684797, 5.998218773829595, 12.703540422818383, 9.55438510826463, 8.491877170008857, 17.72570854287136, 16.913716167250332, 14.023570908960881, 12.443635413546657, 11.473701471978039, 7.444203995843914, 7.164151383992416, 7.200576619389378, 7.486679343969239, 6.797320010173582, 6.045154127107348, 18.29744269180436, 41.415996076199335, 14.397150735594284, 8.642445006970036, 10.787155305615148, 6.4030678816130955, 8.287522591870914, 6.2368206469944605, 4.577855803343597, 4.1433605759710845, 6.390785092996753, 5.60293080825919, 4.734112532504003, 4.856628709398412, 13.266306623993417, 8.247357804361343, 4.880369820769582, 4.787236452446285, 3.7716886849683604, 14.739114018260697, 3.5014164721543195, 6.616388370914378, 3.2703725443602156, 5.890909276034133, 4.646436310247828, 2.7869106089228026, 4.174151403147713, 8.453957470658114, 8.06556772145052, 3.998800676988554, 13.795123632756805, 51.70809039049582, 6.593291641990204, 33.84757680152511, 15.106870469680153, 9.629423090607299, 13.426201450531904, 18.112855817238255, 28.880892725239324, 14.672795207827763, 20.064202580553854, 26.58409236025374, 12.025283080404712, 12.337957376584294, 14.250014337835683, 16.026447867714804, 14.5887072507071, 19.074018737201616, 18.170765694490864, 16.978697346000676, 11.935479193226215, 15.520515199421059, 10.985974955999867, 11.415532323639876, 10.424681071907868, 10.980340132212692, 108.81528660676676, 20.160822230468295, 14.386679280716072, 34.58593567060658, 28.555821785416253, 10.325701175995032, 6.4894534979146625, 6.2274137283458, 4.76376774636338, 3.254506647575648, 5.483062507810438, 7.462581359978935, 10.361261780780165, 5.593015272908015, 5.726376816682724, 5.691014909950139, 4.738337813048542, 4.777938286101068, 6.832985831914517, 5.968676154421729, 7.50099013291778, 2.9253372047451864, 11.705323756434899, 5.166650749277684, 7.969293615280494, 8.632687340342093, 2.402517225744002, 5.453704992596967, 57.5326647279146, 2.039107031162958, 10.743993286114623, 8.10807273224714, 8.869017925279024, 16.6307106435742, 14.069313088048848, 29.03629222813667, 12.022686043489298, 13.36609578180201, 16.407635708570353, 14.69055060466459, 9.4789936073096, 7.878484699653528, 9.527262099828905, 11.291003654559841, 9.34438861405126, 9.121519700124011, 7.897854098264871, 7.660208786410224, 56.86568020458427, 9.236655519172299, 14.997966981646426, 18.899846181467808, 13.44174877906519, 15.108471111044148, 8.283240239641836, 51.80384854007568, 10.541495357809675, 18.39088567822825, 15.710587293662147, 18.25911570687141, 15.88655627546872, 8.731106432048913, 7.997273104261208, 12.938350708249263, 6.2433182291364835, 35.47864836890106, 9.830709104978597, 7.4605062167676985, 6.952945607487256, 10.022813185318661, 6.124225304578631, 5.951192303204001, 6.732456118357675, 15.165967463287133, 5.67414371394852, 5.02094171943746, 11.116505103153282, 5.07309354727875, 17.382968630938098, 44.86377518865706, 38.97985579665757, 11.990076077054518, 59.18200285814351, 27.003622656860326, 37.27135125229037, 19.67346092980596, 38.341797996966605, 36.69374311886699, 26.494533389789623, 20.89019659320982, 30.91480586889112, 25.550024275246695, 25.2484643351599, 17.154688517319652, 19.64020849582191, 14.056942744636157, 14.035615822398162, 14.167575637865024, 27.86768027446467, 24.25724811280423, 38.442812026951756, 11.026771345066775, 22.730249293637872, 12.71379975580834, 8.81682225364644, 6.494723220582015, 6.114267997708131, 14.998339690230985, 7.302043785083871, 7.3121473635517775, 4.418139967485972, 7.370482896972723, 9.187268409942432, 19.518393998447554, 3.5491046098069803, 5.058266424298779, 13.733538183495797, 8.228716317899227, 7.693651043228894, 9.239718463644357, 12.409352318785889, 4.989287822360701, 7.064257753816696, 3.492170205961733, 8.976948195864383, 6.886298116679096, 3.9450800089231683, 4.603028024887653, 6.735486560259912, 7.226327625414466, 9.364701428299874, 29.30235164185289, 35.96048113951249, 28.362263569163133, 40.67795865020088, 18.468199941641725, 19.767567057583996, 14.61800619278743, 20.72325069537774, 16.526562383210447, 21.536836993661943, 16.8088771214445, 16.417322819377926, 20.84219740259087, 19.68529537670294, 11.734588173659215, 11.970445721069446, 13.017569060826174, 15.745577906224083, 13.064604355506425, 13.304633665096896, 11.200245896559965, 42.18135300879838, 15.129574745787385, 42.04708546199867, 71.5934778514495, 30.477077587484814, 10.933002861517732, 8.768763459814778, 9.420065038985637, 52.381708514922884, 7.98175973443924, 6.638629887303031, 14.970380066612025, 8.41513976838227, 10.196588268334633, 3.8275231227718294, 6.589684976346418, 5.226278981206817, 3.2197922153171987, 4.780967177368464, 7.730887662156205, 3.687315000478463, 2.9577071616327335, 3.047160002612041, 5.201431052534311, 5.035068735983221, 4.266474214179959, 3.731043781842686, 25.815609741621888, 9.585708567042833, 4.144689290098145, 9.237408198184406, 6.086292441928566, 20.34561858379242, 38.93610692670384, 30.034844604287247, 20.46449083635507, 8.86694508484483, 32.700283848415914, 10.614213316035364, 26.02218014560522, 18.48681486411941, 12.017843477554463, 18.095985679775133, 15.582036321504518, 13.934234840091193, 9.126970246731135, 10.681556711266085, 9.607477079557988, 9.802091892983398, 9.564955410270917, 24.910170178601646, 37.65194554864704, 23.133226307668924, 34.52321624370325, 43.60387292774311, 74.36873579119704, 13.405522657678764, 15.40287143971284, 8.275197518421026, 39.01654909950548, 11.383090613857531, 94.95118249957551, 24.878941517676637, 9.856267480724634, 10.30796001809818, 25.7893584568361, 81.08534350809853, 14.798297846931902, 15.877452453467166, 8.938014160034676, 7.327002479157977, 13.732732382506898, 6.568511008642933, 12.585951306934, 5.356800627267183, 6.253822962113774, 39.140725497205196, 25.28346897177835, 6.614932145173119, 12.655986339177334, 61.335809310327875, 53.87927387441183, 46.26881557261303, 15.281006692616463, 99.87672560930636, 81.27707084001345, 53.88589813582016, 41.479645596625886, 35.865780728636885, 20.05590913827683, 32.05803897600278, 19.44737210509646, 42.53258676940824, 37.4827703493753, 33.87670372225126, 27.923696052675947, 23.563864582179875, 23.22595884145898, 24.604989543401018, 22.043109175584412, 50.71086873167708, 39.21967385673523, 17.918343462813976, 13.95776012014873, 12.738248102845112, 16.393127957830718, 16.613089801073617, 62.537172200022155, 16.90285636825971, 8.781892469318617, 8.065841977977586, 11.393627988592927, 9.938294355529848, 26.697066076054252, 9.121369489770615, 10.267765591958867, 7.228464257744042, 61.44148824043298, 6.672975370470751, 5.23988936241855, 6.695271753003083, 107.04954536956517, 13.120744069631291, 15.522877793195994, 13.310731763672361, 9.802923726090897, 10.628429915626068, 9.454176604724443, 21.44384677758042, 10.967092253441109, 51.64056974455986, 30.6246091003204, 20.56623114128703, 9.82470352576934, 45.44951001303947, 11.950114244912672, 11.94305119332379, 15.448952750098014, 13.015608040964468, 10.732002000503462, 12.468498041175506, 11.353820866836564, 11.540509096197964, 10.7600678173466, 11.007620245894415, 13.79632716201118, 17.200871980452135, 12.615161117703478, 5.257925863235464, 5.591282913516551, 6.7956451250954695, 5.099595642004842, 5.42039334094814, 4.808017172196823, 2.8928059328331845, 8.52146114774458, 2.850851312973693, 3.085771572441528, 2.9453272175924163, 2.04871907985579, 2.624320058251363, 1.9370184492579519, 2.3695467253488456, 1.9365366572530034, 2.8943254306949453, 5.64700665914384, 4.898748930843023, 2.8440201906178157, 2.8943254306949453, 3.285313540556978, 1.987604293469832, 6.841556991237365, 2.16310050573391, 5.2558170305105065, 1.9076997843414323, 12.480698792981743, 5.692321579697186, 14.011350669098828, 8.805597982662977, 9.524448860903512, 11.638056488766438, 16.44933998574406, 8.472328952075102, 9.91401211061904, 13.665651669504163, 7.108312914856363, 7.991615684720064, 11.302133631338117, 7.806361364399716, 7.517427125300175, 9.768440774090896, 9.774501558680619, 9.152682796074075, 7.087534311038548, 6.573795790069579, 6.93390090005397, 6.353685009435172, 6.171723155082708, 29.117595034097043, 14.272076419759777, 42.048617624522656, 7.663303473199374, 7.562556310594794, 13.832014253188213, 6.431564798094875, 5.906071579781534, 12.783356766837217, 4.1212688223534135, 24.719453907136565, 13.827046690609231, 6.269852721146937, 4.197140477601614, 8.701425154909538, 6.673373957827394, 6.124802812121593, 5.142358687228449, 3.2678820219001996, 8.65220915835966, 7.2558539061896505, 2.883537943824661, 27.32962918443929, 3.6916986862093526, 4.742588252123211, 5.163950723268548, 4.606107550673324, 6.367565682261124, 3.720967422253196, 5.611664460202873, 44.477509293035794, 14.986242982579085, 12.583663669443087, 24.233942758261943, 14.004543019312493, 19.187854104375067, 10.613301211090244, 34.85214992377908, 15.389544537559331, 18.49147932737225, 34.38687218906428, 13.043140510258546, 18.170744045061635, 17.486755749163066, 24.048494114137696, 24.92475496763797, 14.887984465065477, 23.27348463732884, 22.55689914390645, 17.046058045864687, 14.120616020382837, 15.353168166284988, 16.50256551953834, 16.016435046608507, 13.88096679382455, 37.5329645115899, 23.425125140443406, 21.037080363309396, 41.043866338747364, 78.72421566708361, 36.26988224491057, 9.34086744343875, 14.589857895299529, 10.79308097034702, 20.70969549233821, 9.256394896598469, 6.786044463872649, 14.311091313020583, 9.01411110672406, 9.085427112429924, 4.337758364102597, 7.076035426423452, 20.697243789001714, 4.318860304795934, 8.866293041056064, 14.536437656618878, 5.935200486071728, 6.887990521699574, 7.117160448691548, 3.9661125893027456, 7.000769095848319, 3.2800642151371915, 3.637748058456468, 9.717887768107799, 4.981678130438244, 14.464158803682643, 12.060533878013654, 29.24760524998273, 12.074115908387551, 30.20721669288606, 32.50808144073952, 25.2719682974455, 16.535260655497616, 18.646697335278795, 19.650492408509088, 13.959535032061956, 15.001902192123472, 11.824715843314861, 12.818604388963358, 11.578657614700573, 11.486000543082039, 12.227354112057547, 9.933672243341077, 36.47931465971491, 54.21363451298799, 31.154820807743395, 168.1843955354502, 39.18463736920739, 18.07650118106258, 18.92095935297332, 7.338464928380672, 32.38605040577646, 7.471297686486323, 7.257451267194115, 8.05678693037208, 6.889692420108431, 8.634345295841026, 9.48032505594399, 7.365254190472385, 8.921482516174, 19.38068991464135, 15.88534290527722, 3.500513281318883, 5.231521322211108, 14.979280777006686, 10.517474334451471, 6.828353941612005, 7.129291646820308, 9.73726616949483, 8.090503346356101, 4.687704465839295, 12.506676689876942, 19.678214229129726, 37.29093212892243, 13.222010793066419, 13.937263434555357, 32.049846330264046, 24.84083495408986, 26.593023764458973, 17.36355710598, 12.501099472736433, 22.396544626590497, 13.2361123952991, 17.577647183037215, 11.799922249362147, 11.818395144632708, 12.11735158491003, 12.509472571813317, 10.528036716730353, 11.046568806792294, 11.408518416358254, 60.1966659755179, 19.054295886973186, 46.75397893150252, 34.26417690768308, 35.76260910966932, 15.462144282626282, 60.8579329223668, 56.19106062627366, 6.956493819544417, 42.68554997124103, 7.886508886621856, 23.11742690910004, 9.601692299817406, 7.734780307879394, 19.325938106690835, 16.11671663312832, 4.775371767335331, 61.49716023850735, 4.157777698577879, 8.059333636544762, 4.042998792193905, 15.956223178826837, 12.70839593957877, 7.555742769680045, 3.185964643518375, 8.169854295893424, 15.548393779150976, 16.126361897523164, 6.131941402396232, 9.284449969726639, 6.716034530246608, 7.517181868713601, 18.726273670243472, 17.365681490475904, 34.57136368877383, 25.55048874209861, 26.149132343632946, 24.489949202870342, 23.312199609653117, 11.460506535063132, 18.949714276093076, 15.777528953260662, 12.734309541250074, 12.70444914472393, 14.53212206320232, 75.42947811922278, 21.866799326932565, 44.30297633144492, 109.23835728492095, 10.574970946462143, 23.978252096835995, 11.076369506193284, 6.794075032639736, 18.267338936293225, 5.391778300345208, 6.968501630711327, 7.2539617891851815, 5.3350578752408735, 4.039347546533787, 4.048547062458404, 3.8142536128109135, 3.311416203947489, 6.80078036460718, 3.253794386124869, 10.894642726690055, 4.216410202037105, 4.397784041100803, 23.524701103925413, 9.527203422060218, 6.995705400121201, 4.366035493052408, 5.1828520425012785, 5.492083651429749, 4.2041243346093555, 2.085806926795409, 23.09000077789459, 23.402504327067952, 22.016019111910957, 9.357722281211235, 9.096639686712773, 31.1732465498208, 14.935768921396313, 14.082665750056112, 10.947718961298685, 18.719941602916432, 8.753739276280708, 10.405013496934888, 10.958919724609153, 11.144169471903933, 9.967976683739021, 11.872447586517826, 11.184191201319086, 9.742064764954923, 9.291183889837635, 27.620300081751296, 19.48138061890955, 11.401637489434899, 8.204812922325983, 83.1535391188542, 6.868697913201229, 4.578773592226245, 58.632678276402544, 6.920675468458637, 5.521160694227514, 6.299491741484571, 5.799652743044156, 7.346917051893922, 6.771770092875875, 3.6958819644688807, 5.469901283329173, 7.682315231177057, 9.400814871546833, 6.344121317950796, 9.68795003536084, 5.48474829886293, 41.93427172681036, 7.308062965708509, 6.619096316677902, 5.854314053298597, 6.983698826866462, 3.2062567122663075, 12.260367274049429, 8.347448773836092, 7.855918581812096, 27.30303133299483, 11.587470962322852, 21.442941170325458, 26.778315442502336, 10.993336514087044, 12.842607765883162, 13.409097370834658, 16.227950179967028, 16.560021369779793, 22.59821257167785, 12.259438639102358, 14.394171035447899, 15.062198823691968, 17.53038917137123, 16.645437212211302, 10.637362260736694, 12.578101578459359, 8.739324673330136, 11.404382534338438, 9.792381845148485, 10.698229947887388, 10.983793189039217, 48.72636588008919, 6.367418860008145, 8.97038690003772, 3.982770015573725, 8.054900492750363, 7.912472809633195, 7.92633563387315, 4.656018559501278, 35.116110018361354, 4.4986878915167745, 9.947803975593734, 5.936707266013612, 5.2196287157808525, 4.744143897680585, 6.2969177277506825, 2.877732555607299, 4.407217307716031, 8.399820916407702, 3.5333064474382923, 4.040740748889198, 2.931422724545959, 9.271019531818602, 2.9063010681095682, 2.8924296267294483, 2.1096764403502233, 4.067292312021669, 2.854424627651823, 2.984328195158372, 3.4340958765321172, 4.7266055041128805, 7.821001866784945, 6.930108777527631, 14.788260571752609, 26.37267130265121, 5.181422111528123, 17.30991871337814, 8.36330937543487, 11.682215989862916, 14.315145108150126, 8.94129040817661, 5.661112453076269, 9.555940244699187, 10.563943141174777, 8.153438926723693, 11.119229499102762, 6.066561464487968, 8.114918873997473, 7.639509814921154, 7.679927792768647, 8.428538972196579, 7.076865137387532, 6.925079240051503, 6.677277576525361, 98.40225896504998, 74.08640478903449, 90.36805780818241, 17.79500351932274, 20.76703612943587, 54.85588557305172, 18.907335343785387, 15.680669208089, 32.13644040582995, 31.85950203219353, 9.762089917902864, 31.041208893876362, 10.071652752733936, 9.08778679968827, 143.5664580242811, 15.126881918762889, 7.863193239386992, 11.058369645344074, 61.98132178813513, 14.16820476757711, 6.538934474128137, 19.952994373539838, 25.598549075259307, 58.06010308899483, 13.229643939676103, 5.905137701201079, 4.058197186423449, 9.229295426744438, 30.511415503835863, 5.106786762665214, 7.492931590231332, 58.3127067711676, 41.52866644294747, 33.579450335480345, 20.463196473244874, 50.313905401794656, 24.13652507202502, 24.890010187833237, 37.14291818581375, 31.368241587734563, 27.528700123916522, 19.73599921407074, 22.697210607555327, 18.73977634964421, 19.291687152039948, 18.732645273318578, 11.889504336407693, 20.66525686776491, 21.470469939639177, 14.453154577196742, 9.270681726392183, 89.73494086641686, 11.951880644302683, 8.488582495813464, 6.539931542783557, 108.59326724136983, 23.466016553930583, 16.59080790796281, 6.646819992045089, 4.872274602751533, 7.740240282770433, 6.981469150796636, 6.14764790918466, 11.002593988681182, 114.94116714909505, 16.53787671467491, 5.033593675779211, 21.276650850003307, 4.87123408325773, 38.615219733187175, 4.111718611011915, 4.639016970007651, 8.756991301353525, 3.379643272549062, 4.1768669191828804, 28.514967066756206, 9.053748795460768, 6.404357079433158, 13.416622196922411, 14.640766877233665, 13.980889950344928, 13.589780654386347, 7.5816253742270066, 11.09617624351822, 23.618766446645385, 32.32884800061672, 9.832883268322552, 26.42528068379852, 26.589741332426474, 15.625895296252326, 12.25478561802704, 12.475356474969209, 13.890166436632297, 12.333148190150657, 12.229731598950742, 13.039170507030693, 13.053714797325977, 12.349023248163345, 31.087859979091657, 48.014060018646646, 12.452098197640103, 12.64780633992297, 29.453788985111547, 19.100079163681595, 225.2524898377588, 47.27326503818867, 20.54893372137763, 8.083609268383341, 7.7403163348273125, 7.945772311549877, 4.1806108143872365, 5.576857730802844, 12.142416801416635, 5.182546833918833, 6.4243645825246505, 5.0608821333932825, 9.421341377751421, 19.290099173725515, 8.885951827817198, 4.847491245894855, 7.118296497196485, 6.1029476225934465, 4.5440725237034245, 3.8156360888534615, 29.35645722469111, 9.197042620550702, 4.3263995307295, 4.069892314431405, 13.014722881927534, 12.708744837203238, 14.461954906641903, 13.154536195134305, 19.77734768148045, 38.99013138951262, 17.966589079828093, 19.153016197690903, 18.327337300705057, 18.42719805593908, 23.38781211597963, 21.679116398955937, 18.686516904545748, 18.75049659812083, 17.943213147753706, 16.343971854497376, 16.006596195948212, 12.65015168354756, 13.933187751187887, 14.929766716082716, 14.620307723133992, 13.209124404971272], \"Total\": [730.0, 539.0, 537.0, 252.0, 264.0, 546.0, 822.0, 653.0, 146.0, 353.0, 242.0, 195.0, 302.0, 340.0, 362.0, 236.0, 236.0, 284.0, 248.0, 266.0, 391.0, 105.0, 132.0, 291.0, 104.0, 203.0, 284.0, 587.0, 675.0, 79.0, 43.64197106330486, 35.046872967693986, 70.720433722953, 41.44356771154314, 26.9984593719159, 50.040772103302785, 54.06998417253868, 26.83893395326217, 51.87238172057464, 58.84212820822624, 43.64052775650451, 75.71581646293929, 23.52577479385711, 24.894047514871453, 34.15665400554339, 49.1521658864327, 67.32384231392317, 222.61204722592547, 56.013589432783505, 25.697550718619496, 23.507200425168175, 52.65779507171423, 70.66517406621495, 68.96901147436695, 47.85755464827796, 32.85937122479682, 154.94774291148076, 87.49484033089098, 23.746632916146815, 120.97815613329192, 249.91868842750802, 261.8832391253039, 202.13965696259106, 99.62073611578114, 653.6753677702166, 716.063159479305, 139.95596974263879, 365.08894046238754, 90.42594382334173, 625.627783876929, 311.77307898930894, 734.4211875593269, 742.0599031236281, 772.9944924728684, 224.16638408487222, 236.04611530322694, 564.6220710989926, 675.9398226161582, 81.0196477858074, 43.5312107597866, 23.544064165818895, 133.04292002209831, 112.1857165907729, 23.761926909138353, 23.487753341201767, 66.63158686413323, 49.00081498942248, 31.431181558034506, 156.9773024864613, 98.92162509225602, 68.96901147436695, 45.85322322030807, 29.276675385602847, 23.7670243429705, 26.612976454282133, 40.07792784959294, 44.132430919035784, 96.51096791228014, 29.17090120926465, 34.13467175456795, 38.116087211551566, 34.8836749513454, 28.56407700951574, 57.16668432194801, 36.37065004385798, 26.972359299422983, 90.43946984743106, 28.665300950422967, 51.91843500033538, 209.59790696790697, 73.17137075695271, 79.34435593803941, 115.28882992066082, 101.98399989113435, 340.54840036886606, 280.18420447718245, 742.0599031236281, 282.92059878212956, 224.16638408487222, 716.063159479305, 105.7945237362215, 772.9944924728684, 286.13841320353254, 236.04611530322694, 136.19595591860667, 202.13965696259106, 353.45029830323926, 587.0864190485777, 734.4211875593269, 425.56824415864776, 510.1141042461977, 822.9731869803151, 111.71068592709048, 22.89179689085492, 77.71810946512461, 61.666533896166854, 38.709926345866236, 49.20102869838928, 42.278411280385455, 28.884092682052376, 29.217699247869355, 48.886855477571515, 32.629870360273586, 47.15553537413696, 103.9411015192822, 56.86507760234878, 44.53006122596023, 276.3152455784432, 76.69074942597076, 124.33466102630328, 55.65332943781262, 96.17562283732231, 42.68784682786345, 63.70757243833425, 31.431181558034506, 26.421215354028977, 173.2172396807463, 20.895950832417938, 90.43946984743106, 125.85027973716181, 29.746061084785637, 27.601984734093662, 46.52470649028739, 66.63158686413323, 340.4851217878861, 772.9944924728684, 98.92162509225602, 265.60140937783723, 716.063159479305, 625.627783876929, 742.0599031236281, 300.0581659163305, 734.4211875593269, 105.7945237362215, 266.27100875188694, 209.59790696790697, 449.77602152463965, 337.45671819659486, 587.0864190485777, 282.92059878212956, 510.1141042461977, 224.16638408487222, 236.42105055222532, 34.71785682690605, 291.25102222679686, 35.33589477489663, 49.34395744081364, 32.0988570615217, 64.46339020119919, 185.14712062001655, 21.425032221820757, 48.59369459092894, 43.64052775650451, 83.51337477875288, 38.29752065808127, 24.53854269274052, 25.351114077819435, 31.203950900668712, 23.664945498079426, 102.97165549634845, 539.1342841371606, 215.3238369189779, 30.565728678097138, 28.21302977220617, 118.04199224372017, 139.73741349137867, 35.619776363451194, 42.68784682786345, 44.53006122596023, 24.294730123023886, 20.895950832417938, 23.05041725069928, 20.617442625967996, 371.0040176088344, 37.60873117430735, 136.19595591860667, 115.28882992066082, 204.7118322657266, 587.0864190485777, 734.4211875593269, 337.45671819659486, 112.47793751788934, 265.60140937783723, 84.82767909662006, 546.4488937514927, 282.92059878212956, 236.81860792454174, 772.9944924728684, 625.627783876929, 205.115720417732, 202.13965696259106, 197.39703099191888, 716.063159479305, 510.1141042461977, 675.9398226161582, 365.9669808527841, 742.0599031236281, 449.77602152463965, 203.1691795330487, 564.6220710989926, 55.34101635575749, 51.87015756881938, 25.463641157168986, 29.40811894460972, 33.885510012932, 28.50590671567986, 36.43913930547464, 33.58896018367455, 21.50054763539441, 222.39312631065954, 31.18597064905967, 34.48597180898791, 64.80558428920224, 42.80954337376181, 25.549671265578517, 32.30441136525362, 27.457332725704, 32.1126295276064, 108.78982424185239, 27.654545147575213, 54.08626350640084, 28.56442712940033, 33.74368014691305, 35.619776363451194, 114.02596733091025, 105.88419511997978, 26.782441002525097, 57.281213476852145, 27.045657406293618, 33.23849510594745, 47.527930620220594, 95.57465924230316, 158.46284521622462, 224.16638408487222, 337.45671819659486, 587.0864190485777, 730.5823363961963, 734.4211875593269, 653.6753677702166, 158.11526251950028, 716.063159479305, 742.0599031236281, 425.56824415864776, 262.6791546974731, 625.627783876929, 822.9731869803151, 371.0040176088344, 772.9944924728684, 449.77602152463965, 147.43977824623857, 564.6220710989926, 286.13841320353254, 205.115720417732, 236.04611530322694, 53.622926859936484, 50.560129782060486, 38.40136001531799, 30.40071850536188, 53.21585327723723, 65.49751590758197, 174.06246144686094, 27.56098979854488, 23.685238771791006, 33.23849510594745, 31.4596479589042, 39.9803236844934, 33.54425294262099, 25.774123381463184, 22.516871108457295, 26.65269806385126, 61.282893552734215, 29.217699247869355, 36.711011738336886, 539.1342841371606, 28.50590671567986, 48.06849572027859, 128.79253024371843, 26.914114940978074, 185.14712062001655, 35.33589477489663, 25.365171566618844, 96.17562283732231, 38.29752065808127, 28.405413301855816, 90.79688821254214, 96.98253362388145, 222.39312631065954, 291.25102222679686, 120.97815613329192, 222.61204722592547, 675.9398226161582, 365.9669808527841, 653.6753677702166, 262.6791546974731, 127.12948865885919, 158.46284521622462, 546.4488937514927, 249.91868842750802, 291.7275959427799, 261.8832391253039, 734.4211875593269, 716.063159479305, 193.17028933612562, 587.0864190485777, 772.9944924728684, 625.627783876929, 353.45029830323926, 742.0599031236281, 311.77307898930894, 564.6220710989926, 730.5823363961963, 510.1141042461977, 31.399616311039228, 31.74358822755795, 54.08626350640084, 47.44928150649885, 60.76701714926183, 29.44658954549101, 32.45463790059249, 41.8190831176735, 32.62508197952525, 32.51909945113593, 26.085941845396235, 25.84008438580531, 33.487940403153544, 28.56407700951574, 36.810222707754356, 39.8176767266598, 28.331959920819507, 21.809436362313562, 34.558514067890265, 66.48484799851462, 22.582688600894713, 34.029962407137276, 41.31379554305041, 31.473894315142427, 266.27100875188694, 80.49699082644302, 53.19321476645127, 38.40136001531799, 28.665300950422967, 25.025836187354827, 284.9414553470063, 79.34435593803941, 124.33466102630328, 149.28594871163534, 95.85706589723847, 734.4211875593269, 254.95678314134585, 136.19595591860667, 235.3420487551701, 280.18420447718245, 205.115720417732, 391.4074555657499, 587.0864190485777, 286.13841320353254, 163.13513830343658, 188.32502251315302, 742.0599031236281, 365.08894046238754, 371.0040176088344, 222.39312631065954, 276.3152455784432, 625.627783876929, 675.9398226161582, 209.59790696790697, 425.56824415864776, 510.1141042461977, 772.9944924728684, 257.55921613114424, 40.360204289929186, 53.03655341304463, 57.59806343210074, 79.14913537924174, 24.62859197141869, 25.025836187354827, 20.192115144565385, 29.083756462884153, 56.24494856269128, 24.294730123023886, 100.46867547354817, 141.78658091179656, 33.536967336472465, 25.351114077819435, 41.96824636060745, 32.55097927160426, 56.992176762855664, 34.029962407137276, 34.13467175456795, 23.05041725069928, 21.591749681303863, 58.82176054730422, 42.80954337376181, 30.999120752025433, 51.518711846498306, 44.132430919035784, 32.0988570615217, 28.56407700951574, 85.37490140471044, 21.83782639325471, 266.7206787983673, 254.95678314134585, 730.5823363961963, 145.69525059567752, 114.64270841726841, 537.3201622462525, 546.4488937514927, 291.7275959427799, 286.13841320353254, 822.9731869803151, 564.6220710989926, 177.38981978745667, 345.4896604291085, 625.627783876929, 675.9398226161582, 742.0599031236281, 587.0864190485777, 734.4211875593269, 116.0932616036711, 149.28594871163534, 255.93402946377222, 653.6753677702166, 391.4074555657499, 66.19560722152539, 52.56447878909906, 35.75291033963596, 537.3201622462525, 45.55794527973795, 32.57790237374857, 41.110275187782804, 42.61566989055449, 45.43746823173782, 95.84358946468805, 34.20606315912254, 32.88615264578406, 52.87221776187446, 45.330207849191865, 43.383942196348656, 91.78292029162608, 23.32596315048862, 31.40871323431074, 23.848120437168557, 25.776646650461608, 29.272467928300145, 61.282893552734215, 22.71106587637823, 49.8338986236263, 31.24821581528746, 193.17028933612562, 49.057545747760386, 55.34101635575749, 203.9949154257112, 47.625560605041564, 425.56824415864776, 96.38389997675807, 90.42594382334173, 300.0581659163305, 276.3152455784432, 235.3420487551701, 449.77602152463965, 365.9669808527841, 730.5823363961963, 255.93402946377222, 675.9398226161582, 822.9731869803151, 362.5344349772914, 625.627783876929, 365.08894046238754, 742.0599031236281, 236.04611530322694, 216.47030021408284, 196.95892402729544, 311.77307898930894, 203.1691795330487, 257.55921613114424, 564.6220710989926, 510.1141042461977, 716.063159479305, 734.4211875593269, 60.68132735307866, 248.03263465123874, 284.1124298530312, 36.944480182353736, 67.87976350977532, 45.43746823173782, 46.00240788994978, 57.579013751541126, 25.923753248639198, 38.40136001531799, 27.728877015401043, 822.9731869803151, 42.56258448408457, 68.61187445815378, 81.73928748615694, 127.12948865885919, 93.35923739000786, 77.71810946512461, 21.83782639325471, 70.06201951298664, 29.819654705217687, 37.60873117430735, 96.98253362388145, 41.980462125777855, 32.45463790059249, 27.457332725704, 96.38389997675807, 139.73741349137867, 69.97651746386128, 41.04889164989464, 93.61746312936765, 276.3152455784432, 188.32502251315302, 653.6753677702166, 564.6220710989926, 222.39312631065954, 177.38981978745667, 242.7728558006833, 156.9773024864613, 204.7118322657266, 337.45671819659486, 284.9414553470063, 716.063159479305, 510.1141042461977, 197.39703099191888, 734.4211875593269, 42.56258448408457, 112.1857165907729, 33.25390558179653, 242.7728558006833, 95.85706589723847, 32.1126295276064, 57.88481992122593, 68.96901147436695, 31.798806002881555, 29.598711698858054, 40.716803457253356, 30.098673528451762, 61.666533896166854, 33.885510012932, 33.65328314104035, 62.676976660598555, 28.6437129780045, 27.05104913870944, 73.17137075695271, 57.16668432194801, 38.475664018872266, 56.013589432783505, 126.61520857926382, 41.980462125777855, 23.37404324278898, 67.96109700447397, 32.51909945113593, 20.736165215931354, 70.06201951298664, 79.87248667053777, 104.7652723399187, 94.42472908990636, 148.8276667790548, 151.2395613085601, 564.6220710989926, 365.08894046238754, 716.063159479305, 186.94069631259708, 205.115720417732, 653.6753677702166, 675.9398226161582, 209.59790696790697, 510.1141042461977, 276.3152455784432, 730.5823363961963, 362.5344349772914, 337.45671819659486, 51.518711846498306, 25.619407699209663, 52.31233653566072, 148.15070978350428, 44.33546172646385, 29.746061084785637, 37.79892965284372, 33.65328314104035, 49.44721485568978, 41.36812016185529, 56.51667641701247, 31.20513116130896, 34.63514361432058, 93.61746312936765, 58.84212820822624, 26.7099279001934, 23.848120437168557, 254.99409016707168, 114.64270841726841, 31.913939188284086, 69.97651746386128, 20.69691920370004, 98.92162509225602, 76.59988534396567, 50.040772103302785, 95.57465924230316, 197.39703099191888, 81.73928748615694, 56.992176762855664, 38.475664018872266, 42.15466320867216, 67.3196847483843, 108.78982424185239, 222.39312631065954, 345.4896604291085, 255.93402946377222, 147.43977824623857, 100.00834045076239, 265.60140937783723, 730.5823363961963, 675.9398226161582, 510.1141042461977, 587.0864190485777, 742.0599031236281, 772.9944924728684, 337.45671819659486, 291.7275959427799, 254.95678314134585, 224.16638408487222, 93.35923739000786, 26.7099279001934, 266.27100875188694, 43.3389437442521, 49.44721485568978, 38.29752065808127, 38.28205844283741, 44.132430919035784, 43.6166876820286, 33.446051885738136, 31.4596479589042, 31.868191998685063, 133.04292002209831, 32.51909945113593, 34.981639324266204, 26.972359299422983, 75.24232322378606, 38.709926345866236, 96.04373789388433, 46.6290083404254, 28.223877046045935, 44.33546172646385, 20.895950832417938, 24.307476446545763, 70.720433722953, 41.64850944688564, 45.92343897740339, 57.972551274555684, 23.67955907552741, 21.686290032856327, 266.7206787983673, 110.04832705180706, 53.70542674966603, 138.04839178657642, 124.33466102630328, 178.78879390813643, 120.46642632787982, 734.4211875593269, 653.6753677702166, 222.39312631065954, 123.32645013761561, 587.0864190485777, 262.6791546974731, 188.32502251315302, 371.0040176088344, 539.1342841371606, 772.9944924728684, 742.0599031236281, 625.627783876929, 337.45671819659486, 236.04611530322694, 822.9731869803151, 205.115720417732, 286.13841320353254, 345.4896604291085, 79.87248667053777, 46.83274225014348, 39.34318445928841, 34.32836224408115, 35.91509748971129, 106.49864644364877, 20.192115144565385, 22.411620597187863, 31.696021662389025, 47.15553537413696, 45.85322322030807, 21.686290032856327, 114.02596733091025, 33.65328314104035, 35.37838110772507, 32.0988570615217, 54.280597201520095, 32.88615264578406, 34.981639324266204, 25.640880049831168, 58.538594027976856, 32.51909945113593, 59.63409316274152, 43.383942196348656, 26.914114940978074, 45.06523178105401, 31.20513116130896, 61.38556957286161, 60.37142632369526, 25.549671265578517, 42.96462194908102, 53.19321476645127, 141.78658091179656, 92.73273070075133, 391.4074555657499, 63.70757243833425, 625.627783876929, 587.0864190485777, 716.063159479305, 564.6220710989926, 236.42105055222532, 510.1141042461977, 107.4178879041388, 222.39312631065954, 147.43977824623857, 153.71293731229872, 675.9398226161582, 730.5823363961963, 43.65243266040857, 23.05041725069928, 39.307244626891574, 209.59790696790697, 35.96201164015932, 32.88615264578406, 44.33546172646385, 38.709926345866236, 45.92343897740339, 34.8836749513454, 102.97165549634845, 47.15553537413696, 28.223877046045935, 28.331959920819507, 22.5723563909998, 23.67955907552741, 70.720433722953, 48.51129310182277, 93.35923739000786, 26.085941845396235, 49.1521658864327, 82.60364651027535, 39.29324130015037, 75.76000982238064, 57.91470525430146, 108.72381306025859, 40.62225170500875, 24.894047514871453, 84.92147315380086, 33.65328314104035, 94.18270305816944, 90.43946984743106, 284.9414553470063, 178.78879390813643, 625.627783876929, 75.49756259032135, 365.08894046238754, 302.28647230835884, 822.9731869803151, 300.0581659163305, 716.063159479305, 340.4851217878861, 653.6753677702166, 121.84073166950945, 194.01068991203374, 204.7118322657266, 337.45671819659486, 173.2172396807463, 510.1141042461977, 55.65012010237271, 29.819654705217687, 57.522617872524805, 48.46144616436049, 34.42406240965421, 24.307476446545763, 70.66517406621495, 34.11522799814629, 32.235181334894506, 31.209826473463355, 30.565728678097138, 25.47243512029359, 26.972359299422983, 119.25861474763573, 28.6437129780045, 35.96225591987428, 236.81303379470893, 40.93879401906628, 33.58896018367455, 25.438812053568583, 27.43719431673835, 353.45029830323926, 81.43964181529057, 25.474780163804716, 46.10008189206996, 32.620429249437244, 56.24494856269128, 63.13906182081008, 194.01068991203374, 25.351114077819435, 95.57465924230316, 77.34578739785306, 53.96678778169888, 252.84895187984105, 90.06492337468814, 730.5823363961963, 262.6791546974731, 105.88419511997978, 166.37260841541695, 772.9944924728684, 120.46642632787982, 340.54840036886606, 172.95672012306724, 365.9669808527841, 564.6220710989926, 203.1691795330487, 257.55921613114424, 340.4851217878861, 337.45671819659486, 302.28647230835884, 311.77307898930894, 734.4211875593269, 822.9731869803151, 215.3238369189779, 716.063159479305, 291.7275959427799, 51.55220187390221, 36.18126679049814, 34.23488997477975, 50.11358894870814, 65.11154605346604, 124.96837579256346, 96.17562283732231, 236.81303379470893, 340.4851217878861, 75.24232322378606, 539.1342841371606, 35.62662107524966, 26.462296046967133, 56.421321416194914, 48.435990152222345, 32.629870360273586, 100.00834045076239, 38.53786594723036, 35.91509748971129, 47.556395272826535, 41.36812016185529, 59.63409316274152, 48.51129310182277, 41.04889164989464, 77.34578739785306, 27.045657406293618, 56.51667641701247, 194.01068991203374, 25.774123381463184, 63.13906182081008, 65.4846157549797, 188.32502251315302, 772.9944924728684, 161.1933114445624, 675.9398226161582, 255.93402946377222, 119.25861474763573, 265.60140937783723, 345.4896604291085, 125.85027973716181, 822.9731869803151, 130.3188535553141, 742.0599031236281, 449.77602152463965, 510.1141042461977, 276.3152455784432, 425.56824415864776, 254.95678314134585, 121.84073166950945, 28.665477579372727, 54.280597201520095, 39.237871395786684, 25.881270522994676, 38.90465592061447, 58.824142877677794, 52.03711109975619, 28.6437129780045, 34.424719159815275, 39.1995385987134, 23.746632916146815, 57.92446816475945, 81.73928748615694, 46.17883194018977, 29.958054413132842, 34.32467114843827, 147.1526293223592, 25.474780163804716, 21.50054763539441, 391.4074555657499, 27.654545147575213, 82.60364651027535, 36.53403286838165, 98.92162509225602, 41.31379554305041, 41.49443449419414, 33.65328314104035, 546.4488937514927, 32.13774296043016, 63.072213520687605, 114.02596733091025, 291.7275959427799, 115.28882992066082, 282.92059878212956, 143.67002129335214, 76.69074942597076, 166.37260841541695, 254.99409016707168, 128.79253024371843, 340.54840036886606, 742.0599031236281, 196.95892402729544, 215.3238369189779, 587.0864190485777, 734.4211875593269, 716.063159479305, 449.77602152463965, 564.6220710989926, 371.0040176088344, 59.25898731285815, 49.180645511043394, 60.40157695229653, 27.536021132141084, 39.863128373876094, 34.48597180898791, 110.39633302275423, 20.624290610018754, 106.49864644364877, 29.40811894460972, 119.25861474763573, 43.23702316698139, 46.6290083404254, 58.84212820822624, 49.93017245569916, 33.25390558179653, 64.80558428920224, 21.591749681303863, 83.29588366738085, 365.08894046238754, 27.815696519000255, 362.5344349772914, 20.678017971268595, 30.469373156632393, 41.71097742346924, 99.62073611578114, 68.61187445815378, 54.42496845261631, 57.281213476852145, 47.85755464827796, 117.04032523272411, 197.39703099191888, 311.77307898930894, 215.3238369189779, 96.04373789388433, 84.92147315380086, 158.46284521622462, 257.55921613114424, 96.98253362388145, 365.9669808527841, 340.54840036886606, 425.56824415864776, 653.6753677702166, 564.6220710989926, 742.0599031236281, 300.0581659163305, 510.1141042461977, 289.0668094129727, 284.1124298530312, 345.4896604291085, 248.03263465123874, 46.32927395206772, 37.41642185562863, 116.0932616036711, 302.28647230835884, 46.10008189206996, 28.945591512400718, 53.96678778169888, 25.626754468506853, 27.457332725704, 59.63409316274152, 66.48484799851462, 33.446051885738136, 57.92446816475945, 30.47181730952243, 34.71785682690605, 23.664945498079426, 37.79892965284372, 23.487753341201767, 36.944480182353736, 26.021266122401435, 30.559010905724005, 118.04199224372017, 25.84008438580531, 21.686290032856327, 166.37260841541695, 38.50881160462535, 47.54117568434712, 36.4476636465821, 29.276675385602847, 24.121431635454265, 33.54425294262099, 125.85027973716181, 289.0668094129727, 93.61746312936765, 120.97815613329192, 265.60140937783723, 365.08894046238754, 425.56824415864776, 173.2172396807463, 222.61204722592547, 822.9731869803151, 261.8832391253039, 365.9669808527841, 772.9944924728684, 362.5344349772914, 337.45671819659486, 546.4488937514927, 742.0599031236281, 224.16638408487222, 510.1141042461977, 675.9398226161582, 537.3201622462525, 35.812541430133535, 110.04832705180706, 33.293944617823854, 353.45029830323926, 100.7263830228971, 39.9803236844934, 27.34689600710586, 39.29324130015037, 48.886855477571515, 31.629782658943682, 57.579013751541126, 49.057545747760386, 34.63514361432058, 22.5723563909998, 31.20513116130896, 25.025836187354827, 58.65540704039873, 40.62225170500875, 43.64052775650451, 28.345391413115678, 34.32467114843827, 68.96901147436695, 52.71159052804063, 38.56030161866684, 25.697550718619496, 41.35892253294044, 46.17883194018977, 32.51909945113593, 21.83782639325471, 60.68947235800276, 57.23157563678253, 675.9398226161582, 254.99409016707168, 101.98399989113435, 449.77602152463965, 203.9949154257112, 105.7945237362215, 653.6753677702166, 133.04292002209831, 194.01068991203374, 178.78879390813643, 174.06246144686094, 235.3420487551701, 302.28647230835884, 340.54840036886606, 546.4488937514927, 625.627783876929, 537.3201622462525, 257.55921613114424, 365.9669808527841, 772.9944924728684, 564.6220710989926, 742.0599031236281, 587.0864190485777, 734.4211875593269, 105.87688540198913, 30.469373156632393, 45.85322322030807, 57.91470525430146, 29.735122899760682, 77.97208725680261, 50.040772103302785, 27.045657406293618, 26.9984593719159, 51.518711846498306, 75.7145703308527, 23.544064165818895, 35.812541430133535, 35.046872967693986, 67.9274680612322, 44.48908443736808, 40.07792784959294, 153.71293731229872, 48.154024077845044, 44.33546172646385, 49.34395744081364, 32.45463790059249, 20.69691920370004, 44.53006122596023, 34.62574199528146, 114.72513254596427, 26.085941845396235, 35.96201164015932, 110.39633302275423, 55.85795959538018, 120.46642632787982, 114.64270841726841, 100.00834045076239, 74.33744461077967, 224.16638408487222, 148.8276667790548, 149.28594871163534, 730.5823363961963, 822.9731869803151, 772.9944924728684, 625.627783876929, 716.063159479305, 172.95672012306724, 254.95678314134585, 280.18420447718245, 510.1141042461977, 742.0599031236281, 156.9773024864613, 37.43164643459063, 91.78292029162608, 41.110275187782804, 26.421215354028977, 35.62662107524966, 26.67541270540525, 34.59672696317499, 29.272467928300145, 22.411620597187863, 20.617442625967996, 32.629870360273586, 28.665300950422967, 25.697550718619496, 27.711298664686222, 77.97208725680261, 49.057545747760386, 29.429000460689984, 30.077878248781303, 24.53854269274052, 99.34762120178178, 24.121431635454265, 46.46867082114885, 23.05041725069928, 42.15466320867216, 35.615564156053104, 21.425032221820757, 33.293944617823854, 67.9274680612322, 65.06950881921918, 32.30441136525362, 121.84073166950945, 546.4488937514927, 55.65012010237271, 449.77602152463965, 172.95672012306724, 96.80463089381709, 161.1933114445624, 266.7206787983673, 564.6220710989926, 197.39703099191888, 353.45029830323926, 772.9944924728684, 174.06246144686094, 186.94069631259708, 264.9826666246572, 391.4074555657499, 345.4896604291085, 742.0599031236281, 675.9398226161582, 625.627783876929, 236.42105055222532, 822.9731869803151, 215.3238369189779, 337.45671819659486, 194.01068991203374, 340.54840036886606, 146.28928400383106, 56.17073957047833, 46.049079603159264, 114.02596733091025, 96.38389997675807, 41.49443449419414, 29.86346798763362, 35.56490802251105, 29.36900296772129, 22.516871108457295, 39.237871395786684, 53.409950418771864, 74.70923874050956, 40.62225170500875, 43.25428761842445, 43.43968832231711, 36.43913930547464, 36.810222707754356, 53.99462913882974, 48.46144616436049, 69.0517185555234, 27.56098979854488, 120.46642632787982, 53.87835870059048, 84.66434412678277, 94.29346021689113, 26.782441002525097, 61.47199871755444, 653.6753677702166, 23.664945498079426, 126.61520857926382, 96.04373789388433, 116.0932616036711, 291.7275959427799, 236.42105055222532, 730.5823363961963, 216.47030021408284, 340.54840036886606, 716.063159479305, 742.0599031236281, 282.92059878212956, 158.46284521622462, 345.4896604291085, 822.9731869803151, 365.08894046238754, 734.4211875593269, 425.56824415864776, 510.1141042461977, 108.78982424185239, 20.736165215931354, 39.97833079134024, 50.74422377768736, 40.93879401906628, 48.15858895058881, 27.601984734093662, 172.95672012306724, 35.96225591987428, 64.73944216010149, 55.65332943781262, 65.30877032788511, 57.16668432194801, 32.1126295276064, 31.629782658943682, 52.138847413387154, 25.626754468506853, 146.28928400383106, 41.57890312538207, 32.784107623168225, 31.798806002881555, 46.52470649028739, 28.665477579372727, 28.25874366234801, 32.55097927160426, 77.06006805084, 29.17090120926465, 26.65269806385126, 61.282893552734215, 27.994386492863566, 97.48281276963891, 284.1124298530312, 248.03263465123874, 75.49756259032135, 822.9731869803151, 257.55921613114424, 653.6753677702166, 204.7118322657266, 772.9944924728684, 716.063159479305, 391.4074555657499, 265.60140937783723, 742.0599031236281, 734.4211875593269, 730.5823363961963, 286.13841320353254, 587.0864190485777, 282.92059878212956, 289.0668094129727, 510.1141042461977, 52.975390266498636, 60.76701714926183, 99.34762120178178, 29.785083694852485, 70.30859639725801, 42.61566989055449, 34.424719159815275, 28.21302977220617, 29.36900296772129, 75.49756259032135, 37.76134076263287, 38.53786594723036, 23.52577479385711, 39.33382526744549, 49.206841257704326, 106.49864644364877, 20.192115144565385, 29.083756462884153, 79.64909598065447, 48.00924374021746, 50.03538575590946, 60.40157695229653, 81.73928748615694, 33.3239586597773, 49.1521658864327, 25.025836187354827, 70.06201951298664, 53.87835870059048, 31.15258403985925, 36.53403286838165, 56.17073957047833, 61.282893552734215, 83.29588366738085, 340.54840036886606, 564.6220710989926, 425.56824415864776, 772.9944924728684, 235.3420487551701, 311.77307898930894, 188.32502251315302, 362.5344349772914, 255.93402946377222, 675.9398226161582, 365.9669808527841, 345.4896604291085, 716.063159479305, 734.4211875593269, 173.2172396807463, 186.94069631259708, 291.7275959427799, 742.0599031236281, 391.4074555657499, 625.627783876929, 537.3201622462525, 64.46339020119919, 35.96201164015932, 102.97165549634845, 195.43984314288656, 84.92147315380086, 34.558514067890265, 34.43381357808268, 37.266957980353176, 236.81860792454174, 37.79892965284372, 32.335876649109466, 81.43964181529057, 46.46867082114885, 62.3974718398153, 24.121431635454265, 41.66058633369481, 38.28205844283741, 23.7670243429705, 36.684232550670274, 60.76701714926183, 29.40811894460972, 23.685238771791006, 25.463641157168986, 43.6166876820286, 44.132430919035784, 38.116087211551566, 34.20606315912254, 236.81303379470893, 90.43946984743106, 39.29324130015037, 93.36355603959507, 61.229393199378656, 266.27100875188694, 730.5823363961963, 539.1342841371606, 340.54840036886606, 110.04832705180706, 822.9731869803151, 151.2395613085601, 742.0599031236281, 449.77602152463965, 209.59790696790697, 510.1141042461977, 675.9398226161582, 564.6220710989926, 127.08658294610555, 286.13841320353254, 222.61204722592547, 345.4896604291085, 653.6753677702166, 33.74368014691305, 58.538594027976856, 37.554917183553385, 71.10372472209191, 100.46867547354817, 185.14712062001655, 35.615564156053104, 42.15466320867216, 23.67955907552741, 114.64270841726841, 34.59672696317499, 289.0668094129727, 75.76000982238064, 31.85598112308214, 33.87937921813332, 86.21968732876334, 284.9414553470063, 53.19321476645127, 57.579013751541126, 33.67434695451635, 28.345391413115678, 53.99462913882974, 25.923753248639198, 50.9006704619097, 22.582688600894713, 26.914114940978074, 173.2172396807463, 112.47793751788934, 29.971942146198344, 58.84212820822624, 291.25102222679686, 284.1124298530312, 248.03263465123874, 74.70923874050956, 822.9731869803151, 653.6753677702166, 449.77602152463965, 311.77307898930894, 300.0581659163305, 127.08658294610555, 365.08894046238754, 126.61520857926382, 742.0599031236281, 675.9398226161582, 625.627783876929, 716.063159479305, 365.9669808527841, 345.4896604291085, 730.5823363961963, 362.5344349772914, 72.97840213526815, 67.77625671112429, 39.1995385987134, 32.18817304425299, 30.077878248781303, 38.80738301308464, 41.35892253294044, 178.78879390813643, 57.522617872524805, 29.958054413132842, 28.665300950422967, 40.62225170500875, 38.656675276147084, 105.7945237362215, 36.810222707754356, 42.68426146752334, 32.62508197952525, 280.18420447718245, 31.696021662389025, 26.612976454282133, 34.15665400554339, 546.4488937514927, 67.3196847483843, 81.43964181529057, 70.30859639725801, 51.91843500033538, 62.45164969286128, 55.78661548889971, 126.60879632703653, 65.46453431653067, 391.4074555657499, 216.47030021408284, 140.44176083961548, 58.824142877677794, 716.063159479305, 114.72513254596427, 123.32645013761561, 345.4896604291085, 203.1691795330487, 90.43946984743106, 257.55921613114424, 197.39703099191888, 510.1141042461977, 101.98399989113435, 625.627783876929, 40.07792784959294, 65.98062104108021, 53.99462913882974, 23.507200425168175, 27.34689600710586, 42.80954337376181, 34.11522799814629, 38.80738301308464, 40.347562593913224, 27.457332725704, 81.0196477858074, 28.223877046045935, 33.3729711221536, 32.0988570615217, 22.582688600894713, 29.276675385602847, 21.686290032856327, 27.322700696546264, 22.5723563909998, 34.59672696317499, 67.9274680612322, 59.25898731285815, 34.8836749513454, 35.615564156053104, 41.31379554305041, 25.776646650461608, 93.61746312936765, 30.47181730952243, 74.70923874050956, 27.43719431673835, 188.32502251315302, 96.17562283732231, 311.77307898930894, 205.115720417732, 289.0668094129727, 449.77602152463965, 822.9731869803151, 262.6791546974731, 353.45029830323926, 675.9398226161582, 195.43984314288656, 266.27100875188694, 564.6220710989926, 261.8832391253039, 257.55921613114424, 539.1342841371606, 625.627783876929, 716.063159479305, 340.4851217878861, 236.42105055222532, 742.0599031236281, 730.5823363961963, 772.9944924728684, 70.05169681403734, 37.76134076263287, 148.8276667790548, 30.999120752025433, 31.209826473463355, 57.59806343210074, 31.15258403985925, 31.24821581528746, 67.96109700447397, 23.240172667623167, 139.73741349137867, 79.14913537924174, 35.96201164015932, 25.365171566618844, 53.70542674966603, 42.68426146752334, 39.46186417761399, 33.293944617823854, 21.425032221820757, 56.928849564356206, 49.1521658864327, 21.591749681303863, 205.115720417732, 27.728877015401043, 35.96225591987428, 39.34318445928841, 35.15465766240223, 49.34395744081364, 29.276675385602847, 44.53006122596023, 391.4074555657499, 143.67002129335214, 119.25861474763573, 261.8832391253039, 141.78658091179656, 236.81303379470893, 106.49864644364877, 546.4488937514927, 186.94069631259708, 266.7206787983673, 730.5823363961963, 153.71293731229872, 291.7275959427799, 286.13841320353254, 625.627783876929, 716.063159479305, 249.91868842750802, 742.0599031236281, 734.4211875593269, 362.5344349772914, 222.61204722592547, 510.1141042461977, 772.9944924728684, 822.9731869803151, 564.6220710989926, 46.3068705355945, 37.266957980353176, 34.43381357808268, 97.22593471511385, 195.43984314288656, 94.5420902152409, 26.462296046967133, 42.96462194908102, 35.75291033963596, 68.61187445815378, 31.85598112308214, 26.972359299422983, 60.68132735307866, 44.33546172646385, 46.32927395206772, 23.37404324278898, 38.88939507515254, 116.0932616036711, 24.307476446545763, 50.9006704619097, 86.21968732876334, 35.36311293951485, 41.8190831176735, 46.049079603159264, 27.457332725704, 49.00081498942248, 23.544064165818895, 26.83893395326217, 73.17137075695271, 38.116087211551566, 114.64270841726841, 96.38389997675807, 302.28647230835884, 148.15070978350428, 675.9398226161582, 822.9731869803151, 653.6753677702166, 289.0668094129727, 539.1342841371606, 625.627783876929, 300.0581659163305, 537.3201622462525, 284.1124298530312, 716.063159479305, 345.4896604291085, 365.9669808527841, 734.4211875593269, 365.08894046238754, 40.647798777864004, 61.52648347507037, 37.501206814304545, 252.84895187984105, 65.4846157549797, 36.4476636465821, 47.556395272826535, 22.411620597187863, 104.27657566869152, 28.56442712940033, 28.331959920819507, 34.42406240965421, 33.25390558179653, 42.278411280385455, 48.51129310182277, 39.46186417761399, 48.435990152222345, 111.71068592709048, 93.61746312936765, 20.69691920370004, 31.282915992088682, 90.90295633260752, 65.06950881921918, 42.80954337376181, 45.06523178105401, 62.85057029442794, 53.409950418771864, 31.209826473463355, 87.72950621672008, 138.04839178657642, 284.9414553470063, 97.48281276963891, 103.9411015192822, 340.4851217878861, 266.27100875188694, 546.4488937514927, 289.0668094129727, 148.15070978350428, 730.5823363961963, 177.38981978745667, 625.627783876929, 261.8832391253039, 284.1124298530312, 345.4896604291085, 564.6220710989926, 141.78658091179656, 362.5344349772914, 742.0599031236281, 72.9460440868247, 29.598711698858054, 78.80358965544876, 62.85057029442794, 79.87248667053777, 39.863128373876094, 203.9949154257112, 194.01068991203374, 27.0806418768302, 174.06246144686094, 35.91509748971129, 106.49864644364877, 47.030229935305876, 38.53786594723036, 97.22593471511385, 83.51337477875288, 25.776646650461608, 353.45029830323926, 24.121431635454265, 51.6459998531323, 27.05104913870944, 108.78982424185239, 89.3960603157755, 54.06998417253868, 23.67955907552741, 62.14595171919711, 119.25861474763573, 130.3188535553141, 49.8338986236263, 76.35849052239145, 55.65012010237271, 65.06950881921918, 242.7728558006833, 222.39312631065954, 653.6753677702166, 425.56824415864776, 564.6220710989926, 510.1141042461977, 675.9398226161582, 143.67002129335214, 772.9944924728684, 449.77602152463965, 236.04611530322694, 236.42105055222532, 716.063159479305, 132.8052930726588, 42.82106704088433, 107.4178879041388, 264.9826666246572, 28.862551979241175, 75.24232322378606, 35.812541430133535, 23.761926909138353, 67.9274680612322, 26.462296046967133, 37.43164643459063, 41.44356771154314, 30.565728678097138, 28.223877046045935, 29.971942146198344, 28.56442712940033, 26.7099279001934, 56.17073957047833, 27.05104913870944, 91.78292029162608, 35.619776363451194, 37.78872135030687, 205.115720417732, 84.66434412678277, 62.676976660598555, 39.307244626891574, 47.625560605041564, 52.71159052804063, 41.110275187782804, 21.809436362313562, 254.95678314134585, 266.7206787983673, 337.45671819659486, 110.04832705180706, 108.72381306025859, 772.9944924728684, 289.0668094129727, 266.27100875188694, 196.95892402729544, 675.9398226161582, 139.95596974263879, 204.7118322657266, 236.42105055222532, 257.55921613114424, 254.99409016707168, 564.6220710989926, 510.1141042461977, 265.60140937783723, 255.93402946377222, 40.347562593913224, 56.421321416194914, 34.42406240965421, 34.30943331234622, 362.5344349772914, 30.498537876868756, 20.678017971268595, 264.9826666246572, 32.85937122479682, 28.405413301855816, 34.63514361432058, 32.620429249437244, 41.64850944688564, 39.33382526744549, 21.83782639325471, 35.56490802251105, 50.11358894870814, 62.14595171919711, 42.278411280385455, 67.44618122137018, 38.40136001531799, 300.0581659163305, 54.08626350640084, 49.8859508848999, 45.00584646532979, 55.78661548889971, 25.640880049831168, 99.34762120178178, 69.0517185555234, 67.32384231392317, 254.95678314134585, 103.9411015192822, 235.3420487551701, 340.4851217878861, 104.27657566869152, 148.8276667790548, 177.38981978745667, 255.93402946377222, 291.7275959427799, 564.6220710989926, 161.1933114445624, 236.04611530322694, 345.4896604291085, 625.627783876929, 675.9398226161582, 188.32502251315302, 772.9944924728684, 108.72381306025859, 510.1141042461977, 282.92059878212956, 716.063159479305, 27.994386492863566, 236.81860792454174, 32.60311774565817, 47.625560605041564, 22.5723563909998, 47.616603009127246, 50.9006704619097, 54.42496845261631, 34.48597180898791, 262.6791546974731, 35.37838110772507, 80.49699082644302, 48.59369459092894, 45.330207849191865, 41.57890312538207, 58.65540704039873, 26.914114940978074, 41.71097742346924, 79.64909598065447, 36.37065004385798, 41.980462125777855, 32.1126295276064, 103.9411015192822, 34.32836224408115, 34.32467114843827, 25.84008438580531, 50.03538575590946, 35.36311293951485, 37.78872135030687, 43.60703422753146, 62.45164969286128, 118.02712672335608, 105.7945237362215, 282.92059878212956, 653.6753677702166, 75.49756259032135, 425.56824415864776, 151.2395613085601, 266.27100875188694, 449.77602152463965, 204.7118322657266, 90.79688821254214, 340.54840036886606, 510.1141042461977, 255.93402946377222, 742.0599031236281, 115.28882992066082, 371.0040176088344, 337.45671819659486, 365.9669808527841, 675.9398226161582, 772.9944924728684, 734.4211875593269, 546.4488937514927, 104.08788825521574, 79.11797911285518, 155.65638878199823, 33.536967336472465, 45.55794527973795, 127.08658294610555, 51.63876412998515, 43.43968832231711, 94.5420902152409, 95.84358946468805, 31.24821581528746, 100.7263830228971, 33.87937921813332, 30.757385628126467, 537.3201622462525, 57.91470525430146, 31.473894315142427, 45.03386354605317, 254.95678314134585, 60.37142632369526, 28.486512969117587, 95.57465924230316, 128.79253024371843, 302.28647230835884, 75.7145703308527, 34.11522799814629, 23.685238771791006, 57.522617872524805, 193.17028933612562, 33.446051885738136, 49.20102869838928, 425.56824415864776, 311.77307898930894, 280.18420447718245, 158.46284521622462, 675.9398226161582, 254.99409016707168, 276.3152455784432, 742.0599031236281, 510.1141042461977, 391.4074555657499, 262.6791546974731, 625.627783876929, 353.45029830323926, 734.4211875593269, 587.0864190485777, 23.282870110829876, 43.5312107597866, 57.88481992122593, 42.56258448408457, 31.868191998685063, 371.0040176088344, 51.87015756881938, 38.90465592061447, 34.62574199528146, 587.0864190485777, 133.04292002209831, 96.51096791228014, 39.307244626891574, 29.217699247869355, 46.99868155753921, 42.82106704088433, 38.56030161866684, 70.05169681403734, 734.4211875593269, 107.69981025334823, 32.784107623168225, 139.95596974263879, 32.57790237374857, 266.7206787983673, 28.56407700951574, 32.55097927160426, 61.47199871755444, 23.7670243429705, 29.86346798763362, 204.7118322657266, 65.98062104108021, 46.6290083404254, 103.9411015192822, 115.28882992066082, 114.46245165094584, 116.33262925651482, 56.68881701068831, 95.85706589723847, 282.92059878212956, 625.627783876929, 87.72950621672008, 742.0599031236281, 772.9944924728684, 242.7728558006833, 156.9773024864613, 235.3420487551701, 365.9669808527841, 249.91868842750802, 286.13841320353254, 564.6220710989926, 730.5823363961963, 510.1141042461977, 41.96824636060745, 66.20252255208918, 24.62859197141869, 32.620429249437244, 77.34578739785306, 53.21585327723723, 730.5823363961963, 193.17028933612562, 89.02168867283878, 35.615564156053104, 34.71785682690605, 36.53403286838165, 20.624290610018754, 27.711298664686222, 60.37142632369526, 25.776646650461608, 34.59672696317499, 27.56098979854488, 51.500700955552276, 105.88419511997978, 48.886855477571515, 29.36900296772129, 43.383942196348656, 37.79892965284372, 29.44658954549101, 25.47243512029359, 203.9949154257112, 67.3196847483843, 32.30441136525362, 30.47181730952243, 100.7263830228971, 104.7652723399187, 132.8052930726588, 126.60879632703653, 255.93402946377222, 822.9731869803151, 264.9826666246572, 311.77307898930894, 291.25102222679686, 340.54840036886606, 742.0599031236281, 675.9398226161582, 510.1141042461977, 716.063159479305, 625.627783876929, 537.3201622462525, 564.6220710989926, 215.3238369189779, 365.9669808527841, 546.4488937514927, 539.1342841371606, 284.9414553470063], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic26\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic27\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic28\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic29\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic30\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic31\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic32\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic33\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic34\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic35\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic36\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic37\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic38\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic39\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\", \"Topic40\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.3308000564575195, -4.7947998046875, -4.0980000495910645, -4.641300201416016, -5.272200107574463, -4.7683000564575195, -4.699100017547607, -5.483399868011475, -4.860000133514404, -4.793499946594238, -5.107699871063232, -4.559599876403809, -5.729700088500977, -5.68209981918335, -5.3927998542785645, -5.0742998123168945, -4.809999942779541, -3.6456000804901123, -5.027900218963623, -5.821000099182129, -5.927599906921387, -5.138199806213379, -4.844699859619141, -4.877099990844727, -5.252399921417236, -5.64900016784668, -4.139900207519531, -4.719399929046631, -6.046000003814697, -4.435100078582764, -3.94569993019104, -3.952399969100952, -4.214300155639648, -4.799499988555908, -3.6101999282836914, -3.6187000274658203, -4.719900131225586, -4.286499977111816, -4.935200214385986, -4.158400058746338, -4.463099956512451, -4.2382001876831055, -4.441199779510498, -4.4644999504089355, -4.77810001373291, -4.777900218963623, -4.717100143432617, -4.804800033569336, -3.7279000282287598, -4.710000038146973, -5.408699989318848, -3.7520999908447266, -3.967099905014038, -5.555799961090088, -5.593800067901611, -4.596099853515625, -4.934700012207031, -5.399799823760986, -3.849299907684326, -4.349800109863281, -4.711999893188477, -5.127099990844727, -5.583099842071533, -5.791800022125244, -5.694900035858154, -5.285799980163574, -5.192399978637695, -4.464600086212158, -5.666200160980225, -5.53000020980835, -5.4217000007629395, -5.532100200653076, -5.762599945068359, -5.190299987792969, -5.6585001945495605, -5.977799892425537, -4.768099784851074, -5.923799991607666, -5.340199947357178, -4.006100177764893, -5.039899826049805, -5.004899978637695, -4.718800067901611, -4.833899974822998, -4.188399791717529, -4.33519983291626, -3.8859000205993652, -4.404099941253662, -4.586999893188477, -4.037199974060059, -4.955599784851074, -4.167600154876709, -4.569399833679199, -4.69890022277832, -4.929200172424316, -4.827700138092041, -4.686999797821045, -4.596799850463867, -4.601600170135498, -4.721199989318848, -4.752799987792969, -4.810699939727783, -3.6129000186920166, -5.273499965667725, -4.071499824523926, -4.321000099182129, -4.92579984664917, -4.720099925994873, -4.898200035095215, -5.295000076293945, -5.3109002113342285, -4.94290018081665, -5.366399765014648, -5.064700126647949, -4.274600028991699, -4.891600131988525, -5.15749979019165, -3.352099895477295, -4.652699947357178, -4.177199840545654, -5.000699996948242, -4.456699848175049, -5.290200233459473, -4.959499835968018, -5.680600166320801, -5.88539981842041, -4.030099868774414, -6.183599948883057, -4.723700046539307, -4.418499946594238, -5.891300201416016, -6.021699905395508, -5.525199890136719, -5.184800148010254, -3.779599905014038, -3.1229000091552734, -4.889400005340576, -4.21019983291626, -3.5562000274658203, -3.769399881362915, -3.8766000270843506, -4.435999870300293, -4.18720006942749, -4.861800193786621, -4.639999866485596, -4.7153000831604, -4.645299911499023, -4.712500095367432, -4.6570000648498535, -4.774799823760986, -4.728300094604492, -4.83459997177124, -4.873799800872803, -5.2210001945495605, -3.1886000633239746, -5.52370023727417, -5.3541998863220215, -5.7957000732421875, -5.106500148773193, -4.056300163269043, -6.2519001960754395, -5.433300018310547, -5.583000183105469, -4.943600177764893, -5.734300136566162, -6.184000015258789, -6.173699855804443, -5.974699974060059, -6.2617998123168945, -4.808599948883057, -3.1986000537872314, -4.136099815368652, -6.093100070953369, -6.2368998527526855, -4.8109002113342285, -4.664999961853027, -6.041200160980225, -5.861000061035156, -5.825300216674805, -6.433599948883057, -6.644000053405762, -6.571499824523926, -6.688199996948242, -3.842099905014038, -6.093800067901611, -4.864799976348877, -5.0756001472473145, -4.591599941253662, -3.7155001163482666, -3.6233999729156494, -4.373199939727783, -5.200500011444092, -4.62470006942749, -5.398200035095215, -4.205999851226807, -4.655300140380859, -4.767000198364258, -4.047999858856201, -4.1880998611450195, -4.902100086212158, -4.914000034332275, -4.946400165557861, -4.390900135040283, -4.707499980926514, -4.686999797821045, -4.90939998626709, -4.775300025939941, -4.910299777984619, -5.058899879455566, -5.05109977722168, -3.5446999073028564, -4.672900199890137, -5.395999908447266, -5.308000087738037, -5.187900066375732, -5.372799873352051, -5.178500175476074, -5.278900146484375, -5.738800048828125, -3.4553000926971436, -5.452499866485596, -5.352200031280518, -4.770100116729736, -5.1992998123168945, -5.7245001792907715, -5.50600004196167, -5.671299934387207, -5.514900207519531, -4.307400226593018, -5.699100017547607, -5.0482001304626465, -5.6946001052856445, -5.537199974060059, -5.5131001472473145, -4.387800216674805, -4.492400169372559, -5.926599979400635, -5.168399810791016, -5.931399822235107, -5.7256999015808105, -5.437900066375732, -4.9141998291015625, -4.581500053405762, -4.396599769592285, -4.1921000480651855, -4.058700084686279, -4.005199909210205, -4.013000011444092, -4.169000148773193, -4.841000080108643, -4.173399925231934, -4.16510009765625, -4.461299896240234, -4.65500020980835, -4.357500076293945, -4.344399929046631, -4.663099765777588, -4.480800151824951, -4.640100002288818, -4.956699848175049, -4.718999862670898, -4.870800018310547, -4.947800159454346, -4.945700168609619, -3.97189998626709, -4.566699981689453, -5.145400047302246, -5.474999904632568, -4.957200050354004, -4.781000137329102, -3.8059000968933105, -5.650199890136719, -5.8119001388549805, -5.514100074768066, -5.582699775695801, -5.36329984664917, -5.5960001945495605, -5.870299816131592, -6.013599872589111, -5.868899822235107, -5.06220006942749, -5.849800109863281, -5.670499801635742, -3.0155999660491943, -5.973400115966797, -5.4807000160217285, -4.497200012207031, -6.086599826812744, -4.168499946594238, -5.849100112915039, -6.181000232696533, -4.864999771118164, -5.849100112915039, -6.152200222015381, -5.001500129699707, -4.959499835968018, -4.191999912261963, -4.089300155639648, -4.86460018157959, -4.420599937438965, -3.6767001152038574, -4.111100196838379, -3.822499990463257, -4.505000114440918, -4.971700191497803, -4.867599964141846, -4.245200157165527, -4.64739990234375, -4.578100204467773, -4.657899856567383, -4.234000205993652, -4.32480001449585, -4.836900234222412, -4.476399898529053, -4.3983001708984375, -4.4969000816345215, -4.658899784088135, -4.486499786376953, -4.818299770355225, -4.779699802398682, -4.832600116729736, -4.8607001304626465, -5.135900020599365, -5.2642998695373535, -4.74370002746582, -4.895999908447266, -4.740900039672852, -5.47760009765625, -5.478300094604492, -5.254199981689453, -5.504000186920166, -5.510000228881836, -5.74459981918335, -5.873000144958496, -5.640900135040283, -5.811299800872803, -5.566400051116943, -5.54040002822876, -5.893899917602539, -6.17609977722168, -5.7316999435424805, -5.110300064086914, -6.203000068664551, -5.810500144958496, -5.636499881744385, -5.924499988555908, -3.7999000549316406, -5.029399871826172, -5.466000080108643, -5.815999984741211, -6.11269998550415, -6.259799957275391, -3.843899965286255, -5.2104997634887695, -4.829599857330322, -4.6875, -5.078499794006348, -3.743499994277954, -4.491300106048584, -4.929800033569336, -4.6656999588012695, -4.605899810791016, -4.815800189971924, -4.490200042724609, -4.317200183868408, -4.689499855041504, -4.943999767303467, -4.88730001449585, -4.320499897003174, -4.64900016784668, -4.655300140380859, -4.87529993057251, -4.809299945831299, -4.559500217437744, -4.648499965667725, -4.955599784851074, -4.84499979019165, -4.820700168609619, -4.7657999992370605, -5.009099960327148, -4.012199878692627, -3.7939000129699707, -3.8712000846862793, -3.7395999431610107, -5.341100215911865, -5.3628997802734375, -5.577899932861328, -5.231299877166748, -4.581399917602539, -5.4527997970581055, -4.157700061798096, -3.856600046157837, -5.319499969482422, -5.792099952697754, -5.29610013961792, -5.58519983291626, -5.076200008392334, -5.651199817657471, -5.655099868774414, -6.080999851226807, -6.183899879455566, -5.212399959564209, -5.540900230407715, -5.934100151062012, -5.482500076293945, -5.645100116729736, -5.974599838256836, -6.100399971008301, -5.005899906158447, -6.388800144195557, -3.889699935913086, -4.222400188446045, -3.3582000732421875, -4.7256999015808105, -4.916100025177002, -3.8324999809265137, -3.970599889755249, -4.441400051116943, -4.464099884033203, -3.904099941253662, -4.1367998123168945, -4.784999847412109, -4.451000213623047, -4.200099945068359, -4.196400165557861, -4.281899929046631, -4.583099842071533, -4.613699913024902, -5.042900085449219, -5.0177001953125, -4.935699939727783, -4.831299781799316, -4.970099925994873, -3.891700029373169, -4.466800212860107, -5.220300197601318, -2.525099992752075, -5.037700176239014, -5.501399993896484, -5.430300235748291, -5.404399871826172, -5.35890007019043, -4.66540002822876, -5.698599815368652, -5.73799991607666, -5.294600009918213, -5.473800182342529, -5.536900043487549, -4.8105998039245605, -6.182000160217285, -5.884799957275391, -6.200699806213379, -6.157100200653076, -6.079899787902832, -5.37529993057251, -6.369800090789795, -5.5995001792907715, -6.081600189208984, -4.273099899291992, -5.6508002281188965, -5.536099910736084, -4.266499996185303, -5.735000133514404, -3.629499912261963, -5.168000221252441, -5.224699974060059, -4.3024001121521, -4.456999778747559, -4.60699987411499, -4.181000232696533, -4.364999771118164, -3.9790000915527344, -4.65339994430542, -4.105000019073486, -4.01609992980957, -4.502500057220459, -4.298500061035156, -4.57859992980957, -4.311500072479248, -4.815199851989746, -4.869699954986572, -4.919099807739258, -4.819499969482422, -4.961599826812744, -4.901100158691406, -4.786900043487549, -4.841700077056885, -4.929900169372559, -4.9405999183654785, -4.097899913787842, -2.933199882507324, -2.9446001052856445, -5.00629997253418, -4.509699821472168, -4.994999885559082, -4.996799945831299, -4.834000110626221, -5.655399799346924, -5.354599952697754, -5.6921000480651855, -2.32069993019104, -5.371699810028076, -4.897900104522705, -4.7322001457214355, -4.307700157165527, -4.619500160217285, -4.8231000900268555, -6.093100070953369, -4.939599990844727, -5.807400226593018, -5.5802998542785645, -4.6529998779296875, -5.5015997886657715, -5.857100009918213, -6.031000137329102, -4.801499843597412, -4.47599983215332, -5.1880998611450195, -5.7382001876831055, -4.916100025177002, -4.00570011138916, -4.378300189971924, -3.3559000492095947, -3.6982998847961426, -4.408999919891357, -4.609499931335449, -4.454899787902832, -4.788099765777588, -4.718999862670898, -4.521500110626221, -4.672399997711182, -4.61959981918335, -4.775899887084961, -4.909200191497803, -4.909900188446045, -3.9784998893737793, -3.012500047683716, -4.229800224304199, -2.3164000511169434, -3.3510000705718994, -4.47730016708374, -3.9102001190185547, -4.0630998611450195, -4.99370002746582, -5.089799880981445, -4.862400054931641, -5.2393999099731445, -4.597599983215332, -5.210000038146973, -5.236700057983398, -4.650000095367432, -5.436600208282471, -5.503200054168701, -4.538300037384033, -4.791299819946289, -5.201499938964844, -4.8592000007629395, -4.072000026702881, -5.3190999031066895, -5.946400165557861, -4.8902997970581055, -5.639400005340576, -6.095600128173828, -4.913000106811523, -4.860499858856201, -4.595399856567383, -4.697800159454346, -4.356900215148926, -4.416399955749512, -3.584199905395508, -3.9151999950408936, -3.5495998859405518, -4.434999942779541, -4.396999835968018, -4.088399887084961, -4.256100177764893, -4.672900199890137, -4.532100200653076, -4.6493000984191895, -4.5644001960754395, -4.7403998374938965, -4.786200046539307, -3.1635000705718994, -4.426000118255615, -3.758500099182129, -2.8457000255584717, -4.619100093841553, -5.036600112915039, -4.967299938201904, -5.200900077819824, -4.855199813842773, -5.03849983215332, -4.7565999031066895, -5.359499931335449, -5.281400203704834, -4.292399883270264, -4.763700008392334, -5.600900173187256, -5.792799949645996, -3.4339001178741455, -4.247799873352051, -5.54580020904541, -4.7891998291015625, -6.024199962615967, -4.475200176239014, -4.755000114440918, -5.189899921417236, -4.653900146484375, -3.96370005607605, -4.866199970245361, -5.2515997886657715, -5.654200077056885, -5.563399791717529, -5.102799892425537, -4.685800075531006, -4.105800151824951, -3.983799934387207, -4.213600158691406, -4.611700057983398, -4.855800151824951, -4.310400009155273, -3.8968000411987305, -3.9505999088287354, -4.138000011444092, -4.087200164794922, -4.203199863433838, -4.231500148773193, -4.456399917602539, -4.573999881744385, -4.701099872589111, -4.771399974822998, -3.511699914932251, -5.195000171661377, -2.971100091934204, -4.877699851989746, -4.776599884033203, -5.061699867248535, -5.227099895477295, -5.093999862670898, -5.11359977722168, -5.3942999839782715, -5.456299781799316, -5.48360013961792, -4.067599773406982, -5.5096001625061035, -5.446899890899658, -5.759699821472168, -4.757299900054932, -5.532299995422363, -4.644199848175049, -5.379199981689453, -5.930200099945068, -5.480800151824951, -6.233699798583984, -6.090700149536133, -5.026899814605713, -5.5584001541137695, -5.463200092315674, -5.2657999992370605, -6.164899826049805, -6.259500026702881, -3.767899990081787, -4.6641998291015625, -5.36929988861084, -4.53000020980835, -4.688199996948242, -4.413899898529053, -4.779900074005127, -3.599299907684326, -3.686199903488159, -4.420400142669678, -4.861700057983398, -4.20389986038208, -4.577400207519531, -4.789100170135498, -4.537300109863281, -4.421299934387207, -4.328999996185303, -4.3566999435424805, -4.549799919128418, -4.760900020599365, -4.875199794769287, -4.744200229644775, -4.934700012207031, -4.925300121307373, -4.924900054931641, -2.7562999725341797, -3.3338000774383545, -4.239799976348877, -4.406199932098389, -4.412199974060059, -3.366499900817871, -5.06790018081665, -5.068299770355225, -4.774799823760986, -4.502099990844727, -4.560500144958496, -5.325099945068359, -3.720599889755249, -5.0690999031066895, -5.048399925231934, -5.148399829864502, -4.625500202178955, -5.176400184631348, -5.133399963378906, -5.469399929046631, -4.7170000076293945, -5.3404998779296875, -4.736000061035156, -5.067200183868408, -5.552499771118164, -5.062600135803223, -5.468599796295166, -4.820799827575684, -4.849999904632568, -5.73330020904541, -5.223299980163574, -5.021299839019775, -4.398099899291992, -4.7332000732421875, -3.9874000549316406, -5.0208001136779785, -4.086699962615967, -4.216300010681152, -4.162600040435791, -4.360400199890137, -4.607100009918213, -4.444799900054932, -4.847099781036377, -4.6975998878479, -4.8628997802734375, -4.947700023651123, -4.931600093841553, -4.940400123596191, -3.915299892425537, -5.015200138092041, -4.575900077819824, -3.08489990234375, -4.854300022125244, -5.02239990234375, -4.769100189208984, -4.905399799346924, -4.796599864959717, -5.149099826812744, -4.117199897766113, -4.915299892425537, -5.433599948883057, -5.45419979095459, -5.828100204467773, -5.78439998626709, -4.697500228881836, -5.119900226593018, -4.474100112915039, -5.788300037384033, -5.161600112915039, -4.6570000648498535, -5.418099880218506, -4.764800071716309, -5.045000076293945, -4.462100028991699, -5.468200206756592, -5.998799800872803, -4.792799949645996, -5.735599994659424, -4.763400077819824, -4.828800201416016, -3.8485000133514404, -4.309000015258789, -3.562700033187866, -5.057199954986572, -4.109799861907959, -4.255000114440918, -3.757200002670288, -4.372600078582764, -4.0055999755859375, -4.486000061035156, -4.3582000732421875, -4.845600128173828, -4.739699840545654, -4.782700061798096, -4.811800003051758, -4.861800193786621, -4.8520002365112305, -4.0289998054504395, -5.246699810028076, -4.682199954986572, -4.917300224304199, -5.2708001136779785, -5.636000156402588, -4.601099967956543, -5.428800106048584, -5.594399929046631, -5.658699989318848, -5.696400165557861, -5.918499946594238, -5.874000072479248, -4.4008002281188965, -5.8343000411987305, -5.637700080871582, -3.7850000858306885, -5.566500186920166, -5.764699935913086, -6.045400142669678, -5.98859977722168, -3.436000108718872, -4.908199787139893, -6.071499824523926, -5.485199928283691, -5.832799911499023, -5.308000087738037, -5.205699920654297, -4.08650016784668, -6.131999969482422, -4.855599880218506, -5.076000213623047, -5.411399841308594, -4.136499881744385, -5.059899806976318, -3.4818999767303467, -4.313899993896484, -4.968999862670898, -4.708799839019775, -3.7945001125335693, -4.960599899291992, -4.390999794006348, -4.837100028991699, -4.4756999015808105, -4.317999839782715, -4.803299903869629, -4.716100215911865, -4.69350004196167, -4.696700096130371, -4.737599849700928, -4.735400199890137, -4.621099948883057, -4.6793999671936035, -4.937399864196777, -4.841899871826172, -4.958499908447266, -3.5302999019622803, -3.983599901199341, -4.061500072479248, -3.8308000564575195, -3.5992000102996826, -3.114799976348877, -3.572200059890747, -2.92330002784729, -2.670799970626831, -4.34250020980835, -2.4342000484466553, -5.166999816894531, -5.561600208282471, -4.816800117492676, -5.015100002288818, -5.526100158691406, -4.502699851989746, -5.480299949645996, -5.584000110626221, -5.360599994659424, -5.559199810028076, -5.239799976348877, -5.460000038146973, -5.645699977874756, -5.0370001792907715, -6.120100021362305, -5.3867998123168945, -4.176000118255615, -6.216700077056885, -5.411900043487549, -5.381999969482422, -4.520999908447266, -3.4969000816345215, -4.692399978637695, -3.615600109100342, -4.417399883270264, -4.932600021362305, -4.519199848175049, -4.458499908447266, -4.975500106811523, -4.204500198364258, -4.975200176239014, -4.431600093841553, -4.599999904632568, -4.6072001457214355, -4.806399822235107, -4.911900043487549, -4.9629998207092285, -3.3991000652313232, -4.886499881744385, -4.370299816131592, -4.927999973297119, -5.344600200653076, -4.950699806213379, -4.662199974060059, -4.883999824523926, -5.491399765014648, -5.344099998474121, -5.243100166320801, -5.760300159454346, -4.913400173187256, -4.60290002822876, -5.194200038909912, -5.630899906158447, -5.531400203704834, -4.1331000328063965, -5.9253997802734375, -6.095799922943115, -3.212100028991699, -5.888700008392334, -4.894800186157227, -5.735499858856201, -4.76039981842041, -5.644800186157227, -5.653200149536133, -5.908400058746338, -3.1322999000549316, -5.966400146484375, -5.335700035095215, -4.834799766540527, -4.08459997177124, -4.876399993896484, -4.305500030517578, -4.794600009918213, -5.233500003814697, -4.7571001052856445, -4.512400150299072, -4.931099891662598, -4.407599925994873, -4.097499847412109, -4.732699871063232, -4.7058000564575195, -4.390600204467773, -4.419000148773193, -4.455399990081787, -4.5843000411987305, -4.869900226593018, -4.921800136566162, -3.554800033569336, -3.827199935913086, -3.71589994430542, -4.706500053405762, -4.654399871826172, -5.007800102233887, -3.858299970626831, -5.738399982452393, -4.153900146484375, -5.451000213623047, -4.085299968719482, -5.100200176239014, -5.071300029754639, -4.8815999031066895, -5.048799991607666, -5.48330020904541, -4.822000026702881, -5.921500205993652, -4.580100059509277, -3.1356000900268555, -5.755499839782715, -3.203200101852417, -6.074100017547607, -5.6894001960754395, -5.39300012588501, -4.531499862670898, -4.918900012969971, -5.270100116729736, -5.228000164031982, -5.416600227355957, -4.569499969482422, -4.11870002746582, -3.723299980163574, -4.167500019073486, -4.822999954223633, -4.953000068664551, -4.554999828338623, -4.338699817657471, -4.924600124359131, -4.300099849700928, -4.3815999031066895, -4.365699768066406, -4.251800060272217, -4.400000095367432, -4.328999996185303, -4.6066999435424805, -4.524499893188477, -4.660399913787842, -4.697700023651123, -4.727399826049805, -4.78849983215332, -4.087600231170654, -4.763700008392334, -3.6452999114990234, -2.7042999267578125, -4.60890007019043, -5.080399990081787, -4.578800201416016, -5.4969000816345215, -5.445400238037109, -4.715000152587891, -4.66480016708374, -5.4004998207092285, -4.938399791717529, -5.6442999839782715, -5.5503997802734375, -5.988500118255615, -5.526899814605713, -6.049300193786621, -5.603899955749512, -6.005199909210205, -5.853600025177002, -4.508600234985352, -6.0954999923706055, -6.2957000732421875, -4.2617998123168945, -5.743000030517578, -5.533299922943115, -5.812099933624268, -6.064199924468994, -6.282599925994873, -5.9552001953125, -4.648099899291992, -3.905900001525879, -4.999000072479248, -4.798900127410889, -4.170199871063232, -3.9711999893188477, -3.947999954223633, -4.619200229644775, -4.490600109100342, -3.7581000328063965, -4.585100173950195, -4.410799980163574, -4.054999828338623, -4.431300163269043, -4.478499889373779, -4.350299835205078, -4.493199825286865, -4.856800079345703, -4.6605000495910645, -4.634500026702881, -4.8769001960754395, -4.440700054168701, -3.461899995803833, -4.897299766540527, -2.7943999767303467, -4.084400177001953, -5.06850004196167, -5.519599914550781, -5.276599884033203, -5.075300216674805, -5.541900157928467, -4.980599880218506, -5.17140007019043, -5.551599979400635, -6.013500213623047, -5.7270002365112305, -5.950300216674805, -5.105299949645996, -5.517300128936768, -5.487100124359131, -5.929299831390381, -5.747399806976318, -5.056000232696533, -5.336900234222412, -5.6641998291015625, -6.093500137329102, -5.623899936676025, -5.513899803161621, -5.893899917602539, -6.323500156402588, -5.307799816131592, -5.3796000480651855, -3.0975000858306885, -4.020400047302246, -4.858799934387207, -3.5901999473571777, -4.325799942016602, -4.8597002029418945, -3.515500068664551, -4.756400108337402, -4.668399810791016, -4.856599807739258, -4.878399848937988, -4.791399955749512, -4.691299915313721, -4.691400051116943, -4.598400115966797, -4.593299865722656, -4.681300163269043, -4.906400203704834, -4.811999797821045, -4.645699977874756, -4.743500232696533, -4.811299800872803, -4.862100124359131, -5.047699928283691, -2.402100086212158, -4.504899978637695, -4.127399921417236, -4.026800155639648, -4.839900016784668, -4.030399799346924, -4.516900062561035, -5.142000198364258, -5.163099765777588, -4.542600154876709, -4.253699779510498, -5.424300193786621, -5.0395002365112305, -5.084499835968018, -4.463099956512451, -4.926199913024902, -5.042600154876709, -3.734999895095825, -4.9415998458862305, -5.067399978637695, -4.970099925994873, -5.451099872589111, -5.908599853515625, -5.150599956512451, -5.4085001945495605, -4.228600025177002, -5.710000038146973, -5.436800003051758, -4.316999912261963, -5.0671000480651855, -4.34089994430542, -4.40310001373291, -4.554299831390381, -4.85890007019043, -4.108500003814697, -4.393400192260742, -4.511300086975098, -3.775399923324585, -3.8222999572753906, -4.009699821472168, -4.129199981689453, -4.210299968719482, -4.64300012588501, -4.681300163269043, -4.676199913024902, -4.63730001449585, -4.73390007019043, -4.851099967956543, -4.390399932861328, -3.573499917984009, -4.630099773406982, -5.140399932861328, -4.918799877166748, -5.440400123596191, -5.182400226593018, -5.466700077056885, -5.775899887084961, -5.8755998611450195, -5.442299842834473, -5.573800086975098, -5.742300033569336, -5.716800212860107, -4.711900234222412, -5.18720006942749, -5.711900234222412, -5.731200218200684, -5.969600200653076, -4.606599807739258, -6.044000148773193, -5.407599925994873, -6.112199783325195, -5.52370023727417, -5.761000156402588, -6.272200107574463, -5.868199825286865, -5.162499904632568, -5.209499835968018, -5.911099910736084, -4.672800064086914, -3.3515000343322754, -5.411099910736084, -3.7753000259399414, -4.581999778747559, -5.032299995422363, -4.699900150299072, -4.4004998207092285, -3.9339001178741455, -4.611100196838379, -4.2982001304626465, -4.0167999267578125, -4.810100078582764, -4.78439998626709, -4.640399932861328, -4.522900104522705, -4.6168999671936035, -4.348800182342529, -4.397299766540527, -4.465199947357178, -4.817599773406982, -4.554999828338623, -4.9004998207092285, -4.862199783325195, -4.953000068664551, -4.901000022888184, -2.265399932861328, -3.9512999057769775, -4.288700103759766, -3.411600112915039, -3.6031999588012695, -4.6203999519348145, -5.084799766540527, -5.126100063323975, -5.394000053405762, -5.775000095367432, -5.253399848937988, -4.945099830627441, -4.617000102996826, -5.233500003814697, -5.20989990234375, -5.216100215911865, -5.3993000984191895, -5.390999794006348, -5.033299922943115, -5.168499946594238, -4.940000057220459, -5.8815999031066895, -4.494999885559082, -5.31279993057251, -4.87939977645874, -4.799499988555908, -6.078499794006348, -5.258699893951416, -2.9026999473571777, -6.242499828338623, -4.580699920654297, -4.862199783325195, -4.772500038146973, -4.143799781799316, -4.310999870300293, -3.5864999294281006, -4.468200206756592, -4.362299919128418, -4.157299995422363, -4.2677998542785645, -4.705900192260742, -4.890900135040283, -4.700900077819824, -4.531000137329102, -4.720200061798096, -4.7444000244140625, -4.888400077819824, -4.919000148773193, -3.439500093460083, -5.256999969482422, -4.772299766540527, -4.540999889373779, -4.881800174713135, -4.764900207519531, -5.365900039672852, -3.5327000617980957, -5.124899864196777, -4.568299770355225, -4.725800037384033, -4.575500011444092, -4.714700222015381, -5.313300132751465, -5.401100158691406, -4.920000076293945, -5.64870023727417, -3.9112000465393066, -5.194699764251709, -5.470600128173828, -5.540999889373779, -5.175300121307373, -5.667900085449219, -5.696599960327148, -5.573200225830078, -4.761099815368652, -5.74429988861084, -5.866600036621094, -5.071700096130371, -5.856200218200684, -4.62470006942749, -3.676500082015991, -3.8171000480651855, -4.996099948883057, -3.399600028991699, -4.184199810028076, -3.861999988555908, -4.500899791717529, -3.8336000442504883, -3.8775999546051025, -4.203199863433838, -4.440899848937988, -4.048900127410889, -4.239500045776367, -4.251399993896484, -4.637899875640869, -4.502600193023682, -4.837100028991699, -4.838600158691406, -4.82919979095459, -4.037399768829346, -4.17609977722168, -3.7156999111175537, -4.9644999504089355, -4.241199970245361, -4.822199821472168, -5.188199996948242, -5.493899822235107, -5.554200172424316, -4.656899929046631, -5.376699924468994, -5.37529993057251, -5.8790998458862305, -5.367400169372559, -5.146999835968018, -4.393499851226807, -6.098199844360352, -5.743800163269043, -4.744999885559082, -5.257199764251709, -5.32450008392334, -5.141300201416016, -4.846399784088135, -5.757599830627441, -5.409800052642822, -6.114299774169922, -5.170199871063232, -5.435299873352051, -5.992400169372559, -5.838099956512451, -5.457499980926514, -5.3871002197265625, -5.127900123596191, -3.9872000217437744, -3.782399892807007, -4.019800186157227, -3.6591999530792236, -4.448800086975098, -4.380799770355225, -4.682600021362305, -4.333600044250488, -4.559899806976318, -4.295100212097168, -4.542900085449219, -4.566500186920166, -4.327899932861328, -4.385000228881836, -4.902299880981445, -4.882400035858154, -4.798500061035156, -4.60830020904541, -4.794899940490723, -4.776700019836426, -4.94890022277832, -3.4117000102996826, -4.437099933624268, -3.414900064468384, -2.882699966430664, -3.736799955368042, -4.761899948120117, -4.982500076293945, -4.910900115966797, -3.195199966430664, -5.076600074768066, -5.260799884796143, -4.447700023651123, -5.02370023727417, -4.831699848175049, -5.811500072479248, -5.268199920654297, -5.5, -5.984399795532227, -5.589099884033203, -5.108500003814697, -5.848800182342529, -6.069300174713135, -6.0395002365112305, -5.504799842834473, -5.537300109863281, -5.702899932861328, -5.836999893188477, -3.9026999473571777, -4.893499851226807, -5.731900215148926, -4.930500030517578, -5.347700119018555, -4.140900135040283, -3.49180006980896, -3.7513999938964844, -4.135000228881836, -4.971399784088135, -3.666300058364868, -4.791500091552734, -3.8947999477386475, -4.236700057983398, -4.667300224304199, -4.257999897003174, -4.407599925994873, -4.519400119781494, -4.942500114440918, -4.785200119018555, -4.891200065612793, -4.871099948883057, -4.895599842071533, -4.811100006103516, -4.3979997634887695, -4.8850998878479, -4.4847002029418945, -4.251200199127197, -3.7172999382019043, -5.430699825286865, -5.291800022125244, -5.913099765777588, -4.362400054931641, -5.594200134277344, -3.4730000495910645, -4.812300205230713, -5.7382001876831055, -5.693399906158447, -4.776400089263916, -3.6308999061584473, -5.3317999839782715, -5.261499881744385, -5.835999965667725, -6.034800052642822, -5.406599998474121, -6.144100189208984, -5.493800163269043, -6.3480000495910645, -6.19320011138916, -4.3592000007629395, -4.796199798583984, -6.13700008392334, -5.4882001876831055, -3.9100000858306885, -4.039599895477295, -4.19189977645874, -5.299699783325195, -3.4223999977111816, -3.628499984741211, -4.0395002365112305, -4.301199913024902, -4.446599960327148, -5.0278000831604, -4.558800220489502, -5.058599948883057, -4.276100158691406, -4.402500152587891, -4.503600120544434, -4.696899890899658, -4.866600036621094, -4.881100177764893, -4.823400020599365, -4.9334001541137695, -3.2632999420166016, -3.5202999114990234, -4.303599834442139, -4.553400039672852, -4.644899845123291, -4.392600059509277, -4.379300117492676, -3.0536999702453613, -4.361999988555908, -5.0167999267578125, -5.101799964904785, -4.756400108337402, -4.893099784851074, -3.904900074005127, -4.978799819946289, -4.860499858856201, -5.211400032043457, -3.0713999271392822, -5.291399955749512, -5.533199787139893, -5.288099765777588, -2.516200065612793, -4.615300178527832, -4.4471001625061035, -4.600900173187256, -4.906799793243408, -4.825900077819824, -4.942999839782715, -4.124000072479248, -4.794600009918213, -3.2451999187469482, -3.767699956893921, -4.165800094604492, -4.904600143432617, -3.3729000091552734, -4.708700180053711, -4.7093000411987305, -4.451900005340576, -4.623300075531006, -4.816199779510498, -4.666299819946289, -4.759900093078613, -4.743599891662598, -4.813600063323975, -4.790900230407715, -3.941699981689453, -3.7211999893188477, -4.031199932098389, -4.906400203704834, -4.844900131225586, -4.649799823760986, -4.936999797821045, -4.875899791717529, -4.995800018310547, -5.503900051116943, -4.423500061035156, -5.518499851226807, -5.439300060272217, -5.485899925231934, -5.848899841308594, -5.60129976272583, -5.90500020980835, -5.703400135040283, -5.905200004577637, -5.503399848937988, -4.835000038146973, -4.977099895477295, -5.520899772644043, -5.503399848937988, -5.376699924468994, -5.879199981689453, -4.643099784851074, -5.794600009918213, -4.906799793243408, -5.920199871063232, -4.041900157928467, -4.827000141143799, -3.9261999130249023, -4.390699863433838, -4.312300205230713, -4.111800193786621, -3.7657999992370605, -4.429299831390381, -4.272200107574463, -3.951200008392334, -4.604899883270264, -4.48769998550415, -4.14109992980957, -4.511199951171875, -4.548900127410889, -4.2870001792907715, -4.286300182342529, -4.352099895477295, -4.607800006866455, -4.683000087738037, -4.629700183868408, -4.717100143432617, -4.746099948883057, -4.090799808502197, -4.803800106048584, -3.723299980163574, -5.4257001876831055, -5.438899993896484, -4.835100173950195, -5.600900173187256, -5.686100006103516, -4.914000034332275, -6.046000003814697, -4.254499912261963, -4.8354997634887695, -5.626399993896484, -6.027699947357178, -5.298600196838379, -5.564000129699707, -5.649799823760986, -5.8246002197265625, -6.2779998779296875, -5.304299831390381, -5.480299949645996, -6.40310001373291, -4.154099941253662, -6.156000137329102, -5.9054999351501465, -5.820400238037109, -5.934700012207031, -5.610899925231934, -6.148099899291992, -5.737299919128418, -3.667099952697754, -4.755000114440918, -4.929699897766113, -4.274400234222412, -4.822700023651123, -4.507800102233887, -5.099999904632568, -3.9110000133514404, -4.728400230407715, -4.5447998046875, -3.9244000911712646, -4.893899917602539, -4.562300205230713, -4.6006999015808105, -4.2820000648498535, -4.246300220489502, -4.761600017547607, -4.314799785614014, -4.346099853515625, -4.626200199127197, -4.814499855041504, -4.730800151824951, -4.658599853515625, -4.688499927520752, -4.831600189208984, -3.6108999252319336, -4.082300186157227, -4.189799785614014, -3.521399974822998, -2.8701000213623047, -3.6451001167297363, -5.001699924468994, -4.555799961090088, -4.8572001457214355, -4.20550012588501, -5.010799884796143, -5.321199893951416, -4.575099945068359, -5.037300109863281, -5.029399871826172, -5.768700122833252, -5.279399871826172, -4.206099987030029, -5.773099899291992, -5.053800106048584, -4.5594000816345215, -5.4552001953125, -5.306300163269043, -5.273600101470947, -5.85830020904541, -5.29010009765625, -6.0482001304626465, -5.944699764251709, -4.962100028991699, -5.630300045013428, -4.5644001960754395, -4.746099948883057, -3.860300064086914, -4.744999885559082, -3.828000068664551, -3.7546000480651855, -4.006400108337402, -4.430600166320801, -4.310400009155273, -4.257999897003174, -4.599899768829346, -4.527900218963623, -4.765900135040283, -4.685200214385986, -4.786900043487549, -4.795000076293945, -4.732399940490723, -4.940199851989746, -3.7569000720977783, -3.36080002784729, -3.9147000312805176, -2.228600025177002, -3.6854000091552734, -4.459099769592285, -4.413400173187256, -5.360599994659424, -3.875999927520752, -5.342599868774414, -5.371699810028076, -5.267199993133545, -5.423699855804443, -5.19789981842041, -5.104499816894531, -5.356900215148926, -5.165200233459473, -4.389400005340576, -4.5883002281188965, -6.100800037384033, -5.698999881744385, -4.646999835968018, -5.0005998611450195, -5.432600021362305, -5.389500141143799, -5.077700138092041, -5.263000011444092, -5.808700084686279, -4.827400207519531, -4.374199867248535, -3.7348999977111816, -4.7718000411987305, -4.719099998474121, -3.886399984359741, -4.141200065612793, -4.072999954223633, -4.499300003051758, -4.827899932861328, -4.244800090789795, -4.770699977874756, -4.487100124359131, -4.8856000900268555, -4.883999824523926, -4.859000205993652, -4.827199935913086, -4.999599933624268, -4.951600074768066, -4.919300079345703, -3.1440000534057617, -4.294300079345703, -3.396699905395508, -3.7074999809265137, -3.6647000312805176, -4.503200054168701, -3.1331000328063965, -3.2128000259399414, -5.3018999099731445, -3.4876999855041504, -5.176400184631348, -4.10099983215332, -4.979700088500977, -5.195899963378906, -4.280099868774414, -4.461699962615967, -5.678100109100342, -3.1226000785827637, -5.8165998458862305, -5.154799938201904, -5.844600200653076, -4.471700191497803, -4.6992998123168945, -5.219299793243408, -6.082799911499023, -5.14109992980957, -4.497600078582764, -4.461100101470947, -5.428100109100342, -5.013299942016602, -5.337100028991699, -5.224400043487549, -4.311699867248535, -4.3871002197265625, -3.6986000537872314, -4.000899791717529, -3.977799892425537, -4.043300151824951, -4.092599868774414, -4.802700042724609, -4.299799919128418, -4.482999801635742, -4.697299957275391, -4.6996002197265625, -4.565199851989746, -2.7258999347686768, -3.964200019836426, -3.2581000328063965, -2.355600118637085, -4.690700054168701, -3.871999979019165, -4.6442999839782715, -5.1331000328063965, -4.144000053405762, -5.364299774169922, -5.107699871063232, -5.067599773406982, -5.374800205230713, -5.65310001373291, -5.6508002281188965, -5.710400104522705, -5.851799964904785, -5.1321001052856445, -5.86929988861084, -4.660900115966797, -5.610199928283691, -5.567999839782715, -3.8910999298095703, -4.795000076293945, -5.103799819946289, -5.575300216674805, -5.403800010681152, -5.345799922943115, -5.613100051879883, -6.314000129699707, -3.9096999168395996, -3.8963000774383545, -3.95740008354187, -4.812900066375732, -4.84119987487793, -3.609600067138672, -4.345399856567383, -4.404200077056885, -4.656000137329102, -4.11959981918335, -4.879700183868408, -4.706900119781494, -4.65500020980835, -4.638199806213379, -4.749800205230713, -4.574900150299072, -4.6346001625061035, -4.77269983291626, -4.820099830627441, -3.874000072479248, -4.223100185394287, -4.758800029754639, -5.087800025939941, -2.771899938583374, -5.265600204467773, -5.67110013961792, -3.121299982070923, -5.257999897003174, -5.484000205993652, -5.352099895477295, -5.434700012207031, -5.198299884796143, -5.279799938201904, -5.885300159454346, -5.493299961090088, -5.153600215911865, -4.9517998695373535, -5.34499979019165, -4.9217000007629395, -5.490600109100342, -3.456399917602539, -5.20359992980957, -5.302599906921387, -5.4253997802734375, -5.249000072479248, -6.027400016784668, -4.686200141906738, -5.0706000328063965, -5.13129997253418, -3.885499954223633, -4.742599964141846, -4.127200126647949, -3.9049999713897705, -4.795300006866455, -4.639800071716309, -4.59660005569458, -4.405799865722656, -4.3856000900268555, -4.074699878692627, -4.686299800872803, -4.525700092315674, -4.480400085449219, -4.32859992980957, -4.38040018081665, -4.828199863433838, -4.660600185394287, -5.024700164794922, -4.758500099182129, -4.910900115966797, -4.822500228881836, -4.251299858093262, -2.7614998817443848, -4.796599864959717, -4.453800201416016, -5.2657999992370605, -4.561500072479248, -4.5792999267578125, -4.577600002288818, -5.109600067138672, -3.089099884033203, -5.144000053405762, -4.350399971008301, -4.866600036621094, -4.995299816131592, -5.090799808502197, -4.807700157165527, -5.590700149536133, -5.1645002365112305, -4.519499778747559, -5.385499954223633, -5.251299858093262, -5.572299957275391, -4.420899868011475, -5.580900192260742, -5.585700035095215, -5.901199817657471, -5.244800090789795, -5.598899841308594, -5.5543999671936035, -5.414000034332275, -5.0945000648498535, -4.59089994430542, -4.711900234222412, -3.953900098800659, -3.3754000663757324, -5.002699851989746, -3.7964999675750732, -4.523900032043457, -4.189700126647949, -3.9863998889923096, -4.457099914550781, -4.914100170135498, -4.390600204467773, -4.290299892425537, -4.549300193786621, -4.239099979400635, -4.84499979019165, -4.553999900817871, -4.6143999099731445, -4.609099864959717, -4.51609992980957, -4.690899848937988, -4.712600231170654, -4.749000072479248, -3.145900011062622, -3.429800033569336, -3.231100082397461, -4.856100082397461, -4.701600074768066, -3.730299949645996, -4.795499801635742, -4.982600212097168, -4.264999866485596, -4.27370023727417, -5.456500053405762, -4.299699783325195, -5.425300121307373, -5.52810001373291, -2.768199920654297, -5.018499851226807, -5.672800064086914, -5.3317999839782715, -3.6082000732421875, -5.084000110626221, -5.8572001457214355, -4.741600036621094, -4.492499828338623, -3.6735000610351562, -5.152500152587891, -5.959199905395508, -6.3343000411987305, -5.512599945068359, -4.31689977645874, -6.104400157928467, -5.7210001945495605, -3.6691999435424805, -4.008600234985352, -4.221099853515625, -4.716400146484375, -3.816699981689453, -4.551300048828125, -4.520500183105469, -4.120200157165527, -4.2891998291015625, -4.4197998046875, -4.752600193023682, -4.612800121307373, -4.8043999671936035, -4.775300025939941, -4.804699897766113, -4.89709997177124, -4.344299793243408, -4.306099891662598, -4.701900005340576, -5.145899772644043, -2.8759000301361084, -4.891900062561035, -5.234099864959717, -5.494900226593018, -2.6851999759674072, -4.217199802398682, -4.564000129699707, -5.478700160980225, -5.7891998291015625, -5.326399803161621, -5.429500102996826, -5.556700229644775, -4.974699974060059, -2.6284000873565674, -4.5671000480651855, -5.756700038909912, -4.315199851989746, -5.7895002365112305, -3.7191998958587646, -5.959000110626221, -5.8383002281188965, -5.203000068664551, -6.15500020980835, -5.94320011138916, -4.02239990234375, -5.169600009918213, -5.5157999992370605, -4.776299953460693, -4.689000129699707, -4.735099792480469, -4.763500213623047, -5.347099781036377, -4.96619987487793, -4.2108001708984375, -3.8968000411987305, -5.087100028991699, -4.098499774932861, -4.092299938201904, -4.623899936676025, -4.8668999671936035, -4.848999977111816, -4.741600036621094, -4.860499858856201, -4.868899822235107, -4.804800033569336, -4.803699970245361, -4.8592000007629395, -4.063799858093262, -3.6291000843048096, -4.978700160980225, -4.963099956512451, -4.117800235748291, -4.550899982452393, -2.083400011062622, -3.644700050354004, -4.477799892425537, -5.410799980163574, -5.45419979095459, -5.427999973297119, -6.070099830627441, -5.7820000648498535, -5.003900051116943, -5.855299949645996, -5.640500068664551, -5.8790998458862305, -5.257599830627441, -4.540999889373779, -5.316100120544434, -5.922100067138672, -5.537899971008301, -5.691800117492676, -5.986800193786621, -6.161499977111816, -4.121099948883057, -5.281700134277344, -6.035900115966797, -6.0970001220703125, -4.934500217437744, -4.9583001136779785, -4.829100131988525, -4.923799991607666, -4.51609992980957, -3.8373000621795654, -4.612100124359131, -4.548099994659424, -4.592199802398682, -4.5868000984191895, -4.348400115966797, -4.424200057983398, -4.572800159454346, -4.569399833679199, -4.613399982452393, -4.706699848175049, -4.72760009765625, -4.962900161743164, -4.866300106048584, -4.7972002029418945, -4.81820011138916, -4.9197001457214355], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.9413, 2.6967, 2.6914, 2.6825, 2.4802, 2.3669, 2.3587, 2.2749, 2.2394, 2.1797, 2.1644, 2.1615, 2.1603, 2.1514, 2.1244, 2.0789, 2.0286, 1.9971, 1.9947, 1.9807, 1.9633, 1.9461, 1.9454, 1.9374, 1.9274, 1.9069, 1.8651, 1.8572, 1.8347, 1.8174, 1.5813, 1.5278, 1.5249, 1.6473, 0.9553, 0.8556, 1.3869, 0.8614, 1.6084, 0.4509, 0.8428, 0.2108, -0.0025, -0.0667, 0.8576, 0.8062, -0.0051, -0.2728, 2.9255, 2.5646, 2.4806, 2.4053, 2.3609, 2.3242, 2.2978, 2.2529, 2.2215, 2.2006, 2.1427, 2.104, 2.1024, 2.0956, 2.0883, 2.088, 2.0718, 2.0715, 2.0685, 2.0138, 2.0087, 1.9879, 1.9857, 1.964, 1.9334, 1.8119, 1.7959, 1.7756, 1.7754, 1.7687, 1.7583, 1.6968, 1.7154, 1.6694, 1.5819, 1.5894, 1.0292, 1.0775, 0.5528, 0.9989, 1.0487, 0.4371, 1.4311, 0.2303, 0.8222, 0.8852, 1.2048, 0.9115, 0.4934, 0.0762, -0.1525, 0.2735, 0.0607, -0.4755, 2.7193, 2.6439, 2.6235, 2.6053, 2.4662, 2.4321, 2.4056, 2.3898, 2.3624, 2.2157, 2.1965, 2.13, 2.1297, 2.1158, 2.0944, 2.0745, 2.0556, 2.0479, 2.0283, 2.0253, 2.004, 1.9344, 1.9197, 1.8885, 1.8635, 1.825, 1.8197, 1.7945, 1.7642, 1.7086, 1.6829, 1.6641, 1.4382, 1.2749, 1.5644, 1.2559, 0.9182, 0.84, 0.5621, 0.9081, 0.2618, 1.5248, 0.8236, 0.9877, 0.294, 0.5142, 0.016, 0.6281, 0.0852, 0.8011, 0.7087, 2.2799, 2.1853, 1.9596, 1.7951, 1.7836, 1.7756, 1.7707, 1.7316, 1.7314, 1.6891, 1.6795, 1.6685, 1.6639, 1.6416, 1.6329, 1.6223, 1.6051, 1.5596, 1.5399, 1.5351, 1.4714, 1.4662, 1.4434, 1.434, 1.4333, 1.4267, 1.4243, 1.3646, 1.339, 1.3338, 1.2898, 1.327, 1.2692, 1.2251, 1.1349, 0.9574, 0.8256, 0.8534, 1.1248, 0.8415, 1.2093, 0.5387, 0.7477, 0.8139, 0.3498, 0.4213, 0.8225, 0.8251, 0.8165, 0.0835, 0.106, -0.155, 0.2361, -0.3366, 0.029, 0.6752, -0.3391, 3.4899, 2.4265, 2.4149, 2.3589, 2.3372, 2.3252, 2.274, 2.255, 2.2413, 2.1883, 2.1557, 2.1553, 2.1066, 2.092, 2.083, 2.0669, 2.0642, 2.0639, 2.0513, 2.0293, 2.0093, 2.0014, 1.9922, 1.9622, 1.9239, 1.8934, 1.8337, 1.8318, 1.8192, 1.8188, 1.7489, 1.574, 1.4011, 1.2392, 1.0346, 0.6143, 0.4491, 0.4361, 0.3965, 1.1438, 0.3009, 0.2736, 0.5334, 0.8222, 0.2519, -0.0092, 0.4688, -0.083, 0.2993, 1.0981, -0.007, 0.5208, 0.7768, 0.6384, 3.0943, 2.5583, 2.2546, 2.1587, 2.1166, 2.0852, 2.0828, 2.0816, 2.0714, 2.0303, 2.0167, 1.9964, 1.9392, 1.9285, 1.9202, 1.8963, 1.8705, 1.8235, 1.7746, 1.7426, 1.7246, 1.6948, 1.6927, 1.6688, 1.6585, 1.6341, 1.6338, 1.617, 1.5536, 1.5493, 1.538, 1.5141, 1.4516, 1.2846, 1.3879, 1.2221, 0.8554, 1.0345, 0.743, 0.9721, 1.2312, 1.115, 0.4995, 0.8796, 0.7942, 0.8223, 0.2151, 0.1495, 0.9477, 0.1965, -0.0004, 0.1125, 0.5214, -0.0478, 0.4876, -0.0677, -0.3783, -0.0472, 2.4654, 2.3261, 2.3138, 2.2925, 2.2002, 2.1879, 2.09, 2.0606, 2.059, 2.0563, 2.0421, 1.9232, 1.8961, 1.8846, 1.8759, 1.8234, 1.8103, 1.7897, 1.7738, 1.7409, 1.728, 1.7104, 1.6904, 1.6745, 1.6637, 1.6305, 1.6082, 1.5841, 1.5797, 1.5684, 1.5519, 1.4639, 1.3956, 1.3547, 1.4068, 0.7056, 1.0157, 1.2042, 0.9213, 0.8067, 0.9087, 0.5882, 0.3558, 0.7021, 1.0095, 0.9227, 0.1182, 0.499, 0.4766, 0.7684, 0.6173, 0.0498, -0.1164, 0.7474, 0.1497, -0.0072, -0.368, 0.4878, 3.3381, 3.2833, 3.1234, 2.9372, 2.5031, 2.4653, 2.465, 2.4466, 2.437, 2.4051, 2.2806, 2.2372, 2.216, 2.0232, 2.0151, 1.9801, 1.929, 1.8697, 1.8627, 1.8294, 1.7919, 1.7612, 1.7505, 1.6801, 1.6237, 1.6159, 1.6047, 1.5955, 1.5951, 1.5756, 1.5723, 1.2847, 1.0961, 1.3409, 1.3902, 0.929, 0.7741, 0.9309, 0.9275, 0.4311, 0.5751, 1.0848, 0.7521, 0.4093, 0.3356, 0.1568, 0.0899, -0.1646, 1.2508, 1.0246, 0.5675, -0.2658, 0.1083, 2.9638, 2.6193, 2.2512, 2.2364, 2.1915, 2.0631, 1.9016, 1.8915, 1.8729, 1.82, 1.8172, 1.8171, 1.7857, 1.7604, 1.7411, 1.7181, 1.7165, 1.7162, 1.6757, 1.6415, 1.5915, 1.5573, 1.5555, 1.54, 1.5246, 1.5115, 1.5043, 1.4985, 1.4635, 1.4498, 1.3652, 1.3117, 1.3189, 1.0417, 0.9696, 0.9801, 0.7584, 0.7806, 0.4753, 0.8498, 0.427, 0.3191, 0.6525, 0.3109, 0.5694, 0.1272, 0.7689, 0.8009, 0.846, 0.4863, 0.7725, 0.5958, -0.0749, -0.0282, -0.4555, -0.4915, 2.8446, 2.6013, 2.4541, 2.4324, 2.3207, 2.2368, 2.2227, 2.161, 2.1376, 2.0455, 2.0336, 2.0145, 1.9254, 1.9218, 1.9124, 1.8952, 1.8922, 1.8719, 1.8714, 1.8592, 1.8456, 1.8406, 1.8206, 1.8093, 1.7111, 1.7045, 1.6783, 1.6323, 1.6119, 1.5952, 1.5928, 1.4209, 1.4316, 1.2096, 1.0136, 1.2347, 1.2602, 1.1011, 1.204, 1.0075, 0.7052, 0.7234, -0.1452, 0.0376, 0.8537, -0.4609, 3.3186, 3.3155, 3.3141, 3.2396, 3.1342, 3.1015, 3.0794, 2.7514, 2.595, 2.5706, 2.4791, 2.4043, 2.3288, 2.3151, 2.2953, 2.2601, 2.2566, 2.2472, 2.217, 2.2109, 2.1966, 2.1633, 2.135, 1.9919, 1.9501, 1.9389, 1.9269, 1.9207, 1.8858, 1.8072, 1.801, 1.8025, 1.6884, 1.6129, 1.1278, 1.2328, 0.9248, 1.3823, 1.3275, 0.4771, 0.2759, 1.03, 0.2814, 0.7773, -0.1101, 0.4146, 0.4405, 3.9427, 3.3788, 3.3324, 3.2042, 2.6373, 2.6188, 2.4485, 2.3311, 2.2921, 2.2871, 2.257, 2.248, 2.2218, 2.2165, 2.2095, 2.1621, 2.0837, 2.073, 2.0585, 2.0393, 2.0108, 1.9939, 1.9786, 1.9545, 1.9454, 1.8343, 1.7992, 1.7784, 1.7537, 1.7439, 1.7434, 1.7359, 1.6729, 1.5379, 1.2194, 1.2896, 1.443, 1.587, 1.1557, 0.5575, 0.5815, 0.6755, 0.5857, 0.2355, 0.1663, 0.7703, 0.7983, 0.8059, 0.8644, 3.0, 2.5681, 2.4925, 2.4014, 2.3706, 2.3411, 2.176, 2.167, 2.1591, 2.1439, 2.1431, 2.1029, 2.0899, 2.0567, 2.0464, 1.9936, 1.9701, 1.8597, 1.8391, 1.8267, 1.7778, 1.7756, 1.7749, 1.7666, 1.7624, 1.7605, 1.758, 1.7224, 1.7186, 1.712, 1.694, 1.683, 1.6953, 1.5906, 1.537, 1.448, 1.4769, 0.8497, 0.8794, 1.2233, 1.3716, 0.4691, 0.8998, 1.0209, 0.5946, 0.3369, 0.0688, 0.082, 0.0595, 0.4658, 0.7089, -0.409, 0.7898, 0.4664, 0.2782, 3.9114, 3.8677, 3.136, 3.106, 3.0547, 3.0135, 2.975, 2.8702, 2.8171, 2.6926, 2.6622, 2.6464, 2.5911, 2.4629, 2.4336, 2.4309, 2.4285, 2.3786, 2.3599, 2.3345, 2.2615, 2.2258, 2.2239, 2.2108, 2.203, 2.1775, 2.1389, 2.1101, 2.0976, 2.0742, 2.0644, 2.0529, 1.6957, 1.7852, 1.091, 1.8731, 0.5227, 0.4567, 0.3118, 0.3516, 0.9754, 0.3687, 1.5243, 0.9461, 1.1918, 1.0653, -0.3995, -0.4861, 3.3565, 2.8952, 2.8008, 2.618, 2.6113, 2.5327, 2.4873, 2.4866, 2.4245, 2.347, 2.2965, 2.2794, 2.2743, 2.2499, 2.1033, 2.0991, 2.0919, 2.0464, 2.0376, 1.9984, 1.9916, 1.977, 1.959, 1.9557, 1.9442, 1.8972, 1.8756, 1.8347, 1.8136, 1.7964, 1.7395, 1.7146, 1.5473, 1.5529, 1.0467, 1.6668, 1.0382, 1.0817, 0.578, 0.9715, 0.4688, 0.7317, 0.2073, 1.3998, 1.0405, 0.9438, 0.4149, 1.0317, -0.0385, 3.0, 2.4062, 2.3137, 2.25, 2.2386, 2.2214, 2.1891, 2.0896, 1.9807, 1.9487, 1.9318, 1.8921, 1.8794, 1.866, 1.8589, 1.828, 1.7958, 1.7695, 1.7692, 1.7664, 1.7476, 1.7444, 1.74, 1.7389, 1.7321, 1.7303, 1.7104, 1.6971, 1.6937, 1.6833, 1.6326, 1.6238, 1.6484, 1.3789, 1.4877, 0.9724, 1.1633, 1.4168, 1.2251, 0.6034, 1.2961, 0.8266, 1.058, 0.6698, 0.394, 0.9307, 0.7807, 0.5243, 0.53, 0.5992, 0.5705, -0.1721, -0.3442, 0.7386, -0.3675, 0.4138, 3.5752, 3.476, 3.4534, 3.3031, 3.2728, 3.1052, 2.9098, 2.6575, 2.5469, 2.3849, 2.3239, 2.308, 2.2108, 2.1985, 2.1528, 2.0368, 1.9402, 1.9162, 1.883, 1.8256, 1.7664, 1.7201, 1.7064, 1.6877, 1.6628, 1.6305, 1.6268, 1.6042, 1.5821, 1.4909, 1.4843, 1.289, 0.901, 1.2731, 0.9164, 1.0858, 1.3343, 0.947, 0.7446, 1.2375, 0.1307, 1.203, 0.0071, 0.3394, 0.2063, 0.6202, 0.0828, 0.544, 2.8463, 2.8059, 2.6837, 2.4504, 2.45, 2.4364, 2.3114, 2.2122, 2.2017, 2.1652, 2.1364, 2.1204, 2.0756, 2.0417, 2.0214, 2.0174, 1.9809, 1.9236, 1.8851, 1.8843, 1.8663, 1.8396, 1.7392, 1.7144, 1.6934, 1.6821, 1.6694, 1.6236, 1.6124, 1.6117, 1.5681, 1.4769, 1.2877, 1.4243, 1.0974, 1.286, 1.4748, 1.1768, 0.9945, 1.2588, 0.8099, 0.3412, 1.0325, 0.9702, 0.2824, 0.0301, 0.0189, 0.3551, -0.1579, 0.2101, 3.4114, 3.3254, 3.2312, 3.0262, 2.7082, 2.4998, 2.4857, 2.2832, 2.2261, 2.2159, 2.1815, 2.1813, 2.1346, 2.0917, 2.0887, 2.0607, 2.0547, 2.0543, 2.0457, 2.0123, 1.967, 1.9518, 1.945, 1.942, 1.9244, 1.9153, 1.9007, 1.7812, 1.7721, 1.7633, 1.7161, 1.6442, 1.5825, 1.5085, 1.6604, 1.6534, 1.4276, 1.1582, 1.549, 0.8454, 0.836, 0.629, 0.3137, 0.312, 0.1097, 0.7374, 0.289, 0.7211, 0.7011, 0.4757, 0.746, 3.1248, 2.6623, 2.6485, 2.6324, 2.6084, 2.6023, 2.481, 2.3076, 2.29, 2.2449, 2.1863, 2.1377, 2.0506, 1.987, 1.9505, 1.8956, 1.8889, 1.8423, 1.8348, 1.784, 1.7748, 1.7685, 1.7007, 1.6757, 1.6721, 1.6543, 1.6532, 1.6401, 1.6071, 1.5825, 1.5801, 1.5649, 1.4756, 1.5099, 1.4537, 1.2959, 1.1768, 1.0467, 1.2744, 1.1521, 0.5771, 0.8951, 0.7348, 0.3429, 0.7237, 0.7482, 0.3944, -0.0545, 0.779, 0.153, -0.1025, -0.1154, 3.0291, 2.8853, 2.6455, 2.386, 2.3513, 2.2912, 2.2199, 2.1005, 2.0833, 2.0521, 2.0144, 1.9838, 1.9517, 1.9179, 1.8806, 1.8779, 1.8712, 1.8265, 1.7851, 1.7744, 1.7648, 1.7585, 1.7464, 1.7317, 1.7082, 1.702, 1.7017, 1.6724, 1.641, 1.6346, 1.6214, 1.4345, 1.4865, 1.5645, 1.3491, 1.4042, 1.5269, 1.05, 1.4011, 1.1118, 1.0053, 1.0104, 0.7957, 0.6455, 0.5262, 0.1463, 0.0161, 0.0803, 0.5905, 0.3336, -0.2479, -0.0315, -0.3726, -0.1891, -0.5986, 3.9838, 3.1265, 3.0952, 2.9624, 2.8158, 2.6614, 2.6184, 2.6086, 2.5892, 2.5636, 2.4675, 2.4649, 2.4303, 2.4069, 2.3665, 2.3267, 2.3147, 2.2781, 2.2321, 2.1889, 2.1792, 2.1171, 2.1096, 2.1014, 2.095, 2.077, 2.0767, 2.0288, 2.027, 1.9582, 1.9159, 1.9032, 1.8885, 1.8806, 1.5272, 1.6519, 1.531, 0.6789, 0.5129, 0.3882, 0.4802, 0.264, 1.2521, 0.8257, 0.7364, 0.1762, -0.2952, 1.1409, 3.0352, 2.9552, 2.7018, 2.6335, 2.5563, 2.324, 2.322, 2.2048, 2.1626, 2.1464, 2.1206, 2.1186, 2.0594, 2.0095, 1.9799, 1.9679, 1.9542, 1.9132, 1.8783, 1.8429, 1.8211, 1.8018, 1.7982, 1.7831, 1.7143, 1.7114, 1.6745, 1.6672, 1.6631, 1.6618, 1.5726, 1.3932, 1.618, 1.1641, 1.3131, 1.4431, 1.2656, 1.0614, 0.778, 1.1518, 0.8822, 0.381, 1.0786, 1.0329, 0.8281, 0.5555, 0.5863, 0.0899, 0.1347, 0.1442, 0.7649, -0.2198, 0.7755, 0.3645, 0.8273, 0.3165, 3.7972, 3.0684, 2.9297, 2.9001, 2.8766, 2.7022, 2.5666, 2.3507, 2.2742, 2.1589, 2.1251, 2.125, 2.1176, 2.1103, 2.0711, 2.0606, 2.0531, 2.0513, 2.026, 1.9989, 1.8733, 1.8501, 1.7618, 1.7486, 1.73, 1.7022, 1.6819, 1.6708, 1.6628, 1.6416, 1.6263, 1.6212, 1.5213, 1.2285, 1.2715, 0.8678, 1.2024, 0.8553, 0.3171, 0.1709, 0.697, 1.0917, 0.5023, -0.1958, 0.4277, -0.2953, 0.1063, -0.1055, 2.9192, 2.7592, 2.5875, 2.5803, 2.4542, 2.4087, 2.3643, 2.3624, 2.3408, 2.3094, 2.3031, 2.2935, 2.2875, 2.2656, 2.1929, 2.1742, 2.1558, 2.1513, 2.1259, 2.0876, 2.0477, 2.0328, 2.0245, 2.0101, 1.9921, 1.9424, 1.9307, 1.8987, 1.8609, 1.8599, 1.8438, 1.7222, 1.7174, 1.7279, 0.9356, 1.3127, 0.7036, 1.2256, 0.5642, 0.5968, 0.8751, 1.0252, 0.3898, 0.2095, 0.2029, 0.7537, 0.1704, 0.5659, 0.5429, -0.0157, 3.0409, 2.7649, 2.7338, 2.6896, 2.5541, 2.4737, 2.3212, 2.2145, 2.114, 2.0671, 2.0401, 2.0212, 2.0109, 2.0087, 2.0051, 1.9865, 1.9447, 1.9341, 1.9255, 1.9195, 1.8109, 1.8058, 1.7982, 1.7843, 1.7434, 1.7139, 1.6286, 1.6261, 1.6168, 1.6117, 1.5623, 1.5455, 1.4978, 1.2304, 0.9295, 0.9749, 0.7387, 1.1383, 0.925, 1.1274, 0.8214, 0.9433, 0.2369, 0.6026, 0.6366, 0.1465, 0.0641, 0.9913, 0.9349, 0.5738, -0.1696, 0.2834, -0.1674, -0.1874, 3.4703, 3.0286, 2.9987, 2.8901, 2.8696, 2.7435, 2.5266, 2.5191, 2.3857, 2.3393, 2.3111, 2.2006, 2.1857, 2.0829, 2.0535, 2.0503, 1.9031, 1.8954, 1.8567, 1.8326, 1.818, 1.814, 1.7714, 1.7679, 1.7236, 1.7045, 1.6787, 1.6781, 1.65, 1.6452, 1.5812, 1.5858, 1.3227, 0.9625, 1.0068, 1.0825, 1.3758, 0.6689, 1.2377, 0.5439, 0.7027, 1.0356, 0.5555, 0.1244, 0.1926, 1.2608, 0.6064, 0.7515, 0.332, -0.3301, 2.7183, 2.5805, 2.5372, 2.2993, 2.1871, 2.1097, 2.0447, 2.015, 1.9704, 1.9439, 1.9101, 1.9085, 1.9082, 1.8487, 1.8319, 1.8148, 1.765, 1.7424, 1.7335, 1.6953, 1.6689, 1.6527, 1.6489, 1.6245, 1.583, 1.5623, 1.5344, 1.5292, 1.5108, 1.485, 1.4639, 1.3591, 1.3427, 1.4348, 0.9128, 0.937, 0.8999, 1.0047, 0.8976, 1.1754, 0.5892, 1.1483, 0.1626, 0.1295, 0.1057, -0.2225, 0.2789, 0.3221, -0.3691, 0.2217, 3.4946, 3.3116, 3.0758, 3.0231, 2.9995, 2.9969, 2.9466, 2.8082, 2.634, 2.6316, 2.5906, 2.5874, 2.5003, 2.4817, 2.4635, 2.4338, 2.3516, 2.3413, 2.3005, 2.2336, 2.2291, 2.2285, 2.2234, 2.2011, 2.1943, 2.1917, 2.0878, 2.0836, 2.083, 2.0721, 1.8332, 1.903, 1.9375, 2.069, 1.1015, 1.5969, 1.524, 0.7512, 1.1108, 1.7272, 0.8306, 1.003, 0.0699, 1.6097, -0.1815, 3.4156, 3.1376, 3.028, 2.9844, 2.8946, 2.6415, 2.5814, 2.5136, 2.3548, 2.2316, 2.2299, 2.1895, 2.1011, 2.0934, 2.082, 2.07, 2.0665, 2.037, 2.0262, 2.001, 1.9947, 1.9891, 1.9752, 1.972, 1.9503, 1.9195, 1.8658, 1.8367, 1.8277, 1.816, 1.768, 1.655, 1.3796, 1.3338, 1.0692, 0.8275, 0.5694, 1.0479, 0.9082, 0.5808, 1.168, 0.9759, 0.5708, 0.969, 0.948, 0.4712, 0.323, 0.1223, 0.61, 0.8995, -0.191, -0.2628, -0.3483, 2.7081, 2.613, 2.322, 2.1885, 2.1685, 2.1595, 2.0083, 1.92, 1.9152, 1.8563, 1.8538, 1.8413, 1.8393, 1.787, 1.766, 1.7303, 1.723, 1.7181, 1.7056, 1.702, 1.6729, 1.5727, 1.5704, 1.5696, 1.5601, 1.5554, 1.5536, 1.5384, 1.5232, 1.5147, 1.4112, 1.3256, 1.3371, 1.2059, 1.2711, 1.073, 1.28, 0.8337, 1.0889, 0.9171, 0.5298, 1.1192, 0.81, 0.791, 0.3273, 0.2281, 0.7654, 0.1239, 0.103, 0.5288, 0.8282, 0.0827, -0.2607, -0.3533, -0.1196, 3.602, 3.3477, 3.3193, 2.9496, 2.9027, 2.854, 2.7707, 2.732, 2.6143, 2.6142, 2.5761, 2.4321, 2.3674, 2.219, 2.1829, 2.1278, 2.108, 2.0876, 2.0842, 2.0644, 2.0318, 2.0273, 2.0085, 1.9448, 1.8772, 1.8662, 1.841, 1.8135, 1.7932, 1.7772, 1.7419, 1.7336, 1.4765, 1.3049, 0.704, 0.5806, 0.5591, 0.9509, 0.4477, 0.3514, 0.7442, 0.2336, 0.6329, -0.2108, 0.4162, 0.3506, -0.2834, 0.2078, 3.5862, 3.5679, 3.509, 3.2867, 3.1809, 2.9932, 2.7728, 2.578, 2.5251, 2.3533, 2.3325, 2.2422, 2.1203, 2.1059, 2.0619, 2.0159, 2.0027, 1.9428, 1.9206, 1.9174, 1.9061, 1.8913, 1.872, 1.8588, 1.8505, 1.8296, 1.8071, 1.7986, 1.7464, 1.7463, 1.6609, 1.6966, 1.6852, 1.3314, 1.3224, 0.6716, 0.8822, 1.222, 0.2095, 1.099, 0.1223, 0.5946, 0.5147, 0.3441, -0.1152, 1.0942, 0.2034, -0.4806, 3.6144, 3.3661, 3.2845, 3.1999, 3.003, 2.8595, 2.597, 2.5674, 2.4474, 2.401, 2.2905, 2.279, 2.2177, 2.2006, 2.1909, 2.1614, 2.1205, 2.0578, 2.0484, 1.9489, 1.9058, 1.887, 1.8557, 1.8386, 1.8007, 1.7775, 1.7692, 1.717, 1.7113, 1.6994, 1.6919, 1.6483, 1.2443, 1.2566, 0.8669, 0.9938, 0.7342, 0.7702, 0.4394, 1.2779, 0.098, 0.4564, 0.8868, 0.8829, -0.0909, 3.4333, 3.3269, 3.1133, 3.1128, 2.9949, 2.8554, 2.8255, 2.7469, 2.6857, 2.4081, 2.3179, 2.2562, 2.2534, 2.0549, 1.9971, 1.9856, 1.9113, 1.8876, 1.8811, 1.8678, 1.8651, 1.8481, 1.8335, 1.8144, 1.8063, 1.8014, 1.781, 1.7375, 1.7188, 1.6518, 1.5973, 1.5656, 1.2693, 1.5343, 1.5181, 0.7883, 1.0361, 1.0594, 1.1091, 0.4125, 1.2271, 1.0197, 0.9275, 0.8586, 0.7571, 0.137, 0.1788, 0.6934, 0.6831, 3.4766, 2.7922, 2.7506, 2.4249, 2.3831, 2.3649, 2.3479, 2.3472, 2.2979, 2.2176, 2.1512, 2.1284, 2.1206, 2.0963, 2.0792, 1.9835, 1.9802, 1.9669, 1.9588, 1.9151, 1.9095, 1.8877, 1.854, 1.8358, 1.816, 1.7776, 1.7765, 1.7633, 1.7427, 1.7073, 1.6215, 1.6617, 1.4599, 1.3128, 1.6058, 1.4056, 1.2732, 1.0974, 0.9867, 0.6373, 1.2793, 1.0584, 0.7228, 0.2808, 0.1516, 0.9818, -0.2627, 1.3346, 0.0549, 0.492, -0.3481, 3.4648, 2.8193, 2.7672, 2.7309, 2.6656, 2.6235, 2.5389, 2.4737, 2.398, 2.3881, 2.3381, 2.3095, 2.298, 2.2388, 2.2297, 2.1688, 2.1647, 2.1529, 2.151, 2.0688, 2.0596, 2.0066, 1.9834, 1.9313, 1.9266, 1.895, 1.8906, 1.8836, 1.8617, 1.8589, 1.8192, 1.6863, 1.6747, 1.449, 1.1901, 1.7214, 1.1982, 1.5054, 1.2739, 0.9529, 1.2694, 1.6254, 0.827, 0.5232, 0.9539, 0.1996, 1.4557, 0.5779, 0.6123, 0.5364, 0.0159, -0.2931, -0.2636, -0.0044, 3.2569, 3.2474, 2.7694, 2.6794, 2.5275, 2.473, 2.3084, 2.2942, 2.2341, 2.2117, 2.1497, 2.136, 2.1, 2.0939, 1.9933, 1.9706, 1.9262, 1.9089, 1.8989, 1.8636, 1.8415, 1.7466, 1.6975, 1.6632, 1.5686, 1.5592, 1.549, 1.4833, 1.4676, 1.4338, 1.4312, 1.3255, 1.2972, 1.1916, 1.2662, 0.7153, 0.9556, 0.906, 0.3185, 0.5243, 0.6586, 0.7246, -0.0034, 0.376, -0.3263, -0.1318, 3.0033, 2.9303, 2.6835, 2.5953, 2.4406, 2.256, 2.2075, 2.1529, 2.0087, 1.9878, 1.9402, 1.9145, 1.8981, 1.8841, 1.8716, 1.8616, 1.8392, 1.8242, 1.8207, 1.8016, 1.8015, 1.7916, 1.775, 1.7428, 1.737, 1.727, 1.7266, 1.7248, 1.7082, 1.7041, 1.6891, 1.6901, 1.628, 1.6117, 1.5728, 1.5282, 1.6635, 1.5191, 1.1922, 0.7125, 1.4868, 0.3402, 0.3056, 0.9321, 1.1251, 0.738, 0.404, 0.6665, 0.5227, -0.0929, -0.3494, -0.0457, 3.2474, 3.2263, 2.8655, 2.6001, 2.5821, 2.5229, 2.3709, 2.1399, 2.0815, 2.0646, 2.0467, 2.0219, 1.9515, 1.9443, 1.9437, 1.9434, 1.8639, 1.8527, 1.8489, 1.8448, 1.8425, 1.7461, 1.7401, 1.724, 1.6788, 1.649, 1.6089, 1.557, 1.5371, 1.5343, 1.5012, 1.4381, 1.3302, 1.2832, 0.9871, 0.4979, 0.8564, 0.7577, 0.7817, 0.6308, 0.0903, 0.1078, 0.2407, -0.095, -0.004, 0.0548, -0.0156, 0.7131, 0.2793, -0.0526, -0.06, 0.4762]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 3, 6, 8, 10, 13, 16, 32, 1, 2, 6, 8, 10, 11, 13, 14, 18, 20, 23, 30, 33, 36, 1, 2, 4, 5, 7, 9, 10, 11, 12, 13, 14, 16, 18, 19, 21, 22, 24, 25, 26, 28, 29, 38, 39, 1, 5, 9, 13, 17, 20, 21, 25, 27, 29, 30, 4, 5, 7, 9, 12, 15, 22, 23, 32, 36, 1, 3, 7, 12, 13, 16, 18, 25, 33, 35, 36, 37, 39, 40, 4, 5, 6, 11, 13, 15, 16, 19, 21, 23, 27, 28, 29, 38, 40, 4, 6, 7, 9, 13, 16, 19, 20, 32, 34, 40, 2, 5, 9, 16, 18, 24, 29, 34, 38, 39, 40, 4, 6, 8, 9, 13, 18, 19, 20, 23, 31, 32, 38, 39, 40, 6, 7, 8, 11, 18, 26, 28, 34, 35, 40, 6, 11, 18, 22, 23, 27, 30, 31, 34, 35, 36, 38, 2, 11, 15, 18, 34, 2, 4, 6, 8, 11, 12, 13, 16, 18, 19, 20, 23, 25, 32, 33, 38, 2, 8, 14, 23, 33, 40, 4, 5, 6, 8, 9, 13, 18, 26, 27, 28, 31, 37, 38, 39, 4, 27, 32, 4, 5, 7, 9, 14, 16, 18, 21, 22, 23, 26, 31, 32, 35, 40, 2, 4, 10, 12, 15, 16, 18, 22, 23, 31, 33, 4, 5, 16, 19, 21, 23, 27, 37, 5, 11, 12, 18, 25, 32, 34, 37, 7, 8, 9, 11, 13, 15, 28, 34, 1, 2, 3, 4, 5, 6, 7, 8, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 1, 2, 4, 5, 7, 8, 11, 13, 16, 18, 20, 21, 23, 26, 27, 28, 29, 31, 36, 38, 39, 4, 6, 7, 8, 9, 18, 19, 21, 26, 32, 33, 39, 2, 5, 7, 8, 10, 16, 19, 28, 32, 36, 39, 1, 7, 8, 20, 23, 27, 34, 39, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 28, 29, 30, 31, 32, 36, 38, 39, 40, 3, 4, 6, 7, 11, 12, 16, 19, 20, 24, 26, 28, 31, 32, 33, 35, 2, 4, 7, 8, 9, 18, 24, 25, 31, 32, 33, 37, 38, 1, 4, 7, 8, 9, 17, 19, 21, 26, 28, 31, 33, 36, 38, 40, 4, 7, 9, 23, 26, 28, 33, 38, 40, 2, 5, 7, 9, 11, 12, 14, 17, 19, 20, 23, 25, 26, 27, 30, 37, 39, 1, 4, 5, 6, 7, 10, 13, 18, 19, 20, 26, 27, 37, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 30, 32, 33, 36, 39, 4, 5, 7, 9, 14, 16, 18, 19, 31, 32, 16, 25, 33, 36, 1, 2, 4, 11, 13, 15, 21, 24, 25, 36, 1, 2, 4, 8, 9, 13, 14, 15, 16, 18, 19, 23, 25, 26, 29, 31, 34, 39, 40, 2, 5, 6, 8, 9, 15, 16, 21, 28, 31, 33, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 18, 19, 20, 23, 24, 28, 29, 30, 31, 33, 35, 36, 37, 40, 1, 2, 4, 5, 6, 7, 8, 9, 17, 18, 20, 24, 27, 29, 31, 32, 33, 34, 36, 38, 3, 7, 17, 19, 23, 25, 29, 31, 38, 4, 12, 13, 16, 26, 30, 31, 32, 36, 37, 38, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 40, 1, 2, 5, 7, 10, 12, 13, 15, 18, 23, 26, 31, 35, 36, 38, 1, 2, 5, 6, 9, 10, 13, 15, 21, 22, 24, 25, 26, 28, 31, 32, 33, 1, 7, 12, 15, 19, 20, 23, 29, 30, 32, 35, 39, 2, 3, 5, 6, 7, 10, 14, 16, 24, 25, 26, 27, 28, 31, 36, 39, 3, 6, 7, 9, 17, 23, 25, 26, 3, 9, 12, 14, 17, 18, 22, 23, 26, 32, 34, 36, 2, 9, 10, 22, 23, 26, 32, 39, 3, 5, 10, 12, 13, 18, 21, 25, 28, 31, 40, 7, 9, 12, 17, 20, 22, 25, 28, 31, 35, 36, 39, 40, 6, 7, 9, 16, 19, 25, 27, 28, 30, 31, 32, 34, 39, 1, 6, 7, 9, 13, 18, 19, 21, 23, 27, 37, 1, 5, 6, 7, 8, 9, 10, 11, 12, 19, 20, 23, 24, 27, 34, 35, 36, 38, 40, 1, 5, 6, 8, 9, 10, 16, 19, 21, 23, 26, 32, 35, 36, 6, 10, 12, 20, 25, 27, 31, 40, 4, 5, 6, 9, 10, 12, 13, 15, 18, 19, 20, 22, 23, 24, 26, 28, 32, 33, 37, 38, 39, 1, 4, 8, 18, 19, 27, 28, 29, 1, 2, 10, 11, 21, 25, 33, 34, 39, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 40, 1, 7, 11, 12, 18, 21, 24, 26, 30, 32, 33, 36, 37, 7, 9, 10, 11, 13, 16, 19, 22, 25, 26, 34, 36, 37, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 1, 3, 4, 13, 15, 25, 38, 1, 2, 4, 5, 7, 8, 9, 13, 18, 24, 25, 26, 28, 30, 36, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 31, 33, 35, 36, 37, 38, 39, 2, 4, 6, 8, 9, 10, 13, 23, 25, 27, 28, 34, 37, 39, 4, 6, 7, 13, 19, 20, 21, 22, 23, 27, 1, 3, 4, 6, 7, 9, 10, 14, 16, 23, 26, 32, 33, 38, 40, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 16, 18, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 8, 9, 13, 16, 21, 23, 26, 35, 40, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 29, 31, 32, 33, 34, 35, 37, 39, 40, 5, 16, 19, 23, 26, 29, 31, 33, 37, 39, 2, 16, 17, 33, 36, 3, 6, 7, 9, 10, 12, 20, 23, 27, 28, 31, 33, 34, 2, 4, 8, 10, 12, 13, 17, 23, 25, 27, 28, 34, 6, 10, 16, 28, 31, 36, 40, 1, 2, 4, 5, 6, 7, 8, 9, 13, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 1, 2, 3, 4, 6, 7, 9, 11, 12, 13, 15, 17, 18, 20, 21, 23, 25, 27, 28, 29, 31, 35, 36, 37, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 31, 32, 33, 35, 36, 37, 38, 39, 40, 2, 3, 5, 6, 9, 10, 21, 25, 26, 28, 29, 34, 6, 13, 15, 17, 21, 25, 30, 38, 40, 1, 2, 3, 4, 5, 7, 8, 11, 12, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 29, 31, 32, 33, 34, 36, 39, 4, 17, 32, 36, 4, 17, 32, 5, 6, 7, 9, 12, 15, 19, 21, 22, 23, 35, 38, 3, 33, 37, 2, 5, 13, 15, 18, 19, 32, 35, 39, 1, 2, 4, 8, 9, 20, 24, 26, 27, 31, 32, 39, 40, 7, 10, 11, 13, 14, 16, 18, 21, 23, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 15, 16, 18, 20, 21, 22, 23, 25, 26, 27, 32, 38, 39, 1, 7, 8, 11, 12, 16, 23, 26, 29, 32, 39, 2, 7, 11, 13, 17, 18, 27, 30, 33, 4, 5, 8, 9, 24, 2, 3, 7, 8, 9, 10, 14, 15, 24, 25, 27, 28, 31, 40, 2, 3, 6, 7, 8, 9, 16, 19, 20, 21, 24, 25, 26, 28, 32, 33, 34, 36, 40, 2, 5, 6, 9, 10, 13, 21, 23, 24, 26, 28, 13, 16, 20, 22, 28, 29, 30, 38, 5, 7, 9, 14, 17, 20, 23, 27, 28, 30, 31, 39, 3, 4, 5, 6, 9, 13, 20, 23, 24, 40, 1, 5, 7, 9, 13, 14, 16, 18, 20, 21, 24, 26, 31, 32, 38, 4, 7, 8, 9, 19, 21, 30, 32, 37, 40, 1, 4, 5, 9, 16, 18, 19, 22, 23, 24, 25, 26, 1, 2, 4, 5, 10, 13, 16, 18, 21, 27, 28, 29, 40, 2, 4, 7, 10, 12, 13, 17, 19, 20, 23, 26, 27, 28, 31, 32, 33, 36, 40, 2, 4, 8, 10, 14, 16, 21, 25, 27, 29, 30, 39, 2, 4, 6, 7, 9, 10, 11, 13, 15, 18, 21, 24, 28, 32, 34, 38, 1, 2, 4, 7, 11, 16, 21, 26, 40, 1, 2, 3, 4, 6, 8, 9, 11, 12, 13, 15, 16, 18, 20, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 39, 2, 3, 4, 5, 13, 18, 30, 31, 38, 7, 9, 12, 17, 18, 20, 21, 22, 23, 24, 32, 33, 34, 2, 6, 10, 11, 21, 23, 26, 29, 30, 33, 2, 3, 4, 7, 8, 10, 11, 13, 14, 19, 21, 23, 29, 30, 31, 35, 37, 9, 11, 16, 19, 33, 35, 5, 7, 11, 12, 16, 18, 30, 32, 33, 34, 36, 39, 2, 3, 4, 5, 7, 12, 31, 33, 39, 40, 1, 2, 9, 14, 18, 19, 23, 25, 26, 27, 28, 29, 31, 32, 4, 9, 10, 22, 26, 27, 28, 29, 31, 32, 2, 3, 6, 7, 14, 15, 17, 28, 33, 37, 40, 1, 2, 3, 4, 6, 7, 8, 11, 13, 14, 15, 17, 18, 19, 20, 22, 23, 26, 27, 28, 30, 31, 35, 36, 37, 39, 1, 2, 4, 5, 6, 9, 15, 16, 18, 24, 29, 30, 32, 34, 37, 5, 8, 9, 10, 11, 25, 26, 28, 30, 31, 32, 6, 7, 10, 11, 12, 14, 15, 17, 22, 23, 26, 27, 32, 34, 36, 39, 2, 6, 8, 13, 16, 18, 28, 35, 4, 7, 9, 10, 26, 28, 30, 36, 39, 1, 2, 4, 5, 8, 9, 10, 15, 16, 17, 20, 21, 22, 23, 25, 27, 28, 29, 31, 33, 36, 37, 38, 40, 4, 27, 31, 3, 4, 7, 17, 20, 21, 26, 27, 30, 31, 2, 3, 4, 6, 12, 14, 20, 25, 26, 2, 5, 7, 11, 14, 15, 17, 19, 20, 21, 35, 36, 1, 3, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 19, 20, 21, 23, 24, 25, 26, 28, 30, 31, 36, 38, 40, 4, 6, 9, 13, 17, 18, 20, 21, 23, 24, 27, 28, 31, 34, 38, 1, 6, 8, 20, 38, 4, 8, 10, 13, 15, 18, 26, 27, 31, 40, 2, 4, 5, 8, 18, 24, 28, 31, 32, 36, 40, 6, 8, 10, 19, 23, 25, 28, 37, 38, 4, 5, 7, 8, 9, 18, 23, 25, 30, 33, 34, 1, 2, 3, 4, 9, 12, 15, 16, 23, 25, 26, 27, 28, 31, 34, 36, 4, 5, 6, 10, 13, 16, 17, 20, 31, 35, 39, 1, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 18, 19, 20, 21, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 10, 13, 15, 16, 23, 24, 28, 31, 35, 36, 1, 2, 5, 6, 7, 21, 24, 26, 28, 31, 33, 36, 2, 5, 7, 11, 12, 16, 19, 20, 27, 28, 31, 32, 33, 34, 38, 5, 27, 32, 38, 7, 8, 15, 16, 17, 18, 24, 25, 32, 33, 34, 39, 2, 3, 4, 5, 8, 10, 20, 21, 22, 26, 31, 33, 36, 37, 40, 2, 6, 7, 13, 16, 18, 25, 29, 31, 32, 40, 2, 3, 4, 5, 8, 9, 10, 11, 17, 22, 32, 37, 2, 5, 7, 9, 23, 27, 28, 29, 7, 10, 16, 17, 21, 26, 32, 33, 36, 40, 4, 6, 9, 13, 16, 20, 23, 24, 32, 38, 3, 6, 7, 8, 14, 24, 27, 32, 35, 36, 39, 40, 8, 13, 16, 19, 20, 25, 26, 29, 30, 36, 38, 39, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 18, 19, 20, 21, 22, 24, 27, 28, 30, 31, 34, 35, 1, 4, 5, 6, 7, 8, 9, 10, 11, 21, 23, 24, 27, 31, 33, 36, 37, 39, 40, 1, 3, 4, 6, 8, 9, 11, 12, 13, 14, 15, 17, 20, 22, 23, 26, 28, 32, 36, 38, 40, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 19, 21, 26, 27, 30, 32, 35, 1, 4, 7, 11, 15, 17, 18, 20, 25, 28, 31, 32, 33, 34, 35, 38, 40, 4, 6, 7, 8, 10, 13, 21, 22, 25, 29, 31, 32, 37, 40, 1, 5, 10, 17, 18, 19, 22, 29, 34, 36, 1, 3, 4, 9, 12, 13, 18, 27, 28, 3, 7, 9, 16, 23, 26, 31, 32, 34, 35, 36, 40, 3, 7, 17, 18, 20, 26, 35, 37, 38, 1, 4, 5, 6, 9, 10, 14, 15, 16, 17, 19, 20, 22, 23, 24, 25, 28, 32, 33, 35, 37, 39, 1, 4, 6, 10, 15, 20, 21, 23, 26, 27, 28, 33, 35, 40, 24, 25, 39, 8, 9, 16, 21, 23, 34, 35, 38, 7, 9, 10, 11, 23, 24, 25, 29, 31, 40, 1, 2, 3, 4, 5, 7, 8, 9, 12, 13, 14, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 38, 39, 40, 2, 3, 7, 13, 14, 15, 16, 18, 19, 21, 23, 32, 36, 37, 38, 1, 2, 9, 15, 18, 25, 27, 31, 39, 2, 5, 15, 16, 18, 19, 22, 23, 26, 31, 36, 39, 40, 4, 5, 7, 8, 9, 15, 17, 21, 28, 32, 33, 36, 1, 8, 9, 19, 20, 23, 26, 27, 28, 30, 31, 38, 3, 7, 11, 12, 14, 15, 16, 25, 30, 33, 36, 38, 39, 1, 2, 3, 4, 6, 9, 14, 15, 16, 18, 21, 23, 25, 28, 29, 33, 34, 35, 36, 38, 39, 40, 1, 2, 19, 23, 29, 31, 35, 8, 14, 16, 18, 19, 29, 31, 38, 40, 5, 7, 9, 13, 15, 20, 33, 35, 9, 11, 16, 20, 21, 25, 35, 3, 7, 9, 16, 18, 19, 20, 21, 26, 31, 36, 38, 39, 40, 1, 5, 7, 10, 12, 13, 21, 23, 24, 25, 29, 30, 36, 9, 15, 19, 22, 26, 31, 5, 9, 11, 16, 18, 21, 22, 24, 25, 26, 31, 32, 37, 38, 39, 5, 8, 9, 11, 16, 18, 21, 22, 24, 26, 31, 32, 37, 38, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14, 18, 24, 25, 26, 28, 31, 32, 33, 34, 36, 38, 40, 1, 2, 7, 15, 18, 21, 26, 30, 37, 4, 6, 7, 8, 15, 16, 20, 23, 25, 29, 38, 6, 7, 9, 17, 21, 26, 28, 31, 32, 40, 1, 4, 6, 16, 17, 18, 19, 23, 26, 32, 35, 4, 6, 7, 9, 10, 12, 14, 18, 21, 24, 30, 32, 36, 37, 4, 8, 11, 12, 13, 15, 17, 25, 33, 34, 35, 37, 40, 1, 2, 3, 4, 5, 7, 8, 9, 13, 15, 16, 18, 21, 22, 23, 24, 25, 28, 29, 31, 32, 37, 38, 39, 40, 1, 4, 16, 19, 21, 25, 31, 33, 39, 40, 1, 3, 4, 7, 13, 16, 19, 21, 25, 31, 33, 37, 40, 1, 4, 11, 16, 17, 18, 20, 24, 25, 26, 31, 2, 4, 6, 8, 9, 14, 15, 17, 18, 19, 20, 21, 23, 24, 26, 28, 29, 31, 32, 34, 35, 1, 2, 4, 5, 6, 7, 8, 14, 21, 30, 33, 2, 6, 15, 16, 22, 25, 27, 28, 30, 31, 36, 37, 3, 7, 10, 15, 16, 21, 24, 31, 33, 36, 40, 2, 3, 9, 10, 11, 12, 16, 17, 21, 26, 29, 32, 40, 3, 4, 5, 6, 12, 16, 18, 19, 21, 25, 27, 28, 32, 33, 34, 36, 40, 1, 2, 4, 8, 9, 13, 16, 21, 30, 31, 36, 1, 2, 4, 6, 7, 8, 9, 10, 13, 20, 21, 22, 23, 26, 28, 31, 32, 33, 34, 36, 38, 1, 4, 8, 10, 19, 23, 35, 36, 40, 1, 2, 4, 6, 11, 12, 14, 15, 18, 34, 1, 4, 5, 7, 8, 18, 19, 20, 21, 28, 35, 40, 6, 7, 8, 9, 30, 31, 36, 38, 4, 5, 9, 19, 30, 34, 40, 5, 10, 16, 18, 19, 26, 36, 37, 4, 7, 11, 16, 19, 21, 23, 25, 4, 5, 7, 9, 10, 11, 12, 13, 19, 21, 22, 23, 25, 26, 28, 32, 33, 36, 38, 40, 1, 2, 3, 4, 5, 7, 9, 10, 18, 23, 25, 28, 31, 34, 36, 37, 40, 4, 9, 25, 26, 31, 33, 39, 40, 3, 5, 9, 14, 15, 18, 20, 23, 25, 26, 27, 30, 33, 37, 38, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 21, 22, 23, 24, 25, 26, 32, 33, 35, 4, 5, 6, 7, 8, 9, 13, 14, 15, 18, 19, 21, 22, 24, 28, 29, 32, 36, 38, 40, 2, 4, 5, 6, 9, 11, 16, 18, 19, 20, 23, 24, 25, 26, 27, 28, 31, 37, 40, 4, 9, 19, 20, 23, 24, 26, 31, 37, 40, 4, 7, 13, 15, 27, 30, 31, 39, 3, 13, 15, 31, 32, 33, 34, 39, 2, 3, 4, 5, 7, 8, 13, 15, 21, 31, 37, 39, 40, 1, 4, 6, 7, 8, 12, 13, 16, 23, 26, 29, 31, 33, 34, 36, 39, 1, 18, 19, 30, 35, 2, 3, 5, 6, 13, 14, 16, 26, 28, 31, 1, 7, 10, 14, 18, 19, 20, 24, 29, 30, 33, 1, 2, 3, 4, 7, 9, 25, 28, 35, 4, 6, 7, 10, 16, 19, 23, 28, 3, 4, 6, 7, 13, 16, 17, 18, 19, 20, 22, 23, 27, 28, 31, 32, 35, 36, 37, 40, 3, 17, 1, 2, 8, 10, 15, 20, 26, 31, 34, 35, 39, 40, 1, 3, 4, 5, 7, 8, 10, 12, 13, 15, 16, 18, 19, 22, 23, 31, 33, 37, 6, 9, 10, 14, 16, 18, 21, 26, 28, 29, 31, 32, 39, 4, 11, 16, 18, 21, 26, 35, 36, 39, 2, 8, 12, 15, 20, 29, 31, 40, 8, 12, 15, 20, 31, 40, 1, 2, 5, 6, 7, 8, 9, 11, 13, 15, 16, 17, 18, 19, 22, 23, 25, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 2, 8, 12, 13, 14, 15, 16, 18, 21, 25, 27, 30, 31, 32, 39, 1, 3, 4, 6, 9, 14, 15, 20, 22, 23, 25, 26, 27, 3, 4, 8, 10, 11, 13, 14, 15, 18, 19, 20, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32, 36, 38, 39, 40, 3, 6, 10, 11, 13, 21, 24, 25, 26, 27, 29, 33, 4, 6, 7, 8, 11, 12, 13, 16, 17, 20, 21, 22, 27, 30, 32, 35, 39, 2, 9, 19, 39, 1, 4, 8, 13, 14, 18, 25, 26, 31, 35, 36, 38, 39, 1, 2, 4, 7, 8, 9, 12, 13, 14, 16, 18, 21, 22, 23, 25, 26, 27, 28, 29, 33, 34, 36, 37, 40, 2, 4, 5, 15, 17, 22, 27, 31, 36, 40, 4, 6, 7, 10, 13, 21, 23, 31, 33, 36, 6, 8, 9, 14, 15, 23, 26, 30, 31, 36, 39, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 3, 4, 13, 15, 18, 21, 25, 26, 27, 28, 31, 34, 35, 4, 7, 8, 17, 19, 28, 31, 32, 33, 34, 36, 37, 38, 40, 2, 3, 4, 5, 9, 11, 15, 18, 19, 20, 21, 25, 26, 29, 33, 37, 39, 40, 3, 7, 8, 11, 16, 27, 33, 36, 39, 1, 2, 3, 5, 6, 7, 9, 10, 18, 25, 27, 30, 31, 38, 2, 8, 12, 13, 16, 18, 26, 39, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 28, 29, 30, 31, 33, 37, 38, 40, 2, 16, 20, 22, 26, 27, 30, 34, 35, 40, 2, 4, 8, 14, 16, 18, 20, 27, 31, 33, 1, 2, 4, 6, 7, 10, 16, 22, 28, 31, 32, 1, 4, 6, 10, 11, 12, 15, 18, 19, 23, 31, 34, 1, 2, 4, 9, 15, 22, 27, 28, 30, 31, 33, 36, 1, 4, 5, 7, 10, 11, 15, 16, 18, 19, 21, 23, 25, 26, 28, 31, 39, 40, 6, 7, 8, 13, 15, 18, 23, 25, 26, 31, 33, 34, 36, 1, 2, 4, 5, 6, 13, 14, 15, 17, 19, 23, 24, 26, 29, 30, 37, 38, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 20, 23, 24, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 1, 4, 5, 7, 14, 15, 20, 34, 36, 39, 4, 9, 10, 18, 24, 27, 28, 31, 32, 36, 38, 5, 8, 16, 36, 40, 1, 3, 4, 6, 8, 10, 13, 14, 15, 16, 17, 20, 22, 23, 26, 27, 28, 30, 31, 32, 33, 35, 36, 40, 5, 6, 7, 10, 13, 15, 16, 17, 27, 28, 33, 35, 36, 40, 2, 18, 21, 27, 28, 29, 31, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 1, 3, 4, 5, 6, 8, 10, 11, 14, 15, 17, 18, 19, 20, 21, 27, 28, 30, 31, 33, 35, 36, 38, 2, 8, 11, 16, 18, 19, 20, 21, 23, 26, 28, 30, 40, 6, 14, 15, 17, 19, 24, 25, 29, 31, 33, 35, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 37, 38, 39, 1, 2, 6, 7, 8, 9, 19, 20, 22, 24, 25, 27, 31, 38, 39, 4, 16, 20, 26, 29, 31, 40, 1, 6, 7, 9, 10, 12, 13, 28, 30, 32, 35, 1, 2, 4, 6, 7, 9, 12, 18, 19, 21, 26, 30, 34, 37, 40, 5, 6, 8, 14, 16, 18, 21, 23, 28, 34, 36, 8, 22, 26, 40, 4, 7, 8, 12, 25, 33, 34, 35, 36, 39, 5, 6, 12, 17, 18, 23, 26, 29, 30, 31, 35, 38, 2, 3, 4, 5, 11, 16, 26, 27, 28, 29, 38, 40, 4, 12, 17, 18, 20, 23, 27, 38, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 15, 18, 19, 20, 21, 22, 25, 26, 28, 31, 32, 33, 34, 35, 37, 38, 39, 40, 7, 13, 17, 30, 33, 35, 36, 5, 6, 7, 9, 15, 16, 20, 24, 25, 28, 30, 33, 35, 36, 40, 5, 6, 9, 13, 20, 31, 32, 38, 3, 6, 7, 9, 10, 12, 15, 20, 37, 38, 1, 3, 4, 5, 7, 8, 13, 21, 25, 26, 27, 28, 30, 32, 38, 1, 3, 7, 10, 12, 13, 14, 17, 18, 23, 25, 36, 37, 1, 2, 3, 4, 5, 7, 8, 9, 12, 18, 25, 28, 31, 32, 35, 38, 39, 4, 6, 7, 8, 10, 14, 18, 21, 22, 26, 31, 1, 2, 3, 4, 5, 12, 13, 15, 17, 18, 21, 22, 27, 28, 29, 30, 31, 32, 35, 39, 1, 2, 4, 5, 6, 8, 10, 11, 12, 13, 15, 17, 20, 21, 22, 23, 25, 28, 31, 32, 33, 34, 35, 36, 38, 1, 2, 4, 7, 13, 14, 17, 18, 19, 28, 30, 31, 32, 36, 37, 38, 39, 1, 4, 5, 6, 8, 9, 13, 18, 26, 35, 37, 39, 40, 1, 4, 5, 8, 9, 13, 16, 23, 26, 29, 40, 4, 5, 6, 13, 20, 24, 26, 36, 38, 40, 4, 7, 8, 16, 19, 40, 4, 10, 14, 15, 23, 27, 30, 31, 32, 38, 1, 4, 7, 8, 12, 17, 19, 21, 23, 24, 29, 31, 32, 33, 34, 36, 1, 2, 5, 6, 13, 18, 20, 23, 24, 27, 37, 39, 2, 4, 7, 13, 18, 20, 23, 31, 32, 33, 40, 1, 2, 4, 6, 10, 13, 25, 26, 28, 29, 33, 34, 36, 5, 7, 8, 13, 21, 23, 25, 26, 28, 2, 3, 4, 7, 12, 20, 21, 25, 28, 32, 35, 36, 37, 2, 5, 8, 12, 13, 16, 23, 32, 3, 4, 7, 11, 12, 20, 25, 31, 33, 34, 39, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 30, 32, 33, 34, 35, 36, 37, 1, 2, 5, 6, 7, 13, 18, 19, 20, 26, 27, 40, 1, 4, 6, 7, 8, 9, 12, 14, 17, 18, 19, 20, 22, 25, 26, 28, 29, 30, 31, 32, 2, 9, 18, 19, 20, 21, 23, 25, 26, 29, 31, 39, 2, 9, 14, 16, 18, 27, 31, 38, 39, 5, 6, 14, 15, 21, 23, 25, 28, 32, 4, 5, 7, 9, 10, 13, 14, 20, 24, 25, 27, 33, 34, 35, 36, 37, 1, 2, 4, 9, 20, 27, 30, 31, 34, 37, 38, 1, 6, 7, 10, 13, 24, 25, 28, 32, 36, 3, 6, 7, 10, 11, 13, 16, 17, 28, 30, 32, 33, 35, 38, 1, 2, 3, 4, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 5, 7, 8, 11, 14, 16, 21, 23, 25, 30, 31, 32, 35, 37, 38, 39, 1, 2, 4, 7, 9, 20, 24, 26, 31, 32, 33, 37, 38, 40, 1, 2, 4, 8, 9, 13, 15, 25, 28, 31, 32, 33, 34, 36, 37, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 3, 7, 9, 11, 17, 33, 34, 36, 4, 9, 10, 19, 22, 25, 26, 28, 31, 2, 4, 5, 6, 7, 9, 10, 11, 13, 17, 19, 20, 21, 26, 28, 30, 31, 34, 35, 38, 39, 40, 10, 11, 39, 1, 5, 7, 13, 21, 25, 27, 29, 30, 34, 38, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 37, 39, 40, 5, 8, 9, 10, 20, 30, 32, 1, 2, 4, 5, 6, 7, 8, 9, 12, 13, 15, 18, 19, 20, 23, 24, 26, 28, 30, 32, 33, 34, 36, 37, 38, 40, 1, 4, 18, 20, 28, 29, 31, 32, 33, 2, 4, 5, 8, 11, 13, 18, 20, 27, 28, 35, 37, 39, 40, 2, 4, 5, 8, 9, 10, 16, 21, 24, 28, 31, 32, 2, 4, 5, 6, 7, 9, 11, 12, 13, 16, 18, 19, 21, 22, 23, 27, 28, 29, 30, 31, 34, 37, 38, 2, 5, 14, 20, 24, 25, 26, 27, 30, 31, 39, 6, 7, 13, 14, 17, 18, 26, 27, 28, 31, 39, 40, 1, 4, 8, 9, 10, 11, 12, 14, 16, 17, 20, 21, 23, 24, 26, 27, 28, 29, 32, 33, 37, 1, 3, 8, 9, 10, 19, 20, 23, 26, 28, 31, 32, 34, 35, 36, 38, 40, 4, 10, 20, 26, 40, 6, 7, 10, 14, 23, 26, 28, 35, 36, 38, 40, 1, 2, 4, 8, 9, 20, 21, 22, 23, 24, 25, 27, 28, 32, 35, 2, 6, 7, 10, 13, 16, 20, 27, 28, 34, 4, 9, 10, 13, 22, 23, 30, 31, 34, 35, 2, 3, 4, 8, 12, 15, 17, 21, 27, 29, 31, 36, 39, 40, 1, 3, 4, 7, 8, 10, 12, 13, 15, 16, 17, 19, 20, 21, 26, 28, 30, 32, 34, 35, 36, 40, 3, 6, 8, 13, 16, 20, 22, 25, 28, 30, 32, 35, 1, 12, 16, 25, 27, 31, 35, 39, 1, 16, 21, 24, 29, 31, 37, 39, 40, 7, 13, 18, 20, 22, 23, 31, 33, 36, 37, 39, 2, 3, 4, 8, 13, 21, 25, 26, 33, 35, 39, 2, 7, 9, 13, 26, 27, 31, 36, 39, 4, 6, 7, 14, 16, 18, 21, 23, 26, 28, 1, 2, 4, 6, 9, 12, 15, 16, 17, 19, 20, 21, 23, 24, 25, 30, 34, 35, 40, 6, 12, 16, 19, 21, 23, 30, 34, 1, 4, 6, 9, 15, 21, 30, 34, 35, 40, 2, 4, 7, 9, 12, 15, 28, 30, 31, 36, 1, 2, 4, 5, 6, 9, 10, 13, 15, 16, 18, 19, 22, 23, 24, 26, 28, 30, 31, 32, 34, 35, 39, 40, 8, 11, 13, 18, 20, 28, 31, 32, 34, 39, 1, 2, 3, 4, 5, 7, 9, 11, 12, 16, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 32, 37, 38, 39, 1, 9, 13, 22, 23, 29, 31, 1, 3, 8, 14, 15, 19, 22, 30, 32, 33, 35, 8, 18, 40, 4, 8, 9, 17, 23, 26, 28, 29, 31, 2, 3, 4, 5, 6, 7, 10, 13, 15, 16, 17, 18, 19, 24, 26, 31, 32, 34, 36, 37, 39, 4, 6, 10, 13, 16, 17, 18, 19, 24, 26, 31, 39, 1, 2, 3, 4, 6, 14, 18, 20, 22, 23, 26, 27, 28, 29, 30, 34, 39, 1, 2, 3, 7, 10, 14, 17, 22, 23, 25, 26, 28, 30, 37, 5, 7, 9, 10, 13, 14, 21, 23, 25, 27, 30, 39, 40, 2, 3, 5, 7, 10, 14, 20, 27, 28, 32, 33, 35, 36, 39, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 36, 37, 38, 39, 40, 4, 5, 7, 9, 11, 21, 23, 26, 32, 33, 5, 7, 9, 10, 11, 25, 28, 29, 37, 39, 6, 7, 18, 20, 26, 29, 39, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 39, 40, 5, 8, 15, 16, 18, 24, 25, 28, 30, 31, 32, 33, 37, 5, 10, 11, 15, 19, 21, 22, 28, 31, 32, 33, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 38, 40, 1, 7, 8, 12, 15, 21, 22, 23, 26, 28, 32, 35, 38, 8, 12, 20, 22, 1, 4, 5, 8, 12, 21, 32, 36, 37, 38, 3, 7, 11, 13, 16, 18, 22, 23, 24, 25, 26, 27, 28, 31, 33, 36, 37, 4, 5, 6, 7, 10, 18, 20, 24, 25, 30, 31, 32, 33, 40, 1, 2, 3, 4, 6, 8, 10, 13, 16, 17, 18, 19, 20, 22, 23, 25, 28, 31, 32, 33, 3, 4, 6, 8, 10, 13, 16, 17, 19, 20, 22, 23, 25, 28, 31, 32, 33, 40, 2, 3, 4, 19, 21, 23, 25, 26, 28, 31, 33, 1, 3, 7, 11, 15, 18, 21, 23, 26, 30, 31, 37, 1, 2, 3, 4, 5, 6, 7, 9, 16, 18, 20, 25, 28, 32, 33, 39, 2, 4, 13, 16, 18, 28, 31, 36, 3, 4, 6, 9, 10, 15, 21, 22, 23, 24, 37, 39, 40, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 1, 12, 16, 19, 21, 25, 26, 28, 1, 2, 4, 6, 7, 13, 16, 19, 20, 21, 26, 29, 30, 31, 34, 36, 2, 3, 4, 6, 7, 9, 12, 14, 17, 18, 19, 20, 22, 28, 37, 38, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 34, 35, 36, 37, 38, 39, 40, 2, 3, 4, 5, 8, 11, 13, 14, 19, 21, 22, 29, 30, 33, 35, 40, 14, 15, 19, 22, 23, 26, 27, 31, 34, 19, 26, 31, 34, 40, 4, 5, 8, 10, 14, 18, 24, 28, 30, 32, 38, 40, 1, 3, 7, 18, 26, 31, 35, 39, 1, 2, 4, 5, 6, 7, 8, 12, 13, 15, 16, 18, 19, 21, 22, 23, 24, 26, 27, 29, 31, 34, 36, 38, 39, 40, 1, 3, 21, 23, 28, 32, 34, 40, 2, 4, 8, 16, 18, 23, 24, 26, 27, 29, 31, 33, 38, 40, 3, 7, 9, 10, 14, 15, 16, 23, 26, 30, 4, 5, 13, 16, 26, 28, 30, 31, 34, 35, 39, 40, 1, 2, 5, 7, 9, 14, 15, 17, 18, 24, 28, 35, 2, 3, 5, 6, 9, 12, 13, 16, 18, 21, 23, 25, 29, 31, 35, 36, 39, 1, 2, 3, 4, 5, 7, 9, 10, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 33, 2, 4, 5, 16, 19, 23, 27, 30, 32, 35, 40, 2, 4, 7, 18, 21, 22, 29, 40, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 33, 34, 39, 4, 5, 6, 21, 24, 26, 28, 30, 38, 40, 1, 5, 6, 7, 8, 13, 14, 17, 20, 23, 27, 30, 31, 32, 37, 40, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 38, 39, 40, 2, 4, 7, 16, 23, 25, 31, 39, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 31, 32, 34, 36, 38, 40, 1, 2, 4, 9, 16, 24, 27, 28, 29, 34, 38, 39, 40, 1, 4, 5, 7, 8, 9, 13, 16, 19, 21, 22, 24, 25, 28, 35, 36, 1, 2, 4, 5, 8, 9, 10, 11, 13, 15, 18, 19, 20, 23, 24, 25, 26, 31, 32, 33, 40, 1, 7, 8, 9, 10, 14, 15, 17, 23, 24, 33, 39, 40, 4, 5, 6, 8, 23, 28, 2, 8, 11, 19, 21, 22, 25, 29, 31, 39, 40, 3, 9, 12, 20, 21, 23, 25, 26, 28, 31, 32, 4, 5, 8, 9, 11, 16, 18, 19, 20, 24, 27, 29, 30, 31, 33, 37, 40, 4, 6, 14, 17, 27, 28, 40, 4, 6, 10, 14, 15, 19, 26, 28, 29, 31, 32, 36, 40, 1, 4, 6, 8, 13, 15, 16, 17, 19, 20, 23, 27, 28, 30, 32, 33, 36, 38, 40, 1, 3, 5, 6, 7, 13, 14, 15, 16, 18, 19, 20, 21, 22, 26, 31, 33, 34, 35, 37, 38, 39, 2, 5, 7, 12, 14, 18, 21, 24, 27, 28, 34, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 16, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 36, 38, 39, 5, 7, 8, 10, 12, 13, 17, 18, 26, 32, 36, 38, 2, 3, 5, 6, 7, 16, 19, 21, 25, 26, 31, 33, 38, 4, 5, 6, 8, 9, 11, 12, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 31, 32, 35, 38, 40, 2, 6, 7, 8, 18, 21, 26, 34, 36, 38, 1, 9, 15, 16, 18, 20, 26, 30, 31, 34, 40, 2, 4, 8, 14, 26, 33, 6, 8, 13, 14, 17, 24, 26, 30, 32, 33, 37, 39, 6, 12, 13, 14, 15, 22, 31, 32, 36, 7, 10, 16, 17, 19, 26, 30, 31, 32, 34, 36, 40, 1, 11, 12, 14, 16, 34, 5, 7, 11, 14, 18, 24, 34, 1, 4, 5, 6, 17, 18, 19, 20, 25, 26, 29, 31, 36, 38, 3, 7, 15, 20, 21, 23, 25, 26, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 28, 30, 31, 33, 35, 36, 38, 40, 1, 7, 9, 12, 13, 15, 19, 23, 28, 33, 36, 40, 1, 4, 6, 13, 16, 18, 22, 30, 31, 32, 34, 35, 37, 40, 4, 16, 18, 23, 26, 28, 30, 40, 4, 8, 9, 13, 16, 18, 19, 21, 24, 31, 37, 38, 40, 6, 13, 16, 20, 26, 28, 33, 1, 6, 10, 14, 18, 26, 27, 28, 37, 39, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 1, 2, 3, 6, 9, 10, 13, 16, 18, 20, 21, 23, 25, 27, 28, 29, 31, 32, 33, 38, 1, 5, 6, 7, 9, 13, 15, 18, 21, 30, 31, 32, 33, 34, 38, 1, 4, 7, 13, 26, 27, 4, 6, 8, 14, 15, 16, 19, 26, 27, 3, 4, 6, 7, 9, 15, 16, 18, 23, 26, 27, 29, 38, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 37, 38, 39, 2, 4, 6, 7, 8, 13, 15, 16, 18, 19, 23, 27, 28, 30, 34, 35, 36, 39, 1, 2, 3, 4, 5, 7, 8, 10, 12, 13, 15, 17, 18, 19, 20, 22, 23, 24, 25, 28, 29, 30, 32, 33, 34, 36, 37, 38, 1, 4, 6, 8, 9, 10, 12, 15, 16, 19, 20, 21, 23, 25, 26, 28, 30, 31, 32, 33, 35, 37, 5, 21, 22, 28, 1, 5, 6, 10, 11, 12, 13, 17, 19, 28, 35, 36, 40, 4, 6, 7, 10, 12, 13, 21, 26, 27, 38, 2, 11, 15, 16, 20, 22, 25, 29, 30, 35, 36, 2, 4, 8, 13, 19, 22, 25, 27, 31, 35, 37, 40, 1, 4, 6, 11, 12, 13, 15, 20, 21, 22, 26, 27, 35, 39, 2, 4, 13, 16, 28, 31, 33, 3, 33, 1, 2, 3, 4, 8, 9, 13, 15, 16, 18, 21, 24, 26, 28, 31, 33, 35, 37, 38, 39, 40, 2, 3, 5, 7, 9, 15, 18, 23, 25, 26, 28, 33, 34, 36, 37, 39, 40, 1, 4, 5, 9, 10, 14, 19, 21, 23, 26, 29, 33, 36, 38, 1, 2, 3, 4, 12, 13, 16, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 31, 35, 36, 1, 2, 4, 5, 13, 18, 19, 21, 23, 25, 26, 28, 35, 37, 38, 40, 2, 3, 4, 9, 12, 18, 25, 27, 28, 31, 38, 1, 4, 5, 12, 13, 19, 20, 22, 25, 28, 29, 31, 33, 38, 1, 5, 6, 7, 9, 13, 18, 19, 20, 34, 40, 2, 3, 4, 10, 13, 15, 19, 28, 1, 5, 8, 11, 13, 21, 24, 25, 31, 36, 38, 39, 3, 4, 13, 17, 18, 25, 26, 27, 28, 31, 37, 38, 5, 12, 13, 16, 18, 23, 26, 28, 32, 1, 4, 7, 8, 9, 16, 18, 20, 21, 25, 26, 27, 30, 4, 7, 8, 9, 16, 18, 20, 21, 26, 27, 30, 2, 4, 6, 15, 17, 19, 28, 31, 32, 37, 1, 2, 3, 4, 5, 10, 11, 14, 15, 18, 21, 29, 4, 18, 20, 23, 28, 30, 40, 1, 2, 5, 11, 13, 14, 18, 21, 23, 24, 25, 28, 32, 33, 38, 1, 4, 6, 7, 10, 13, 14, 16, 20, 27, 30, 31, 4, 8, 9, 12, 16, 20, 21, 23, 24, 26, 27, 28, 31, 32, 4, 8, 16, 17, 18, 20, 24, 28, 30, 3, 14, 17, 21, 27, 30, 33, 34, 36, 39, 8, 15, 22, 27, 31, 32, 2, 4, 5, 8, 9, 13, 20, 21, 27, 31, 32, 35, 36, 37, 38, 1, 7, 8, 11, 12, 21, 23, 30, 31, 32, 34, 35, 36, 39, 40, 2, 7, 9, 19, 25, 26, 27, 29, 32, 33, 34, 36, 2, 5, 9, 23, 24, 25, 26, 32, 36, 1, 6, 7, 8, 10, 11, 12, 13, 18, 20, 25, 28, 1, 8, 16, 19, 22, 31, 38, 40, 1, 3, 5, 6, 10, 12, 15, 16, 18, 19, 21, 22, 23, 26, 28, 30, 32, 36, 40, 1, 5, 6, 12, 14, 18, 23, 31, 40, 4, 13, 22, 23, 26, 32, 35, 37, 39, 4, 5, 7, 9, 10, 11, 13, 14, 16, 17, 19, 21, 22, 26, 27, 30, 31, 33, 34, 35, 36, 39, 40, 2, 4, 7, 8, 13, 26, 34, 38, 39, 4, 13, 16, 18, 22, 26, 29, 35, 1, 3, 4, 6, 15, 18, 22, 25, 32, 35, 4, 9, 15, 16, 19, 20, 25, 26, 27, 28, 32, 1, 4, 6, 7, 13, 15, 16, 26, 28, 31, 32, 35, 39, 1, 5, 16, 17, 19, 27, 31, 33, 38, 1, 3, 4, 6, 9, 10, 11, 18, 20, 23, 25, 4, 5, 7, 9, 14, 20, 21, 22, 23, 26, 28, 39, 2, 4, 6, 11, 17, 19, 22, 23, 24, 25, 28, 40, 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 32, 34, 36, 37, 38, 39, 40, 1, 3, 5, 6, 7, 8, 20, 21, 22, 23, 25, 26, 27, 35, 36, 37, 40, 4, 8, 9, 12, 19, 20, 21, 28, 30, 4, 16, 24, 25, 28, 30, 31, 33, 37, 4, 6, 8, 9, 18, 19, 25, 28, 29, 32, 40, 1, 7, 16, 23, 25, 26, 28, 33, 35, 37, 39, 40, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 16, 17, 19, 20, 22, 28, 30, 31, 32, 34, 35, 36, 37, 38, 4, 9, 13, 15, 17, 20, 22, 23, 26, 27, 28, 30, 31, 33, 34, 35, 38, 39, 40, 1, 4, 6, 10, 11, 16, 19, 28, 38, 1, 6, 9, 11, 18, 23, 28, 36, 37, 2, 9, 13, 14, 16, 22, 23, 24, 27, 32, 5, 13, 15, 17, 18, 19, 20, 23, 24, 26, 30, 33, 39, 40, 2, 3, 5, 7, 9, 13, 17, 22, 23, 25, 26, 27, 28, 32, 37, 38, 39, 40, 4, 7, 9, 23, 28, 30, 36, 4, 5, 7, 9, 15, 24, 26, 34, 35, 3, 7, 9, 10, 11, 14, 15, 16, 17, 20, 21, 23, 26, 28, 31, 33, 34, 35, 36, 38, 39, 1, 3, 4, 7, 10, 13, 14, 17, 21, 25, 26, 32, 33, 35, 36, 39, 1, 2, 9, 16, 19, 23, 26, 27, 31, 38, 1, 4, 5, 6, 7, 8, 10, 14, 15, 16, 17, 20, 23, 25, 26, 27, 28, 32, 33, 36, 38, 40, 4, 6, 9, 10, 12, 14, 17, 18, 20, 21, 22, 24, 28, 36, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 4, 6, 7, 9, 10, 13, 20, 25, 26, 30, 33, 35, 37, 40, 2, 3, 5, 6, 7, 13, 16, 18, 19, 21, 24, 25, 26, 31, 32, 36, 37, 38, 39, 40, 1, 2, 3, 4, 5, 7, 9, 15, 16, 18, 20, 22, 25, 26, 27, 29, 31, 36, 39, 1, 2, 4, 5, 6, 9, 23, 26, 31, 37, 38, 39, 1, 2, 4, 5, 6, 8, 9, 15, 16, 18, 19, 20, 31, 32, 37, 1, 2, 5, 7, 9, 11, 12, 13, 15, 18, 23, 26, 27, 31, 37, 38, 40, 6, 9, 16, 22, 36, 40, 2, 3, 4, 7, 9, 10, 12, 13, 17, 18, 19, 20, 22, 23, 25, 28, 29, 31, 32, 33, 36, 38, 40, 2, 6, 9, 13, 18, 20, 22, 27, 28, 30, 31, 33, 34, 38, 2, 4, 7, 8, 12, 14, 16, 19, 20, 22, 23, 24, 28, 31, 33, 37, 38, 5, 11, 25, 28, 37, 39, 8, 15, 18, 21, 24, 26, 28, 29, 40, 2, 5, 7, 9, 10, 13, 17, 18, 19, 23, 27, 28, 30, 31, 38, 40, 1, 2, 3, 4, 5, 7, 9, 10, 13, 16, 18, 19, 20, 22, 23, 24, 25, 27, 30, 31, 38, 39, 40, 2, 4, 6, 7, 18, 19, 20, 26, 28, 29, 31, 39, 8, 18, 20, 25, 36, 39, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 17, 18, 20, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 36, 38, 39, 40, 7, 10, 19, 20, 22, 25, 26, 28, 30, 36, 40, 3, 6, 7, 16, 21, 25, 30, 31, 32, 2, 4, 16, 20, 26, 28, 31, 34, 1, 2, 3, 4, 5, 6, 9, 13, 15, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 31, 33, 35, 38, 2, 4, 7, 11, 17, 22, 28, 29, 32, 33, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 28, 30, 31, 32, 36, 38, 40, 9, 16, 20, 31, 32, 40, 6, 9, 10, 16, 24, 25, 28, 30, 32, 6, 9, 10, 24, 28, 32, 4, 8, 9, 12, 13, 15, 16, 18, 19, 21, 22, 29, 31, 32, 36, 40, 1, 4, 5, 8, 12, 14, 15, 16, 18, 20, 21, 24, 25, 29, 31, 33, 34, 35, 38, 39, 40, 7, 8, 9, 11, 16, 20, 39, 40, 1, 2, 5, 6, 7, 8, 9, 15, 16, 18, 19, 20, 23, 25, 26, 28, 30, 31, 32, 33, 38, 40, 6, 7, 8, 14, 16, 18, 23, 26, 27, 28, 30, 31, 36, 37, 1, 3, 4, 5, 7, 13, 14, 15, 16, 18, 24, 25, 30, 34, 36, 37, 40, 4, 8, 16, 21, 24, 26, 31, 32, 38, 40, 3, 4, 5, 8, 10, 13, 15, 16, 26, 27, 28, 30, 36, 39, 40, 1, 2, 3, 4, 5, 7, 8, 9, 10, 13, 15, 16, 17, 20, 21, 22, 23, 25, 27, 28, 30, 31, 33, 36, 40, 3, 5, 11, 12, 13, 14, 17, 18, 19, 21, 22, 24, 26, 27, 30, 31, 32, 33, 37, 39, 6, 14, 17, 29, 30, 32, 35, 3, 4, 9, 10, 14, 16, 30, 31, 33, 3, 6, 14, 18, 27, 32, 33, 35, 39, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 2, 3, 4, 9, 10, 12, 13, 15, 16, 17, 19, 20, 24, 25, 28, 30, 32, 33, 34, 35, 39, 2, 4, 5, 6, 7, 16, 18, 27, 28, 30, 39, 40, 2, 5, 6, 7, 16, 17, 20, 21, 28, 30, 40, 2, 4, 6, 7, 9, 10, 11, 13, 15, 17, 21, 22, 24, 25, 26, 28, 32, 37, 4, 5, 7, 8, 13, 20, 26, 28, 36, 1, 2, 7, 9, 10, 12, 32, 1, 3, 6, 7, 8, 9, 14, 15, 16, 17, 19, 20, 23, 26, 27, 28, 31, 32, 33, 35, 36, 39, 4, 7, 10, 17, 20, 26, 27, 28, 30, 32, 36, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 36, 37, 38, 39, 1, 9, 16, 17, 19, 20, 23, 27, 32, 38, 1, 6, 38, 40, 7, 19, 38, 2, 3, 7, 9, 11, 14, 15, 16, 17, 19, 20, 22, 25, 26, 30, 32, 33, 37, 38, 39, 1, 4, 5, 7, 9, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 25, 28, 30, 31, 34, 38, 39, 40, 2, 4, 5, 8, 9, 10, 11, 13, 16, 18, 19, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 34, 35, 38, 39, 40, 2, 3, 4, 5, 7, 8, 13, 14, 16, 18, 19, 26, 27, 35, 37, 40, 3, 4, 5, 6, 8, 9, 12, 13, 16, 18, 19, 21, 27, 32, 38, 40, 1, 6, 7, 8, 9, 20, 21, 24, 28, 30, 31, 34, 35, 37, 38, 40, 1, 3, 4, 6, 7, 9, 11, 15, 16, 19, 20, 21, 23, 25, 26, 28, 30, 31, 33, 34, 36, 38, 40, 1, 3, 4, 5, 7, 8, 9, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 40, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 32, 35, 37, 39, 40, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 18, 19, 20, 21, 25, 26, 28, 30, 31, 36, 39, 40, 1, 2, 3, 4, 5, 7, 8, 9, 10, 14, 18, 20, 22, 23, 25, 26, 27, 30, 31, 33, 37, 38, 39, 40, 2, 4, 5, 12, 15, 16, 18, 19, 25, 33, 34, 37, 1, 2, 3, 5, 7, 13, 14, 15, 16, 18, 21, 22, 25, 26, 27, 30, 31, 33, 34, 35, 37, 2, 5, 6, 7, 9, 13, 14, 18, 21, 25, 26, 28, 29, 30, 31, 34, 35, 37, 39, 2, 4, 5, 16, 23, 30, 31, 32, 34, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 6, 8, 9, 13, 15, 20, 30, 31, 35, 1, 2, 4, 5, 6, 7, 9, 10, 13, 14, 17, 18, 19, 23, 26, 31, 32, 35, 36, 39, 40, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 1, 2, 3, 4, 7, 10, 14, 15, 16, 19, 20, 22, 25, 26, 28, 29, 30, 31, 35, 40, 1, 2, 4, 6, 7, 8, 12, 13, 14, 16, 17, 18, 20, 21, 25, 26, 27, 29, 30, 31, 32, 38, 40, 2, 3, 4, 7, 11, 18, 20, 21, 23, 25, 27, 28, 31, 32, 34, 36, 38, 39, 2, 3, 4, 5, 6, 7, 11, 12, 16, 17, 18, 20, 24, 26, 32, 33, 34, 35, 38, 2, 4, 6, 9, 10, 11, 13, 15, 18, 20, 21, 22, 23, 24, 28, 29, 31, 35, 39, 40, 1, 4, 6, 10, 12, 13, 15, 20, 22, 25, 27, 28, 33, 35, 39, 4, 6, 8, 13, 16, 28, 29, 1, 3, 7, 8, 9, 11, 14, 15, 21, 28, 33, 36, 38, 1, 8, 9, 12, 13, 16, 17, 19, 24, 25, 36, 39, 4, 5, 7, 8, 9, 15, 19, 23, 4, 5, 6, 7, 9, 10, 12, 13, 16, 19, 20, 21, 23, 28, 31, 32, 40, 8, 12, 20, 31, 40, 6, 21, 22, 34, 35, 40, 1, 4, 6, 7, 9, 10, 13, 15, 20, 21, 23, 24, 28, 30, 31, 34, 35, 40, 1, 2, 3, 4, 8, 22, 23, 25, 31, 33, 35, 3, 4, 5, 7, 21, 22, 23, 26, 27, 28, 30, 31, 32, 40, 4, 27, 32, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 35, 36, 37, 38, 39, 40, 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 16, 21, 22, 25, 26, 30, 31, 35, 36, 38, 39, 40, 1, 13, 14, 17, 23, 3, 7, 10, 13, 15, 16, 18, 19, 21, 28, 30, 31, 39, 2, 5, 6, 9, 13, 14, 17, 18, 23, 26, 28, 30, 31, 34, 39, 2, 4, 7, 8, 9, 13, 19, 21, 33, 40, 4, 10, 18, 20, 21, 23, 24, 25, 26, 27, 28, 31, 32, 34, 3, 4, 6, 10, 16, 17, 30, 31, 8, 18, 25, 38, 4, 5, 7, 9, 10, 12, 15, 18, 20, 26, 27, 28, 33, 34, 36, 38, 39, 40, 2, 6, 7, 16, 18, 27, 28, 31, 40, 1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 16, 17, 20, 21, 23, 25, 27, 28, 32, 34, 36, 38, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 36, 37, 38, 39, 40, 1, 2, 5, 6, 7, 10, 17, 19, 20, 23, 25, 29, 30, 31, 38, 40, 1, 8, 9, 10, 11, 12, 19, 22, 28, 32, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 5, 6, 16, 26, 27, 29, 32, 34, 3, 6, 9, 10, 12, 16, 20, 25, 28, 30, 33, 40, 1, 2, 13, 18, 22, 23, 29, 30, 31, 39, 40, 2, 4, 5, 17, 18, 32, 36, 6, 17, 18, 21, 23, 27, 31, 35, 2, 4, 7, 8, 13, 14, 16, 22, 25, 27, 30, 31, 32, 36, 4, 6, 9, 12, 13, 14, 20, 24, 25, 26, 27, 30, 33, 39, 2, 3, 7, 10, 14, 15, 22, 23, 26, 31, 36, 40, 1, 3, 4, 5, 10, 11, 12, 13, 14, 19, 21, 22, 26, 27, 29, 31, 36, 37, 1, 2, 4, 5, 6, 7, 13, 18, 19, 20, 23, 24, 25, 26, 28, 31, 36, 37, 38, 3, 4, 8, 10, 21, 28, 36, 4, 5, 8, 12, 18, 19, 20, 22, 23, 26, 28, 39, 2, 5, 6, 7, 8, 9, 13, 16, 22, 23, 24, 25, 26, 28, 29, 32, 38, 39, 2, 5, 7, 10, 14, 16, 19, 25, 26, 28, 31, 32, 2, 4, 6, 7, 8, 9, 10, 11, 13, 14, 16, 18, 19, 20, 21, 22, 24, 27, 30, 35, 1, 2, 3, 4, 13, 16, 18, 28, 34, 35, 36, 38, 39, 2, 3, 4, 5, 7, 8, 10, 11, 13, 16, 17, 18, 19, 20, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 2, 3, 5, 7, 9, 15, 22, 23, 26, 28, 33, 34, 36, 37, 39, 40, 2, 3, 4, 6, 7, 9, 14, 15, 18, 21, 22, 24, 26, 29, 30, 31, 33, 36, 37, 38, 39, 2, 3, 4, 5, 6, 9, 10, 11, 13, 16, 17, 18, 20, 26, 27, 28, 29, 30, 31, 35, 36, 40, 7, 9, 10, 13, 16, 17, 18, 21, 22, 24, 28, 31, 36, 38, 1, 2, 5, 6, 7, 10, 16, 19, 25, 26, 28, 29, 31, 32, 2, 3, 5, 6, 8, 9, 10, 13, 16, 18, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 36, 38, 39, 40, 4, 5, 6, 7, 8, 9, 11, 14, 15, 16, 18, 19, 20, 25, 26, 28, 29, 30, 31, 34, 35, 39, 40, 1, 4, 6, 9, 16, 18, 25, 26, 31, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 3, 4, 5, 7, 9, 13, 14, 16, 17, 18, 19, 20, 24, 26, 31, 32, 34, 36, 40, 4, 7, 8, 11, 21, 23, 25, 31, 39, 40, 6, 12, 13, 18, 19, 23, 25, 26, 31, 32, 36, 1, 4, 5, 6, 7, 9, 12, 13, 14, 16, 18, 24, 25, 27, 29, 30, 31, 33, 34, 37, 39, 1, 3, 6, 7, 8, 9, 10, 17, 18, 20, 21, 25, 26, 28, 29, 33, 37, 38, 40, 1, 5, 7, 8, 15, 16, 18, 19, 21, 23, 24, 27, 28, 30, 32, 36, 37, 38, 39, 1, 2, 7, 8, 9, 10, 14, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 31, 32, 33, 36, 38, 40, 1, 9, 19, 27, 38, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 18, 19, 20, 23, 24, 25, 26, 28, 30, 31, 32, 34, 36, 37, 38, 39, 2, 4, 7, 9, 11, 17, 19, 25, 28, 31, 32, 34, 1, 4, 5, 6, 7, 22, 25, 26, 31, 32, 35, 39, 1, 5, 6, 11, 13, 17, 19, 26, 30, 33, 34, 2, 3, 5, 9, 12, 18, 21, 24, 26, 29, 31], \"Freq\": [0.029754416394897256, 0.037516438063131326, 0.10608096279919892, 0.07244553557018463, 0.020698724448624178, 0.027167075838819235, 0.01940505417058517, 0.007762021668234067, 0.023286065004702202, 0.010349362224312089, 0.001293670278039011, 0.015524043336468134, 0.025873405560780223, 0.007762021668234067, 0.056921492233716495, 0.06597718417998957, 0.01940505417058517, 0.007762021668234067, 0.037516438063131326, 0.014230373058429124, 0.018111383892546157, 0.0349290975070533, 0.0038810108341170335, 0.04915947056548243, 0.05304048139959946, 0.007762021668234067, 0.018111383892546157, 0.0038810108341170335, 0.007762021668234067, 0.02199239472666319, 0.007762021668234067, 0.011643032502351101, 0.024579735282741214, 0.04010377861920935, 0.016817713614507145, 0.009055691946273078, 0.014230373058429124, 0.0349290975070533, 0.014230373058429124, 0.041139605840988316, 0.08227921168197663, 0.08227921168197663, 0.12341881752296495, 0.12341881752296495, 0.28797724088691823, 0.16455842336395327, 0.024559884742667796, 0.07367965422800338, 0.09823953897067118, 0.09823953897067118, 0.12279942371333898, 0.19647907794134237, 0.04911976948533559, 0.024559884742667796, 0.07367965422800338, 0.07367965422800338, 0.04911976948533559, 0.024559884742667796, 0.024559884742667796, 0.04911976948533559, 0.07108150830368698, 0.015795890734152662, 0.007897945367076331, 0.023693836101228993, 0.039489726835381656, 0.07108150830368698, 0.04738767220245799, 0.13426507124029763, 0.015795890734152662, 0.04738767220245799, 0.015795890734152662, 0.07897945367076331, 0.031591781468305324, 0.007897945367076331, 0.023693836101228993, 0.007897945367076331, 0.08687739903783964, 0.015795890734152662, 0.015795890734152662, 0.1500609619744503, 0.015795890734152662, 0.039489726835381656, 0.04738767220245799, 0.015358259181541392, 0.07679129590770696, 0.030716518363082784, 0.015358259181541392, 0.706479922350904, 0.015358259181541392, 0.030716518363082784, 0.030716518363082784, 0.015358259181541392, 0.015358259181541392, 0.046074777544624176, 0.038334824401844804, 0.038334824401844804, 0.2300089464110688, 0.15333929760737922, 0.038334824401844804, 0.1150044732055344, 0.1150044732055344, 0.038334824401844804, 0.1150044732055344, 0.07666964880368961, 0.019241666393433193, 0.2501416631146315, 0.028862499590149786, 0.009620833196716597, 0.05772499918029957, 0.019241666393433193, 0.028862499590149786, 0.07696666557373277, 0.13469166475403235, 0.028862499590149786, 0.11544999836059915, 0.08658749877044937, 0.12507083155731574, 0.009620833196716597, 0.05956731315008185, 0.019855771050027286, 0.07942308420010914, 0.009927885525013643, 0.009927885525013643, 0.019855771050027286, 0.029783656575040927, 0.03971154210005457, 0.1886298249752592, 0.04963942762506821, 0.009927885525013643, 0.029783656575040927, 0.009927885525013643, 0.3077644512754229, 0.12906251182517736, 0.0636765977988302, 0.0955148966982453, 0.0318382988994151, 0.22286809229590568, 0.0636765977988302, 0.0636765977988302, 0.0318382988994151, 0.0636765977988302, 0.0955148966982453, 0.1273531955976604, 0.0955148966982453, 0.08931506017438755, 0.20840180707357095, 0.08931506017438755, 0.1786301203487751, 0.08931506017438755, 0.02977168672479585, 0.0595433734495917, 0.02977168672479585, 0.1190867468991834, 0.08931506017438755, 0.02977168672479585, 0.03398008713669218, 0.06796017427338436, 0.10194026141007655, 0.06796017427338436, 0.10194026141007655, 0.10194026141007655, 0.06796017427338436, 0.03398008713669218, 0.16990043568346092, 0.03398008713669218, 0.03398008713669218, 0.03398008713669218, 0.06796017427338436, 0.06796017427338436, 0.03696714293306364, 0.03696714293306364, 0.11090142879919092, 0.14786857173225457, 0.14786857173225457, 0.03696714293306364, 0.11090142879919092, 0.14786857173225457, 0.11090142879919092, 0.07393428586612728, 0.0736079253755318, 0.058886340300425444, 0.029443170150212722, 0.13249426567595723, 0.11777268060085089, 0.058886340300425444, 0.08832951045063817, 0.014721585075106361, 0.0736079253755318, 0.26498853135191447, 0.014721585075106361, 0.058886340300425444, 0.03378525424262235, 0.20271152545573412, 0.0675705084852447, 0.03378525424262235, 0.6419198306098247, 0.0259904546289182, 0.0259904546289182, 0.07797136388675459, 0.0259904546289182, 0.12995227314459099, 0.07797136388675459, 0.0259904546289182, 0.12995227314459099, 0.1039618185156728, 0.0259904546289182, 0.0259904546289182, 0.0259904546289182, 0.07797136388675459, 0.0519809092578364, 0.12995227314459099, 0.0259904546289182, 0.08923941895799153, 0.1338591284369873, 0.1338591284369873, 0.22309854739497884, 0.31233796635297034, 0.04461970947899577, 0.03997171141553133, 0.019985855707765666, 0.059957567123297, 0.09992927853882834, 0.03997171141553133, 0.019985855707765666, 0.019985855707765666, 0.15988684566212533, 0.019985855707765666, 0.179872701369891, 0.059957567123297, 0.07994342283106266, 0.119915134246594, 0.03997171141553133, 0.08050026518347901, 0.24150079555043702, 0.6171686997400058, 0.1517692773042827, 0.018971159663035336, 0.11382695797821203, 0.03794231932607067, 0.018971159663035336, 0.0948557983151767, 0.056913478989106016, 0.11382695797821203, 0.03794231932607067, 0.018971159663035336, 0.018971159663035336, 0.056913478989106016, 0.07588463865214135, 0.0948557983151767, 0.056913478989106016, 0.04831636970497655, 0.0966327394099531, 0.0966327394099531, 0.0966327394099531, 0.04831636970497655, 0.04831636970497655, 0.14494910911492964, 0.0966327394099531, 0.04831636970497655, 0.04831636970497655, 0.1932654788199062, 0.10201264506752401, 0.23802950515755603, 0.13601686009003203, 0.23802950515755603, 0.06800843004501601, 0.03400421502250801, 0.13601686009003203, 0.03400421502250801, 0.17464868734194108, 0.02757610852767491, 0.07353628940713308, 0.009192036175891635, 0.5239460620258233, 0.009192036175891635, 0.14707257881426616, 0.02757610852767491, 0.08446103213412365, 0.042230516067061824, 0.08446103213412365, 0.042230516067061824, 0.12669154820118547, 0.12669154820118547, 0.3378441285364946, 0.12669154820118547, 0.010979983798317662, 0.0036599945994392207, 0.0036599945994392207, 0.0878398703865413, 0.012809981098037273, 0.04391993519327065, 0.010979983798317662, 0.05672991629130792, 0.0018299972997196103, 0.0018299972997196103, 0.007319989198878441, 0.007319989198878441, 0.009149986498598052, 0.1775097380728022, 0.0018299972997196103, 0.04025994059383143, 0.021959967596635325, 0.09515985958541974, 0.014639978397756883, 0.0018299972997196103, 0.005489991899158831, 0.1958097110699983, 0.0018299972997196103, 0.06404990549018637, 0.007319989198878441, 0.04940992709242948, 0.0036599945994392207, 0.014639978397756883, 0.012809981098037273, 0.0036599945994392207, 0.027449959495794155, 0.08736489438907949, 0.06989191551126359, 0.026209468316723845, 0.07862840495017154, 0.03494595775563179, 0.05241893663344769, 0.05241893663344769, 0.008736489438907948, 0.008736489438907948, 0.043682447194539745, 0.07862840495017154, 0.043682447194539745, 0.017472978877815896, 0.043682447194539745, 0.026209468316723845, 0.10483787326689538, 0.026209468316723845, 0.017472978877815896, 0.008736489438907948, 0.026209468316723845, 0.12231085214471128, 0.06393265897929054, 0.03196632948964527, 0.06393265897929054, 0.12786531795858108, 0.15983164744822634, 0.0958989884689358, 0.03196632948964527, 0.06393265897929054, 0.06393265897929054, 0.03196632948964527, 0.15983164744822634, 0.0958989884689358, 0.04836053442788695, 0.04836053442788695, 0.0967210688557739, 0.04836053442788695, 0.04836053442788695, 0.04836053442788695, 0.1934421377115478, 0.0967210688557739, 0.04836053442788695, 0.24180267213943477, 0.04836053442788695, 0.12437072746505341, 0.041456909155017804, 0.041456909155017804, 0.12437072746505341, 0.16582763662007122, 0.16582763662007122, 0.16582763662007122, 0.12437072746505341, 0.004922006390429579, 0.049220063904295785, 0.03445404473300705, 0.10336213419902116, 0.03445404473300705, 0.014766019171288735, 0.02953203834257747, 0.03937605112343663, 0.08859611502773242, 0.014766019171288735, 0.02953203834257747, 0.014766019171288735, 0.004922006390429579, 0.07875210224687326, 0.024610031952147893, 0.004922006390429579, 0.009844012780859158, 0.009844012780859158, 0.024610031952147893, 0.02953203834257747, 0.03445404473300705, 0.049220063904295785, 0.06398608307558452, 0.009844012780859158, 0.02953203834257747, 0.04429805751386621, 0.03937605112343663, 0.04429805751386621, 0.024610031952147893, 0.019688025561718316, 0.052697358596867444, 0.1405262895916465, 0.08782893099477908, 0.03513157239791163, 0.052697358596867444, 0.052697358596867444, 0.10539471719373489, 0.052697358596867444, 0.03513157239791163, 0.052697358596867444, 0.017565786198955814, 0.052697358596867444, 0.15809207579060233, 0.03513157239791163, 0.03513157239791163, 0.03513157239791163, 0.16496818156301382, 0.08248409078150691, 0.027494696927168973, 0.054989393854337945, 0.027494696927168973, 0.16496818156301382, 0.054989393854337945, 0.027494696927168973, 0.08248409078150691, 0.027494696927168973, 0.08248409078150691, 0.10997878770867589, 0.08248409078150691, 0.06712917023896588, 0.09589881462709411, 0.05753928877625646, 0.01917976292541882, 0.00958988146270941, 0.00958988146270941, 0.01917976292541882, 0.00958988146270941, 0.06712917023896588, 0.12466845901522235, 0.06712917023896588, 0.30687620680670114, 0.10548869608980352, 0.01917976292541882, 0.00958988146270941, 0.1270894526094776, 0.158861815761847, 0.0635447263047388, 0.0317723631523694, 0.0635447263047388, 0.0635447263047388, 0.1270894526094776, 0.2541789052189552, 0.0317723631523694, 0.036747839399139906, 0.018373919699569953, 0.09186959849784976, 0.09186959849784976, 0.036747839399139906, 0.018373919699569953, 0.018373919699569953, 0.018373919699569953, 0.14699135759655962, 0.1102435181974197, 0.036747839399139906, 0.07349567879827981, 0.036747839399139906, 0.036747839399139906, 0.036747839399139906, 0.14699135759655962, 0.036747839399139906, 0.029811365950243297, 0.029811365950243297, 0.029811365950243297, 0.1788681957014598, 0.05962273190048659, 0.14905682975121648, 0.029811365950243297, 0.0894340978507299, 0.0894340978507299, 0.11924546380097319, 0.0894340978507299, 0.029811365950243297, 0.029811365950243297, 0.029994184535906278, 0.003332687170656253, 0.07331911775443757, 0.049990307559843794, 0.003332687170656253, 0.003332687170656253, 0.019996123023937516, 0.03332687170656253, 0.11664405097296886, 0.02332881019459377, 0.009998061511968758, 0.003332687170656253, 0.009998061511968758, 0.03332687170656253, 0.036659558877218786, 0.05332299473050005, 0.02332881019459377, 0.013330748682625012, 0.006665374341312506, 0.019996123023937516, 0.013330748682625012, 0.036659558877218786, 0.006665374341312506, 0.11997673814362511, 0.009998061511968758, 0.04665762038918754, 0.036659558877218786, 0.13997286116756263, 0.016663435853281266, 0.171482736819646, 0.1286120526147345, 0.0428706842049115, 0.21435342102455748, 0.0428706842049115, 0.085741368409823, 0.0428706842049115, 0.1286120526147345, 0.0428706842049115, 0.0428706842049115, 0.07999742554566733, 0.02666580851522244, 0.8266400639718957, 0.02666580851522244, 0.09434316495179551, 0.031447721650598504, 0.031447721650598504, 0.22013405155418952, 0.09434316495179551, 0.031447721650598504, 0.06289544330119701, 0.06289544330119701, 0.22013405155418952, 0.09434316495179551, 0.10541739846159796, 0.05856522136755442, 0.03513913282053265, 0.12884348700861972, 0.03513913282053265, 0.011713044273510885, 0.03513913282053265, 0.03513913282053265, 0.05856522136755442, 0.03513913282053265, 0.04685217709404354, 0.02342608854702177, 0.03513913282053265, 0.011713044273510885, 0.11713044273510884, 0.02342608854702177, 0.08199130991457619, 0.08199130991457619, 0.03513913282053265, 0.0860578804040592, 0.0430289402020296, 0.0860578804040592, 0.1290868206060888, 0.0430289402020296, 0.0430289402020296, 0.0860578804040592, 0.0860578804040592, 0.1290868206060888, 0.1721157608081184, 0.0430289402020296, 0.04559339091765998, 0.005065932324184442, 0.010131864648368884, 0.11651644345624217, 0.005065932324184442, 0.005065932324184442, 0.050659323241844426, 0.06585712021439775, 0.015197796972553327, 0.08105491718695107, 0.010131864648368884, 0.015197796972553327, 0.020263729296737767, 0.030395593945106655, 0.1317142404287955, 0.040527458593475535, 0.07598898486276663, 0.010131864648368884, 0.050659323241844426, 0.055725255566028864, 0.010131864648368884, 0.030395593945106655, 0.035461526269291097, 0.010131864648368884, 0.020263729296737767, 0.015197796972553327, 0.030395593945106655, 0.07240949399368792, 0.02896379759747517, 0.02896379759747517, 0.05792759519495034, 0.043445696396212756, 0.043445696396212756, 0.07240949399368792, 0.014481898798737585, 0.043445696396212756, 0.02896379759747517, 0.014481898798737585, 0.11585519038990068, 0.014481898798737585, 0.043445696396212756, 0.05792759519495034, 0.014481898798737585, 0.043445696396212756, 0.014481898798737585, 0.11585519038990068, 0.08689139279242551, 0.061302527952424885, 0.2145588478334871, 0.030651263976212442, 0.061302527952424885, 0.061302527952424885, 0.15325631988106223, 0.2145588478334871, 0.12260505590484977, 0.030651263976212442, 0.05862484636211939, 0.029312423181059696, 0.029312423181059696, 0.26381180862953724, 0.05862484636211939, 0.14656211590529847, 0.029312423181059696, 0.029312423181059696, 0.08793726954317908, 0.029312423181059696, 0.17587453908635817, 0.020009959556074205, 0.017786630716510404, 0.04001991911214841, 0.05335989214953121, 0.031126603753893207, 0.008893315358255202, 0.015563301876946604, 0.013339973037382803, 0.08670982474298822, 0.0022233288395638005, 0.006669986518691402, 0.006669986518691402, 0.017786630716510404, 0.026679946074765606, 0.037796590272584606, 0.05113656330996741, 0.028903274914329407, 0.013339973037382803, 0.07114652286604162, 0.006669986518691402, 0.07559318054516921, 0.015563301876946604, 0.0022233288395638005, 0.020009959556074205, 0.04001991911214841, 0.12005975733644522, 0.026679946074765606, 0.028903274914329407, 0.0022233288395638005, 0.022233288395638005, 0.03557326143302081, 0.013339973037382803, 0.031126603753893207, 0.013339973037382803, 0.0871014908523771, 0.021775372713094276, 0.04355074542618855, 0.021775372713094276, 0.0871014908523771, 0.06532611813928284, 0.13065223627856568, 0.15242760899165994, 0.021775372713094276, 0.06532611813928284, 0.06532611813928284, 0.06532611813928284, 0.021775372713094276, 0.021775372713094276, 0.0871014908523771, 0.03959872770652354, 0.013199575902174514, 0.07919745541304708, 0.013199575902174514, 0.013199575902174514, 0.11879618311957063, 0.0923970313152216, 0.0923970313152216, 0.013199575902174514, 0.013199575902174514, 0.013199575902174514, 0.052798303608698055, 0.013199575902174514, 0.32998939755436285, 0.013199575902174514, 0.052798303608698055, 0.013199575902174514, 0.28119171845471375, 0.040170245493530536, 0.040170245493530536, 0.08034049098706107, 0.1205107364805916, 0.1205107364805916, 0.040170245493530536, 0.08034049098706107, 0.040170245493530536, 0.040170245493530536, 0.040170245493530536, 0.040170245493530536, 0.030893068170930075, 0.06178613634186015, 0.09267920451279023, 0.015446534085465037, 0.046339602256395114, 0.046339602256395114, 0.015446534085465037, 0.046339602256395114, 0.046339602256395114, 0.2780376135383707, 0.09267920451279023, 0.030893068170930075, 0.07723267042732519, 0.046339602256395114, 0.015446534085465037, 0.030893068170930075, 0.2758208935747834, 0.09194029785826113, 0.030646765952753712, 0.030646765952753712, 0.21452736166927597, 0.18388059571652227, 0.030646765952753712, 0.09194029785826113, 0.025948504812624895, 0.05189700962524979, 0.05189700962524979, 0.025948504812624895, 0.18163953368837427, 0.025948504812624895, 0.025948504812624895, 0.025948504812624895, 0.18163953368837427, 0.025948504812624895, 0.20758803850099916, 0.10379401925049958, 0.3397884045701142, 0.08494710114252856, 0.04247355057126428, 0.12742065171379285, 0.04247355057126428, 0.04247355057126428, 0.12742065171379285, 0.12742065171379285, 0.1011179297882065, 0.0202235859576413, 0.0404471719152826, 0.12134151574584778, 0.26290661744933685, 0.0202235859576413, 0.0202235859576413, 0.12134151574584778, 0.1415651017034891, 0.1011179297882065, 0.0202235859576413, 0.10009361373268637, 0.033364537910895456, 0.033364537910895456, 0.10009361373268637, 0.06672907582179091, 0.06672907582179091, 0.06672907582179091, 0.2335517653762682, 0.033364537910895456, 0.13345815164358182, 0.033364537910895456, 0.033364537910895456, 0.033364537910895456, 0.06185080496511234, 0.03092540248255617, 0.12370160993022468, 0.09277620744766851, 0.03092540248255617, 0.12370160993022468, 0.2164778173778932, 0.03092540248255617, 0.03092540248255617, 0.09277620744766851, 0.06185080496511234, 0.03092540248255617, 0.03092540248255617, 0.23348528681578287, 0.038914214469297145, 0.07782842893859429, 0.038914214469297145, 0.038914214469297145, 0.038914214469297145, 0.038914214469297145, 0.11674264340789144, 0.1945710723464857, 0.11674264340789144, 0.038914214469297145, 0.1606091324340797, 0.010038070777129982, 0.040152283108519926, 0.020076141554259963, 0.030114212331389945, 0.10038070777129982, 0.05019035388564991, 0.010038070777129982, 0.020076141554259963, 0.17064720321120969, 0.020076141554259963, 0.08030456621703985, 0.010038070777129982, 0.010038070777129982, 0.05019035388564991, 0.030114212331389945, 0.040152283108519926, 0.10038070777129982, 0.020076141554259963, 0.06002697588233918, 0.06002697588233918, 0.07203237105880701, 0.09604316141174268, 0.06002697588233918, 0.10804855658821051, 0.06002697588233918, 0.2040917179999532, 0.036016185529403506, 0.036016185529403506, 0.10804855658821051, 0.012005395176467835, 0.06002697588233918, 0.02401079035293567, 0.0793673267352506, 0.026455775578416867, 0.1587346534705012, 0.18519042904891808, 0.0793673267352506, 0.21164620462733494, 0.0793673267352506, 0.1587346534705012, 0.01724954272348719, 0.051748628170461566, 0.03449908544697438, 0.10349725634092313, 0.01724954272348719, 0.01724954272348719, 0.13799634178789752, 0.03449908544697438, 0.01724954272348719, 0.03449908544697438, 0.01724954272348719, 0.03449908544697438, 0.10349725634092313, 0.03449908544697438, 0.01724954272348719, 0.08624771361743595, 0.051748628170461566, 0.01724954272348719, 0.06899817089394876, 0.051748628170461566, 0.01724954272348719, 0.15455191801523532, 0.03863797950380883, 0.03863797950380883, 0.42501777454189715, 0.11591393851142649, 0.07727595900761766, 0.03863797950380883, 0.11591393851142649, 0.01727568646427295, 0.01727568646427295, 0.12092980524991065, 0.345513729285459, 0.01727568646427295, 0.01727568646427295, 0.05182705939281885, 0.01727568646427295, 0.36278941574973195, 0.02367074621831509, 0.00443826491593408, 0.01775305966373632, 0.0443826491593408, 0.011835373109157546, 0.0636151304617218, 0.025150167856959785, 0.03698554096611733, 0.06213570882307712, 0.01775305966373632, 0.020711902941025706, 0.02367074621831509, 0.005917686554578773, 0.00443826491593408, 0.0014794216386446932, 0.005917686554578773, 0.0665739737390112, 0.011835373109157546, 0.00887652983186816, 0.02367074621831509, 0.07692992520952405, 0.02662958949560448, 0.00887652983186816, 0.005917686554578773, 0.03254727605018325, 0.02367074621831509, 0.054738600629853654, 0.011835373109157546, 0.020711902941025706, 0.00443826491593408, 0.0443826491593408, 0.0029588432772893864, 0.03402669768882795, 0.028109011134249174, 0.025150167856959785, 0.011835373109157546, 0.07397108193223466, 0.03254727605018325, 0.03125805511975659, 0.007814513779939148, 0.011721770669908722, 0.04688708267963489, 0.019536284449847872, 0.007814513779939148, 0.03516531200972617, 0.04688708267963489, 0.09377416535926977, 0.011721770669908722, 0.04688708267963489, 0.003907256889969574, 0.07814513779939149, 0.02735079822978702, 0.007814513779939148, 0.03125805511975659, 0.003907256889969574, 0.015629027559878297, 0.003907256889969574, 0.007814513779939148, 0.06642336712948275, 0.015629027559878297, 0.05470159645957404, 0.003907256889969574, 0.011721770669908722, 0.03516531200972617, 0.019536284449847872, 0.02735079822978702, 0.03516531200972617, 0.06251611023951319, 0.03125805511975659, 0.019536284449847872, 0.07814513779939149, 0.11311221404183533, 0.028278053510458832, 0.056556107020917665, 0.028278053510458832, 0.16966832106275298, 0.08483416053137649, 0.028278053510458832, 0.056556107020917665, 0.028278053510458832, 0.16966832106275298, 0.028278053510458832, 0.11311221404183533, 0.08483416053137649, 0.04764120971340884, 0.07146181457011326, 0.16674423399693095, 0.11910302428352211, 0.11910302428352211, 0.04764120971340884, 0.04764120971340884, 0.02382060485670442, 0.07146181457011326, 0.04764120971340884, 0.07146181457011326, 0.02382060485670442, 0.09528241942681769, 0.03948687822634116, 0.025870713320706274, 0.03812526173577767, 0.11709901818845998, 0.03540202875465069, 0.032678795773523714, 0.0571878926036665, 0.0231474803395793, 0.02450909683014279, 0.017701014377325346, 0.004084849471690464, 0.009531315433944417, 0.0571878926036665, 0.0027232329811269764, 0.006808082452817441, 0.025870713320706274, 0.004084849471690464, 0.03676364524521418, 0.010892931924507906, 0.004084849471690464, 0.009531315433944417, 0.006808082452817441, 0.006808082452817441, 0.012254548415071394, 0.03540202875465069, 0.027232329811269763, 0.004084849471690464, 0.02450909683014279, 0.012254548415071394, 0.0013616164905634882, 0.031317179282960225, 0.016339397886761857, 0.013616164905634881, 0.009531315433944417, 0.006808082452817441, 0.008169698943380928, 0.009531315433944417, 0.025870713320706274, 0.15658589641480114, 0.008169698943380928, 0.1914246464341028, 0.1914246464341028, 0.1914246464341028, 0.1435684848255771, 0.0478561616085257, 0.0957123232170514, 0.0957123232170514, 0.022219335453902617, 0.08887734181561047, 0.08887734181561047, 0.04443867090780523, 0.04443867090780523, 0.04443867090780523, 0.15553534817731832, 0.04443867090780523, 0.04443867090780523, 0.06665800636170785, 0.04443867090780523, 0.04443867090780523, 0.04443867090780523, 0.04443867090780523, 0.1333160127234157, 0.022219335453902617, 0.23359023308934407, 0.03144483906971939, 0.022460599335513854, 0.013476359601308312, 0.004492119867102771, 0.08984239734205542, 0.008984239734205542, 0.049413318538130475, 0.004492119867102771, 0.022460599335513854, 0.008984239734205542, 0.004492119867102771, 0.017968479468411083, 0.008984239734205542, 0.004492119867102771, 0.008984239734205542, 0.040429078803924934, 0.08535027747495263, 0.017968479468411083, 0.017968479468411083, 0.004492119867102771, 0.004492119867102771, 0.004492119867102771, 0.04492119867102771, 0.04492119867102771, 0.022460599335513854, 0.06288967813943878, 0.03144483906971939, 0.017968479468411083, 0.013476359601308312, 0.008984239734205542, 0.017968479468411083, 0.017968479468411083, 0.09150775230739933, 0.03050258410246644, 0.03050258410246644, 0.12201033640986576, 0.03050258410246644, 0.03050258410246644, 0.03050258410246644, 0.06100516820493288, 0.2135180887172651, 0.06100516820493288, 0.06100516820493288, 0.03050258410246644, 0.03050258410246644, 0.1525129205123322, 0.20009842999602442, 0.20009842999602442, 0.10004921499801221, 0.12506151874751528, 0.050024607499006106, 0.025012303749503053, 0.17508612624652137, 0.025012303749503053, 0.025012303749503053, 0.025012303749503053, 0.033128254901193505, 0.033128254901193505, 0.11594889215417727, 0.016564127450596752, 0.04969238235179026, 0.04969238235179026, 0.033128254901193505, 0.06625650980238701, 0.016564127450596752, 0.016564127450596752, 0.033128254901193505, 0.04969238235179026, 0.016564127450596752, 0.23189778430835453, 0.19876952940716103, 0.04249134420684487, 0.02549480652410692, 0.004249134420684487, 0.01699653768273795, 0.07223528515163627, 0.02549480652410692, 0.11047749493779666, 0.05098961304821384, 0.01274740326205346, 0.01699653768273795, 0.004249134420684487, 0.02549480652410692, 0.02549480652410692, 0.04249134420684487, 0.004249134420684487, 0.059487881889582817, 0.07648441957232076, 0.01274740326205346, 0.004249134420684487, 0.04674047862752936, 0.021245672103422434, 0.02549480652410692, 0.008498268841368974, 0.0339930753654759, 0.01699653768273795, 0.08923182283437422, 0.008498268841368974, 0.01274740326205346, 0.05098961304821384, 0.05098961304821384, 0.024324818927438877, 0.26757300820182767, 0.024324818927438877, 0.024324818927438877, 0.024324818927438877, 0.3405474649841443, 0.048649637854877754, 0.09729927570975551, 0.12162409463719438, 0.018389661670681297, 0.04290921056492303, 0.09194830835340649, 0.05516898501204389, 0.07968853390628562, 0.018389661670681297, 0.018389661670681297, 0.030649436117802163, 0.02451954889424173, 0.018389661670681297, 0.030649436117802163, 0.006129887223560433, 0.018389661670681297, 0.09807819557696693, 0.04903909778848346, 0.030649436117802163, 0.02451954889424173, 0.030649436117802163, 0.02451954889424173, 0.012259774447120866, 0.02451954889424173, 0.04903909778848346, 0.018389661670681297, 0.036779323341362594, 0.012259774447120866, 0.030649436117802163, 0.012259774447120866, 0.012259774447120866, 0.012259774447120866, 0.012259774447120866, 0.061298872235604325, 0.031067311544062427, 0.09320193463218727, 0.031067311544062427, 0.031067311544062427, 0.031067311544062427, 0.434942361616874, 0.062134623088124855, 0.12426924617624971, 0.062134623088124855, 0.062134623088124855, 0.05809889536567599, 0.2904944768283799, 0.05809889536567599, 0.23239558146270395, 0.31954392451121794, 0.05451933599094626, 0.05451933599094626, 0.10903867198189252, 0.08177900398641938, 0.10903867198189252, 0.02725966799547313, 0.02725966799547313, 0.02725966799547313, 0.13629833997736565, 0.08177900398641938, 0.10903867198189252, 0.05451933599094626, 0.08177900398641938, 0.042525839289987995, 0.17010335715995198, 0.021262919644993997, 0.14884043751495796, 0.042525839289987995, 0.042525839289987995, 0.06378875893498198, 0.06378875893498198, 0.021262919644993997, 0.06378875893498198, 0.06378875893498198, 0.21262919644993997, 0.07714932250811271, 0.23144796752433813, 0.07714932250811271, 0.2700226287783945, 0.07714932250811271, 0.07714932250811271, 0.11572398376216907, 0.05059600112299952, 0.0316225007018747, 0.02529800056149976, 0.06956950154412433, 0.0316225007018747, 0.05059600112299952, 0.01264900028074988, 0.0316225007018747, 0.03794700084224964, 0.02529800056149976, 0.08221850182487422, 0.0316225007018747, 0.01897350042112482, 0.01264900028074988, 0.00632450014037494, 0.01897350042112482, 0.01897350042112482, 0.01264900028074988, 0.01264900028074988, 0.05059600112299952, 0.05059600112299952, 0.02529800056149976, 0.01897350042112482, 0.01264900028074988, 0.01897350042112482, 0.05059600112299952, 0.04427150098262458, 0.01264900028074988, 0.02529800056149976, 0.04427150098262458, 0.06956950154412433, 0.03099480231068107, 0.05120880381764698, 0.05255640391811138, 0.036385202712538645, 0.029647202210216675, 0.02560440190882349, 0.032342402411145464, 0.03099480231068107, 0.045818403415789404, 0.014823601105108337, 0.016171201205572732, 0.026952002009287886, 0.002695200200928789, 0.0013476001004643944, 0.01886640140650152, 0.026952002009287886, 0.04986120371718259, 0.02829960210975228, 0.02560440190882349, 0.012128400904179548, 0.00943320070325076, 0.02560440190882349, 0.020214001506965916, 0.04177560311439622, 0.02156160160743031, 0.03503760261207425, 0.05794680431996896, 0.006738000502321971, 0.00943320070325076, 0.03099480231068107, 0.00943320070325076, 0.014823601105108337, 0.008085600602786366, 0.00943320070325076, 0.008085600602786366, 0.014823601105108337, 0.04986120371718259, 0.03503760261207425, 0.03099480231068107, 0.019610919380833813, 0.1470818953562536, 0.02941637907125072, 0.03922183876166763, 0.02941637907125072, 0.04902729845208453, 0.019610919380833813, 0.02941637907125072, 0.019610919380833813, 0.019610919380833813, 0.019610919380833813, 0.02941637907125072, 0.019610919380833813, 0.05883275814250144, 0.08824913721375216, 0.009805459690416907, 0.02941637907125072, 0.04902729845208453, 0.05883275814250144, 0.10786005659458597, 0.019610919380833813, 0.009805459690416907, 0.06863821783291835, 0.009805459690416907, 0.017033267463767654, 0.03236320818115854, 0.030659881434781773, 0.13456281296376446, 0.040879841913042364, 0.03236320818115854, 0.040879841913042364, 0.02895655468840501, 0.030659881434781773, 0.010219960478260591, 0.013626613971014123, 0.023846574449274714, 0.0391765151666656, 0.011923287224637357, 0.0017033267463767653, 0.018736594210144418, 0.015329940717390887, 0.04769314889854943, 0.010219960478260591, 0.011923287224637357, 0.015329940717390887, 0.0017033267463767653, 0.015329940717390887, 0.006813306985507061, 0.03406653492753531, 0.018736594210144418, 0.0017033267463767653, 0.023846574449274714, 0.018736594210144418, 0.015329940717390887, 0.0017033267463767653, 0.0034066534927535306, 0.0017033267463767653, 0.013626613971014123, 0.011923287224637357, 0.03236320818115854, 0.18566261535506742, 0.013626613971014123, 0.018867720207225366, 0.005390777202064391, 0.03234466321238634, 0.18598181347122147, 0.035040051813418537, 0.03234466321238634, 0.04582160621754732, 0.021563108808257563, 0.013476943005160976, 0.018867720207225366, 0.008086165803096585, 0.01617233160619317, 0.043126217616515125, 0.0026953886010321953, 0.013476943005160976, 0.005390777202064391, 0.043126217616515125, 0.0026953886010321953, 0.008086165803096585, 0.010781554404128781, 0.010781554404128781, 0.013476943005160976, 0.021563108808257563, 0.0026953886010321953, 0.005390777202064391, 0.02695388601032195, 0.013476943005160976, 0.005390777202064391, 0.0026953886010321953, 0.013476943005160976, 0.021563108808257563, 0.03234466321238634, 0.24258497409289756, 0.01617233160619317, 0.031615772096278964, 0.0948473162888369, 0.031615772096278964, 0.06323154419255793, 0.06323154419255793, 0.06323154419255793, 0.12646308838511586, 0.2529261767702317, 0.06323154419255793, 0.06323154419255793, 0.031615772096278964, 0.0948473162888369, 0.10970166410185914, 0.10970166410185914, 0.03656722136728638, 0.07313444273457276, 0.1828361068364319, 0.14626888546914552, 0.21940332820371827, 0.10970166410185914, 0.07313444273457276, 0.006795665185223125, 0.04756965629656188, 0.09513931259312376, 0.09513931259312376, 0.054365321481785, 0.0271826607408925, 0.01359133037044625, 0.006795665185223125, 0.0271826607408925, 0.006795665185223125, 0.01359133037044625, 0.24464394666803252, 0.006795665185223125, 0.020386995555669377, 0.04756965629656188, 0.01359133037044625, 0.04756965629656188, 0.01359133037044625, 0.01359133037044625, 0.020386995555669377, 0.0271826607408925, 0.0271826607408925, 0.01359133037044625, 0.0271826607408925, 0.020386995555669377, 0.033978325926115625, 0.020386995555669377, 0.05986400221845089, 0.7383226940275609, 0.03990933481230059, 0.15963733924920237, 0.05527722430451872, 0.8567969767200402, 0.02763861215225936, 0.017266771808801144, 0.03453354361760229, 0.017266771808801144, 0.12086740266160802, 0.017266771808801144, 0.08633385904400573, 0.017266771808801144, 0.03453354361760229, 0.24173480532321603, 0.06906708723520458, 0.03453354361760229, 0.25900157713201716, 0.09751898143879964, 0.8776708329491967, 0.01625316357313327, 0.07632180857438138, 0.12720301429063563, 0.050881205716254256, 0.20352482286501702, 0.10176241143250851, 0.07632180857438138, 0.050881205716254256, 0.10176241143250851, 0.1780842200068899, 0.032258979472334276, 0.032258979472334276, 0.032258979472334276, 0.1290359178893371, 0.06451795894466855, 0.06451795894466855, 0.032258979472334276, 0.09677693841700284, 0.032258979472334276, 0.2580718357786742, 0.09677693841700284, 0.032258979472334276, 0.032258979472334276, 0.21525811348245938, 0.12300463627569107, 0.12300463627569107, 0.1845069544135366, 0.061502318137845534, 0.030751159068922767, 0.0922534772067683, 0.0922534772067683, 0.030751159068922767, 0.04459243399601496, 0.2548139085486569, 0.019111043141149267, 0.03185173856858211, 0.006370347713716422, 0.025481390854865688, 0.06370347713716422, 0.03185173856858211, 0.09555521570574634, 0.006370347713716422, 0.006370347713716422, 0.050962781709731377, 0.06370347713716422, 0.006370347713716422, 0.025481390854865688, 0.038222086282298534, 0.019111043141149267, 0.038222086282298534, 0.025481390854865688, 0.012740695427432844, 0.012740695427432844, 0.03185173856858211, 0.07644417256459707, 0.22641987671335276, 0.08490745376750729, 0.08490745376750729, 0.014151242294584547, 0.05660496917833819, 0.2830248458916909, 0.08490745376750729, 0.05660496917833819, 0.042453726883753644, 0.05660496917833819, 0.014151242294584547, 0.5554208297593077, 0.09874148084609914, 0.012342685105762393, 0.012342685105762393, 0.07405611063457436, 0.03702805531728718, 0.012342685105762393, 0.11108416595186155, 0.08639879574033675, 0.01806978017121226, 0.7408609870197026, 0.01806978017121226, 0.1806978017121226, 0.01806978017121226, 0.05846916643684594, 0.05846916643684594, 0.05846916643684594, 0.11693833287369187, 0.2631112489658067, 0.02923458321842297, 0.05846916643684594, 0.02923458321842297, 0.0877037496552689, 0.02923458321842297, 0.11693833287369187, 0.02923458321842297, 0.02923458321842297, 0.02923458321842297, 0.06241093995238953, 0.06241093995238953, 0.14562552655557556, 0.08321458660318604, 0.06241093995238953, 0.06241093995238953, 0.02080364665079651, 0.04160729330159302, 0.08321458660318604, 0.04160729330159302, 0.02080364665079651, 0.02080364665079651, 0.02080364665079651, 0.10401823325398254, 0.02080364665079651, 0.04160729330159302, 0.02080364665079651, 0.04160729330159302, 0.02080364665079651, 0.08882228753571378, 0.04441114376785689, 0.17764457507142756, 0.04441114376785689, 0.04441114376785689, 0.04441114376785689, 0.04441114376785689, 0.08882228753571378, 0.13323343130357065, 0.04441114376785689, 0.17764457507142756, 0.029516479436103957, 0.08854943830831187, 0.11806591774441583, 0.029516479436103957, 0.2951647943610396, 0.059032958872207913, 0.029516479436103957, 0.2951647943610396, 0.02541736297514964, 0.12708681487574822, 0.05083472595029928, 0.1779215408260475, 0.07625208892544892, 0.07625208892544892, 0.02541736297514964, 0.10166945190059856, 0.02541736297514964, 0.05083472595029928, 0.12708681487574822, 0.10166945190059856, 0.03628316716160884, 0.07256633432321768, 0.07256633432321768, 0.21769900296965308, 0.07256633432321768, 0.03628316716160884, 0.03628316716160884, 0.10884950148482654, 0.10884950148482654, 0.1814158358080442, 0.04441102411643477, 0.04441102411643477, 0.04441102411643477, 0.022205512058217386, 0.06661653617465216, 0.04441102411643477, 0.08882204823286954, 0.04441102411643477, 0.04441102411643477, 0.08882204823286954, 0.04441102411643477, 0.06661653617465216, 0.06661653617465216, 0.022205512058217386, 0.24426063264039125, 0.04848651616236563, 0.04848651616236563, 0.04848651616236563, 0.04848651616236563, 0.24243258081182814, 0.09697303232473126, 0.04848651616236563, 0.09697303232473126, 0.04848651616236563, 0.1939460646494625, 0.062044012695998234, 0.062044012695998234, 0.09306601904399735, 0.12408802539199647, 0.21715404443599381, 0.031022006347999117, 0.12408802539199647, 0.031022006347999117, 0.031022006347999117, 0.062044012695998234, 0.031022006347999117, 0.062044012695998234, 0.1733488307176798, 0.1733488307176798, 0.07704392476341325, 0.038521962381706625, 0.038521962381706625, 0.038521962381706625, 0.038521962381706625, 0.05778294357255993, 0.05778294357255993, 0.019260981190853312, 0.07704392476341325, 0.19260981190853313, 0.019260981190853312, 0.03916454948370666, 0.0652742491395111, 0.0652742491395111, 0.013054849827902221, 0.09138394879531554, 0.026109699655804442, 0.013054849827902221, 0.07832909896741332, 0.026109699655804442, 0.052219399311608884, 0.03916454948370666, 0.07832909896741332, 0.0652742491395111, 0.03916454948370666, 0.0652742491395111, 0.0652742491395111, 0.10443879862321777, 0.03916454948370666, 0.06139516196964944, 0.04911612957571955, 0.012279032393929887, 0.07367419436357933, 0.012279032393929887, 0.17190645351501843, 0.024558064787859775, 0.14734838872715866, 0.18418548590894832, 0.1964645183028782, 0.024558064787859775, 0.012279032393929887, 0.011598279128372514, 0.011598279128372514, 0.023196558256745027, 0.05799139564186257, 0.06958967477023509, 0.023196558256745027, 0.023196558256745027, 0.046393116513490054, 0.023196558256745027, 0.034794837385117544, 0.034794837385117544, 0.0811879538986076, 0.30155525733768535, 0.17397418692558772, 0.023196558256745027, 0.05799139564186257, 0.27497375987189865, 0.045828959978649775, 0.27497375987189865, 0.022914479989324887, 0.06874343996797466, 0.022914479989324887, 0.11457239994662444, 0.09165791995729955, 0.06874343996797466, 0.12861187724324807, 0.014290208582583118, 0.014290208582583118, 0.0357255214564578, 0.007145104291291559, 0.05001573003904092, 0.014290208582583118, 0.02143531287387468, 0.02143531287387468, 0.0357255214564578, 0.007145104291291559, 0.02143531287387468, 0.06430593862162404, 0.028580417165166235, 0.05001573003904092, 0.028580417165166235, 0.0357255214564578, 0.014290208582583118, 0.0357255214564578, 0.04287062574774936, 0.014290208582583118, 0.0714510429129156, 0.06430593862162404, 0.007145104291291559, 0.02143531287387468, 0.15004719011712275, 0.043683770425181935, 0.4368377042518194, 0.043683770425181935, 0.08736754085036387, 0.043683770425181935, 0.1310513112755458, 0.043683770425181935, 0.1310513112755458, 0.043683770425181935, 0.0868638425451955, 0.04343192127259775, 0.021715960636298874, 0.04343192127259775, 0.021715960636298874, 0.021715960636298874, 0.021715960636298874, 0.021715960636298874, 0.06514788190889663, 0.3040234489081843, 0.15201172445409214, 0.10857980318149438, 0.04343192127259775, 0.02576829258656352, 0.07730487775969057, 0.02576829258656352, 0.05153658517312704, 0.05153658517312704, 0.02576829258656352, 0.10307317034625409, 0.41229268138501635, 0.1288414629328176, 0.05153658517312704, 0.04786446570716499, 0.03190964380477666, 0.14359339712149496, 0.09572893141432998, 0.03190964380477666, 0.01595482190238833, 0.14359339712149496, 0.04786446570716499, 0.01595482190238833, 0.04786446570716499, 0.06381928760955333, 0.01595482190238833, 0.04786446570716499, 0.01595482190238833, 0.01595482190238833, 0.11168375331671831, 0.04786446570716499, 0.030071655719964768, 0.42100318007950677, 0.030071655719964768, 0.21050159003975338, 0.21050159003975338, 0.060143311439929535, 0.031847522915381225, 0.31847522915381227, 0.09554256874614368, 0.031847522915381225, 0.031847522915381225, 0.1273900916615249, 0.031847522915381225, 0.09554256874614368, 0.06369504583076245, 0.06369504583076245, 0.031847522915381225, 0.031847522915381225, 0.059722992095735485, 0.059722992095735485, 0.17916897628720643, 0.11944598419147097, 0.17916897628720643, 0.029861496047867742, 0.11944598419147097, 0.059722992095735485, 0.08958448814360322, 0.08958448814360322, 0.032100065879623836, 0.06420013175924767, 0.032100065879623836, 0.032100065879623836, 0.06420013175924767, 0.032100065879623836, 0.032100065879623836, 0.12840026351849534, 0.12840026351849534, 0.06420013175924767, 0.032100065879623836, 0.06420013175924767, 0.19260039527774303, 0.032100065879623836, 0.03606348715256607, 0.1081904614576982, 0.2163809229153964, 0.07212697430513214, 0.03606348715256607, 0.03606348715256607, 0.2163809229153964, 0.07212697430513214, 0.14425394861026428, 0.07212697430513214, 0.02290823074579689, 0.06872469223739067, 0.04581646149159378, 0.04581646149159378, 0.04581646149159378, 0.36653169193275026, 0.09163292298318756, 0.13744938447478133, 0.02290823074579689, 0.06872469223739067, 0.04581646149159378, 0.011247721824628035, 0.01499696243283738, 0.033743165473884104, 0.03749240608209345, 0.02249544364925607, 0.033743165473884104, 0.12747418067911773, 0.00749848121641869, 0.13122342128732706, 0.003749240608209345, 0.00749848121641869, 0.02999392486567476, 0.003749240608209345, 0.003749240608209345, 0.03749240608209345, 0.01499696243283738, 0.06748633094776821, 0.018746203041046725, 0.01499696243283738, 0.041241646690302794, 0.018746203041046725, 0.06748633094776821, 0.08623253398881493, 0.00749848121641869, 0.003749240608209345, 0.14622038372016444, 0.12244694300074774, 0.26530170983495344, 0.040815647666915915, 0.08163129533383183, 0.020407823833457957, 0.040815647666915915, 0.020407823833457957, 0.040815647666915915, 0.020407823833457957, 0.020407823833457957, 0.040815647666915915, 0.020407823833457957, 0.1428547668342057, 0.08163129533383183, 0.020407823833457957, 0.042782499784606395, 0.08556499956921279, 0.12834749935381917, 0.12834749935381917, 0.12834749935381917, 0.042782499784606395, 0.08556499956921279, 0.042782499784606395, 0.042782499784606395, 0.042782499784606395, 0.17112999913842558, 0.0713653422318665, 0.0428192053391199, 0.1855498898028529, 0.0999114791246131, 0.0142730684463733, 0.0142730684463733, 0.0428192053391199, 0.0570922737854932, 0.0142730684463733, 0.0856384106782398, 0.1284576160173597, 0.0285461368927466, 0.0713653422318665, 0.0428192053391199, 0.0428192053391199, 0.0285461368927466, 0.2945889037857423, 0.1262523873367467, 0.0420841291122489, 0.0420841291122489, 0.0420841291122489, 0.0420841291122489, 0.0841682582244978, 0.2945889037857423, 0.1841739204435344, 0.12278261362902293, 0.30695653407255735, 0.030695653407255732, 0.030695653407255732, 0.061391306814511465, 0.030695653407255732, 0.061391306814511465, 0.15347826703627868, 0.008445282309223211, 0.016890564618446423, 0.11401131117451334, 0.016890564618446423, 0.021113205773058027, 0.016890564618446423, 0.016890564618446423, 0.012667923463834815, 0.038003770391504446, 0.004222641154611606, 0.021113205773058027, 0.004222641154611606, 0.008445282309223211, 0.012667923463834815, 0.042226411546116054, 0.21957734003980348, 0.016890564618446423, 0.004222641154611606, 0.05067169385533926, 0.038003770391504446, 0.016890564618446423, 0.20690941657596867, 0.07600754078300889, 0.012667923463834815, 0.31025361740326135, 0.6515325965468488, 0.015512680870163068, 0.11574571731128233, 0.14468214663910292, 0.17361857596692348, 0.028936429327820583, 0.08680928798346174, 0.028936429327820583, 0.057872858655641166, 0.3183007226060264, 0.028936429327820583, 0.028936429327820583, 0.03902171854141408, 0.07804343708282815, 0.07804343708282815, 0.11706515562424225, 0.03902171854141408, 0.03902171854141408, 0.27315202978989855, 0.2341303112484845, 0.03902171854141408, 0.018488982879759384, 0.16640084591783447, 0.2773347431963908, 0.03697796575951877, 0.03697796575951877, 0.018488982879759384, 0.11093389727855632, 0.05546694863927816, 0.018488982879759384, 0.018488982879759384, 0.07395593151903754, 0.1294228801583157, 0.0737716036117047, 0.00962238307978757, 0.028867149239362712, 0.044904454372341994, 0.016037305132979286, 0.00962238307978757, 0.06735668155851299, 0.03528207129255442, 0.0032074610265958566, 0.006414922053191713, 0.054526837452129565, 0.0032074610265958566, 0.12509098003723843, 0.006414922053191713, 0.006414922053191713, 0.022452227186170997, 0.006414922053191713, 0.01924476615957514, 0.06414922053191714, 0.13150590209043012, 0.044904454372341994, 0.0032074610265958566, 0.025659688212766853, 0.13471336311702597, 0.060941759505321276, 0.11617550278942083, 0.03872516759647361, 0.03872516759647361, 0.07745033519294722, 0.07745033519294722, 0.05808775139471042, 0.019362583798236806, 0.03872516759647361, 0.05808775139471042, 0.019362583798236806, 0.03872516759647361, 0.19362583798236807, 0.03872516759647361, 0.15490067038589445, 0.03872516759647361, 0.03770984106799235, 0.09427460266998086, 0.7164869802918545, 0.09427460266998086, 0.05656476160198852, 0.06876690782885429, 0.3094510852298443, 0.10315036174328145, 0.06876690782885429, 0.034383453914427146, 0.034383453914427146, 0.17191726957213574, 0.06876690782885429, 0.034383453914427146, 0.034383453914427146, 0.02548558228129008, 0.07645674684387024, 0.07645674684387024, 0.10194232912516032, 0.4077693165006413, 0.1274279114064504, 0.02548558228129008, 0.02548558228129008, 0.02548558228129008, 0.02548558228129008, 0.02548558228129008, 0.5967596674382257, 0.055946218822333665, 0.018648739607444554, 0.09324369803722278, 0.07459495842977822, 0.03729747921488911, 0.055946218822333665, 0.03729747921488911, 0.018648739607444554, 0.023359277422540906, 0.18687421938032725, 0.04671855484508181, 0.16351494195778635, 0.04671855484508181, 0.04671855484508181, 0.023359277422540906, 0.09343710969016362, 0.16351494195778635, 0.16351494195778635, 0.04671855484508181, 0.23765726152993288, 0.014853578845620805, 0.02970715769124161, 0.04456073653686242, 0.05941431538248322, 0.04456073653686242, 0.04456073653686242, 0.04456073653686242, 0.10397505191934564, 0.10397505191934564, 0.02970715769124161, 0.014853578845620805, 0.02970715769124161, 0.02970715769124161, 0.04456073653686242, 0.11882863076496644, 0.11529031623166605, 0.038430105410555346, 0.07686021082111069, 0.11529031623166605, 0.07686021082111069, 0.11529031623166605, 0.07686021082111069, 0.15372042164222138, 0.038430105410555346, 0.038430105410555346, 0.15372042164222138, 0.02284155363188291, 0.04949003286907964, 0.04949003286907964, 0.07233158650096255, 0.019034628026569092, 0.057103884079707276, 0.02664847923719673, 0.007613851210627637, 0.0609108096850211, 0.011420776815941456, 0.09898006573815928, 0.03426233044782437, 0.007613851210627637, 0.007613851210627637, 0.015227702421255275, 0.02664847923719673, 0.019034628026569092, 0.03426233044782437, 0.05329695847439346, 0.007613851210627637, 0.03045540484251055, 0.015227702421255275, 0.007613851210627637, 0.019034628026569092, 0.0038069256053138187, 0.015227702421255275, 0.13324239618598366, 0.07613851210627637, 0.02284155363188291, 0.19280363146932175, 0.4820090786733044, 0.09640181573466088, 0.010711312859406763, 0.08569050287525411, 0.021422625718813527, 0.010711312859406763, 0.03213393857822029, 0.010711312859406763, 0.03213393857822029, 0.09836537122244517, 0.03278845707414839, 0.06557691414829678, 0.03278845707414839, 0.09836537122244517, 0.06557691414829678, 0.03278845707414839, 0.03278845707414839, 0.13115382829659356, 0.09836537122244517, 0.06557691414829678, 0.22951919951903874, 0.08228257227293484, 0.010285321534116855, 0.010285321534116855, 0.010285321534116855, 0.010285321534116855, 0.010285321534116855, 0.051426607670584275, 0.010285321534116855, 0.02057064306823371, 0.02057064306823371, 0.02057064306823371, 0.4216981828987911, 0.030855964602350565, 0.19542110914822025, 0.08228257227293484, 0.021595067609488625, 0.06478520282846588, 0.8206125691605678, 0.06478520282846588, 0.025703864391977013, 0.025703864391977013, 0.051407728783954026, 0.025703864391977013, 0.025703864391977013, 0.4112618302716322, 0.025703864391977013, 0.051407728783954026, 0.025703864391977013, 0.051407728783954026, 0.051407728783954026, 0.2056309151358161, 0.07976871078409233, 0.053179140522728215, 0.18612699182954875, 0.026589570261364107, 0.07976871078409233, 0.18612699182954875, 0.026589570261364107, 0.053179140522728215, 0.053179140522728215, 0.026589570261364107, 0.026589570261364107, 0.026589570261364107, 0.07976871078409233, 0.026589570261364107, 0.026589570261364107, 0.07714185304766488, 0.025713951015888296, 0.07714185304766488, 0.05142790203177659, 0.1285697550794415, 0.05142790203177659, 0.15428370609532976, 0.07714185304766488, 0.10285580406355319, 0.17999765711121807, 0.07714185304766488, 0.16399856768925933, 0.1366654730743828, 0.013666547307438277, 0.04099964192231483, 0.027333094614876555, 0.15033202038182106, 0.0683327365371914, 0.15033202038182106, 0.05466618922975311, 0.013666547307438277, 0.1366654730743828, 0.027333094614876555, 0.17442691456990347, 0.034885382913980696, 0.13954153165592278, 0.06977076582796139, 0.2093122974838842, 0.034885382913980696, 0.034885382913980696, 0.27908306331184557, 0.012928952353360588, 0.025857904706721176, 0.15514742824032707, 0.14221847588696646, 0.05171580941344235, 0.06464476176680294, 0.025857904706721176, 0.03878685706008177, 0.07757371412016353, 0.37493961824745703, 0.029898895194455342, 0.059797790388910685, 0.029898895194455342, 0.2092922663611874, 0.029898895194455342, 0.23919116155564274, 0.059797790388910685, 0.08969668558336603, 0.059797790388910685, 0.14949447597227672, 0.11637481660901583, 0.04654992664360633, 0.023274963321803167, 0.04654992664360633, 0.04654992664360633, 0.023274963321803167, 0.09309985328721267, 0.3491244498270475, 0.0698248899654095, 0.04654992664360633, 0.0698248899654095, 0.023274963321803167, 0.021277192611791187, 0.08510877044716475, 0.10638596305895594, 0.06383157783537356, 0.08510877044716475, 0.021277192611791187, 0.021277192611791187, 0.10638596305895594, 0.042554385223582375, 0.10638596305895594, 0.12766315567074712, 0.1702175408943295, 0.01573199122484339, 0.04719597367453017, 0.007865995612421696, 0.09439194734906034, 0.03146398244968678, 0.007865995612421696, 0.09439194734906034, 0.18878389469812068, 0.023597986837265085, 0.03146398244968678, 0.007865995612421696, 0.03932997806210847, 0.05506196928695187, 0.07865995612421695, 0.03146398244968678, 0.03146398244968678, 0.03146398244968678, 0.023597986837265085, 0.03146398244968678, 0.04719597367453017, 0.007865995612421696, 0.007865995612421696, 0.03146398244968678, 0.04719597367453017, 0.06211412313263162, 0.08695977238568427, 0.0496912985061053, 0.037268473879578976, 0.1490738955183159, 0.11180542163873691, 0.02484564925305265, 0.012422824626526325, 0.037268473879578976, 0.02484564925305265, 0.037268473879578976, 0.02484564925305265, 0.012422824626526325, 0.012422824626526325, 0.012422824626526325, 0.012422824626526325, 0.12422824626526324, 0.11180542163873691, 0.037268473879578976, 0.14826624481675685, 0.010590446058339776, 0.03177133817501933, 0.06354267635003866, 0.010590446058339776, 0.03177133817501933, 0.09531401452505799, 0.03177133817501933, 0.052952230291698876, 0.010590446058339776, 0.0423617842333591, 0.0847235684667182, 0.0847235684667182, 0.0423617842333591, 0.02118089211667955, 0.010590446058339776, 0.052952230291698876, 0.052952230291698876, 0.010590446058339776, 0.07413312240837842, 0.0423617842333591, 0.027260750620840428, 0.009086916873613477, 0.07269533498890782, 0.009086916873613477, 0.054521501241680856, 0.027260750620840428, 0.06360841811529433, 0.018173833747226954, 0.018173833747226954, 0.12721683623058866, 0.009086916873613477, 0.32712900745008516, 0.027260750620840428, 0.08178225186252129, 0.009086916873613477, 0.018173833747226954, 0.08178225186252129, 0.07655939584985774, 0.10718315418980084, 0.015311879169971548, 0.015311879169971548, 0.04593563750991465, 0.07655939584985774, 0.030623758339943095, 0.015311879169971548, 0.2756138250594879, 0.07655939584985774, 0.030623758339943095, 0.030623758339943095, 0.015311879169971548, 0.015311879169971548, 0.04593563750991465, 0.06124751667988619, 0.06124751667988619, 0.06922180701272583, 0.04614787134181722, 0.02307393567090861, 0.02307393567090861, 0.04614787134181722, 0.2768872280509033, 0.04614787134181722, 0.02307393567090861, 0.06922180701272583, 0.09229574268363444, 0.04614787134181722, 0.09229574268363444, 0.06922180701272583, 0.04614787134181722, 0.14789805031950107, 0.14789805031950107, 0.07394902515975053, 0.14789805031950107, 0.1109235377396258, 0.03697451257987527, 0.1848725628993763, 0.03697451257987527, 0.07394902515975053, 0.03697451257987527, 0.16772810295631468, 0.04193202573907867, 0.04193202573907867, 0.20966012869539336, 0.08386405147815734, 0.04193202573907867, 0.125796077217236, 0.04193202573907867, 0.16772810295631468, 0.03395979009573027, 0.23771853067011187, 0.10187937028719081, 0.03395979009573027, 0.03395979009573027, 0.06791958019146054, 0.06791958019146054, 0.03395979009573027, 0.06791958019146054, 0.06791958019146054, 0.03395979009573027, 0.16979895047865134, 0.07739912804226974, 0.19349782010567435, 0.07739912804226974, 0.1160986920634046, 0.15479825608453948, 0.07739912804226974, 0.03869956402113487, 0.07739912804226974, 0.1160986920634046, 0.20572641691700888, 0.01142924538427827, 0.04571698153711308, 0.02285849076855654, 0.04571698153711308, 0.06857547230566963, 0.01142924538427827, 0.03428773615283481, 0.01142924538427827, 0.06857547230566963, 0.04571698153711308, 0.01142924538427827, 0.01142924538427827, 0.01142924538427827, 0.02285849076855654, 0.04571698153711308, 0.09143396307422616, 0.0800047176899479, 0.02285849076855654, 0.03428773615283481, 0.05714622692139135, 0.01142924538427827, 0.15267754603258782, 0.10687428222281148, 0.21374856444562296, 0.0916065276195527, 0.04580326380977635, 0.030535509206517566, 0.015267754603258783, 0.030535509206517566, 0.015267754603258783, 0.015267754603258783, 0.15267754603258782, 0.015267754603258783, 0.06107101841303513, 0.030535509206517566, 0.7450990053184311, 0.23925197418481733, 0.006835770690994781, 0.07626691303522026, 0.2288007391056608, 0.010895273290745753, 0.04358109316298301, 0.44670620492057583, 0.021790546581491505, 0.11984800619820328, 0.021790546581491505, 0.06649405198922355, 0.033247025994611774, 0.06649405198922355, 0.033247025994611774, 0.16623512997305886, 0.033247025994611774, 0.033247025994611774, 0.43221133792995303, 0.06649405198922355, 0.033247025994611774, 0.013397108148893855, 0.013397108148893855, 0.03349277037223464, 0.020095662223340784, 0.03349277037223464, 0.10717686519115084, 0.0736840948189162, 0.0468898785211285, 0.013397108148893855, 0.006698554074446928, 0.020095662223340784, 0.020095662223340784, 0.006698554074446928, 0.020095662223340784, 0.013397108148893855, 0.05358843259557542, 0.02679421629778771, 0.04019132444668157, 0.03349277037223464, 0.02679421629778771, 0.006698554074446928, 0.1205739733400447, 0.013397108148893855, 0.013397108148893855, 0.013397108148893855, 0.020095662223340784, 0.06698554074446927, 0.0468898785211285, 0.05358843259557542, 0.013397108148893855, 0.05826086271694588, 0.05826086271694588, 0.05826086271694588, 0.02913043135847294, 0.17478258815083764, 0.02913043135847294, 0.02913043135847294, 0.11652172543389176, 0.1456521567923647, 0.02913043135847294, 0.05826086271694588, 0.02913043135847294, 0.02913043135847294, 0.08739129407541882, 0.08739129407541882, 0.21055616674817879, 0.12633370004890726, 0.042111233349635756, 0.042111233349635756, 0.2947786334474503, 0.16844493339854302, 0.042111233349635756, 0.042111233349635756, 0.042111233349635756, 0.08743950891548105, 0.029146502971827016, 0.029146502971827016, 0.11658601188730806, 0.05829300594365403, 0.11658601188730806, 0.029146502971827016, 0.029146502971827016, 0.08743950891548105, 0.029146502971827016, 0.23317202377461613, 0.029146502971827016, 0.08743950891548105, 0.07534342148072541, 0.05022894765381694, 0.17580131678835928, 0.10045789530763388, 0.05022894765381694, 0.02511447382690847, 0.05022894765381694, 0.07534342148072541, 0.07534342148072541, 0.12557236913454234, 0.10045789530763388, 0.10045789530763388, 0.04721190751146767, 0.007868651251911279, 0.10229246627484662, 0.007868651251911279, 0.0708178612672015, 0.015737302503822558, 0.06294921001529023, 0.0708178612672015, 0.15737302503822556, 0.007868651251911279, 0.007868651251911279, 0.43277581885512034, 0.04350423412167207, 0.015819721498789845, 0.019774651873487304, 0.003954930374697461, 0.007909860749394922, 0.011864791124092382, 0.12260284161562128, 0.03954930374697461, 0.007909860749394922, 0.6644283029491734, 0.027684512622882226, 0.003954930374697461, 0.019774651873487304, 0.021235321721067544, 0.04247064344213509, 0.10617660860533772, 0.12741193032640527, 0.03185298258160132, 0.010617660860533772, 0.010617660860533772, 0.0743236260237364, 0.010617660860533772, 0.05308830430266886, 0.021235321721067544, 0.010617660860533772, 0.10617660860533772, 0.06370596516320264, 0.021235321721067544, 0.021235321721067544, 0.08494128688427018, 0.010617660860533772, 0.0743236260237364, 0.05308830430266886, 0.021235321721067544, 0.021235321721067544, 0.45845473855543556, 0.024129196766075556, 0.09651678706430222, 0.12064598383037778, 0.07238759029822667, 0.024129196766075556, 0.1689043773625289, 0.06953786437300807, 0.01738446609325202, 0.3129203896785363, 0.01738446609325202, 0.03476893218650404, 0.2955359235852843, 0.052153398279756055, 0.15646019483926815, 0.01738446609325202, 0.1750428943437021, 0.035008578868740425, 0.1750428943437021, 0.07001715773748085, 0.035008578868740425, 0.07001715773748085, 0.24506005208118295, 0.1400343154749617, 0.09644984881116897, 0.09644984881116897, 0.048224924405584485, 0.14467477321675346, 0.048224924405584485, 0.43402431965026034, 0.048224924405584485, 0.05674057429388304, 0.11348114858776608, 0.24587582194015983, 0.07565409905851071, 0.13239467335239374, 0.018913524764627678, 0.018913524764627678, 0.037827049529255356, 0.11348114858776608, 0.018913524764627678, 0.018913524764627678, 0.018913524764627678, 0.018913524764627678, 0.0945676238231384, 0.07203138935442338, 0.024010463118141128, 0.12005231559070564, 0.024010463118141128, 0.07203138935442338, 0.14406277870884676, 0.048020926236282256, 0.024010463118141128, 0.048020926236282256, 0.048020926236282256, 0.14406277870884676, 0.024010463118141128, 0.1680732418269879, 0.06622343656953009, 0.03311171828476504, 0.6456785065529184, 0.01655585914238252, 0.1490027322814427, 0.04966757742714757, 0.06496483072342674, 0.06496483072342674, 0.021654943574475578, 0.1082747178723779, 0.25985932289370695, 0.1082747178723779, 0.021654943574475578, 0.021654943574475578, 0.021654943574475578, 0.021654943574475578, 0.043309887148951155, 0.043309887148951155, 0.043309887148951155, 0.08661977429790231, 0.021654943574475578, 0.08740069167819242, 0.029133563892730804, 0.08740069167819242, 0.029133563892730804, 0.08740069167819242, 0.26220207503457726, 0.11653425557092321, 0.029133563892730804, 0.029133563892730804, 0.029133563892730804, 0.029133563892730804, 0.05826712778546161, 0.08740069167819242, 0.05826712778546161, 0.03597233481409336, 0.01798616740704668, 0.20234438332927515, 0.11691008814580342, 0.06295158592466338, 0.01798616740704668, 0.09892392073875673, 0.05845504407290171, 0.08093775333171006, 0.01798616740704668, 0.01798616740704668, 0.01348962555528501, 0.03147579296233169, 0.00449654185176167, 0.01348962555528501, 0.05395850222114004, 0.00899308370352334, 0.01798616740704668, 0.07644121147994838, 0.00899308370352334, 0.01348962555528501, 0.01798616740704668, 0.2550394217650455, 0.12751971088252276, 0.0850131405883485, 0.04250657029417425, 0.0850131405883485, 0.0850131405883485, 0.170026281176697, 0.04250657029417425, 0.04250657029417425, 0.36789785805100017, 0.1414991761734616, 0.08489950570407696, 0.05659967046938465, 0.028299835234692323, 0.028299835234692323, 0.028299835234692323, 0.028299835234692323, 0.028299835234692323, 0.1414991761734616, 0.028299835234692323, 0.2254967131220074, 0.01879139276016728, 0.03758278552033456, 0.05637417828050185, 0.05637417828050185, 0.03758278552033456, 0.07516557104066912, 0.09395696380083642, 0.03758278552033456, 0.3570364624431784, 0.039254509501943737, 0.1177635285058312, 0.039254509501943737, 0.19627254750971868, 0.039254509501943737, 0.2355270570116624, 0.039254509501943737, 0.039254509501943737, 0.1177635285058312, 0.039254509501943737, 0.07850901900388747, 0.05623520799587297, 0.05623520799587297, 0.028117603997936485, 0.028117603997936485, 0.11247041599174594, 0.08435281199380945, 0.028117603997936485, 0.14058801998968243, 0.028117603997936485, 0.1687056239876189, 0.028117603997936485, 0.028117603997936485, 0.14058801998968243, 0.028117603997936485, 0.020613756840104223, 0.06184127052031267, 0.041227513680208445, 0.041227513680208445, 0.06184127052031267, 0.10306878420052111, 0.14429629788072956, 0.12368254104062534, 0.185523811560938, 0.020613756840104223, 0.08245502736041689, 0.041227513680208445, 0.020613756840104223, 0.006863641717293343, 0.06863641717293342, 0.027454566869173372, 0.02059092515188003, 0.0480454920210534, 0.08236370060752012, 0.10295462575940015, 0.10981826747669349, 0.006863641717293343, 0.02059092515188003, 0.0480454920210534, 0.013727283434586686, 0.027454566869173372, 0.006863641717293343, 0.04118185030376006, 0.006863641717293343, 0.03431820858646671, 0.06863641717293342, 0.027454566869173372, 0.07550005889022678, 0.006863641717293343, 0.013727283434586686, 0.08922734232481346, 0.02059092515188003, 0.02059092515188003, 0.07190184860673349, 0.17975462151683372, 0.10785277291010023, 0.17975462151683372, 0.035950924303366744, 0.07190184860673349, 0.035950924303366744, 0.07190184860673349, 0.10785277291010023, 0.07190184860673349, 0.021493956124339277, 0.15045769287037494, 0.10746978062169639, 0.06448186837301784, 0.021493956124339277, 0.06448186837301784, 0.12896373674603567, 0.021493956124339277, 0.21493956124339278, 0.021493956124339277, 0.04298791224867855, 0.04298791224867855, 0.04298791224867855, 0.02409961750749776, 0.04819923501499552, 0.07229885252249328, 0.09639847002999104, 0.04819923501499552, 0.19279694005998207, 0.02409961750749776, 0.2409961750749776, 0.14459770504498656, 0.02409961750749776, 0.04819923501499552, 0.06777249221177559, 0.22026059968827066, 0.016943123052943897, 0.016943123052943897, 0.008471561526471948, 0.008471561526471948, 0.03388624610588779, 0.042357807632359745, 0.11860186137060728, 0.016943123052943897, 0.15248810747649508, 0.025414684579415845, 0.05082936915883169, 0.016943123052943897, 0.008471561526471948, 0.008471561526471948, 0.016943123052943897, 0.042357807632359745, 0.042357807632359745, 0.016943123052943897, 0.05930093068530364, 0.03115375722205211, 0.03115375722205211, 0.3115375722205211, 0.06230751444410422, 0.06230751444410422, 0.03115375722205211, 0.12461502888820844, 0.09346127166615634, 0.03115375722205211, 0.09346127166615634, 0.12461502888820844, 0.07040911458483633, 0.14081822916967265, 0.035204557292418164, 0.07040911458483633, 0.035204557292418164, 0.14081822916967265, 0.035204557292418164, 0.10561367187725448, 0.035204557292418164, 0.07040911458483633, 0.21122734375450897, 0.035204557292418164, 0.03529582855526907, 0.17647914277634535, 0.03529582855526907, 0.14118331422107627, 0.07059165711053814, 0.03529582855526907, 0.03529582855526907, 0.07059165711053814, 0.2470707998868835, 0.03529582855526907, 0.07059165711053814, 0.28515004510912245, 0.15007897111006444, 0.06003158844402578, 0.03001579422201289, 0.03001579422201289, 0.03001579422201289, 0.015007897111006445, 0.04502369133301933, 0.015007897111006445, 0.03001579422201289, 0.15007897111006444, 0.06003158844402578, 0.09004738266603866, 0.0158548423177189, 0.0317096846354378, 0.0951290539063134, 0.0158548423177189, 0.0158548423177189, 0.1109838962240323, 0.1744032654949079, 0.0317096846354378, 0.0317096846354378, 0.0634193692708756, 0.0475645269531567, 0.0317096846354378, 0.0317096846354378, 0.1109838962240323, 0.0158548423177189, 0.0792742115885945, 0.0951290539063134, 0.21270078569827094, 0.04254015713965419, 0.08508031427930839, 0.08508031427930839, 0.08508031427930839, 0.04254015713965419, 0.04254015713965419, 0.04254015713965419, 0.21270078569827094, 0.04254015713965419, 0.04254015713965419, 0.005745064109100211, 0.011490128218200422, 0.005745064109100211, 0.21831243614580803, 0.011490128218200422, 0.017235192327300634, 0.08617596163650316, 0.04021544876370148, 0.05745064109100211, 0.011490128218200422, 0.0517055769819019, 0.005745064109100211, 0.06894076930920254, 0.022980256436400844, 0.028725320545501056, 0.017235192327300634, 0.005745064109100211, 0.028725320545501056, 0.2470377566913091, 0.017235192327300634, 0.04021544876370148, 0.05343072481449273, 0.026715362407246365, 0.08014608722173909, 0.026715362407246365, 0.05343072481449273, 0.48087652333043457, 0.18700753685072455, 0.05343072481449273, 0.026715362407246365, 0.02971478282843955, 0.0594295656568791, 0.08914434848531866, 0.0594295656568791, 0.14857391414219775, 0.14857391414219775, 0.08914434848531866, 0.08914434848531866, 0.17828869697063732, 0.08914434848531866, 0.03981340433868004, 0.059720106508020065, 0.04976675542335005, 0.00995335108467001, 0.25878712820142025, 0.03981340433868004, 0.00995335108467001, 0.03981340433868004, 0.00995335108467001, 0.43794744772548044, 0.00995335108467001, 0.03981340433868004, 0.03200182710946246, 0.0960054813283874, 0.06400365421892493, 0.1920109626567748, 0.06400365421892493, 0.1920109626567748, 0.03200182710946246, 0.32001827109462466, 0.0775896115239716, 0.1163844172859574, 0.193974028809929, 0.0775896115239716, 0.0775896115239716, 0.193974028809929, 0.193974028809929, 0.20298108572296703, 0.05799459592084773, 0.05799459592084773, 0.05799459592084773, 0.3189702775646625, 0.08699189388127158, 0.05799459592084773, 0.1449864898021193, 0.036316067423145686, 0.036316067423145686, 0.036316067423145686, 0.036316067423145686, 0.5447410113471853, 0.07263213484629137, 0.036316067423145686, 0.14526426969258274, 0.009058271888377723, 0.018116543776755447, 0.02717481566513317, 0.09964099077215495, 0.02717481566513317, 0.04529135944188862, 0.02717481566513317, 0.036233087553510894, 0.3079812442048426, 0.02717481566513317, 0.09058271888377724, 0.06340790321864406, 0.018116543776755447, 0.018116543776755447, 0.09964099077215495, 0.009058271888377723, 0.04529135944188862, 0.009058271888377723, 0.009058271888377723, 0.009058271888377723, 0.060199986010681036, 0.04013332400712069, 0.060199986010681036, 0.060199986010681036, 0.04013332400712069, 0.020066662003560345, 0.1805999580320431, 0.060199986010681036, 0.060199986010681036, 0.020066662003560345, 0.04013332400712069, 0.020066662003560345, 0.020066662003560345, 0.12039997202136207, 0.020066662003560345, 0.04013332400712069, 0.08026664801424138, 0.026482110534315574, 0.07944633160294672, 0.1059284421372623, 0.18537477374020903, 0.37074954748041805, 0.07944633160294672, 0.05296422106863115, 0.07944633160294672, 0.04438011125110478, 0.04438011125110478, 0.02219005562555239, 0.06657016687665718, 0.04438011125110478, 0.02219005562555239, 0.02219005562555239, 0.04438011125110478, 0.08876022250220957, 0.11095027812776197, 0.08876022250220957, 0.04438011125110478, 0.15533038937886676, 0.02219005562555239, 0.15533038937886676, 0.041505340138431907, 0.09961281633223656, 0.02490320408305914, 0.033204272110745524, 0.008301068027686381, 0.016602136055372762, 0.07470961224917742, 0.016602136055372762, 0.08301068027686381, 0.016602136055372762, 0.10791388435992295, 0.11621495238760933, 0.04980640816611828, 0.08301068027686381, 0.033204272110745524, 0.09961281633223656, 0.02490320408305914, 0.02490320408305914, 0.016602136055372762, 0.02490320408305914, 0.008301068027686381, 0.007764425453150633, 0.007764425453150633, 0.14752408360986202, 0.023293276359451897, 0.015528850906301265, 0.03105770181260253, 0.015528850906301265, 0.007764425453150633, 0.015528850906301265, 0.12423080725041012, 0.09317310543780759, 0.046586552718903794, 0.023293276359451897, 0.046586552718903794, 0.03882212726575316, 0.007764425453150633, 0.06211540362520506, 0.015528850906301265, 0.20187506178191644, 0.06987982907835569, 0.018560327822106436, 0.05568098346631931, 0.05568098346631931, 0.018560327822106436, 0.09280163911053219, 0.018560327822106436, 0.05568098346631931, 0.11136196693263863, 0.018560327822106436, 0.018560327822106436, 0.05568098346631931, 0.09280163911053219, 0.05568098346631931, 0.12992229475474507, 0.05568098346631931, 0.05568098346631931, 0.018560327822106436, 0.018560327822106436, 0.07424131128842575, 0.1361980181757037, 0.06809900908785185, 0.06809900908785185, 0.034049504543925926, 0.034049504543925926, 0.17024752271962962, 0.20429702726355553, 0.034049504543925926, 0.034049504543925926, 0.17024752271962962, 0.128850093898199, 0.04295003129939966, 0.08590006259879931, 0.04295003129939966, 0.04295003129939966, 0.04295003129939966, 0.04295003129939966, 0.515400375592796, 0.08565103020884553, 0.057100686805897014, 0.07137585850737127, 0.4139799793427534, 0.014275171701474253, 0.08565103020884553, 0.08565103020884553, 0.15702688871621678, 0.023425871162639066, 0.2108328404637516, 0.2108328404637516, 0.04685174232527813, 0.023425871162639066, 0.04685174232527813, 0.023425871162639066, 0.04685174232527813, 0.0702776134879172, 0.04685174232527813, 0.04685174232527813, 0.0702776134879172, 0.04685174232527813, 0.035092535740861346, 0.017546267870430673, 0.035092535740861346, 0.017546267870430673, 0.17546267870430673, 0.07018507148172269, 0.08773133935215337, 0.035092535740861346, 0.08773133935215337, 0.05263880361129202, 0.017546267870430673, 0.035092535740861346, 0.10527760722258404, 0.017546267870430673, 0.08773133935215337, 0.08773133935215337, 0.05062523232402679, 0.016875077441342264, 0.7762535623017441, 0.08437538720671131, 0.03375015488268453, 0.2545243164094486, 0.19089323730708643, 0.03181553955118108, 0.06363107910236215, 0.06363107910236215, 0.06363107910236215, 0.09544661865354322, 0.03181553955118108, 0.09544661865354322, 0.03181553955118108, 0.02716636647214151, 0.19016456530499057, 0.02716636647214151, 0.05433273294428302, 0.10866546588856604, 0.02716636647214151, 0.10866546588856604, 0.13583183236070756, 0.24449729824927358, 0.05433273294428302, 0.02716636647214151, 0.13755513669229724, 0.0917034244615315, 0.04585171223076575, 0.04585171223076575, 0.183406848923063, 0.183406848923063, 0.04585171223076575, 0.13755513669229724, 0.0917034244615315, 0.2852655142422488, 0.0815044326406425, 0.12225664896096375, 0.0815044326406425, 0.0815044326406425, 0.0815044326406425, 0.163008865281285, 0.0815044326406425, 0.05911836766609929, 0.016890962190314083, 0.021113702737892606, 0.04222740547578521, 0.004222740547578521, 0.18580058409345493, 0.3842693898296454, 0.004222740547578521, 0.004222740547578521, 0.012668221642735563, 0.004222740547578521, 0.008445481095157041, 0.10979125423704154, 0.004222740547578521, 0.0802320704039919, 0.033781924380628166, 0.008445481095157041, 0.004222740547578521, 0.004222740547578521, 0.004222740547578521, 0.019397813549186925, 0.9504928639101594, 0.24413980103595634, 0.02034498341966303, 0.10172491709831515, 0.08137993367865212, 0.10172491709831515, 0.02034498341966303, 0.1424148839376412, 0.1424148839376412, 0.061034950258989085, 0.02034498341966303, 0.02034498341966303, 0.04068996683932606, 0.0840043125132902, 0.0420021562566451, 0.14700754689825787, 0.0420021562566451, 0.0420021562566451, 0.02100107812832255, 0.0420021562566451, 0.0420021562566451, 0.02100107812832255, 0.02100107812832255, 0.06300323438496766, 0.10500539064161275, 0.02100107812832255, 0.02100107812832255, 0.0420021562566451, 0.02100107812832255, 0.0420021562566451, 0.1680086250265804, 0.025868753400452034, 0.07760626020135611, 0.05173750680090407, 0.025868753400452034, 0.10347501360180814, 0.18108127380316424, 0.05173750680090407, 0.025868753400452034, 0.05173750680090407, 0.2586875340045204, 0.05173750680090407, 0.025868753400452034, 0.025868753400452034, 0.06982335012000014, 0.13964670024000028, 0.2094700503600004, 0.3142050755400006, 0.03491167506000007, 0.1047350251800002, 0.03491167506000007, 0.03491167506000007, 0.03491167506000007, 0.10107501442268618, 0.5053750721134309, 0.03790313040850732, 0.03790313040850732, 0.025268753605671546, 0.025268753605671546, 0.17688127523970082, 0.07580626081701464, 0.6076593189849103, 0.017361694828140296, 0.03472338965628059, 0.03472338965628059, 0.24306372759396414, 0.05208508448442088, 0.09918039876746856, 0.07273229242947694, 0.03306013292248952, 0.026448106337991616, 0.026448106337991616, 0.01983607975349371, 0.03967215950698742, 0.07934431901397485, 0.03967215950698742, 0.026448106337991616, 0.026448106337991616, 0.026448106337991616, 0.05289621267598323, 0.013224053168995808, 0.01983607975349371, 0.006612026584497904, 0.01983607975349371, 0.07273229242947694, 0.01983607975349371, 0.03306013292248952, 0.006612026584497904, 0.01983607975349371, 0.006612026584497904, 0.026448106337991616, 0.05289621267598323, 0.07273229242947694, 0.013224053168995808, 0.026448106337991616, 0.032046011754627506, 0.09613803526388252, 0.12818404701851002, 0.12818404701851002, 0.06409202350925501, 0.032046011754627506, 0.09613803526388252, 0.032046011754627506, 0.12818404701851002, 0.06409202350925501, 0.06409202350925501, 0.032046011754627506, 0.032046011754627506, 0.032046011754627506, 0.032046011754627506, 0.08482567249557407, 0.25447701748672225, 0.021206418123893518, 0.021206418123893518, 0.08482567249557407, 0.1060320906194676, 0.12723850874336112, 0.06361925437168056, 0.021206418123893518, 0.021206418123893518, 0.042412836247787036, 0.08482567249557407, 0.021206418123893518, 0.07156196604096693, 0.09107886587032155, 0.05204506621161231, 0.05855069948806385, 0.05204506621161231, 0.04553943293516077, 0.019516899829354617, 0.026022533105806156, 0.006505633276451539, 0.026022533105806156, 0.026022533105806156, 0.026022533105806156, 0.1171013989761277, 0.026022533105806156, 0.05855069948806385, 0.019516899829354617, 0.05855069948806385, 0.032528166382257695, 0.013011266552903078, 0.08457323259387, 0.019516899829354617, 0.026022533105806156, 0.006505633276451539, 0.013011266552903078, 0.006505633276451539, 0.16048903234080777, 0.045854009240230795, 0.045854009240230795, 0.045854009240230795, 0.20634304158103858, 0.09170801848046159, 0.045854009240230795, 0.09170801848046159, 0.045854009240230795, 0.11463502310057698, 0.045854009240230795, 0.045854009240230795, 0.056014171230162946, 0.04001012230725925, 0.008002024461451849, 0.008002024461451849, 0.032008097845807396, 0.04001012230725925, 0.02400607338435555, 0.008002024461451849, 0.6001518346088888, 0.04001012230725925, 0.032008097845807396, 0.008002024461451849, 0.02400607338435555, 0.016004048922903698, 0.04001012230725925, 0.008002024461451849, 0.008002024461451849, 0.3905244008444698, 0.045944047158172914, 0.06891607073725937, 0.48241249516081564, 0.10169364339223286, 0.025423410848058214, 0.05084682169611643, 0.025423410848058214, 0.05084682169611643, 0.10169364339223286, 0.025423410848058214, 0.1779638759364075, 0.025423410848058214, 0.07627023254417464, 0.1779638759364075, 0.10169364339223286, 0.05084682169611643, 0.02897534660297979, 0.043463019904469685, 0.06519452985670453, 0.03621918325372474, 0.043463019904469685, 0.0072438366507449475, 0.043463019904469685, 0.11590138641191916, 0.0072438366507449475, 0.043463019904469685, 0.03621918325372474, 0.043463019904469685, 0.02897534660297979, 0.014487673301489895, 0.07243836650744948, 0.0072438366507449475, 0.014487673301489895, 0.08692603980893937, 0.021731509952234843, 0.14487673301489895, 0.0072438366507449475, 0.02897534660297979, 0.043463019904469685, 0.021731509952234843, 0.10682551374917032, 0.25249666886167527, 0.009711410340833665, 0.12624833443083763, 0.029134231022500994, 0.009711410340833665, 0.40787923431501394, 0.009711410340833665, 0.009711410340833665, 0.01942282068166733, 0.026040744379915402, 0.26040744379915404, 0.13020372189957702, 0.20832595503932322, 0.026040744379915402, 0.026040744379915402, 0.026040744379915402, 0.0781222331397462, 0.026040744379915402, 0.13020372189957702, 0.09386218755384568, 0.02346554688846142, 0.28158656266153703, 0.04693109377692284, 0.02346554688846142, 0.02346554688846142, 0.30505210954999845, 0.02346554688846142, 0.02346554688846142, 0.02346554688846142, 0.1173277344423071, 0.03187973145464259, 0.014168769535396707, 0.0017710961919245884, 0.03719302003041636, 0.02302425049501965, 0.024795346686944237, 0.005313288575773766, 0.04781959718196389, 0.03719302003041636, 0.07792823244468189, 0.04781959718196389, 0.019482058111170472, 0.010626577151547531, 0.0460485009900393, 0.015939865727321294, 0.030108635262718002, 0.03542192383849177, 0.008855480959622943, 0.017710961919245885, 0.005313288575773766, 0.05136178956581307, 0.017710961919245885, 0.06375946290928518, 0.024795346686944237, 0.026566442878868828, 0.012397673343472118, 0.019482058111170472, 0.024795346686944237, 0.012397673343472118, 0.02302425049501965, 0.0460485009900393, 0.021253154303095063, 0.04073521241426553, 0.005313288575773766, 0.021253154303095063, 0.02302425049501965, 0.028337539070793415, 0.05089933876216891, 0.10179867752433781, 0.025449669381084453, 0.10179867752433781, 0.025449669381084453, 0.15269801628650673, 0.05089933876216891, 0.05089933876216891, 0.10179867752433781, 0.17814768566759118, 0.05089933876216891, 0.025449669381084453, 0.07634900814325336, 0.09589801647154274, 0.023974504117885685, 0.04794900823577137, 0.09589801647154274, 0.16782152882519977, 0.1438470247073141, 0.023974504117885685, 0.04794900823577137, 0.023974504117885685, 0.07192351235365704, 0.023974504117885685, 0.09589801647154274, 0.07192351235365704, 0.023974504117885685, 0.12287970625410653, 0.13233199135057627, 0.009452285096469733, 0.0283568552894092, 0.009452285096469733, 0.009452285096469733, 0.047261425482348664, 0.03780914038587893, 0.018904570192939466, 0.0567137105788184, 0.0850705658682276, 0.009452285096469733, 0.0567137105788184, 0.2552116976046828, 0.018904570192939466, 0.06616599567528814, 0.018904570192939466, 0.009452285096469733, 0.3311382707158423, 0.04730546724512033, 0.023652733622560165, 0.023652733622560165, 0.04730546724512033, 0.023652733622560165, 0.21287460260304147, 0.141916401735361, 0.11826366811280083, 0.07201050834883761, 0.04800700556589174, 0.07201050834883761, 0.09601401113178348, 0.04800700556589174, 0.07201050834883761, 0.02400350278294587, 0.07201050834883761, 0.12001751391472934, 0.02400350278294587, 0.16802451948062108, 0.02400350278294587, 0.02400350278294587, 0.14402101669767522, 0.15613163453905285, 0.11709872590428964, 0.39032908634763214, 0.07806581726952642, 0.07806581726952642, 0.03903290863476321, 0.03903290863476321, 0.07806581726952642, 0.05139653341970987, 0.09545070492231833, 0.03671180958550705, 0.18355904792753525, 0.05873889533681128, 0.02936944766840564, 0.09545070492231833, 0.02202708575130423, 0.02936944766840564, 0.02202708575130423, 0.01468472383420282, 0.00734236191710141, 0.00734236191710141, 0.04405417150260846, 0.02936944766840564, 0.04405417150260846, 0.00734236191710141, 0.00734236191710141, 0.00734236191710141, 0.04405417150260846, 0.02202708575130423, 0.01468472383420282, 0.02936944766840564, 0.01468472383420282, 0.02936944766840564, 0.03671180958550705, 0.02202708575130423, 0.2245625081659857, 0.024951389796220632, 0.024951389796220632, 0.12475694898110316, 0.024951389796220632, 0.09980555918488253, 0.34931945714708884, 0.049902779592441264, 0.024951389796220632, 0.024951389796220632, 0.12348356967986766, 0.20580594946644612, 0.28812832925302456, 0.04116118989328922, 0.04116118989328922, 0.08232237978657844, 0.04116118989328922, 0.04116118989328922, 0.04116118989328922, 0.04116118989328922, 0.030812237162003403, 0.061624474324006806, 0.09243671148601021, 0.09243671148601021, 0.21568566013402382, 0.15406118581001702, 0.061624474324006806, 0.09243671148601021, 0.09243671148601021, 0.061624474324006806, 0.030812237162003403, 0.08695197020054002, 0.15216594785094503, 0.021737992550135005, 0.26085591060162006, 0.021737992550135005, 0.021737992550135005, 0.04347598510027001, 0.15216594785094503, 0.08695197020054002, 0.021737992550135005, 0.021737992550135005, 0.08695197020054002, 0.17738645007984633, 0.05068184287995609, 0.15204552863986828, 0.025340921439978046, 0.025340921439978046, 0.025340921439978046, 0.05068184287995609, 0.025340921439978046, 0.025340921439978046, 0.15204552863986828, 0.17738645007984633, 0.07602276431993414, 0.042937677534514974, 0.21468838767257487, 0.007156279589085829, 0.021468838767257487, 0.14312559178171658, 0.028625118356343317, 0.042937677534514974, 0.042937677534514974, 0.021468838767257487, 0.050093957123600805, 0.007156279589085829, 0.007156279589085829, 0.08587535506902995, 0.028625118356343317, 0.014312559178171659, 0.17890698972714572, 0.014312559178171659, 0.042937677534514974, 0.0744803689698802, 0.0744803689698802, 0.01862009224247005, 0.13034064569729034, 0.01862009224247005, 0.13034064569729034, 0.0744803689698802, 0.05586027672741015, 0.01862009224247005, 0.16758083018223044, 0.01862009224247005, 0.05586027672741015, 0.1117205534548203, 0.016012387261473863, 0.09607432356884317, 0.06404954904589545, 0.016012387261473863, 0.09607432356884317, 0.016012387261473863, 0.016012387261473863, 0.032024774522947726, 0.09607432356884317, 0.048037161784421585, 0.016012387261473863, 0.06404954904589545, 0.06404954904589545, 0.17613625987621248, 0.032024774522947726, 0.08006193630736931, 0.032024774522947726, 0.048037161784421585, 0.021361167661704275, 0.028481556882272364, 0.035601946102840454, 0.014240778441136182, 0.04984272454397664, 0.05696311376454473, 0.05696311376454473, 0.04272233532340855, 0.078324281426249, 0.021361167661704275, 0.014240778441136182, 0.007120389220568091, 0.035601946102840454, 0.007120389220568091, 0.05696311376454473, 0.035601946102840454, 0.007120389220568091, 0.021361167661704275, 0.035601946102840454, 0.007120389220568091, 0.05696311376454473, 0.04272233532340855, 0.1495281736319299, 0.014240778441136182, 0.007120389220568091, 0.035601946102840454, 0.007120389220568091, 0.014240778441136182, 0.014240778441136182, 0.007120389220568091, 0.028481556882272364, 0.019278908082614506, 0.15423126466091605, 0.2506258050739886, 0.05783672424784352, 0.019278908082614506, 0.05783672424784352, 0.05783672424784352, 0.09639454041307254, 0.019278908082614506, 0.2313468969913741, 0.06502503249727036, 0.03251251624863518, 0.03251251624863518, 0.09753754874590555, 0.03251251624863518, 0.09753754874590555, 0.1625625812431759, 0.03251251624863518, 0.03251251624863518, 0.03251251624863518, 0.29261264623771666, 0.12262254335813214, 0.0919669075185991, 0.1839338150371982, 0.1839338150371982, 0.39852326591392945, 0.0037096509327000653, 0.009274127331750164, 0.2448369615582043, 0.1539505137070527, 0.007419301865400131, 0.018548254663500328, 0.03338685839430059, 0.0018548254663500326, 0.005564476399050098, 0.024112731062550425, 0.27451416901980485, 0.012983778264450229, 0.007419301865400131, 0.0018548254663500326, 0.005564476399050098, 0.05564476399050098, 0.022257905596200393, 0.018548254663500328, 0.011128952798100196, 0.03524168386065062, 0.009274127331750164, 0.005564476399050098, 0.007419301865400131, 0.02782238199525049, 0.03167611209802325, 0.09502833629406975, 0.07919028024505813, 0.04751416814703487, 0.03167611209802325, 0.04751416814703487, 0.17421861653912787, 0.126704448392093, 0.04751416814703487, 0.09502833629406975, 0.09502833629406975, 0.04751416814703487, 0.03167611209802325, 0.04751416814703487, 0.06851342114523576, 0.10962147383237722, 0.013702684229047152, 0.013702684229047152, 0.013702684229047152, 0.6988368956814048, 0.05481073691618861, 0.14510283333488957, 0.03436646052668437, 0.04964044298298854, 0.019092478070380208, 0.015273982456304165, 0.06109592982521666, 0.015273982456304165, 0.038184956140760416, 0.007636991228152082, 0.03054796491260833, 0.015273982456304165, 0.019092478070380208, 0.007636991228152082, 0.019092478070380208, 0.003818495614076041, 0.019092478070380208, 0.015273982456304165, 0.003818495614076041, 0.0649144254392927, 0.015273982456304165, 0.03054796491260833, 0.003818495614076041, 0.003818495614076041, 0.026729469298532288, 0.0649144254392927, 0.03054796491260833, 0.091643894737825, 0.0458219473689125, 0.02291097368445625, 0.02291097368445625, 0.011455486842228125, 0.011455486842228125, 0.02291097368445625, 0.003818495614076041, 0.019092478070380208, 0.19838292107508368, 0.016531910089590304, 0.02479786513438546, 0.008265955044795152, 0.10745741558233698, 0.04132977522397577, 0.03306382017918061, 0.008265955044795152, 0.008265955044795152, 0.02479786513438546, 0.016531910089590304, 0.016531910089590304, 0.008265955044795152, 0.11572337062713214, 0.02479786513438546, 0.03306382017918061, 0.09092550549274668, 0.03306382017918061, 0.07439359540315638, 0.05786168531356607, 0.016531910089590304, 0.02479786513438546, 0.008265955044795152, 0.0400559401587189, 0.0801118803174378, 0.060083910238078346, 0.0801118803174378, 0.0801118803174378, 0.20027970079359447, 0.02002797007935945, 0.060083910238078346, 0.0801118803174378, 0.10013985039679724, 0.1602237606348756, 0.02002797007935945, 0.02002797007935945, 0.031549700800041255, 0.12619880320016502, 0.06309940160008251, 0.09464910240012377, 0.031549700800041255, 0.031549700800041255, 0.09464910240012377, 0.2208479056002888, 0.031549700800041255, 0.12619880320016502, 0.09464910240012377, 0.15605075492908738, 0.028009109859066964, 0.032010411267505104, 0.028009109859066964, 0.06402082253501021, 0.028009109859066964, 0.024007808450628828, 0.05601821971813393, 0.016005205633752552, 0.024007808450628828, 0.020006507042190688, 0.004001301408438138, 0.012003904225314414, 0.008002602816876276, 0.020006507042190688, 0.028009109859066964, 0.04401431549281952, 0.008002602816876276, 0.020006507042190688, 0.032010411267505104, 0.016005205633752552, 0.03601171267594324, 0.012003904225314414, 0.032010411267505104, 0.03601171267594324, 0.012003904225314414, 0.004001301408438138, 0.06001952112657207, 0.004001301408438138, 0.016005205633752552, 0.024007808450628828, 0.048015616901257656, 0.048015616901257656, 0.27735288320213647, 0.07924368091489613, 0.07924368091489613, 0.013207280152482688, 0.05282912060993075, 0.013207280152482688, 0.09245096106737881, 0.07924368091489613, 0.06603640076241343, 0.013207280152482688, 0.026414560304965376, 0.05282912060993075, 0.05282912060993075, 0.039621840457448064, 0.05282912060993075, 0.10573615219106328, 0.015105164598723325, 0.0604206583948933, 0.03021032919744665, 0.015105164598723325, 0.03021032919744665, 0.7250479007387196, 0.03743926242469413, 0.11231778727408237, 0.03743926242469413, 0.03743926242469413, 0.07487852484938826, 0.11231778727408237, 0.299514099397553, 0.03743926242469413, 0.03743926242469413, 0.03743926242469413, 0.11231778727408237, 0.14580525446052972, 0.04165864413157992, 0.08331728826315984, 0.02082932206578996, 0.12497593239473977, 0.02082932206578996, 0.06248796619736988, 0.02082932206578996, 0.08331728826315984, 0.06248796619736988, 0.1666345765263197, 0.04165864413157992, 0.02082932206578996, 0.02082932206578996, 0.02082932206578996, 0.018422789201958074, 0.05526836760587422, 0.018422789201958074, 0.0736911568078323, 0.018422789201958074, 0.515838097654826, 0.018422789201958074, 0.018422789201958074, 0.09211394600979037, 0.09211394600979037, 0.03684557840391615, 0.32482571513970204, 0.040603214392462755, 0.08120642878492551, 0.48723857270955306, 0.02335299115842276, 0.04670598231684552, 0.02335299115842276, 0.07005897347526828, 0.02335299115842276, 0.04670598231684552, 0.02335299115842276, 0.5137658054853007, 0.02335299115842276, 0.16347093810895932, 0.09669281524878956, 0.09669281524878956, 0.12086601906098696, 0.16921242668538175, 0.14503922287318435, 0.02417320381219739, 0.09669281524878956, 0.04834640762439478, 0.04834640762439478, 0.04834640762439478, 0.04834640762439478, 0.04834640762439478, 0.08129911316530969, 0.3455212309525662, 0.020324778291327422, 0.040649556582654844, 0.060974334873982274, 0.060974334873982274, 0.020324778291327422, 0.060974334873982274, 0.10162389145663712, 0.040649556582654844, 0.14227344803929196, 0.020324778291327422, 0.11246311474656617, 0.03748770491552206, 0.07497540983104412, 0.07497540983104412, 0.07497540983104412, 0.22492622949313235, 0.11246311474656617, 0.14995081966208823, 0.009769830975885635, 0.009769830975885635, 0.009769830975885635, 0.16120221110211297, 0.009769830975885635, 0.024424577439714087, 0.014654746463828451, 0.03907932390354254, 0.07815864780708508, 0.01953966195177127, 0.004884915487942817, 0.034194408415599724, 0.01953966195177127, 0.03907932390354254, 0.004884915487942817, 0.004884915487942817, 0.004884915487942817, 0.09769830975885635, 0.009769830975885635, 0.009769830975885635, 0.029309492927656902, 0.014654746463828451, 0.048849154879428175, 0.009769830975885635, 0.048849154879428175, 0.04396423939148535, 0.009769830975885635, 0.1416625491503417, 0.03907932390354254, 0.07974235434164856, 0.17277510107357186, 0.2923886325860447, 0.053161569561099034, 0.026580784780549517, 0.31896941736659423, 0.03987117717082428, 0.10708180319955747, 0.06692612699972342, 0.06692612699972342, 0.0803113523996681, 0.013385225399944684, 0.026770450799889368, 0.04015567619983405, 0.13385225399944684, 0.053540901599778735, 0.20077838099917025, 0.06692612699972342, 0.026770450799889368, 0.04015567619983405, 0.026770450799889368, 0.013385225399944684, 0.0267262327717627, 0.0534524655435254, 0.16035739663057622, 0.0534524655435254, 0.3741672588046778, 0.1069049310870508, 0.0534524655435254, 0.13363116385881352, 0.05413528597853396, 0.05413528597853396, 0.10827057195706792, 0.10827057195706792, 0.3248117158712038, 0.02706764298926698, 0.05413528597853396, 0.1624058579356019, 0.02706764298926698, 0.02706764298926698, 0.05992873672168681, 0.11985747344337362, 0.14982184180421704, 0.029964368360843405, 0.029964368360843405, 0.05992873672168681, 0.029964368360843405, 0.05992873672168681, 0.08989310508253022, 0.029964368360843405, 0.08989310508253022, 0.05992873672168681, 0.08989310508253022, 0.029964368360843405, 0.029964368360843405, 0.03150242476784202, 0.06300484953568404, 0.2835218229105782, 0.12600969907136808, 0.03150242476784202, 0.03150242476784202, 0.03150242476784202, 0.06300484953568404, 0.06300484953568404, 0.03150242476784202, 0.06300484953568404, 0.03150242476784202, 0.06300484953568404, 0.012976889656264706, 0.09083822759385295, 0.10381511725011765, 0.07786133793758823, 0.07786133793758823, 0.051907558625058824, 0.025953779312529412, 0.06488444828132353, 0.012976889656264706, 0.09083822759385295, 0.1946533448439706, 0.012976889656264706, 0.012976889656264706, 0.012976889656264706, 0.012976889656264706, 0.051907558625058824, 0.051907558625058824, 0.11827241113354305, 0.11827241113354305, 0.11827241113354305, 0.03942413704451435, 0.0788482740890287, 0.03942413704451435, 0.11827241113354305, 0.03942413704451435, 0.0788482740890287, 0.03942413704451435, 0.1576965481780574, 0.009999166024481079, 0.019998332048962158, 0.019998332048962158, 0.10999082626929187, 0.009999166024481079, 0.05999499614688647, 0.039996664097924316, 0.04999583012240539, 0.1899841544651405, 0.029997498073443235, 0.029997498073443235, 0.07999332819584863, 0.009999166024481079, 0.1399883243427351, 0.019998332048962158, 0.019998332048962158, 0.06999416217136754, 0.009999166024481079, 0.039996664097924316, 0.019998332048962158, 0.009197617079947007, 0.06438331955962906, 0.027592851239841024, 0.009197617079947007, 0.03679046831978803, 0.027592851239841024, 0.09197617079947008, 0.027592851239841024, 0.04598808539973504, 0.009197617079947007, 0.08277855371952307, 0.03679046831978803, 0.03679046831978803, 0.027592851239841024, 0.027592851239841024, 0.009197617079947007, 0.03679046831978803, 0.07358093663957606, 0.027592851239841024, 0.03679046831978803, 0.04598808539973504, 0.027592851239841024, 0.08277855371952307, 0.08277855371952307, 0.018395234159894015, 0.2278864882902388, 0.1139432441451194, 0.13293378483597262, 0.018990540690853232, 0.018990540690853232, 0.018990540690853232, 0.037981081381706465, 0.07596216276341293, 0.037981081381706465, 0.07596216276341293, 0.018990540690853232, 0.037981081381706465, 0.018990540690853232, 0.0569716220725597, 0.037981081381706465, 0.037981081381706465, 0.018990540690853232, 0.03843410899897882, 0.03843410899897882, 0.03843410899897882, 0.09608527249744706, 0.09608527249744706, 0.09608527249744706, 0.07686821799795764, 0.32668992649131995, 0.07686821799795764, 0.01921705449948941, 0.01921705449948941, 0.01921705449948941, 0.05765116349846823, 0.030955524578156913, 0.061911049156313826, 0.18573314746894148, 0.12382209831262765, 0.09286657373447074, 0.030955524578156913, 0.061911049156313826, 0.12382209831262765, 0.061911049156313826, 0.09286657373447074, 0.12382209831262765, 0.13721509605603655, 0.2195441536896585, 0.05488603842241462, 0.02744301921120731, 0.08232905763362193, 0.13721509605603655, 0.02744301921120731, 0.02744301921120731, 0.13721509605603655, 0.10977207684482924, 0.2761219873222275, 0.07889199637777927, 0.19722999094444818, 0.15778399275555854, 0.15778399275555854, 0.07889199637777927, 0.2800462532742041, 0.04667437554570069, 0.04667437554570069, 0.04667437554570069, 0.14002312663710206, 0.09334875109140138, 0.04667437554570069, 0.14002312663710206, 0.04667437554570069, 0.09334875109140138, 0.05308167766031177, 0.1592450329809353, 0.035387785106874514, 0.017693892553437257, 0.1238572478740608, 0.14155114042749806, 0.017693892553437257, 0.035387785106874514, 0.07077557021374903, 0.035387785106874514, 0.08846946276718629, 0.017693892553437257, 0.05308167766031177, 0.08846946276718629, 0.035387785106874514, 0.035387785106874514, 0.1120136883608613, 0.0373378961202871, 0.1493515844811484, 0.1120136883608613, 0.0373378961202871, 0.1120136883608613, 0.1120136883608613, 0.0373378961202871, 0.0746757922405742, 0.0746757922405742, 0.0373378961202871, 0.0373378961202871, 0.07173758428797655, 0.023912528095992185, 0.21521275286392966, 0.04782505619198437, 0.07173758428797655, 0.04782505619198437, 0.023912528095992185, 0.07173758428797655, 0.16738769667194528, 0.1434751685759531, 0.09565011238396874, 0.07170178661933543, 0.03585089330966772, 0.10755267992900314, 0.10755267992900314, 0.03585089330966772, 0.01792544665483386, 0.03585089330966772, 0.05377633996450157, 0.14340357323867087, 0.1613290198935047, 0.01792544665483386, 0.07170178661933543, 0.125478126583837, 0.039958704776677345, 0.11987611433003202, 0.31966963821341876, 0.07991740955335469, 0.11987611433003202, 0.039958704776677345, 0.039958704776677345, 0.11987611433003202, 0.07991740955335469, 0.028872408070123614, 0.028872408070123614, 0.08661722421037085, 0.08661722421037085, 0.11548963228049446, 0.05774481614024723, 0.11548963228049446, 0.028872408070123614, 0.11548963228049446, 0.028872408070123614, 0.028872408070123614, 0.1732344484207417, 0.05774481614024723, 0.18537495902729445, 0.03707499180545889, 0.03707499180545889, 0.03707499180545889, 0.18537495902729445, 0.18537495902729445, 0.03707499180545889, 0.25952494263821224, 0.40540627825936526, 0.03243250226074922, 0.04864875339112383, 0.16216251130374612, 0.03243250226074922, 0.01621625113037461, 0.04864875339112383, 0.08108125565187306, 0.03243250226074922, 0.06486500452149845, 0.04864875339112383, 0.07395458204185622, 0.02739058594142823, 0.01917341015899976, 0.013695292970714115, 0.01917341015899976, 0.04656399610042799, 0.005478117188285646, 0.0712155234477134, 0.005478117188285646, 0.05478117188285646, 0.00821717578242847, 0.00821717578242847, 0.0356077617238567, 0.01917341015899976, 0.013695292970714115, 0.021912468753142584, 0.1917341015899976, 0.08764987501257034, 0.002739058594142823, 0.010956234376571292, 0.024651527347285405, 0.02739058594142823, 0.010956234376571292, 0.08764987501257034, 0.00821717578242847, 0.02739058594142823, 0.010956234376571292, 0.010956234376571292, 0.021912468753142584, 0.010956234376571292, 0.013695292970714115, 0.06017119588672758, 0.06017119588672758, 0.15042798971681895, 0.21059918560354654, 0.09025679383009137, 0.12034239177345515, 0.06017119588672758, 0.03008559794336379, 0.06017119588672758, 0.06017119588672758, 0.03008559794336379, 0.03008559794336379, 0.10668758927137537, 0.16003138390706306, 0.07112505951425024, 0.0444531621964064, 0.05334379463568768, 0.02667189731784384, 0.00889063243928128, 0.01778126487856256, 0.01778126487856256, 0.01778126487856256, 0.00889063243928128, 0.01778126487856256, 0.03556252975712512, 0.00889063243928128, 0.00889063243928128, 0.22226581098203202, 0.07112505951425024, 0.00889063243928128, 0.0444531621964064, 0.0444531621964064, 0.04685567774252615, 0.04685567774252615, 0.07028351661378922, 0.023427838871263075, 0.04685567774252615, 0.07028351661378922, 0.0937113554850523, 0.04685567774252615, 0.023427838871263075, 0.23427838871263074, 0.16399487209884153, 0.0937113554850523, 0.25245061869828067, 0.04207510311638011, 0.04207510311638011, 0.08415020623276022, 0.08415020623276022, 0.12622530934914034, 0.08415020623276022, 0.12622530934914034, 0.12622530934914034, 0.03139127927456681, 0.06278255854913362, 0.03139127927456681, 0.06278255854913362, 0.03139127927456681, 0.06278255854913362, 0.06278255854913362, 0.31391279274566813, 0.2825215134711013, 0.07489241178164678, 0.09361551472705847, 0.03744620589082339, 0.05616930883623508, 0.07489241178164678, 0.018723102945411695, 0.018723102945411695, 0.03744620589082339, 0.13106172061788185, 0.13106172061788185, 0.018723102945411695, 0.14978482356329356, 0.018723102945411695, 0.018723102945411695, 0.07489241178164678, 0.018723102945411695, 0.17612559541559863, 0.132094196561699, 0.132094196561699, 0.17612559541559863, 0.04403139885389966, 0.04403139885389966, 0.04403139885389966, 0.132094196561699, 0.04403139885389966, 0.04403139885389966, 0.04403139885389966, 0.06591813617928483, 0.016479534044821206, 0.04943860213446362, 0.49438602134463616, 0.03295906808964241, 0.016479534044821206, 0.016479534044821206, 0.016479534044821206, 0.2307134766274969, 0.03295906808964241, 0.44758475507556356, 0.008951695101511272, 0.04475847550755636, 0.008951695101511272, 0.026855085304533816, 0.026855085304533816, 0.026855085304533816, 0.0626618657105789, 0.0626618657105789, 0.008951695101511272, 0.017903390203022545, 0.17008220692871415, 0.05371017060906763, 0.008951695101511272, 0.05637302079669346, 0.005637302079669346, 0.05637302079669346, 0.02818651039834673, 0.02818651039834673, 0.07892222911537085, 0.022549208318677384, 0.10147143743404823, 0.03382381247801607, 0.016911906239008036, 0.005637302079669346, 0.03382381247801607, 0.005637302079669346, 0.016911906239008036, 0.05073571871702411, 0.005637302079669346, 0.011274604159338692, 0.005637302079669346, 0.011274604159338692, 0.02818651039834673, 0.011274604159338692, 0.02818651039834673, 0.016911906239008036, 0.016911906239008036, 0.0732849270357015, 0.05073571871702411, 0.039461114557685424, 0.0732849270357015, 0.011274604159338692, 0.02818651039834673, 0.022549208318677384, 0.18153384974072237, 0.051866814211634964, 0.025933407105817482, 0.025933407105817482, 0.025933407105817482, 0.025933407105817482, 0.051866814211634964, 0.10373362842326993, 0.025933407105817482, 0.051866814211634964, 0.051866814211634964, 0.025933407105817482, 0.051866814211634964, 0.051866814211634964, 0.025933407105817482, 0.025933407105817482, 0.15560044263490488, 0.06544714441740587, 0.13089428883481175, 0.13089428883481175, 0.032723572208702936, 0.032723572208702936, 0.1636178610435147, 0.032723572208702936, 0.032723572208702936, 0.06544714441740587, 0.032723572208702936, 0.06544714441740587, 0.032723572208702936, 0.032723572208702936, 0.09817071662610881, 0.33290189141838006, 0.03698909904648667, 0.05548364856973001, 0.05548364856973001, 0.018494549523243337, 0.018494549523243337, 0.03698909904648667, 0.05548364856973001, 0.11096729713946002, 0.018494549523243337, 0.018494549523243337, 0.03698909904648667, 0.1479563961859467, 0.018494549523243337, 0.03698909904648667, 0.0740158173177624, 0.04608532021671998, 0.0740158173177624, 0.055860994202084825, 0.030723546811146654, 0.030723546811146654, 0.015361773405573327, 0.0027930497101042416, 0.026533972245990294, 0.02513744739093817, 0.03910269594145938, 0.006982624275260603, 0.009775673985364846, 0.02094787282578181, 0.02094787282578181, 0.013965248550521206, 0.03630964623135514, 0.009775673985364846, 0.01675829826062545, 0.0013965248550521208, 0.015361773405573327, 0.011172198840416966, 0.022344397680833933, 0.051671419636928465, 0.029327021956094535, 0.011172198840416966, 0.03910269594145938, 0.06284361847734543, 0.012568723695469085, 0.034913121376303015, 0.01815482311567757, 0.012568723695469085, 0.02094787282578181, 0.012568723695469085, 0.015361773405573327, 0.008379149130312725, 0.02513744739093817, 0.013965248550521206, 0.026533972245990294, 0.06364301837297127, 0.04773226377972846, 0.06364301837297127, 0.01591075459324282, 0.06364301837297127, 0.15910754593242818, 0.5409656561702558, 0.01591075459324282, 0.03281984157860254, 0.16409920789301272, 0.03281984157860254, 0.16409920789301272, 0.29537857420742286, 0.16409920789301272, 0.03281984157860254, 0.03281984157860254, 0.03281984157860254, 0.03707168979133733, 0.004119076643481925, 0.012357229930445777, 0.020595383217409627, 0.0164763065739277, 0.05354799636526503, 0.08650060951312043, 0.39955043441774674, 0.00823815328696385, 0.004119076643481925, 0.0164763065739277, 0.004119076643481925, 0.00823815328696385, 0.00823815328696385, 0.05354799636526503, 0.00823815328696385, 0.00823815328696385, 0.07826245622615659, 0.0329526131478554, 0.03707168979133733, 0.0659052262957108, 0.028833536504373477, 0.18795851090742483, 0.42290664954170587, 0.32892739408799343, 0.1450714775081793, 0.024178579584696554, 0.04835715916939311, 0.024178579584696554, 0.09671431833878621, 0.024178579584696554, 0.04835715916939311, 0.4110358529398414, 0.04835715916939311, 0.07253573875408965, 0.024178579584696554, 0.01822659624554547, 0.01822659624554547, 0.0036453192491090938, 0.010935957747327281, 0.021871915494654563, 0.014581276996436375, 0.0036453192491090938, 0.04131361815656973, 0.05589489515300611, 0.21264362286469715, 0.00607553208184849, 0.015796383412806075, 0.001215106416369698, 0.023087021911024262, 0.021871915494654563, 0.03037766040924245, 0.002430212832739396, 0.010935957747327281, 0.04738915023841822, 0.001215106416369698, 0.020656809078284866, 0.019441702661915167, 0.013366170580066677, 0.07169127856581219, 0.0036453192491090938, 0.04009851174020003, 0.1215106416369698, 0.019441702661915167, 0.019441702661915167, 0.04009851174020003, 0.01215106416369698, 0.0036453192491090938, 0.0036453192491090938, 0.002430212832739396, 0.04738915023841822, 0.1821007178646775, 0.0364201435729355, 0.072840287145871, 0.145680574291742, 0.2549410050105485, 0.10926043071880649, 0.145680574291742, 0.042720318745338, 0.0085440637490676, 0.0683525099925408, 0.0085440637490676, 0.085440637490676, 0.0512643824944056, 0.0341762549962704, 0.0341762549962704, 0.0085440637490676, 0.0256321912472028, 0.0085440637490676, 0.085440637490676, 0.1452490837341492, 0.042720318745338, 0.0598084462434732, 0.0170881274981352, 0.0085440637490676, 0.0341762549962704, 0.0085440637490676, 0.0256321912472028, 0.0085440637490676, 0.0341762549962704, 0.0512643824944056, 0.0085440637490676, 0.042720318745338, 0.0256321912472028, 0.29807443223830543, 0.07451860805957636, 0.07451860805957636, 0.11177791208936454, 0.11177791208936454, 0.03725930402978818, 0.03725930402978818, 0.14903721611915272, 0.07451860805957636, 0.033224055507121174, 0.033224055507121174, 0.033224055507121174, 0.033224055507121174, 0.16612027753560585, 0.033224055507121174, 0.06644811101424235, 0.06644811101424235, 0.06644811101424235, 0.16612027753560585, 0.033224055507121174, 0.06644811101424235, 0.033224055507121174, 0.06644811101424235, 0.035279103591326334, 0.10583731077397901, 0.035279103591326334, 0.07055820718265267, 0.10583731077397901, 0.035279103591326334, 0.10583731077397901, 0.10583731077397901, 0.035279103591326334, 0.24695372513928435, 0.035279103591326334, 0.035279103591326334, 0.048097285761561345, 0.028292521036212555, 0.005658504207242511, 0.04526803365794009, 0.005658504207242511, 0.036780277347076326, 0.022634016828970045, 0.0028292521036212557, 0.008487756310863766, 0.1782428825281391, 0.005658504207242511, 0.005658504207242511, 0.1980476472534879, 0.0028292521036212557, 0.05658504207242511, 0.005658504207242511, 0.031121773139833813, 0.005658504207242511, 0.028292521036212555, 0.033951025243455064, 0.1725843783208966, 0.01980476472534879, 0.053755789968803855, 0.23909818679214967, 0.034156883827449955, 0.034156883827449955, 0.13662753530979982, 0.06831376765489991, 0.034156883827449955, 0.10247065148234986, 0.06831376765489991, 0.10247065148234986, 0.13662753530979982, 0.034156883827449955, 0.054743477327160846, 0.027371738663580423, 0.054743477327160846, 0.027371738663580423, 0.054743477327160846, 0.19160217064506296, 0.13685869331790212, 0.054743477327160846, 0.054743477327160846, 0.027371738663580423, 0.054743477327160846, 0.21897390930864338, 0.04724918301255016, 0.0067498832875071664, 0.03374941643753583, 0.04724918301255016, 0.0067498832875071664, 0.03374941643753583, 0.3172445145128368, 0.013499766575014333, 0.0067498832875071664, 0.06749883287507166, 0.0202496498625215, 0.04724918301255016, 0.026999533150028666, 0.04724918301255016, 0.026999533150028666, 0.013499766575014333, 0.03374941643753583, 0.013499766575014333, 0.080998599450086, 0.08774848273759316, 0.013499766575014333, 0.007547663496167321, 0.04151214922892027, 0.015095326992334642, 0.045285980977003926, 0.007547663496167321, 0.015095326992334642, 0.022642990488501963, 0.05283364447317125, 0.0037738317480836606, 0.011321495244250981, 0.0037738317480836606, 0.0037738317480836606, 0.007547663496167321, 0.411347660541119, 0.22265607313693597, 0.04905981272508759, 0.06792897146550589, 0.48966156190911936, 0.028803621288771727, 0.20162534902140208, 0.028803621288771727, 0.23042897031017381, 0.060238562898416545, 0.007529820362302068, 0.030119281449208272, 0.015059640724604136, 0.03764910181151034, 0.015059640724604136, 0.007529820362302068, 0.5647365271726551, 0.045178922173812405, 0.09035784434762481, 0.10541748507222895, 0.041533392863841063, 0.14536687502344373, 0.18690026788728478, 0.06230008929576159, 0.020766696431920532, 0.020766696431920532, 0.020766696431920532, 0.12460017859152318, 0.020766696431920532, 0.041533392863841063, 0.020766696431920532, 0.020766696431920532, 0.18690026788728478, 0.020766696431920532, 0.020766696431920532, 0.09536025336071484, 0.19072050672142968, 0.06357350224047656, 0.06357350224047656, 0.22250725784166797, 0.12714700448095312, 0.03178675112023828, 0.06357350224047656, 0.03178675112023828, 0.06357350224047656, 0.30398858903832304, 0.06079771780766461, 0.06079771780766461, 0.08106362374355282, 0.10132952967944102, 0.06079771780766461, 0.020265905935888204, 0.12159543561532922, 0.06079771780766461, 0.08106362374355282, 0.1769139074675202, 0.1879710266842402, 0.033171357650160035, 0.033171357650160035, 0.06634271530032007, 0.07739983451704008, 0.05528559608360006, 0.033171357650160035, 0.11057119216720011, 0.12162831138392013, 0.022114238433440025, 0.05528559608360006, 0.022114238433440025, 0.022114238433440025, 0.03716978183029876, 0.02123987533159929, 0.026549844164499115, 0.0690295948276977, 0.015929906498699468, 0.11681931432379611, 0.02123987533159929, 0.0690295948276977, 0.010619937665799646, 0.06371962599479787, 0.09557943899219681, 0.026549844164499115, 0.03716978183029876, 0.02123987533159929, 0.07964953249349735, 0.07964953249349735, 0.06371962599479787, 0.03716978183029876, 0.02123987533159929, 0.010619937665799646, 0.058409657161898054, 0.010619937665799646, 0.10934062591705371, 0.03644687530568457, 0.10934062591705371, 0.10934062591705371, 0.18223437652842286, 0.03644687530568457, 0.03644687530568457, 0.03644687530568457, 0.10934062591705371, 0.07289375061136914, 0.07289375061136914, 0.03644687530568457, 0.5957567764821095, 0.022913722172388823, 0.06874116651716647, 0.022913722172388823, 0.045827444344777646, 0.06874116651716647, 0.022913722172388823, 0.11456861086194411, 0.26349185135462516, 0.14638436186368065, 0.02927687237273613, 0.02927687237273613, 0.2049381066091529, 0.11710748949094452, 0.02927687237273613, 0.02927687237273613, 0.11710748949094452, 0.0172638615715144, 0.0172638615715144, 0.2762217851442304, 0.20716633885817282, 0.0345277231430288, 0.0345277231430288, 0.0172638615715144, 0.1381108925721152, 0.10358316942908641, 0.0345277231430288, 0.1208470310006008, 0.33072034192192745, 0.07516371407316533, 0.022549114221949596, 0.022549114221949596, 0.19542565659022984, 0.07516371407316533, 0.007516371407316533, 0.015032742814633066, 0.04509822844389919, 0.03006548562926613, 0.17287654236828026, 0.12551700454688633, 0.06275850227344316, 0.03137925113672158, 0.1882755068203295, 0.06275850227344316, 0.06275850227344316, 0.06275850227344316, 0.06275850227344316, 0.2824132602304943, 0.10072155682807317, 0.06714770455204877, 0.033573852276024385, 0.033573852276024385, 0.06714770455204877, 0.13429540910409754, 0.06714770455204877, 0.033573852276024385, 0.3693123750362683, 0.033573852276024385, 0.0051543551566844556, 0.015463065470053368, 0.02577177578342228, 0.010308710313368911, 0.056697906723529014, 0.0051543551566844556, 0.03608048609679119, 0.17009372017058705, 0.13401323407379584, 0.020617420626737822, 0.010308710313368911, 0.056697906723529014, 0.05154355156684456, 0.02577177578342228, 0.010308710313368911, 0.020617420626737822, 0.2886438887743295, 0.02577177578342228, 0.030926130940106735, 0.01796941315059918, 0.01796941315059918, 0.6289294602709713, 0.03593882630119836, 0.03593882630119836, 0.12578589205419427, 0.03593882630119836, 0.12578589205419427, 0.012689777259795886, 0.02537955451959177, 0.012689777259795886, 0.07613866355877531, 0.05075910903918354, 0.06344888629897943, 0.012689777259795886, 0.5964195312104067, 0.05075910903918354, 0.07613866355877531, 0.2006669311580093, 0.11466681780457674, 0.08600011335343255, 0.05733340890228837, 0.028666704451144186, 0.1433335222557209, 0.05733340890228837, 0.08600011335343255, 0.05733340890228837, 0.11466681780457674, 0.022206203314908725, 0.04441240662981745, 0.022206203314908725, 0.011103101657454362, 0.011103101657454362, 0.04441240662981745, 0.022206203314908725, 0.011103101657454362, 0.022206203314908725, 0.13323721988945236, 0.03330930497236309, 0.04441240662981745, 0.04441240662981745, 0.022206203314908725, 0.06661860994472618, 0.011103101657454362, 0.07772171160218054, 0.011103101657454362, 0.0888248132596349, 0.022206203314908725, 0.055515508287271816, 0.055515508287271816, 0.07772171160218054, 0.055515508287271816, 0.0369267465870366, 0.0369267465870366, 0.0369267465870366, 0.0738534931740732, 0.0369267465870366, 0.2215604795222196, 0.0369267465870366, 0.0738534931740732, 0.2584872261092562, 0.1107802397611098, 0.03099025296930924, 0.09297075890792772, 0.01033008432310308, 0.09297075890792772, 0.02066016864620616, 0.01033008432310308, 0.07231059026172156, 0.02066016864620616, 0.01033008432310308, 0.03099025296930924, 0.02066016864620616, 0.06198050593861848, 0.08264067458482464, 0.03099025296930924, 0.1033008432310308, 0.03099025296930924, 0.01033008432310308, 0.02066016864620616, 0.04132033729241232, 0.02066016864620616, 0.02066016864620616, 0.02066016864620616, 0.0516504216155154, 0.03099025296930924, 0.04132033729241232, 0.3703915050205462, 0.14815660200821848, 0.07407830100410924, 0.14815660200821848, 0.03703915050205462, 0.07407830100410924, 0.03703915050205462, 0.45653145759248515, 0.02853321609953032, 0.02853321609953032, 0.02853321609953032, 0.02853321609953032, 0.02853321609953032, 0.1426660804976516, 0.02853321609953032, 0.08559964829859096, 0.02853321609953032, 0.08559964829859096, 0.19062030686869536, 0.02382753835858692, 0.7386536891161946, 0.13664717337115484, 0.10248538002836614, 0.2049707600567323, 0.10248538002836614, 0.2049707600567323, 0.10248538002836614, 0.03416179334278871, 0.06832358668557742, 0.03416179334278871, 0.020823845925381793, 0.010411922962690896, 0.12494307555229076, 0.010411922962690896, 0.010411922962690896, 0.020823845925381793, 0.020823845925381793, 0.15617884444036345, 0.03123576888807269, 0.07288346073883627, 0.010411922962690896, 0.03123576888807269, 0.13535499851498164, 0.08329538370152717, 0.010411922962690896, 0.020823845925381793, 0.010411922962690896, 0.03123576888807269, 0.020823845925381793, 0.05205961481345448, 0.09370730666421806, 0.15012114237761504, 0.02144587748251643, 0.02144587748251643, 0.15012114237761504, 0.08578350993006573, 0.02144587748251643, 0.04289175496503286, 0.21445877482516432, 0.0643376324475493, 0.02144587748251643, 0.02144587748251643, 0.1286752648950986, 0.040644754852799246, 0.020322377426399623, 0.040644754852799246, 0.040644754852799246, 0.060967132279198866, 0.020322377426399623, 0.040644754852799246, 0.040644754852799246, 0.020322377426399623, 0.08128950970559849, 0.1829013968375966, 0.1016118871319981, 0.14225664198479734, 0.020322377426399623, 0.040644754852799246, 0.060967132279198866, 0.040644754852799246, 0.1029361117384094, 0.02573402793460235, 0.4117444469536376, 0.06433506983650587, 0.18013819554221644, 0.012867013967301175, 0.012867013967301175, 0.012867013967301175, 0.06433506983650587, 0.02573402793460235, 0.02573402793460235, 0.012867013967301175, 0.012867013967301175, 0.012867013967301175, 0.040768447942410684, 0.10192111985602671, 0.18345801574084808, 0.08153689588482137, 0.020384223971205342, 0.020384223971205342, 0.1426895677984374, 0.16307379176964273, 0.020384223971205342, 0.10192111985602671, 0.020384223971205342, 0.020384223971205342, 0.08153689588482137, 0.12557376923667277, 0.20405737500959326, 0.06278688461833638, 0.015696721154584096, 0.10987704808208867, 0.04709016346375229, 0.015696721154584096, 0.015696721154584096, 0.03139344230916819, 0.07848360577292048, 0.10987704808208867, 0.03139344230916819, 0.09418032692750458, 0.04709016346375229, 0.025847793276368866, 0.03994658960893371, 0.009399197555043225, 0.01644859572132564, 0.0375967902201729, 0.01879839511008645, 0.03289719144265128, 0.014098796332564837, 0.15978635843573483, 0.004699598777521612, 0.004699598777521612, 0.002349799388760806, 0.014098796332564837, 0.004699598777521612, 0.03054739205389048, 0.028197592665129674, 0.021148194498847254, 0.04934578716397693, 0.0751935804403458, 0.002349799388760806, 0.004699598777521612, 0.01879839511008645, 0.002349799388760806, 0.06579438288530257, 0.002349799388760806, 0.01879839511008645, 0.009399197555043225, 0.002349799388760806, 0.01174899694380403, 0.06109478410778096, 0.01174899694380403, 0.03994658960893371, 0.13628836454812676, 0.0070493981662824185, 0.01174899694380403, 0.25637779092353835, 0.032047223865442294, 0.032047223865442294, 0.1602361193272115, 0.09614167159632689, 0.032047223865442294, 0.09614167159632689, 0.09614167159632689, 0.032047223865442294, 0.12818889546176918, 0.23608911292605292, 0.11804455646302646, 0.11804455646302646, 0.05902227823151323, 0.14755569557878306, 0.08853341734726984, 0.05902227823151323, 0.05902227823151323, 0.029511139115756615, 0.05902227823151323, 0.02551050435151861, 0.05102100870303722, 0.3061260522182233, 0.02551050435151861, 0.05102100870303722, 0.45918907832733497, 0.05102100870303722, 0.006843855580554975, 0.00273754223222199, 0.012318940044998955, 0.03558804901888587, 0.017794024509442934, 0.07938872473443771, 0.06570101357332776, 0.01368771116110995, 0.023269108973886916, 0.00821262669666597, 0.004106313348332985, 0.001368771116110995, 0.0821262669666597, 0.00821262669666597, 0.004106313348332985, 0.001368771116110995, 0.00547508446444398, 0.02463788008999791, 0.039694362367218856, 0.034219277902774875, 0.009581397812776966, 0.053382073528328806, 0.034219277902774875, 0.00821262669666597, 0.04653821794777383, 0.01095016892888796, 0.03011296455444189, 0.00273754223222199, 0.004106313348332985, 0.00273754223222199, 0.017794024509442934, 0.30797350112497385, 0.08253983974051686, 0.06190487980538764, 0.06190487980538764, 0.288889439091809, 0.04126991987025843, 0.12380975961077528, 0.06190487980538764, 0.020634959935129215, 0.020634959935129215, 0.06190487980538764, 0.08253983974051686, 0.04126991987025843, 0.020634959935129215, 0.01736743884351848, 0.2431441438092587, 0.01736743884351848, 0.03473487768703696, 0.10420463306111087, 0.13893951074814784, 0.03473487768703696, 0.2778790214962957, 0.03473487768703696, 0.03473487768703696, 0.03473487768703696, 0.05077200766294883, 0.040617606130359064, 0.04569480689665394, 0.020308803065179532, 0.04569480689665394, 0.03554040536406418, 0.03554040536406418, 0.025386003831474414, 0.09646681455960278, 0.025386003831474414, 0.010154401532589766, 0.030463204597769297, 0.010154401532589766, 0.010154401532589766, 0.05584920842924371, 0.005077200766294883, 0.10154401532589766, 0.005077200766294883, 0.03554040536406418, 0.015231602298884648, 0.010154401532589766, 0.020308803065179532, 0.005077200766294883, 0.020308803065179532, 0.010154401532589766, 0.020308803065179532, 0.005077200766294883, 0.05077200766294883, 0.015231602298884648, 0.020308803065179532, 0.05584920842924371, 0.025386003831474414, 0.03554040536406418, 0.008722752749004069, 0.008722752749004069, 0.10467303298804884, 0.10467303298804884, 0.034891010996016276, 0.02616825824701221, 0.07850477474103662, 0.04361376374502035, 0.008722752749004069, 0.3401873572111587, 0.12211853848605697, 0.017445505498008138, 0.0872275274900407, 0.1358729624462803, 0.6793648122314014, 0.01941042320661147, 0.15528338565289176, 0.09557974908254303, 0.07646379926603443, 0.11469569889905164, 0.019115949816508607, 0.36320304651366353, 0.05734784944952582, 0.09557974908254303, 0.09557974908254303, 0.05734784944952582, 0.019115949816508607, 0.014714300446535886, 0.014714300446535886, 0.1030001031257512, 0.014714300446535886, 0.058857201786143544, 0.08828580267921532, 0.04414290133960766, 0.08828580267921532, 0.04414290133960766, 0.04414290133960766, 0.014714300446535886, 0.07357150223267943, 0.058857201786143544, 0.19128590580496652, 0.029428600893071772, 0.1030001031257512, 0.014714300446535886, 0.039413352912088806, 0.019706676456044403, 0.0591200293681332, 0.07882670582417761, 0.019706676456044403, 0.0591200293681332, 0.019706676456044403, 0.0591200293681332, 0.37442685266484366, 0.019706676456044403, 0.039413352912088806, 0.09853338228022201, 0.07882670582417761, 0.039413352912088806, 0.0035197333693470965, 0.024638133585429673, 0.0035197333693470965, 0.0035197333693470965, 0.010559200108041289, 0.03167760032412387, 0.33085493671862704, 0.007039466738694193, 0.02815786695477677, 0.010559200108041289, 0.0035197333693470965, 0.052796000540206445, 0.014078933477388386, 0.0035197333693470965, 0.024638133585429673, 0.15838800162061933, 0.1900656019447432, 0.007039466738694193, 0.042236800432165156, 0.042236800432165156, 0.004031727524106295, 0.004031727524106295, 0.012095182572318884, 0.02419036514463777, 0.383014114790098, 0.00806345504821259, 0.028222092668744064, 0.012095182572318884, 0.05241245781338184, 0.02419036514463777, 0.004031727524106295, 0.028222092668744064, 0.15723737344014552, 0.18545946610888958, 0.004031727524106295, 0.03225382019285036, 0.03225382019285036, 0.004031727524106295, 0.03622927878678272, 0.1811463939339136, 0.14491711514713088, 0.10868783636034816, 0.03622927878678272, 0.03622927878678272, 0.28983423029426175, 0.03622927878678272, 0.03622927878678272, 0.03622927878678272, 0.03622927878678272, 0.044301976394397466, 0.044301976394397466, 0.08860395278879493, 0.044301976394397466, 0.08860395278879493, 0.044301976394397466, 0.1329059291831924, 0.044301976394397466, 0.044301976394397466, 0.08860395278879493, 0.08860395278879493, 0.17720790557758986, 0.03528032697565223, 0.05292049046347835, 0.12348114441478282, 0.017640163487826117, 0.08820081743913058, 0.017640163487826117, 0.07056065395130447, 0.05292049046347835, 0.03528032697565223, 0.03528032697565223, 0.03528032697565223, 0.12348114441478282, 0.07056065395130447, 0.017640163487826117, 0.05292049046347835, 0.14112130790260893, 0.15724004688492818, 0.15724004688492818, 0.11793003516369613, 0.19655005860616023, 0.07862002344246409, 0.11793003516369613, 0.07862002344246409, 0.07862002344246409, 0.02833479648187345, 0.009444932160624484, 0.009444932160624484, 0.10389425376686932, 0.037779728642497935, 0.02833479648187345, 0.0566695929637469, 0.6611452512437138, 0.009444932160624484, 0.009444932160624484, 0.009444932160624484, 0.02833479648187345, 0.009444932160624484, 0.04619571363882385, 0.013858714091647155, 0.04157614227494146, 0.013858714091647155, 0.0046195713638823846, 0.013858714091647155, 0.018478285455529538, 0.0923914272776477, 0.0323369995471767, 0.013858714091647155, 0.009239142727764769, 0.0046195713638823846, 0.013858714091647155, 0.060054427730471004, 0.07391314182211815, 0.009239142727764769, 0.018478285455529538, 0.0323369995471767, 0.013858714091647155, 0.05543485636658862, 0.013858714091647155, 0.0046195713638823846, 0.0046195713638823846, 0.05543485636658862, 0.14320671228035392, 0.023097856819411924, 0.018478285455529538, 0.02771742818329431, 0.023097856819411924, 0.0323369995471767, 0.0323369995471767, 0.04157614227494146, 0.2719140263482715, 0.11896238652736878, 0.01699462664676697, 0.20393551976120364, 0.01699462664676697, 0.06797850658706787, 0.03398925329353394, 0.2209301464079706, 0.13747901965488174, 0.06110178651328077, 0.06110178651328077, 0.06110178651328077, 0.04582633988496058, 0.015275446628320193, 0.12220357302656154, 0.04582633988496058, 0.030550893256640385, 0.030550893256640385, 0.030550893256640385, 0.1680299129115221, 0.030550893256640385, 0.04582633988496058, 0.07637723314160096, 0.015275446628320193, 0.24261631344629836, 0.14152618284367405, 0.050545065301312156, 0.020218026120524864, 0.020218026120524864, 0.08087210448209946, 0.09098111754236189, 0.010109013060262432, 0.030327039180787295, 0.1920712481449862, 0.020218026120524864, 0.010109013060262432, 0.020218026120524864, 0.010109013060262432, 0.020218026120524864, 0.010109013060262432, 0.07201982535557364, 0.07201982535557364, 0.0593104444104724, 0.016945841260134972, 0.04236460315033743, 0.033891682520269945, 0.0889656666157086, 0.021182301575168715, 0.008472920630067486, 0.008472920630067486, 0.05083752378040492, 0.004236460315033743, 0.04660106346537117, 0.04660106346537117, 0.008472920630067486, 0.01270938094510123, 0.02541876189020246, 0.008472920630067486, 0.01270938094510123, 0.021182301575168715, 0.02541876189020246, 0.021182301575168715, 0.016945841260134972, 0.04236460315033743, 0.008472920630067486, 0.008472920630067486, 0.01270938094510123, 0.055073984095438656, 0.004236460315033743, 0.0593104444104724, 0.021182301575168715, 0.016945841260134972, 0.0296552222052362, 0.004236460315033743, 0.08792742770817001, 0.24619679758287605, 0.08792742770817001, 0.035170971083268006, 0.08792742770817001, 0.035170971083268006, 0.035170971083268006, 0.017585485541634003, 0.07034194216653601, 0.017585485541634003, 0.017585485541634003, 0.08792742770817001, 0.017585485541634003, 0.035170971083268006, 0.05275645662490201, 0.035170971083268006, 0.1502366512091402, 0.028169372101713788, 0.23474476751428158, 0.046948953502856314, 0.009389790700571263, 0.18779581401142525, 0.009389790700571263, 0.10328769770628389, 0.21596518611313906, 0.376287577314933, 0.10034335395064879, 0.0752575154629866, 0.376287577314933, 0.025085838487662198, 0.14848097891114137, 0.05939239156445655, 0.029696195782228273, 0.05939239156445655, 0.029696195782228273, 0.05939239156445655, 0.05939239156445655, 0.2672657620400545, 0.029696195782228273, 0.029696195782228273, 0.08908858734668482, 0.1187847831289131, 0.08714668044414914, 0.11619557392553219, 0.029048893481383047, 0.3195378282952135, 0.2614400413324474, 0.08714668044414914, 0.029048893481383047, 0.08714668044414914, 0.01392078863770965, 0.034801971594274124, 0.09744552046396755, 0.0278415772754193, 0.06264354886969342, 0.0278415772754193, 0.0556831545508386, 0.006960394318854825, 0.006960394318854825, 0.006960394318854825, 0.0278415772754193, 0.12528709773938684, 0.020881182956564475, 0.01392078863770965, 0.020881182956564475, 0.048722760231983776, 0.020881182956564475, 0.01392078863770965, 0.020881182956564475, 0.06960394318854825, 0.10440591478282238, 0.07656433750740307, 0.006960394318854825, 0.04176236591312895, 0.006960394318854825, 0.020881182956564475, 0.06136618873710031, 0.2659201511941014, 0.14318777371990074, 0.06136618873710031, 0.06136618873710031, 0.10227698122850053, 0.06136618873710031, 0.18409856621130094, 0.04604084599226973, 0.023020422996134864, 0.023020422996134864, 0.04604084599226973, 0.09208169198453946, 0.04604084599226973, 0.1381225379768092, 0.04604084599226973, 0.023020422996134864, 0.04604084599226973, 0.0690612689884046, 0.023020422996134864, 0.36832676793815783, 0.023020422996134864, 0.09122380572494831, 0.09122380572494831, 0.2432634819331955, 0.060815870483298874, 0.09122380572494831, 0.15203967620824718, 0.12163174096659775, 0.030407935241649437, 0.060815870483298874, 0.030407935241649437, 0.22459433541555457, 0.1684457515616659, 0.02807429192694432, 0.02807429192694432, 0.02807429192694432, 0.05614858385388864, 0.02807429192694432, 0.11229716770777728, 0.11229716770777728, 0.11229716770777728, 0.05614858385388864, 0.05614858385388864, 0.07232084235438466, 0.03616042117719233, 0.18080210588596163, 0.03616042117719233, 0.03616042117719233, 0.03616042117719233, 0.03616042117719233, 0.07232084235438466, 0.21696252706315394, 0.03616042117719233, 0.10848126353157697, 0.07232084235438466, 0.021040259631555543, 0.1472818174208888, 0.12624155778933327, 0.08416103852622217, 0.08416103852622217, 0.021040259631555543, 0.021040259631555543, 0.042080519263111085, 0.021040259631555543, 0.021040259631555543, 0.10520129815777772, 0.06312077889466663, 0.021040259631555543, 0.021040259631555543, 0.042080519263111085, 0.06312077889466663, 0.08416103852622217, 0.07583652682537621, 0.08475847115777342, 0.06691458249297902, 0.04907069382818461, 0.07583652682537621, 0.035687777329588805, 0.04014874949578741, 0.04014874949578741, 0.031226805163390207, 0.004460972166198601, 0.026765832997191607, 0.008921944332397201, 0.031226805163390207, 0.013382916498595803, 0.05799263816058181, 0.026765832997191607, 0.05799263816058181, 0.026765832997191607, 0.022304860830993003, 0.031226805163390207, 0.008921944332397201, 0.08475847115777342, 0.008921944332397201, 0.008921944332397201, 0.04907069382818461, 0.035687777329588805, 0.2098851321122882, 0.13117820757018014, 0.026235641514036026, 0.05247128302807205, 0.05247128302807205, 0.07870692454210808, 0.1049425660561441, 0.026235641514036026, 0.13117820757018014, 0.07870692454210808, 0.07870692454210808, 0.0667600095927208, 0.0667600095927208, 0.0333800047963604, 0.2670400383708832, 0.0667600095927208, 0.0667600095927208, 0.3004200431672436, 0.10014001438908118, 0.015530409123326715, 0.04659122736998015, 0.05435643193164351, 0.011647806842495037, 0.007765204561663358, 0.023295613684990075, 0.04659122736998015, 0.0737694433358019, 0.011647806842495037, 0.015530409123326715, 0.007765204561663358, 0.003882602280831679, 0.015530409123326715, 0.06600423877413854, 0.007765204561663358, 0.08153464789746526, 0.04659122736998015, 0.03106081824665343, 0.011647806842495037, 0.019413011404158395, 0.10483026158245533, 0.03106081824665343, 0.003882602280831679, 0.019413011404158395, 0.04659122736998015, 0.03106081824665343, 0.03106081824665343, 0.007765204561663358, 0.007765204561663358, 0.042708625089148466, 0.023295613684990075, 0.015530409123326715, 0.011647806842495037, 0.019413011404158395, 0.003882602280831679, 0.014313120027765122, 0.16221536031467138, 0.08110768015733569, 0.004771040009255041, 0.03339728006478528, 0.009542080018510082, 0.06202352012031553, 0.042939360083295366, 0.028626240055530244, 0.042939360083295366, 0.18129952035169156, 0.009542080018510082, 0.03816832007404033, 0.023855200046275202, 0.009542080018510082, 0.014313120027765122, 0.023855200046275202, 0.014313120027765122, 0.047710400092550405, 0.05725248011106049, 0.019084160037020164, 0.042939360083295366, 0.019084160037020164, 0.014313120027765122, 0.07408144224336263, 0.07408144224336263, 0.018520360560840658, 0.018520360560840658, 0.1296425239258846, 0.037040721121681316, 0.2592850478517692, 0.24076468729092854, 0.018520360560840658, 0.1296425239258846, 0.05653169922925865, 0.05653169922925865, 0.028265849614629326, 0.05653169922925865, 0.08479754884388797, 0.1130633984585173, 0.08479754884388797, 0.05653169922925865, 0.028265849614629326, 0.05653169922925865, 0.028265849614629326, 0.05653169922925865, 0.028265849614629326, 0.028265849614629326, 0.1130633984585173, 0.1130633984585173, 0.05349290014025772, 0.021397160056103086, 0.037445030098180405, 0.048143610126231945, 0.016047870042077316, 0.021397160056103086, 0.05349290014025772, 0.016047870042077316, 0.06419148016830926, 0.010698580028051543, 0.010698580028051543, 0.010698580028051543, 0.010698580028051543, 0.03209574008415463, 0.06954077018233504, 0.010698580028051543, 0.016047870042077316, 0.016047870042077316, 0.06419148016830926, 0.02674645007012886, 0.02674645007012886, 0.06419148016830926, 0.010698580028051543, 0.03209574008415463, 0.016047870042077316, 0.021397160056103086, 0.08023935021038657, 0.016047870042077316, 0.016047870042077316, 0.016047870042077316, 0.0053492900140257715, 0.037445030098180405, 0.04279432011220617, 0.016047870042077316, 0.14550786217401437, 0.19401048289868583, 0.09700524144934292, 0.04850262072467146, 0.19401048289868583, 0.04850262072467146, 0.04850262072467146, 0.14550786217401437, 0.02399498743812017, 0.03085069813472593, 0.034278553483028815, 0.010283566044908644, 0.05827354092114898, 0.02399498743812017, 0.06855710696605763, 0.010283566044908644, 0.02742284278642305, 0.020567132089817287, 0.020567132089817287, 0.04798997487624034, 0.02742284278642305, 0.1268306478872066, 0.0034278553483028814, 0.0034278553483028814, 0.010283566044908644, 0.017139276741514407, 0.05827354092114898, 0.020567132089817287, 0.04456211952793746, 0.010283566044908644, 0.06855710696605763, 0.06170139626945186, 0.02742284278642305, 0.02399498743812017, 0.05827354092114898, 0.04798997487624034, 0.010283566044908644, 0.09760541588322541, 0.0813378465693545, 0.0325351386277418, 0.1301405545109672, 0.0162675693138709, 0.0813378465693545, 0.048802707941612705, 0.048802707941612705, 0.1301405545109672, 0.048802707941612705, 0.0325351386277418, 0.14640812382483812, 0.0813378465693545, 0.08990969471694157, 0.11238711839617696, 0.022477423679235393, 0.04495484735847079, 0.022477423679235393, 0.08990969471694157, 0.06743227103770617, 0.022477423679235393, 0.11238711839617696, 0.04495484735847079, 0.13486454207541235, 0.022477423679235393, 0.04495484735847079, 0.11238711839617696, 0.06743227103770617, 0.022477423679235393, 0.047725738580406005, 0.09545147716081201, 0.11454177259297442, 0.028635443148243605, 0.009545147716081202, 0.03818059086432481, 0.019090295432162403, 0.09545147716081201, 0.047725738580406005, 0.019090295432162403, 0.07636118172864961, 0.047725738580406005, 0.03818059086432481, 0.047725738580406005, 0.03818059086432481, 0.019090295432162403, 0.019090295432162403, 0.019090295432162403, 0.028635443148243605, 0.047725738580406005, 0.12408692030905562, 0.04610000610244974, 0.09220001220489948, 0.06915000915367461, 0.2305000305122487, 0.09220001220489948, 0.06915000915367461, 0.02305000305122487, 0.04610000610244974, 0.04610000610244974, 0.02305000305122487, 0.02305000305122487, 0.02305000305122487, 0.1613500213585741, 0.13313835776982288, 0.026627671553964578, 0.10651068621585831, 0.026627671553964578, 0.026627671553964578, 0.6124364457411853, 0.13452105153505833, 0.06726052576752917, 0.03363026288376458, 0.10089078865129375, 0.06726052576752917, 0.2017815773025875, 0.06726052576752917, 0.10089078865129375, 0.06726052576752917, 0.06726052576752917, 0.03363026288376458, 0.05707276223619945, 0.5517033682832614, 0.0380485081574663, 0.01902425407873315, 0.01902425407873315, 0.0380485081574663, 0.01902425407873315, 0.05707276223619945, 0.0760970163149326, 0.0380485081574663, 0.05707276223619945, 0.01633202531901211, 0.09799215191407265, 0.01633202531901211, 0.08166012659506054, 0.048996075957036325, 0.08166012659506054, 0.09799215191407265, 0.048996075957036325, 0.03266405063802422, 0.048996075957036325, 0.09799215191407265, 0.01633202531901211, 0.01633202531901211, 0.11432417723308476, 0.01633202531901211, 0.048996075957036325, 0.09799215191407265, 0.05124824143480855, 0.06833098857974473, 0.06833098857974473, 0.017082747144936183, 0.05124824143480855, 0.649144391507575, 0.06833098857974473, 0.3024621706914396, 0.14042886496388265, 0.005401110190918564, 0.010802220381837127, 0.005401110190918564, 0.005401110190918564, 0.010802220381837127, 0.39968215412797375, 0.005401110190918564, 0.010802220381837127, 0.032406661145511384, 0.005401110190918564, 0.05401110190918564, 0.00343346434410555, 0.45665075776603814, 0.0961370016349554, 0.00343346434410555, 0.0068669286882111, 0.010300393032316649, 0.0068669286882111, 0.0274677147528444, 0.0137338573764222, 0.0137338573764222, 0.02403425040873885, 0.0068669286882111, 0.20944132499043855, 0.0068669286882111, 0.020600786064633298, 0.00343346434410555, 0.00343346434410555, 0.0068669286882111, 0.0618023581938999, 0.0128488140811301, 0.0513952563245204, 0.0128488140811301, 0.0513952563245204, 0.044970849283955346, 0.0128488140811301, 0.0128488140811301, 0.0128488140811301, 0.0128488140811301, 0.00642440704056505, 0.0128488140811301, 0.05781966336508544, 0.00642440704056505, 0.00642440704056505, 0.00642440704056505, 0.019273221121695147, 0.00642440704056505, 0.0128488140811301, 0.00642440704056505, 0.00642440704056505, 0.5781966336508544, 0.038546442243390294, 0.11279634115635692, 0.03759878038545231, 0.15039512154180923, 0.05639817057817846, 0.05639817057817846, 0.15039512154180923, 0.03759878038545231, 0.03759878038545231, 0.018799390192726154, 0.2819908528908923, 0.03759878038545231, 0.017192081127900723, 0.06876832451160289, 0.05157624338370217, 0.09455644620345398, 0.017192081127900723, 0.06876832451160289, 0.034384162255801445, 0.04298020281975181, 0.017192081127900723, 0.008596040563950361, 0.017192081127900723, 0.008596040563950361, 0.008596040563950361, 0.04298020281975181, 0.017192081127900723, 0.017192081127900723, 0.017192081127900723, 0.04298020281975181, 0.06017228394765253, 0.06017228394765253, 0.008596040563950361, 0.017192081127900723, 0.034384162255801445, 0.025788121691851084, 0.017192081127900723, 0.008596040563950361, 0.05157624338370217, 0.12034456789530507, 0.03670205714122562, 0.04893607618830082, 0.04893607618830082, 0.19574430475320329, 0.07340411428245124, 0.012234019047075205, 0.03670205714122562, 0.2691484190356545, 0.14680822856490247, 0.02446803809415041, 0.03670205714122562, 0.03670205714122562, 0.0961971020161419, 0.0320657006720473, 0.1923942040322838, 0.0320657006720473, 0.0641314013440946, 0.0961971020161419, 0.0961971020161419, 0.0320657006720473, 0.0961971020161419, 0.0641314013440946, 0.0320657006720473, 0.0641314013440946, 0.0961971020161419, 0.05048502056796907, 0.08834878599394588, 0.08203815842294974, 0.025242510283984537, 0.012621255141992268, 0.006310627570996134, 0.006310627570996134, 0.0378637654259768, 0.06941690328095748, 0.10728066870693428, 0.03155313785498067, 0.006310627570996134, 0.006310627570996134, 0.05048502056796907, 0.05048502056796907, 0.03155313785498067, 0.012621255141992268, 0.06310627570996134, 0.012621255141992268, 0.0189318827129884, 0.025242510283984537, 0.006310627570996134, 0.12621255141992269, 0.06310627570996134, 0.1404173267657023, 0.07020866338285114, 0.07020866338285114, 0.07020866338285114, 0.1404173267657023, 0.1053129950742767, 0.03510433169142557, 0.07020866338285114, 0.03510433169142557, 0.24573032183997898, 0.0656344181800736, 0.09845162727011038, 0.0656344181800736, 0.0656344181800736, 0.164086045450184, 0.19690325454022076, 0.0656344181800736, 0.0656344181800736, 0.0328172090900368, 0.0328172090900368, 0.1312688363601472, 0.04952428177239002, 0.14857284531717008, 0.29714569063434015, 0.14857284531717008, 0.1980971270895601, 0.09904856354478005, 0.04129160968351208, 0.08258321936702416, 0.12387482905053625, 0.04129160968351208, 0.22710385325931645, 0.04129160968351208, 0.02064580484175604, 0.04129160968351208, 0.04129160968351208, 0.18581224357580436, 0.061937414525268124, 0.061937414525268124, 0.04511061624528429, 0.18044246498113717, 0.13533184873585288, 0.04511061624528429, 0.15788715685849503, 0.11277654061321073, 0.06766592436792644, 0.2029977731037793, 0.022555308122642146, 0.06708110786737607, 0.02515541545026603, 0.20124332360212824, 0.10062166180106412, 0.22639873905239427, 0.03354055393368804, 0.03354055393368804, 0.10900680028448613, 0.00838513848342201, 0.13416221573475215, 0.01677027696684402, 0.05031083090053206, 0.013708762586354112, 0.08225257551812466, 0.013708762586354112, 0.013708762586354112, 0.027417525172708224, 0.8225257551812467, 0.012519955765554822, 0.012519955765554822, 0.10015964612443858, 0.36307871720108986, 0.012519955765554822, 0.03755986729666447, 0.4507184075599736, 0.01422301185405259, 0.09956108297836813, 0.05689204741621036, 0.01422301185405259, 0.07111505927026296, 0.02844602370810518, 0.04266903556215777, 0.01422301185405259, 0.05689204741621036, 0.32712927264320957, 0.18489915410268368, 0.04266903556215777, 0.02844602370810518, 0.01422301185405259, 0.18924186238229004, 0.11354511742937402, 0.037848372476458, 0.075696744952916, 0.075696744952916, 0.34063535228812203, 0.075696744952916, 0.075696744952916, 0.06470207395662714, 0.05391839496385594, 0.07548575294939831, 0.04313471597108475, 0.010783678992771188, 0.04313471597108475, 0.010783678992771188, 0.04313471597108475, 0.07548575294939831, 0.010783678992771188, 0.010783678992771188, 0.04313471597108475, 0.021567357985542376, 0.021567357985542376, 0.021567357985542376, 0.010783678992771188, 0.010783678992771188, 0.04313471597108475, 0.05391839496385594, 0.05391839496385594, 0.03235103697831357, 0.021567357985542376, 0.010783678992771188, 0.04313471597108475, 0.010783678992771188, 0.05391839496385594, 0.03235103697831357, 0.04313471597108475, 0.021567357985542376, 0.10825909086040192, 0.036086363620133975, 0.036086363620133975, 0.036086363620133975, 0.07217272724026795, 0.036086363620133975, 0.1443454544805359, 0.18043181810066986, 0.036086363620133975, 0.07217272724026795, 0.036086363620133975, 0.21651818172080384, 0.06173640480014077, 0.28810322240065694, 0.12347280960028154, 0.020578801600046923, 0.041157603200093845, 0.041157603200093845, 0.020578801600046923, 0.020578801600046923, 0.041157603200093845, 0.020578801600046923, 0.041157603200093845, 0.06173640480014077, 0.12347280960028154, 0.041157603200093845, 0.028077612237683545, 0.028077612237683545, 0.08423283671305064, 0.1403880611884177, 0.028077612237683545, 0.36500895908988606, 0.08423283671305064, 0.22462089790146836, 0.18495276284685236, 0.04623819071171309, 0.09247638142342618, 0.04623819071171309, 0.09247638142342618, 0.06935728606756963, 0.023119095355856545, 0.04623819071171309, 0.13871457213513927, 0.06935728606756963, 0.04623819071171309, 0.06935728606756963, 0.09247638142342618, 0.10974640346734824, 0.10974640346734824, 0.05487320173367412, 0.1371830043341853, 0.02743660086683706, 0.02743660086683706, 0.49385881560306705, 0.03715522513718073, 0.14862090054872293, 0.03715522513718073, 0.07431045027436146, 0.03715522513718073, 0.03715522513718073, 0.07431045027436146, 0.22293135082308438, 0.11146567541154219, 0.03715522513718073, 0.11146567541154219, 0.0028944426260339314, 0.017366655756203588, 0.014472213130169657, 0.037627754138441105, 0.0028944426260339314, 0.0028944426260339314, 0.02315554100827145, 0.0549944098946447, 0.031838868886373245, 0.011577770504135726, 0.011577770504135726, 0.04341663939050897, 0.031838868886373245, 0.0028944426260339314, 0.008683327878101794, 0.05788885252067863, 0.04052219676447504, 0.04052219676447504, 0.028944426260339315, 0.005788885252067863, 0.04341663939050897, 0.028944426260339315, 0.017366655756203588, 0.0463110820165429, 0.028944426260339315, 0.06657218039878042, 0.04341663939050897, 0.014472213130169657, 0.017366655756203588, 0.034733311512407175, 0.034733311512407175, 0.028944426260339315, 0.04341663939050897, 0.011577770504135726, 0.02604998363430538, 0.011577770504135726, 0.028944426260339315, 0.026078764583342493, 0.03911814687501374, 0.23470888125008244, 0.013039382291671247, 0.013039382291671247, 0.026078764583342493, 0.026078764583342493, 0.09127567604169873, 0.15647258750005497, 0.026078764583342493, 0.026078764583342493, 0.026078764583342493, 0.026078764583342493, 0.013039382291671247, 0.052157529166684986, 0.052157529166684986, 0.026078764583342493, 0.026078764583342493, 0.026078764583342493, 0.07823629375002748, 0.028445732841526034, 0.0853371985245781, 0.11378293136610414, 0.028445732841526034, 0.05689146568305207, 0.028445732841526034, 0.028445732841526034, 0.11378293136610414, 0.05689146568305207, 0.028445732841526034, 0.14222866420763017, 0.028445732841526034, 0.028445732841526034, 0.05689146568305207, 0.11378293136610414, 0.08228147825190293, 0.03291259130076117, 0.26330073040608937, 0.08228147825190293, 0.39495109560913405, 0.13165036520304468, 0.11775584700337274, 0.02355116940067455, 0.0942046776026982, 0.035326754101011826, 0.08242909290236092, 0.0942046776026982, 0.12953143170371, 0.0471023388013491, 0.35326754101011826, 0.056166088001041704, 0.15726504640291677, 0.03369965280062502, 0.10109895840187506, 0.07863252320145839, 0.01123321760020834, 0.01123321760020834, 0.04493287040083336, 0.02246643520041668, 0.056166088001041704, 0.02246643520041668, 0.04493287040083336, 0.12356539360229174, 0.23589756960437513, 0.0178841764520863, 0.03321347055387456, 0.0025548823502980432, 0.015329294101788259, 0.012774411751490217, 0.0025548823502980432, 0.05109764700596087, 0.030658588203576517, 0.010219529401192173, 0.0025548823502980432, 0.0025548823502980432, 0.022993941152682388, 0.012774411751490217, 0.0025548823502980432, 0.22993941152682387, 0.0025548823502980432, 0.0051097647005960865, 0.007664647050894129, 0.0025548823502980432, 0.04087811760476869, 0.0051097647005960865, 0.06642694110774912, 0.03321347055387456, 0.007664647050894129, 0.13285388221549824, 0.0051097647005960865, 0.1124148234131139, 0.007664647050894129, 0.0025548823502980432, 0.015329294101788259, 0.012774411751490217, 0.0715367058083452, 0.0178841764520863, 0.021519875269272376, 0.021519875269272376, 0.021519875269272376, 0.021519875269272376, 0.04303975053854475, 0.06455962580781713, 0.021519875269272376, 0.04303975053854475, 0.04303975053854475, 0.021519875269272376, 0.15063912688490663, 0.172159002154179, 0.021519875269272376, 0.021519875269272376, 0.0860795010770895, 0.04303975053854475, 0.10759937634636188, 0.04303975053854475, 0.015346973560898432, 0.007673486780449216, 0.046040920682695295, 0.015346973560898432, 0.007673486780449216, 0.06906138102404294, 0.046040920682695295, 0.06906138102404294, 0.007673486780449216, 0.03836743390224608, 0.046040920682695295, 0.09208184136539059, 0.023020460341347648, 0.030693947121796863, 0.007673486780449216, 0.023020460341347648, 0.015346973560898432, 0.007673486780449216, 0.023020460341347648, 0.0997553281458398, 0.015346973560898432, 0.007673486780449216, 0.023020460341347648, 0.03836743390224608, 0.12277578848718745, 0.046040920682695295, 0.030693947121796863, 0.030693947121796863, 0.003459407885778263, 0.02075644731466958, 0.003459407885778263, 0.02075644731466958, 0.027675263086226105, 0.027675263086226105, 0.003459407885778263, 0.017297039428891315, 0.04497230251511742, 0.05189111828667395, 0.11761986811646094, 0.003459407885778263, 0.02421585520044784, 0.04843171040089568, 0.02075644731466958, 0.32864374914893496, 0.03459407885778263, 0.013837631543113052, 0.05880993405823047, 0.05880993405823047, 0.05189111828667395, 0.006918815771556526, 0.17781107377373281, 0.029635178962288802, 0.029635178962288802, 0.7408794740572201, 0.024361193684082495, 0.04872238736816499, 0.12180596842041247, 0.14616716210449496, 0.04872238736816499, 0.024361193684082495, 0.12180596842041247, 0.14616716210449496, 0.024361193684082495, 0.024361193684082495, 0.07308358105224748, 0.07308358105224748, 0.07308358105224748, 0.08444077846417952, 0.21110194616044883, 0.04222038923208976, 0.12666116769626928, 0.04222038923208976, 0.08444077846417952, 0.04222038923208976, 0.04222038923208976, 0.12666116769626928, 0.16888155692835904, 0.3119826753674157, 0.42786195478959865, 0.008913790724783306, 0.08022411652304974, 0.008913790724783306, 0.008913790724783306, 0.01782758144956661, 0.01782758144956661, 0.026741372174349916, 0.044568953623916524, 0.03565516289913322, 0.22659073592265383, 0.04531814718453077, 0.1359544415535923, 0.20393166233038845, 0.022659073592265384, 0.022659073592265384, 0.022659073592265384, 0.11329536796132691, 0.04531814718453077, 0.06797722077679615, 0.022659073592265384, 0.022659073592265384, 0.3397229755948943, 0.01998370444675849, 0.05995111334027547, 0.03996740889351698, 0.09991852223379244, 0.09991852223379244, 0.01998370444675849, 0.03996740889351698, 0.01998370444675849, 0.15986963557406791, 0.01998370444675849, 0.01998370444675849, 0.01998370444675849, 0.01998370444675849, 0.032041190643923176, 0.1602059532196159, 0.09612357193176954, 0.22428833450746224, 0.06408238128784635, 0.2563295251513854, 0.1602059532196159, 0.07380473457848698, 0.8856568149418437, 0.08289213312285176, 0.21759184944748589, 0.04144606656142588, 0.02072303328071294, 0.01036151664035647, 0.02072303328071294, 0.02072303328071294, 0.01036151664035647, 0.01036151664035647, 0.01036151664035647, 0.06216909984213882, 0.02072303328071294, 0.06216909984213882, 0.02072303328071294, 0.06216909984213882, 0.03108454992106941, 0.03108454992106941, 0.01036151664035647, 0.04144606656142588, 0.17614578288606, 0.02072303328071294, 0.044002969335389726, 0.044002969335389726, 0.022001484667694863, 0.044002969335389726, 0.033002227001542296, 0.022001484667694863, 0.05500371166923716, 0.08800593867077945, 0.022001484667694863, 0.08800593867077945, 0.08800593867077945, 0.16501113500771147, 0.06600445400308459, 0.07700519633693202, 0.011000742333847431, 0.05500371166923716, 0.05500371166923716, 0.039000221445464174, 0.039000221445464174, 0.07800044289092835, 0.11700066433639252, 0.07800044289092835, 0.07800044289092835, 0.07800044289092835, 0.039000221445464174, 0.039000221445464174, 0.039000221445464174, 0.039000221445464174, 0.11700066433639252, 0.11700066433639252, 0.07800044289092835, 0.03536581493150311, 0.07073162986300623, 0.09430883981734163, 0.17682907465751554, 0.011788604977167704, 0.08252023484017393, 0.07073162986300623, 0.03536581493150311, 0.011788604977167704, 0.023577209954335408, 0.023577209954335408, 0.023577209954335408, 0.011788604977167704, 0.12967465474884474, 0.011788604977167704, 0.011788604977167704, 0.03536581493150311, 0.03536581493150311, 0.023577209954335408, 0.07073162986300623, 0.05292584476356617, 0.07938876714534925, 0.10585168952713234, 0.05292584476356617, 0.026462922381783084, 0.1323146119089154, 0.026462922381783084, 0.026462922381783084, 0.026462922381783084, 0.026462922381783084, 0.026462922381783084, 0.026462922381783084, 0.10585168952713234, 0.07938876714534925, 0.07938876714534925, 0.05292584476356617, 0.033617896404827693, 0.16808948202413845, 0.06723579280965539, 0.10085368921448308, 0.16808948202413845, 0.13447158561931077, 0.10085368921448308, 0.033617896404827693, 0.033617896404827693, 0.033617896404827693, 0.033617896404827693, 0.2089534259218534, 0.06268602777655602, 0.04179068518437068, 0.06268602777655602, 0.06268602777655602, 0.14626739814529738, 0.04179068518437068, 0.04179068518437068, 0.06268602777655602, 0.02089534259218534, 0.06268602777655602, 0.04179068518437068, 0.08358137036874136, 0.02089534259218534, 0.08171934953421986, 0.08171934953421986, 0.16343869906843972, 0.027239783178073285, 0.10895913271229314, 0.027239783178073285, 0.08171934953421986, 0.08171934953421986, 0.08171934953421986, 0.10895913271229314, 0.10895913271229314, 0.025833167210528373, 0.3616643409473972, 0.025833167210528373, 0.10333266884211349, 0.15499900326317023, 0.15499900326317023, 0.07749950163158512, 0.07749950163158512, 0.232086537064368, 0.017852810543412923, 0.017852810543412923, 0.14282248434730338, 0.12496967380389046, 0.035705621086825845, 0.017852810543412923, 0.10711686326047754, 0.05355843163023877, 0.08926405271706461, 0.10711686326047754, 0.035705621086825845, 0.06001687915949917, 0.06001687915949917, 0.12003375831899835, 0.06001687915949917, 0.12003375831899835, 0.09002531873924875, 0.15004219789874793, 0.030008439579749586, 0.06001687915949917, 0.12003375831899835, 0.030008439579749586, 0.06001687915949917, 0.09488867175143532, 0.07116650381357649, 0.02372216793785883, 0.02372216793785883, 0.11861083968929416, 0.14233300762715298, 0.04744433587571766, 0.35583251906788244, 0.07116650381357649, 0.037059830355110304, 0.07411966071022061, 0.018529915177555152, 0.018529915177555152, 0.1111794910653309, 0.16676923659799636, 0.037059830355110304, 0.3150085580184376, 0.018529915177555152, 0.037059830355110304, 0.07411966071022061, 0.037059830355110304, 0.018529915177555152, 0.10845967718031516, 0.021691935436063032, 0.021691935436063032, 0.0650758063081891, 0.17353548348850426, 0.043383870872126064, 0.3687629024130715, 0.021691935436063032, 0.08676774174425213, 0.043383870872126064, 0.021691935436063032, 0.09823053320568008, 0.03929221328227203, 0.05893831992340805, 0.03929221328227203, 0.019646106641136016, 0.07858442656454406, 0.2553993863347682, 0.07858442656454406, 0.17681495977022416, 0.15716885312908813, 0.21748898062102726, 0.24648751137049754, 0.01449926537473515, 0.04349779612420545, 0.01449926537473515, 0.01449926537473515, 0.24648751137049754, 0.01449926537473515, 0.0289985307494703, 0.0289985307494703, 0.10149485762314606, 0.01449926537473515, 0.0289044683638544, 0.08671340509156321, 0.0578089367277088, 0.2312357469108352, 0.3179491520023984, 0.08671340509156321, 0.17342681018312642, 0.289171221803575, 0.01927808145357167, 0.01927808145357167, 0.057834244360715006, 0.03855616290714334, 0.01927808145357167, 0.07711232581428668, 0.03855616290714334, 0.01927808145357167, 0.01927808145357167, 0.01927808145357167, 0.09639040726785834, 0.057834244360715006, 0.09639040726785834, 0.11566848872143001, 0.1383362481759111, 0.0461120827253037, 0.0461120827253037, 0.0461120827253037, 0.0461120827253037, 0.1383362481759111, 0.0922241654506074, 0.0461120827253037, 0.1383362481759111, 0.0922241654506074, 0.0922241654506074, 0.0461120827253037, 0.04306882183282454, 0.09475140803221398, 0.008613764366564906, 0.008613764366564906, 0.06029635056595435, 0.3790056321288559, 0.034455057466259625, 0.008613764366564906, 0.07752387929908416, 0.025841293099694723, 0.008613764366564906, 0.017227528733129813, 0.051682586199389445, 0.18088905169786304, 0.2535395655353164, 0.04225659425588607, 0.08451318851177214, 0.04225659425588607, 0.1267697827676582, 0.16902637702354428, 0.08451318851177214, 0.08451318851177214, 0.04225659425588607, 0.05568688768206591, 0.1670606630461977, 0.1670606630461977, 0.027843443841032954, 0.08353033152309886, 0.05568688768206591, 0.13921721920516478, 0.22274755072826363, 0.05568688768206591, 0.027843443841032954, 0.055614241494949354, 0.16684272448484805, 0.08342136224242402, 0.4171068112121202, 0.16684272448484805, 0.055614241494949354, 0.020997128165965183, 0.08398851266386073, 0.020997128165965183, 0.020997128165965183, 0.16797702532772146, 0.041994256331930366, 0.041994256331930366, 0.06299138449789554, 0.041994256331930366, 0.06299138449789554, 0.020997128165965183, 0.10498564082982591, 0.020997128165965183, 0.18897415349368665, 0.06299138449789554, 0.01043219913565922, 0.11475419049225141, 0.031296597406977655, 0.3651269697480727, 0.02086439827131844, 0.01043219913565922, 0.01043219913565922, 0.01043219913565922, 0.08345759308527376, 0.01043219913565922, 0.02086439827131844, 0.06259319481395531, 0.06259319481395531, 0.11475419049225141, 0.06259319481395531, 0.06938498028446592, 0.1156416338074432, 0.02312832676148864, 0.2312832676148864, 0.06938498028446592, 0.04625665352297728, 0.06938498028446592, 0.04625665352297728, 0.04625665352297728, 0.1156416338074432, 0.02312832676148864, 0.09251330704595456, 0.2056844235616007, 0.13712294904106714, 0.10284221178080034, 0.10284221178080034, 0.034280737260266786, 0.2056844235616007, 0.034280737260266786, 0.034280737260266786, 0.10284221178080034, 0.15667135199140655, 0.031334270398281315, 0.09400281119484394, 0.06266854079656263, 0.06266854079656263, 0.09400281119484394, 0.09400281119484394, 0.09400281119484394, 0.031334270398281315, 0.031334270398281315, 0.031334270398281315, 0.18800562238968788, 0.09262797269883993, 0.1389419590482599, 0.1389419590482599, 0.18525594539767987, 0.09262797269883993, 0.1389419590482599, 0.04631398634941997, 0.09262797269883993, 0.061723075933542204, 0.015430768983385551, 0.1851692278006266, 0.07715384491692776, 0.015430768983385551, 0.030861537966771102, 0.015430768983385551, 0.015430768983385551, 0.030861537966771102, 0.20059999678401216, 0.015430768983385551, 0.04629230695015665, 0.061723075933542204, 0.015430768983385551, 0.07715384491692776, 0.015430768983385551, 0.030861537966771102, 0.015430768983385551, 0.061723075933542204, 0.1956972341454815, 0.1956972341454815, 0.0782788936581926, 0.0391394468290963, 0.0391394468290963, 0.1174183404872889, 0.1174183404872889, 0.1174183404872889, 0.0782788936581926, 0.17323485475560693, 0.03464697095112138, 0.06929394190224276, 0.03464697095112138, 0.10394091285336415, 0.06929394190224276, 0.3811166804623352, 0.03464697095112138, 0.03464697095112138, 0.11700712140016563, 0.04875296725006901, 0.06825415415009661, 0.05362826397507591, 0.04387767052506211, 0.058503560700082814, 0.05362826397507591, 0.004875296725006901, 0.019501186900027605, 0.04387767052506211, 0.019501186900027605, 0.014625890175020704, 0.019501186900027605, 0.009750593450013802, 0.004875296725006901, 0.04387767052506211, 0.13163301157518634, 0.024376483625034504, 0.03900237380005521, 0.11700712140016563, 0.004875296725006901, 0.03900237380005521, 0.009750593450013802, 0.21005404788683285, 0.10502702394341643, 0.17504503990569403, 0.14003603192455522, 0.035009007981138804, 0.035009007981138804, 0.07001801596227761, 0.07001801596227761, 0.14003603192455522, 0.22901466128030104, 0.09814914054870046, 0.22901466128030104, 0.09814914054870046, 0.03271638018290015, 0.0654327603658003, 0.03271638018290015, 0.16358190091450076, 0.12458836794732628, 0.08305891196488419, 0.1453530959385473, 0.08305891196488419, 0.04152945598244209, 0.06229418397366314, 0.020764727991221046, 0.3114709198683157, 0.08305891196488419, 0.020764727991221046, 0.10616183209861535, 0.03538727736620512, 0.07077455473241023, 0.07077455473241023, 0.10616183209861535, 0.07077455473241023, 0.2123236641972307, 0.03538727736620512, 0.07077455473241023, 0.07077455473241023, 0.10616183209861535, 0.46662609747668343, 0.014140184772020711, 0.042420554316062134, 0.042420554316062134, 0.1414018477202071, 0.09898129340414498, 0.014140184772020711, 0.014140184772020711, 0.028280369544041423, 0.042420554316062134, 0.014140184772020711, 0.042420554316062134, 0.028280369544041423, 0.0458122868312298, 0.0152707622770766, 0.0305415245541532, 0.1221660982166128, 0.0458122868312298, 0.0916245736624596, 0.0152707622770766, 0.5955597288059874, 0.0152707622770766, 0.08984188458279088, 0.2335888999152563, 0.017968376916558178, 0.07187350766623271, 0.08984188458279088, 0.035936753833116356, 0.017968376916558178, 0.017968376916558178, 0.035936753833116356, 0.08984188458279088, 0.28749403066493084, 0.20216173276384686, 0.02888024753769241, 0.08664074261307723, 0.05776049507538482, 0.02888024753769241, 0.02888024753769241, 0.02888024753769241, 0.08664074261307723, 0.05776049507538482, 0.08664074261307723, 0.08664074261307723, 0.20216173276384686, 0.028068898195195806, 0.028068898195195806, 0.028068898195195806, 0.05613779639039161, 0.28068898195195807, 0.028068898195195806, 0.028068898195195806, 0.3087578801471539, 0.05613779639039161, 0.08420669458558741, 0.028068898195195806, 0.028068898195195806, 0.002936440162152713, 0.08222032454027596, 0.044046602432290693, 0.008809320486458138, 0.02055508113506899, 0.011745760648610852, 0.011745760648610852, 0.044046602432290693, 0.011745760648610852, 0.02055508113506899, 0.011745760648610852, 0.005872880324305426, 0.011745760648610852, 0.0704745638916651, 0.002936440162152713, 0.07928388437812324, 0.05872880324305425, 0.026427961459374417, 0.03230084178367984, 0.002936440162152713, 0.03230084178367984, 0.03817372210798527, 0.08515676470242867, 0.05872880324305425, 0.03230084178367984, 0.005872880324305426, 0.011745760648610852, 0.011745760648610852, 0.008809320486458138, 0.011745760648610852, 0.029364401621527127, 0.029364401621527127, 0.017618640972916277, 0.052855922918748834, 0.0687962401741584, 0.0458641601161056, 0.0229320800580528, 0.0229320800580528, 0.0458641601161056, 0.0687962401741584, 0.0229320800580528, 0.0229320800580528, 0.0458641601161056, 0.0687962401741584, 0.0458641601161056, 0.0917283202322112, 0.0917283202322112, 0.0917283202322112, 0.0458641601161056, 0.0687962401741584, 0.0917283202322112, 0.04066640401356474, 0.04066640401356474, 0.04066640401356474, 0.02033320200678237, 0.711662070237383, 0.02033320200678237, 0.02033320200678237, 0.04066640401356474, 0.06099960602034712, 0.035721447235677904, 0.035721447235677904, 0.07144289447135581, 0.1786072361783895, 0.1071643417070337, 0.035721447235677904, 0.035721447235677904, 0.07144289447135581, 0.39293591959245694, 0.16876753006824693, 0.028127921678041158, 0.028127921678041158, 0.15470356922922637, 0.014063960839020579, 0.028127921678041158, 0.014063960839020579, 0.49223862936572027, 0.014063960839020579, 0.014063960839020579, 0.028127921678041158, 0.06322540415262363, 0.27397675132803573, 0.02107513471754121, 0.06322540415262363, 0.08430053887016484, 0.08430053887016484, 0.04215026943508242, 0.12645080830524727, 0.06322540415262363, 0.02107513471754121, 0.10537567358770605, 0.04215026943508242, 0.015688933436936385, 0.019611166796170483, 0.027455633514638676, 0.027455633514638676, 0.07844466718468193, 0.09413360062161831, 0.007844466718468193, 0.027455633514638676, 0.007844466718468193, 0.003922233359234096, 0.027455633514638676, 0.047066800310809155, 0.019611166796170483, 0.015688933436936385, 0.027455633514638676, 0.05098903367004325, 0.003922233359234096, 0.003922233359234096, 0.011766700077702289, 0.027455633514638676, 0.09021136726238423, 0.1059003006993206, 0.003922233359234096, 0.243178468272514, 0.06412551178131735, 0.03847530706879041, 0.01282510235626347, 0.02565020471252694, 0.05130040942505388, 0.02565020471252694, 0.17955143298768858, 0.1667263306314251, 0.03847530706879041, 0.02565020471252694, 0.02565020471252694, 0.02565020471252694, 0.03847530706879041, 0.01282510235626347, 0.06412551178131735, 0.01282510235626347, 0.1282510235626347, 0.02565020471252694, 0.01282510235626347, 0.1006047866635785, 0.0335349288878595, 0.067069857775719, 0.1676746444392975, 0.0335349288878595, 0.335349288878595, 0.0335349288878595, 0.0335349288878595, 0.134139715551438, 0.2130290306564831, 0.12173087466084749, 0.060865437330423745, 0.060865437330423745, 0.060865437330423745, 0.060865437330423745, 0.18259631199127124, 0.2130290306564831, 0.030432718665211873, 0.2398958944968601, 0.021808717681532738, 0.08723487072613095, 0.10904358840766369, 0.08723487072613095, 0.26170461217839286, 0.021808717681532738, 0.06542615304459821, 0.021808717681532738, 0.06542615304459821, 0.04924461563703409, 0.008207435939505682, 0.04924461563703409, 0.008207435939505682, 0.6073502595234205, 0.016414871879011365, 0.03282974375802273, 0.11490410315307954, 0.016414871879011365, 0.016414871879011365, 0.016414871879011365, 0.008207435939505682, 0.008207435939505682, 0.04103717969752841, 0.032052580674010844, 0.06410516134802169, 0.016026290337005422, 0.016026290337005422, 0.11218403235903797, 0.08013145168502711, 0.016026290337005422, 0.06410516134802169, 0.08013145168502711, 0.048078871011016266, 0.016026290337005422, 0.16026290337005422, 0.09615774202203253, 0.032052580674010844, 0.032052580674010844, 0.048078871011016266, 0.048078871011016266, 0.032052580674010844, 0.04956928923140743, 0.024784644615703715, 0.024784644615703715, 0.024784644615703715, 0.04956928923140743, 0.12392322307851858, 0.693970049239704, 0.0890143166750799, 0.035605726670031965, 0.07121145334006393, 0.035605726670031965, 0.05340859000504794, 0.3560572667003196, 0.12462004334511187, 0.10681718001009588, 0.12462004334511187, 0.23523855827762186, 0.05066676639825701, 0.10857164228197931, 0.11580975176744461, 0.03257149268459379, 0.003619054742732644, 0.007238109485465288, 0.021714328456395864, 0.05066676639825701, 0.03257149268459379, 0.003619054742732644, 0.028952437941861153, 0.039809602170059084, 0.04342865691279173, 0.025333383199128506, 0.010857164228197932, 0.003619054742732644, 0.028952437941861153, 0.028952437941861153, 0.09047636856831609, 0.025333383199128506, 0.00804280955725168, 0.23324147716029872, 0.08847090512976848, 0.11259933380152352, 0.03217123822900672, 0.11259933380152352, 0.00804280955725168, 0.0804280955725168, 0.01608561911450336, 0.0402140477862584, 0.05629966690076176, 0.00804280955725168, 0.06434247645801344, 0.0402140477862584, 0.07238528601526512, 0.03217123822900672, 0.11477032591786188, 0.04173466397013159, 0.25040798382078955, 0.08346932794026318, 0.04173466397013159, 0.05216832996266449, 0.010433665992532898, 0.04173466397013159, 0.020867331985065796, 0.33387731176105273, 0.02105695709560093, 0.04211391419120186, 0.017547464246334108, 0.014037971397067287, 0.13336072827213924, 0.0035094928492668217, 0.059661378437535974, 0.010528478547800466, 0.0631708712868028, 0.017547464246334108, 0.024566449944867754, 0.014037971397067287, 0.0315854356434014, 0.0035094928492668217, 0.02105695709560093, 0.010528478547800466, 0.28426892079061256, 0.007018985698533643, 0.1298512354228724, 0.010528478547800466, 0.03860442134193504, 0.04562340704046868, 0.016768931109105635, 0.016768931109105635, 0.08384465554552818, 0.06707572443642254, 0.06707572443642254, 0.06707572443642254, 0.15092037998195074, 0.15092037998195074, 0.25153396663658456, 0.016768931109105635, 0.016768931109105635, 0.016768931109105635, 0.016768931109105635, 0.016768931109105635, 0.014276322276853229, 0.08565793366111937, 0.014276322276853229, 0.010707241707639922, 0.02498356398449315, 0.06424345024583954, 0.017845402846066536, 0.046398047399773, 0.028552644553706458, 0.007138161138426614, 0.003569080569213307, 0.010707241707639922, 0.007138161138426614, 0.007138161138426614, 0.03212172512291977, 0.003569080569213307, 0.007138161138426614, 0.02498356398449315, 0.017845402846066536, 0.02498356398449315, 0.014276322276853229, 0.017845402846066536, 0.003569080569213307, 0.02498356398449315, 0.21771391472201174, 0.010707241707639922, 0.010707241707639922, 0.010707241707639922, 0.010707241707639922, 0.03212172512291977, 0.003569080569213307, 0.003569080569213307, 0.12134873935325245, 0.04282896683055969, 0.02498356398449315, 0.08129489368751038, 0.05655296952174635, 0.10957137844838355, 0.01060368178532744, 0.028276484760873175, 0.024741924165764026, 0.003534560595109147, 0.028276484760873175, 0.01060368178532744, 0.024741924165764026, 0.003534560595109147, 0.003534560595109147, 0.003534560595109147, 0.01060368178532744, 0.1060368178532744, 0.017672802975545732, 0.007069121190218294, 0.007069121190218294, 0.03181104535598232, 0.04948384833152805, 0.014138242380436587, 0.003534560595109147, 0.024741924165764026, 0.01060368178532744, 0.02120736357065488, 0.02120736357065488, 0.02120736357065488, 0.003534560595109147, 0.035345605951091465, 0.0530184089266372, 0.0530184089266372, 0.08482945428261952, 0.035345605951091465, 0.1683546095213563, 0.024050658503050902, 0.024050658503050902, 0.024050658503050902, 0.07215197550915271, 0.07215197550915271, 0.024050658503050902, 0.24050658503050903, 0.048101317006101804, 0.024050658503050902, 0.048101317006101804, 0.07215197550915271, 0.12025329251525452, 0.024050658503050902, 0.05302594674645667, 0.09544670414362201, 0.04242075739716534, 0.02121037869858267, 0.02121037869858267, 0.02121037869858267, 0.02121037869858267, 0.11665708284220468, 0.04242075739716534, 0.010605189349291335, 0.09544670414362201, 0.031815568047874006, 0.010605189349291335, 0.02121037869858267, 0.05302594674645667, 0.04242075739716534, 0.05302594674645667, 0.11665708284220468, 0.05302594674645667, 0.07423632544503934, 0.09416519543617632, 0.10761736621277294, 0.05380868310638647, 0.09416519543617632, 0.06726085388298308, 0.04035651232978985, 0.026904341553193236, 0.026904341553193236, 0.06726085388298308, 0.026904341553193236, 0.04035651232978985, 0.0807130246595797, 0.013452170776596618, 0.013452170776596618, 0.013452170776596618, 0.0807130246595797, 0.05380868310638647, 0.04035651232978985, 0.05380868310638647, 0.0701608975272443, 0.0701608975272443, 0.0701608975272443, 0.2455631413453551, 0.1403217950544886, 0.0701608975272443, 0.03508044876362215, 0.03508044876362215, 0.03508044876362215, 0.0701608975272443, 0.0701608975272443, 0.03508044876362215, 0.031116061922309157, 0.031116061922309157, 0.06223212384461831, 0.031116061922309157, 0.09334818576692747, 0.06223212384461831, 0.06223212384461831, 0.031116061922309157, 0.1555803096115458, 0.18669637153385493, 0.031116061922309157, 0.031116061922309157, 0.031116061922309157, 0.09334818576692747, 0.031116061922309157, 0.02206034446889984, 0.04412068893779968, 0.06618103340669954, 0.06618103340669954, 0.24266378915789827, 0.06618103340669954, 0.02206034446889984, 0.06618103340669954, 0.02206034446889984, 0.02206034446889984, 0.04412068893779968, 0.02206034446889984, 0.04412068893779968, 0.02206034446889984, 0.11030172234449921, 0.04412068893779968, 0.02206034446889984, 0.030213485213708303, 0.7855506155564159, 0.060426970427416606, 0.030213485213708303, 0.015106742606854151, 0.030213485213708303, 0.06973188718517997, 0.008716485898147497, 0.08716485898147497, 0.043582429490737484, 0.03486594359258999, 0.03486594359258999, 0.008716485898147497, 0.03486594359258999, 0.06101540128703248, 0.06101540128703248, 0.043582429490737484, 0.008716485898147497, 0.09588134487962247, 0.06101540128703248, 0.02614945769444249, 0.05229891538888498, 0.10459783077776996, 0.05229891538888498, 0.017432971796294994, 0.02614945769444249, 0.03486594359258999, 0.017432971796294994, 0.008716485898147497, 0.07790424775506875, 0.02596808258502292, 0.02596808258502292, 0.02596808258502292, 0.1558084955101375, 0.12984041292511458, 0.05193616517004584, 0.07790424775506875, 0.05193616517004584, 0.02596808258502292, 0.10387233034009168, 0.05193616517004584, 0.05193616517004584, 0.07790424775506875, 0.05100153365160521, 0.17000511217201736, 0.05100153365160521, 0.15300460095481563, 0.03400102243440347, 0.017000511217201735, 0.03400102243440347, 0.03400102243440347, 0.05100153365160521, 0.05100153365160521, 0.017000511217201735, 0.017000511217201735, 0.1870056233892191, 0.03400102243440347, 0.03400102243440347, 0.017000511217201735, 0.03400102243440347, 0.18684237598300552, 0.3425443559688434, 0.28026356397450825, 0.06228079199433517, 0.09342118799150276, 0.031140395997167586, 0.024617049967141515, 0.07385114990142455, 0.1723193497699906, 0.12308524983570758, 0.1477022998028491, 0.04923409993428303, 0.04923409993428303, 0.27078754963855667, 0.024617049967141515, 0.038834422889236046, 0.019417211444618023, 0.07766884577847209, 0.019417211444618023, 0.038834422889236046, 0.019417211444618023, 0.038834422889236046, 0.09708605722309012, 0.019417211444618023, 0.07766884577847209, 0.07766884577847209, 0.058251634333854066, 0.019417211444618023, 0.058251634333854066, 0.13592048011232616, 0.1747549030015622, 0.15482282415930027, 0.011058773154235733, 0.011058773154235733, 0.04423509261694293, 0.0331763194627072, 0.0663526389254144, 0.15482282415930027, 0.011058773154235733, 0.011058773154235733, 0.04423509261694293, 0.05529386577117867, 0.011058773154235733, 0.04423509261694293, 0.022117546308471465, 0.0331763194627072, 0.011058773154235733, 0.04423509261694293, 0.04423509261694293, 0.011058773154235733, 0.0663526389254144, 0.011058773154235733, 0.11058773154235733, 0.011058773154235733, 0.08499911355100029, 0.050999468130600176, 0.01699982271020006, 0.03399964542040012, 0.3569962769142012, 0.01699982271020006, 0.01699982271020006, 0.01699982271020006, 0.03399964542040012, 0.16999822710200058, 0.050999468130600176, 0.10199893626120035, 0.03488516795965004, 0.5930478553140507, 0.03488516795965004, 0.20931100775790024, 0.03488516795965004, 0.03488516795965004, 0.1434651687638259, 0.0742061217743927, 0.11872979483902832, 0.01484122435487854, 0.039576598279676106, 0.034629523494716596, 0.039576598279676106, 0.009894149569919027, 0.019788299139838053, 0.009894149569919027, 0.024735373924797566, 0.009894149569919027, 0.034629523494716596, 0.004947074784959513, 0.039576598279676106, 0.02968244870975708, 0.034629523494716596, 0.019788299139838053, 0.024735373924797566, 0.019788299139838053, 0.004947074784959513, 0.02968244870975708, 0.01484122435487854, 0.024735373924797566, 0.019788299139838053, 0.039576598279676106, 0.05936489741951416, 0.039576598279676106, 0.004947074784959513, 0.16545123183925692, 0.045123063228888254, 0.09024612645777651, 0.24065633722073734, 0.045123063228888254, 0.015041021076296084, 0.030082042152592168, 0.18049225291555301, 0.030082042152592168, 0.030082042152592168, 0.10528714753407259, 0.12213354398449959, 0.07328012639069975, 0.09770683518759966, 0.17098696157829943, 0.024426708796899916, 0.3175472143596989, 0.024426708796899916, 0.04885341759379983, 0.07328012639069975, 0.2980276528926559, 0.08515075796933026, 0.04257537898466513, 0.17030151593866052, 0.08515075796933026, 0.08515075796933026, 0.1277261369539954, 0.04257537898466513, 0.03543404287769092, 0.023622695251793946, 0.023622695251793946, 0.011811347625896973, 0.07086808575538184, 0.09449078100717578, 0.10630212863307276, 0.08267943338127881, 0.023622695251793946, 0.011811347625896973, 0.023622695251793946, 0.03543404287769092, 0.03543404287769092, 0.03543404287769092, 0.09449078100717578, 0.059056738129484865, 0.023622695251793946, 0.011811347625896973, 0.023622695251793946, 0.023622695251793946, 0.023622695251793946, 0.11811347625896973, 0.023622695251793946, 0.021027666084931264, 0.12616599650958757, 0.04205533216986253, 0.021027666084931264, 0.1682213286794501, 0.08411066433972506, 0.06308299825479378, 0.021027666084931264, 0.04205533216986253, 0.399525655613694, 0.009924360746582584, 0.006616240497721723, 0.0033081202488608615, 0.036389322737469476, 0.0430055632351912, 0.0033081202488608615, 0.006616240497721723, 0.0033081202488608615, 0.039697442986330336, 0.05623804423063465, 0.0033081202488608615, 0.3705094678724165, 0.036389322737469476, 0.0033081202488608615, 0.006616240497721723, 0.0430055632351912, 0.0033081202488608615, 0.0033081202488608615, 0.09593548721696499, 0.0033081202488608615, 0.19187097443392997, 0.029773082239747756, 0.021584624896876223, 0.021584624896876223, 0.6043694971125342, 0.021584624896876223, 0.194261624071886, 0.08633849958750489, 0.07262623738703221, 0.15562765154364044, 0.15562765154364044, 0.04150070707830412, 0.3008801263177049, 0.03112553030872809, 0.11412694446533633, 0.01037517676957603, 0.12450212123491236, 0.06602480544689585, 0.2640992217875834, 0.2640992217875834, 0.06602480544689585, 0.19807441634068754, 0.11004134241149308, 0.1336904656288665, 0.014854496180985166, 0.07427248090492583, 0.07427248090492583, 0.059417984723940664, 0.014854496180985166, 0.014854496180985166, 0.014854496180985166, 0.014854496180985166, 0.029708992361970332, 0.029708992361970332, 0.19310845035280716, 0.0445634885429555, 0.059417984723940664, 0.07427248090492583, 0.1336904656288665, 0.014105707233635684, 0.03526426808408921, 0.04936997531772489, 0.24684987658862448, 0.014105707233635684, 0.042317121700907054, 0.014105707233635684, 0.03526426808408921, 0.07052853616817842, 0.021158560850453527, 0.021158560850453527, 0.021158560850453527, 0.06347568255136057, 0.021158560850453527, 0.09873995063544978, 0.07758138978499626, 0.014105707233635684, 0.007052853616817842, 0.028211414467271368, 0.028211414467271368, 0.07758138978499626, 0.13819030087142428, 0.03454757521785607, 0.13819030087142428, 0.03454757521785607, 0.03454757521785607, 0.3454757521785607, 0.1036427256535682, 0.1036427256535682, 0.010353559063733158, 0.005176779531866579, 0.010353559063733158, 0.06729813391426552, 0.005176779531866579, 0.04659101578679921, 0.18636406314719683, 0.010353559063733158, 0.020707118127466315, 0.010353559063733158, 0.04141423625493263, 0.025883897659332893, 0.020707118127466315, 0.015530338595599737, 0.03623745672306605, 0.04141423625493263, 0.010353559063733158, 0.010353559063733158, 0.015530338595599737, 0.020707118127466315, 0.16048016548786395, 0.2433086379977292, 0.029385868489536296, 0.14692934244768147, 0.17631521093721778, 0.05877173697907259, 0.11754347395814518, 0.029385868489536296, 0.029385868489536296, 0.029385868489536296, 0.08815760546860889, 0.05877173697907259, 0.05877173697907259, 0.029385868489536296, 0.05877173697907259, 0.05877173697907259, 0.008769932177798936, 0.017539864355597873, 0.026309796533396806, 0.15785877920038083, 0.008769932177798936, 0.008769932177798936, 0.09646925395578829, 0.008769932177798936, 0.017539864355597873, 0.15785877920038083, 0.30694762622296273, 0.026309796533396806, 0.035079728711195746, 0.008769932177798936, 0.017539864355597873, 0.008769932177798936, 0.07015945742239149, 0.12445562097363674, 0.3022493652216892, 0.17779374424805247, 0.05333812327441574, 0.017779374424805247, 0.017779374424805247, 0.08889687212402624, 0.035558748849610494, 0.05333812327441574, 0.12445562097363674, 0.02612189732412627, 0.02612189732412627, 0.02612189732412627, 0.05224379464825254, 0.05224379464825254, 0.20897517859301015, 0.05224379464825254, 0.13060948662063135, 0.0783656919723788, 0.13060948662063135, 0.02612189732412627, 0.02612189732412627, 0.02612189732412627, 0.05224379464825254, 0.02612189732412627, 0.01468492947282107, 0.03524383073477057, 0.1262903934662612, 0.04405478841846321, 0.002936985894564214, 0.011747943578256856, 0.008810957683692642, 0.005873971789128428, 0.002936985894564214, 0.01468492947282107, 0.026432873051077926, 0.05286574610215585, 0.34362734966401304, 0.005873971789128428, 0.020558901261949498, 0.002936985894564214, 0.01468492947282107, 0.011747943578256856, 0.023495887156513712, 0.017621915367385284, 0.020558901261949498, 0.008810957683692642, 0.09398354862605485, 0.07929861915323377, 0.008810957683692642, 0.05022028123171088, 0.03766521092378316, 0.01255507030792772, 0.02511014061585544, 0.01255507030792772, 0.01255507030792772, 0.03766521092378316, 0.13810577338720492, 0.08788549215549404, 0.01255507030792772, 0.02511014061585544, 0.02511014061585544, 0.1757709843109881, 0.03766521092378316, 0.02511014061585544, 0.07533042184756632, 0.01255507030792772, 0.02511014061585544, 0.10044056246342176, 0.03766521092378316, 0.03778961576974009, 0.03778961576974009, 0.22673769461844054, 0.03778961576974009, 0.03778961576974009, 0.3401065419276608, 0.18894807884870046, 0.31159019253501785, 0.03462113250389087, 0.1384845300155635, 0.10386339751167262, 0.03462113250389087, 0.10386339751167262, 0.03462113250389087, 0.03462113250389087, 0.10386339751167262, 0.3080324677055572, 0.17112914872530954, 0.034225829745061914, 0.06845165949012383, 0.034225829745061914, 0.034225829745061914, 0.06845165949012383, 0.06845165949012383, 0.17112914872530954, 0.034225829745061914, 0.03012032209746079, 0.037650402621825986, 0.10542112734111277, 0.12048128838984316, 0.015060161048730395, 0.007530080524365198, 0.011295120786547796, 0.003765040262182599, 0.03388536235964339, 0.015060161048730395, 0.041415442884008585, 0.03012032209746079, 0.003765040262182599, 0.007530080524365198, 0.045180483146191185, 0.06777072471928677, 0.018825201310912993, 0.02635528183527819, 0.09789104681674757, 0.011295120786547796, 0.003765040262182599, 0.003765040262182599, 0.011295120786547796, 0.07906584550583458, 0.011295120786547796, 0.015060161048730395, 0.018825201310912993, 0.007530080524365198, 0.003765040262182599, 0.007530080524365198, 0.022590241573095592, 0.037650402621825986, 0.007530080524365198, 0.007530080524365198, 0.022590241573095592, 0.023837849278249498, 0.18275684446657947, 0.10329734687241449, 0.047675698556498995, 0.015891899518833, 0.03972974879708249, 0.023837849278249498, 0.0079459497594165, 0.07945949759416499, 0.09535139711299799, 0.023837849278249498, 0.127135196150664, 0.015891899518833, 0.031783799037666, 0.031783799037666, 0.015891899518833, 0.015891899518833, 0.0079459497594165, 0.03972974879708249, 0.03972974879708249, 0.023837849278249498, 0.03289395939190143, 0.03289395939190143, 0.03289395939190143, 0.23025771574331, 0.09868187817570429, 0.16446979695950714, 0.06578791878380286, 0.06578791878380286, 0.09868187817570429, 0.03289395939190143, 0.03289395939190143, 0.06578791878380286, 0.04428170700455838, 0.08856341400911676, 0.08856341400911676, 0.1771268280182335, 0.04428170700455838, 0.04428170700455838, 0.04428170700455838, 0.04428170700455838, 0.22140853502279187, 0.08856341400911676, 0.08856341400911676, 0.013245460723366428, 0.03973638217009928, 0.026490921446732856, 0.013245460723366428, 0.03973638217009928, 0.11920914651029785, 0.09271822506356499, 0.013245460723366428, 0.06622730361683214, 0.03973638217009928, 0.013245460723366428, 0.026490921446732856, 0.03973638217009928, 0.15894552868039713, 0.1986819108504964, 0.013245460723366428, 0.013245460723366428, 0.06622730361683214, 0.2126676946235264, 0.03544461577058773, 0.03544461577058773, 0.03544461577058773, 0.14177846308235093, 0.03544461577058773, 0.2126676946235264, 0.14177846308235093, 0.1063338473117632, 0.02796975100769327, 0.11187900403077308, 0.02796975100769327, 0.3915765141077058, 0.0839092530230798, 0.02796975100769327, 0.30766726108462594, 0.011546194845768038, 0.19051221495517265, 0.011546194845768038, 0.028865487114420097, 0.03463858453730412, 0.04618477938307215, 0.005773097422884019, 0.03463858453730412, 0.05195787680595618, 0.01731929226865206, 0.03463858453730412, 0.09814265618902833, 0.005773097422884019, 0.06927716907460824, 0.023092389691536076, 0.22515079949247677, 0.01731929226865206, 0.01731929226865206, 0.005773097422884019, 0.028865487114420097, 0.028865487114420097, 0.011546194845768038, 0.12406657313755776, 0.08861898081254126, 0.0354475923250165, 0.2481331462751155, 0.01772379616250825, 0.0354475923250165, 0.05317138848752475, 0.01772379616250825, 0.01772379616250825, 0.01772379616250825, 0.3367521270876568, 0.0826098131618482, 0.004589434064547123, 0.004589434064547123, 0.027536604387282733, 0.0336558498066789, 0.05660302012941451, 0.01988754761303753, 0.006119245419396163, 0.09484830400064052, 0.026006793032433695, 0.05813283148426355, 0.0015298113548490408, 0.01682792490333945, 0.009178868129094245, 0.0030596227096980817, 0.03518566116152794, 0.01835773625818849, 0.05201358606486739, 0.006119245419396163, 0.08872905858124437, 0.05660302012941451, 0.0015298113548490408, 0.015298113548490408, 0.12391471974277231, 0.0030596227096980817, 0.0015298113548490408, 0.03824528387122602, 0.006119245419396163, 0.05354339741971643, 0.009178868129094245, 0.03977509522607506, 0.0015298113548490408, 0.006119245419396163, 0.0846183956985366, 0.010577299462317075, 0.02115459892463415, 0.010577299462317075, 0.03173189838695122, 0.05288649731158537, 0.010577299462317075, 0.02115459892463415, 0.3807827806434147, 0.3384735827941464, 0.012639352157536577, 0.012639352157536577, 0.9353120596577067, 0.012639352157536577, 0.009607265713260265, 0.03842906285304106, 0.9415120398995059, 0.05370765458908957, 0.14322041223757218, 0.035805103059393044, 0.05370765458908957, 0.05370765458908957, 0.017902551529696522, 0.017902551529696522, 0.017902551529696522, 0.017902551529696522, 0.017902551529696522, 0.017902551529696522, 0.08951275764848261, 0.07161020611878609, 0.017902551529696522, 0.017902551529696522, 0.05370765458908957, 0.05370765458908957, 0.05370765458908957, 0.05370765458908957, 0.12531786070787565, 0.023127173070545064, 0.011563586535272532, 0.03469075960581759, 0.03469075960581759, 0.09250869228218025, 0.005781793267636266, 0.011563586535272532, 0.08672689901454399, 0.011563586535272532, 0.03469075960581759, 0.005781793267636266, 0.011563586535272532, 0.017345379802908796, 0.04047255287345386, 0.08672689901454399, 0.3006532499170858, 0.028908966338181328, 0.005781793267636266, 0.017345379802908796, 0.005781793267636266, 0.04047255287345386, 0.028908966338181328, 0.04047255287345386, 0.009288335321428257, 0.24149671835713468, 0.009288335321428257, 0.032509173624998895, 0.004644167660714129, 0.004644167660714129, 0.013932502982142384, 0.009288335321428257, 0.06501834724999779, 0.09288335321428257, 0.11610419151785321, 0.04644167660714128, 0.004644167660714129, 0.004644167660714129, 0.05108584426785541, 0.004644167660714129, 0.013932502982142384, 0.009288335321428257, 0.009288335321428257, 0.018576670642856515, 0.018576670642856515, 0.032509173624998895, 0.02322083830357064, 0.06501834724999779, 0.032509173624998895, 0.06037417958928367, 0.055856618642087644, 0.06516605508243559, 0.018618872880695882, 0.018618872880695882, 0.04654718220173971, 0.027928309321043822, 0.07447549152278353, 0.037237745761391765, 0.018618872880695882, 0.027928309321043822, 0.037237745761391765, 0.009309436440347941, 0.06516605508243559, 0.4096152033753094, 0.018618872880695882, 0.055856618642087644, 0.04185209794846461, 0.020926048974232305, 0.10463024487116153, 0.052315122435580765, 0.010463024487116153, 0.052315122435580765, 0.08370419589692922, 0.020926048974232305, 0.15694536730674227, 0.08370419589692922, 0.031389073461348456, 0.020926048974232305, 0.04185209794846461, 0.020926048974232305, 0.20926048974232306, 0.052315122435580765, 0.019365296920793824, 0.0774611876831753, 0.03873059384158765, 0.09682648460396913, 0.019365296920793824, 0.019365296920793824, 0.03873059384158765, 0.058095890762381475, 0.019365296920793824, 0.019365296920793824, 0.019365296920793824, 0.03873059384158765, 0.0774611876831753, 0.03873059384158765, 0.36794064149508265, 0.019365296920793824, 0.0358586626421137, 0.008275075994333933, 0.0055167173295559545, 0.008275075994333933, 0.03034194531255775, 0.07723404261378336, 0.024825227983001796, 0.0055167173295559545, 0.03310030397733573, 0.1820516718753465, 0.05516717329555955, 0.0027583586647779773, 0.022066869318223818, 0.0055167173295559545, 0.05792553196033752, 0.0606838906251155, 0.0027583586647779773, 0.046892097301225615, 0.03034194531255775, 0.008275075994333933, 0.2289437691765721, 0.04965045596600359, 0.013791793323889887, 0.030053024038160563, 0.006010604807632112, 0.060106048076321125, 0.006010604807632112, 0.012021209615264225, 0.01803181442289634, 0.0901590721144817, 0.030053024038160563, 0.10819088653737803, 0.006010604807632112, 0.11420149134501015, 0.012021209615264225, 0.14425451538317072, 0.01803181442289634, 0.02404241923052845, 0.01803181442289634, 0.02404241923052845, 0.012021209615264225, 0.01803181442289634, 0.030053024038160563, 0.012021209615264225, 0.04207423365342479, 0.02404241923052845, 0.030053024038160563, 0.01803181442289634, 0.07212725769158536, 0.01803181442289634, 0.006782430168403428, 0.07460673185243771, 0.020347290505210284, 0.06782430168403428, 0.06782430168403428, 0.006782430168403428, 0.027129720673613713, 0.027129720673613713, 0.03391215084201714, 0.054259441347227426, 0.04069458101042057, 0.027129720673613713, 0.006782430168403428, 0.06782430168403428, 0.027129720673613713, 0.04069458101042057, 0.07460673185243771, 0.027129720673613713, 0.013564860336806856, 0.03391215084201714, 0.006782430168403428, 0.04069458101042057, 0.006782430168403428, 0.03391215084201714, 0.020347290505210284, 0.013564860336806856, 0.020347290505210284, 0.020347290505210284, 0.006782430168403428, 0.061041871515630855, 0.013564860336806856, 0.014826636317893592, 0.029653272635787185, 0.05930654527157437, 0.04447990895368078, 0.04447990895368078, 0.029653272635787185, 0.014826636317893592, 0.029653272635787185, 0.029653272635787185, 0.014826636317893592, 0.014826636317893592, 0.014826636317893592, 0.029653272635787185, 0.029653272635787185, 0.10378645422525515, 0.029653272635787185, 0.014826636317893592, 0.014826636317893592, 0.029653272635787185, 0.04447990895368078, 0.04447990895368078, 0.14826636317893593, 0.08895981790736156, 0.05930654527157437, 0.025417885559746816, 0.025417885559746816, 0.08472628519915605, 0.08472628519915605, 0.01694525703983121, 0.08472628519915605, 0.06778102815932484, 0.01694525703983121, 0.08472628519915605, 0.008472628519915605, 0.025417885559746816, 0.03389051407966242, 0.01694525703983121, 0.03389051407966242, 0.042363142599578026, 0.05083577111949363, 0.01694525703983121, 0.01694525703983121, 0.008472628519915605, 0.025417885559746816, 0.06778102815932484, 0.05930839963940924, 0.03389051407966242, 0.05083577111949363, 0.03067191327532387, 0.15335956637661935, 0.03067191327532387, 0.03067191327532387, 0.09201573982597161, 0.06134382655064774, 0.09201573982597161, 0.03067191327532387, 0.18403147965194322, 0.06134382655064774, 0.03067191327532387, 0.18403147965194322, 0.01502228882802343, 0.048822438691076143, 0.06760029972610543, 0.0037555722070058573, 0.1502228882802343, 0.2929346321464569, 0.0037555722070058573, 0.007511144414011715, 0.04131129427706443, 0.0037555722070058573, 0.0037555722070058573, 0.0037555722070058573, 0.011266716621017572, 0.01877786103502929, 0.07511144414011715, 0.03004457765604686, 0.011266716621017572, 0.09388930517514643, 0.01877786103502929, 0.052578010898082006, 0.04506686648407029, 0.1512394909270029, 0.0504131636423343, 0.02520658182116715, 0.12603290910583576, 0.012603290910583575, 0.0504131636423343, 0.012603290910583575, 0.1008263272846686, 0.06301645455291788, 0.07561974546350145, 0.06301645455291788, 0.012603290910583575, 0.037809872731750724, 0.0504131636423343, 0.0504131636423343, 0.02520658182116715, 0.012603290910583575, 0.012603290910583575, 0.0504131636423343, 0.10979880917771852, 0.14639841223695801, 0.07319920611847901, 0.14639841223695801, 0.07319920611847901, 0.07319920611847901, 0.07319920611847901, 0.07319920611847901, 0.07319920611847901, 0.10979880917771852, 0.02795849711485447, 0.06640143064777936, 0.0419377456722817, 0.06989624278713617, 0.03844293353292489, 0.017474060696784043, 0.05591699422970894, 0.06640143064777936, 0.034948121393568087, 0.006989624278713617, 0.0034948121393568086, 0.03844293353292489, 0.006989624278713617, 0.034948121393568087, 0.04543255781163851, 0.0034948121393568086, 0.02096887283614085, 0.010484436418070425, 0.02096887283614085, 0.010484436418070425, 0.05941180636906574, 0.02795849711485447, 0.03844293353292489, 0.0034948121393568086, 0.006989624278713617, 0.0034948121393568086, 0.05941180636906574, 0.02096887283614085, 0.02096887283614085, 0.02096887283614085, 0.0034948121393568086, 0.02096887283614085, 0.017474060696784043, 0.0419377456722817, 0.02795849711485447, 0.10629298005747546, 0.10629298005747546, 0.10629298005747546, 0.14172397340996729, 0.14172397340996729, 0.07086198670498364, 0.10629298005747546, 0.03543099335249182, 0.14172397340996729, 0.01857013485256185, 0.08356560683652832, 0.06499547198396648, 0.0371402697051237, 0.04642533713140462, 0.05571040455768555, 0.06499547198396648, 0.027855202278842774, 0.09285067426280924, 0.027855202278842774, 0.009285067426280925, 0.0371402697051237, 0.027855202278842774, 0.027855202278842774, 0.05571040455768555, 0.0371402697051237, 0.01857013485256185, 0.04642533713140462, 0.01857013485256185, 0.1578461462467757, 0.009285067426280925, 0.029405185771457343, 0.031365531489554496, 0.03332587720765166, 0.056850025824817525, 0.009801728590485781, 0.02548449433526303, 0.027444840053360186, 0.017643111462874405, 0.039206914361943124, 0.029405185771457343, 0.02156380289906872, 0.02548449433526303, 0.009801728590485781, 0.009801728590485781, 0.011762074308582936, 0.015682765744777248, 0.03332587720765166, 0.015682765744777248, 0.03528622292574881, 0.031365531489554496, 0.009801728590485781, 0.013722420026680093, 0.017643111462874405, 0.015682765744777248, 0.027444840053360186, 0.009801728590485781, 0.03528622292574881, 0.03724656864384596, 0.023524148617165872, 0.003920691436194312, 0.029405185771457343, 0.011762074308582936, 0.019603457180971562, 0.047048297234331744, 0.02156380289906872, 0.02156380289906872, 0.02156380289906872, 0.06077071726101184, 0.023524148617165872, 0.03724656864384596, 0.032580947182156546, 0.016290473591078273, 0.09774284154646963, 0.048871420773234815, 0.048871420773234815, 0.048871420773234815, 0.06516189436431309, 0.016290473591078273, 0.1140333151375479, 0.06516189436431309, 0.06516189436431309, 0.016290473591078273, 0.048871420773234815, 0.032580947182156546, 0.09774284154646963, 0.048871420773234815, 0.016290473591078273, 0.016290473591078273, 0.016290473591078273, 0.06516189436431309, 0.03213245218217388, 0.01071081739405796, 0.04284326957623184, 0.0535540869702898, 0.01071081739405796, 0.07497572175840572, 0.04284326957623184, 0.02142163478811592, 0.01071081739405796, 0.04284326957623184, 0.01071081739405796, 0.03213245218217388, 0.01071081739405796, 0.06426490436434776, 0.08568653915246369, 0.03213245218217388, 0.09639735654652165, 0.11781899133463757, 0.01071081739405796, 0.03213245218217388, 0.0535540869702898, 0.0535540869702898, 0.04284326957623184, 0.013096120590633715, 0.02619224118126743, 0.13096120590633714, 0.03928836177190114, 0.03928836177190114, 0.013096120590633715, 0.013096120590633715, 0.013096120590633715, 0.07857672354380228, 0.10476896472506972, 0.013096120590633715, 0.14405732649697087, 0.03928836177190114, 0.03928836177190114, 0.11786508531570343, 0.03928836177190114, 0.07857672354380228, 0.03928836177190114, 0.08413759109699502, 0.04206879554849751, 0.10517198887124377, 0.06310319332274626, 0.06310319332274626, 0.021034397774248755, 0.06310319332274626, 0.04206879554849751, 0.06310319332274626, 0.04206879554849751, 0.04206879554849751, 0.14724078441974128, 0.04206879554849751, 0.021034397774248755, 0.021034397774248755, 0.021034397774248755, 0.021034397774248755, 0.021034397774248755, 0.04206879554849751, 0.027965958552016706, 0.039152341972823385, 0.005593191710403341, 0.005593191710403341, 0.005593191710403341, 0.016779575131210023, 0.10067745078726013, 0.06152510881443675, 0.06152510881443675, 0.005593191710403341, 0.05033872539363007, 0.016779575131210023, 0.04474553368322673, 0.016779575131210023, 0.08949106736645346, 0.3523710777554105, 0.016779575131210023, 0.011186383420806682, 0.022372766841613365, 0.03355915026242005, 0.12861457423410597, 0.057162032992935985, 0.11432406598587197, 0.14290508248233996, 0.10003355773763797, 0.057162032992935985, 0.057162032992935985, 0.014290508248233996, 0.057162032992935985, 0.028581016496467992, 0.014290508248233996, 0.057162032992935985, 0.014290508248233996, 0.04287152474470199, 0.08574304948940398, 0.2611135088686184, 0.1305567544343092, 0.07833405266058552, 0.2611135088686184, 0.02611135088686184, 0.10444540354744736, 0.10444540354744736, 0.02135258265806411, 0.08541033063225643, 0.02135258265806411, 0.02135258265806411, 0.12811549594838464, 0.04270516531612822, 0.34164132252902574, 0.06405774797419232, 0.02135258265806411, 0.06405774797419232, 0.08541033063225643, 0.04270516531612822, 0.02135258265806411, 0.0669714582656038, 0.0669714582656038, 0.1004571873984057, 0.0669714582656038, 0.0669714582656038, 0.0334857291328019, 0.0669714582656038, 0.0334857291328019, 0.2009143747968114, 0.0669714582656038, 0.0334857291328019, 0.1339429165312076, 0.21691581309003485, 0.04338316261800697, 0.04338316261800697, 0.17353265047202787, 0.04338316261800697, 0.21691581309003485, 0.08676632523601394, 0.13014948785402092, 0.028332840388507766, 0.15110848207204142, 0.09444280129502589, 0.04722140064751294, 0.018888560259005177, 0.018888560259005177, 0.009444280129502589, 0.009444280129502589, 0.13221992181303624, 0.037777120518010354, 0.028332840388507766, 0.018888560259005177, 0.07555424103602071, 0.0849985211655233, 0.037777120518010354, 0.037777120518010354, 0.1794413224605492, 0.7433064457378304, 0.049553763049188695, 0.049553763049188695, 0.09910752609837739, 0.024776881524594348, 0.05584635773201931, 0.3909245041241352, 0.13961589433004828, 0.027923178866009656, 0.3071549675261062, 0.05584635773201931, 0.04902082965710828, 0.009804165931421657, 0.044118746691397454, 0.0049020829657108285, 0.17647498676558981, 0.02941249779426497, 0.019608331862843314, 0.02451041482855414, 0.04902082965710828, 0.07353124448566242, 0.0049020829657108285, 0.009804165931421657, 0.02451041482855414, 0.0049020829657108285, 0.014706248897132485, 0.2990270609083605, 0.009804165931421657, 0.14216040600561403, 0.0224567398397606, 0.0224567398397606, 0.24702413823736663, 0.22456739839760603, 0.06737021951928181, 0.0898269593590424, 0.0224567398397606, 0.0224567398397606, 0.13474043903856361, 0.06737021951928181, 0.0224567398397606, 0.005116663951008687, 0.06651663136311294, 0.005116663951008687, 0.015349991853026062, 0.005116663951008687, 0.005116663951008687, 0.015349991853026062, 0.005116663951008687, 0.3683998044726255, 0.010233327902017374, 0.03581664765706081, 0.005116663951008687, 0.40421645212968627, 0.0409333116080695, 0.0871236638717681, 0.26137099161530436, 0.6098656471023768, 0.003921659515107984, 0.03921659515107984, 0.011764978545323953, 0.01960829757553992, 0.03137327612086387, 0.03921659515107984, 0.04313825466618783, 0.06274655224172775, 0.015686638060431937, 0.1019631473928076, 0.01960829757553992, 0.003921659515107984, 0.023529957090647906, 0.007843319030215968, 0.09411982836259163, 0.003921659515107984, 0.08235484981726768, 0.011764978545323953, 0.02745161660575589, 0.015686638060431937, 0.003921659515107984, 0.023529957090647906, 0.015686638060431937, 0.02745161660575589, 0.04705991418129581, 0.011764978545323953, 0.03921659515107984, 0.03137327612086387, 0.011764978545323953, 0.09411982836259163, 0.015686638060431937, 0.01960829757553992, 0.013438361584806281, 0.0067191807924031404, 0.07391098871643455, 0.026876723169612562, 0.020157542377209424, 0.0067191807924031404, 0.013438361584806281, 0.0067191807924031404, 0.08734935030124083, 0.0067191807924031404, 0.0067191807924031404, 0.04031508475441885, 0.06719180792403141, 0.026876723169612562, 0.060472627131628265, 0.013438361584806281, 0.2822055932809319, 0.03359590396201571, 0.08734935030124083, 0.04703426554682199, 0.053753446339225124, 0.013438361584806281, 0.029209966812707232, 0.029209966812707232, 0.029209966812707232, 0.8470890375685097, 0.029209966812707232, 0.024204989806805945, 0.14522993884083565, 0.024204989806805945, 0.04840997961361189, 0.024204989806805945, 0.04840997961361189, 0.19363991845444756, 0.12102494903402972, 0.024204989806805945, 0.04840997961361189, 0.07261496942041783, 0.09681995922722378, 0.09681995922722378, 0.07577982627485347, 0.015155965254970694, 0.03031193050994139, 0.015155965254970694, 0.04546789576491208, 0.015155965254970694, 0.015155965254970694, 0.13640368729473626, 0.03031193050994139, 0.015155965254970694, 0.09093579152982416, 0.2576514093345018, 0.04546789576491208, 0.04546789576491208, 0.13640368729473626, 0.20507008388218206, 0.14647863134441577, 0.029295726268883153, 0.17577435761329893, 0.058591452537766306, 0.11718290507553261, 0.08788717880664947, 0.029295726268883153, 0.058591452537766306, 0.029295726268883153, 0.04419579333932081, 0.29463862226213877, 0.08839158667864162, 0.058927724452427756, 0.07365965556553469, 0.014731931113106939, 0.014731931113106939, 0.08839158667864162, 0.014731931113106939, 0.04419579333932081, 0.13258738001796244, 0.029463862226213878, 0.014731931113106939, 0.04419579333932081, 0.2287481936791013, 0.020795290334463753, 0.1351693871740144, 0.010397645167231876, 0.020795290334463753, 0.4886893228598982, 0.06238587100339126, 0.020795290334463753, 0.2385427376225446, 0.1490892110140904, 0.029817842202818075, 0.5367211596507254, 0.27540498825406784, 0.05987064962044953, 0.03592238977226972, 0.03592238977226972, 0.03592238977226972, 0.04789651969635962, 0.02394825984817981, 0.02394825984817981, 0.02394825984817981, 0.02394825984817981, 0.03592238977226972, 0.011974129924089905, 0.07184477954453944, 0.19158607878543849, 0.011974129924089905, 0.03592238977226972, 0.02394825984817981, 0.03592238977226972, 0.11777437005266667, 0.11777437005266667, 0.11777437005266667, 0.19629061675444445, 0.07851624670177777, 0.039258123350888886, 0.07851624670177777, 0.07851624670177777, 0.15703249340355555, 0.03355852583886851, 0.011186175279622837, 0.044744701118491347, 0.07830322695735986, 0.011186175279622837, 0.03355852583886851, 0.044744701118491347, 0.06711705167773702, 0.06711705167773702, 0.044744701118491347, 0.011186175279622837, 0.022372350559245673, 0.044744701118491347, 0.044744701118491347, 0.011186175279622837, 0.022372350559245673, 0.011186175279622837, 0.022372350559245673, 0.03355852583886851, 0.044744701118491347, 0.14542027863509688, 0.06711705167773702, 0.055930876398114185, 0.021148709001677927, 0.012689225401006756, 0.0592163852046982, 0.07190561060570495, 0.012689225401006756, 0.033837934402684684, 0.021148709001677927, 0.016918967201342342, 0.02537845080201351, 0.042297418003355854, 0.0042297418003355855, 0.021148709001677927, 0.03806767620302027, 0.042297418003355854, 0.012689225401006756, 0.042297418003355854, 0.042297418003355854, 0.0042297418003355855, 0.012689225401006756, 0.05075690160402702, 0.0592163852046982, 0.033837934402684684, 0.021148709001677927, 0.05498664340436261, 0.008459483600671171, 0.0296081926023491, 0.02537845080201351, 0.0042297418003355855, 0.05498664340436261, 0.04652715980369144, 0.016918967201342342, 0.012689225401006756, 0.016918967201342342, 0.016918967201342342, 0.029633429891220066, 0.00889002896736602, 0.05037683081507411, 0.12149706255400226, 0.06223020277156213, 0.020743400923854044, 0.014816714945610033, 0.020743400923854044, 0.056303516793318124, 0.02370674391297605, 0.02667008690209806, 0.03852345885858608, 0.00889002896736602, 0.020743400923854044, 0.05334017380419612, 0.01778005793473204, 0.056303516793318124, 0.011853371956488026, 0.0029633429891220064, 0.03259677288034207, 0.020743400923854044, 0.02370674391297605, 0.01778005793473204, 0.014816714945610033, 0.01778005793473204, 0.0029633429891220064, 0.02667008690209806, 0.0029633429891220064, 0.06519354576068415, 0.011853371956488026, 0.02370674391297605, 0.03259677288034207, 0.029633429891220066, 0.029633429891220066, 0.06983092286647237, 0.034915461433236185, 0.13966184573294474, 0.034915461433236185, 0.08728865358309047, 0.017457730716618092, 0.06983092286647237, 0.15711957644956284, 0.05237319214985428, 0.06983092286647237, 0.06983092286647237, 0.017457730716618092, 0.017457730716618092, 0.034915461433236185, 0.06983092286647237, 0.05237319214985428, 0.02914947326238398, 0.04372420989357597, 0.01457473663119199, 0.18947157620549587, 0.02914947326238398, 0.05829894652476796, 0.17489683957430388, 0.04372420989357597, 0.08744841978715194, 0.3060694692550318, 0.008197460855646965, 0.010929947807529287, 0.0054649739037646436, 0.06557968684517572, 0.010929947807529287, 0.07650963465270501, 0.021859895615058574, 0.019127408663176252, 0.09017206941211661, 0.027324869518823215, 0.010929947807529287, 0.019127408663176252, 0.0027324869518823218, 0.013662434759411608, 0.060114712941411075, 0.024592382566940893, 0.019127408663176252, 0.060114712941411075, 0.05464973903764643, 0.024592382566940893, 0.01639492171129393, 0.008197460855646965, 0.04645227818199947, 0.013662434759411608, 0.06557968684517572, 0.0054649739037646436, 0.0054649739037646436, 0.019127408663176252, 0.030057356470705537, 0.008197460855646965, 0.0054649739037646436, 0.010929947807529287, 0.021859895615058574, 0.04371979123011715, 0.038254817326352504, 0.038254817326352504, 0.2356300877382876, 0.1178150438691438, 0.1570867251588584, 0.0392716812897146, 0.1178150438691438, 0.0785433625794292, 0.0785433625794292, 0.1178150438691438, 0.09613590989496397, 0.03204530329832132, 0.021363535532214217, 0.1388629809593924, 0.11749944542717819, 0.053408838830535536, 0.11749944542717819, 0.09613590989496397, 0.04272707106442843, 0.07477237436274975, 0.17090828425771373, 0.03204530329832132, 0.029508858958150974, 0.04426328843722646, 0.014754429479075487, 0.08852657687445292, 0.014754429479075487, 0.029508858958150974, 0.575422749683944, 0.029508858958150974, 0.05901771791630195, 0.014754429479075487, 0.07377214739537744, 0.09302088644047284, 0.18604177288094567, 0.2325522161011821, 0.09302088644047284, 0.2325522161011821, 0.09302088644047284, 0.04651044322023642, 0.06007098356646222, 0.09010647534969334, 0.12014196713292444, 0.27031942604908, 0.12014196713292444, 0.06007098356646222, 0.15017745891615555, 0.06007098356646222, 0.11434569898001504, 0.08575927423501127, 0.05717284949000752, 0.05717284949000752, 0.20010497321502632, 0.08575927423501127, 0.05717284949000752, 0.02858642474500376, 0.11434569898001504, 0.02858642474500376, 0.02858642474500376, 0.02858642474500376, 0.05717284949000752, 0.02858642474500376, 0.06527106943078628, 0.17949544093466227, 0.19581320829235885, 0.03263553471539314, 0.04895330207308971, 0.03263553471539314, 0.03263553471539314, 0.01631776735769657, 0.17949544093466227, 0.11422437150387599, 0.01631776735769657, 0.03263553471539314, 0.01631776735769657, 0.03263553471539314, 0.020131332545324503, 0.07045966390863576, 0.030196998817986754, 0.06039399763597351, 0.030196998817986754, 0.010065666272662251, 0.020131332545324503, 0.15098499408993377, 0.3824953183611655, 0.07045966390863576, 0.12078799527194702, 0.010065666272662251, 0.11934108640962583, 0.03409745325989309, 0.06819490651978619, 0.08524363314973274, 0.03409745325989309, 0.03409745325989309, 0.017048726629946546, 0.05114617988983964, 0.05114617988983964, 0.05114617988983964, 0.11934108640962583, 0.03409745325989309, 0.017048726629946546, 0.06819490651978619, 0.017048726629946546, 0.017048726629946546, 0.06819490651978619, 0.10229235977967928, 0.07709515312478585, 0.04405437321416334, 0.07709515312478585, 0.02202718660708167, 0.1211495263389492, 0.02202718660708167, 0.06608155982124501, 0.07709515312478585, 0.02202718660708167, 0.09912233973186753, 0.03304077991062251, 0.03304077991062251, 0.04405437321416334, 0.02202718660708167, 0.08810874642832668, 0.04405437321416334, 0.02202718660708167, 0.06608155982124501, 0.02202718660708167, 0.09158420641249178, 0.13737630961873767, 0.13737630961873767, 0.18316841282498356, 0.09158420641249178, 0.13737630961873767, 0.18316841282498356, 0.03775337925664683, 0.05663006888497025, 0.03775337925664683, 0.018876689628323416, 0.018876689628323416, 0.05663006888497025, 0.018876689628323416, 0.018876689628323416, 0.05663006888497025, 0.5285473095930556, 0.05663006888497025, 0.09438344814161707, 0.0396224925650478, 0.013207497521682602, 0.013207497521682602, 0.05282999008673041, 0.0396224925650478, 0.0792449851300956, 0.09245248265177822, 0.0792449851300956, 0.1452824727385086, 0.0396224925650478, 0.05282999008673041, 0.013207497521682602, 0.013207497521682602, 0.013207497521682602, 0.0396224925650478, 0.026414995043365204, 0.1716974677818738, 0.026414995043365204, 0.0500271011923595, 0.0500271011923595, 0.100054202384719, 0.02501355059617975, 0.02501355059617975, 0.02501355059617975, 0.02501355059617975, 0.3752032589426963, 0.07504065178853925, 0.1500813035770785, 0.0500271011923595, 0.02501355059617975, 0.010311135032605038, 0.04124454013042015, 0.12373362039126044, 0.020622270065210075, 0.010311135032605038, 0.13404475542386549, 0.17528929555428563, 0.010311135032605038, 0.010311135032605038, 0.010311135032605038, 0.010311135032605038, 0.0824890802608403, 0.12373362039126044, 0.03093340509781511, 0.03093340509781511, 0.051555675163025186, 0.04124454013042015, 0.051555675163025186, 0.010311135032605038, 0.020622270065210075, 0.020045723941541475, 0.0801828957661659, 0.0801828957661659, 0.14032006759079033, 0.04009144788308295, 0.04009144788308295, 0.10022861970770737, 0.12027434364924884, 0.10022861970770737, 0.020045723941541475, 0.14032006759079033, 0.06013717182462442, 0.04009144788308295, 0.01240746270472792, 0.031018656761819797, 0.043426119466547716, 0.00620373135236396, 0.018611194057091877, 0.06824104487600356, 0.02481492540945584, 0.00620373135236396, 0.01240746270472792, 0.01240746270472792, 0.0930559702854594, 0.07444477622836751, 0.043426119466547716, 0.05583358217127564, 0.08064850758073147, 0.02481492540945584, 0.05583358217127564, 0.031018656761819797, 0.05583358217127564, 0.018611194057091877, 0.018611194057091877, 0.018611194057091877, 0.02481492540945584, 0.018611194057091877, 0.07444477622836751, 0.018611194057091877, 0.01240746270472792, 0.02481492540945584, 0.01240746270472792, 0.030736362334569712, 0.015368181167284856, 0.030736362334569712, 0.07684090583642428, 0.04610454350185456, 0.030736362334569712, 0.015368181167284856, 0.12294544933827885, 0.09220908700370913, 0.04610454350185456, 0.16904999284013342, 0.12294544933827885, 0.061472724669139424, 0.015368181167284856, 0.04610454350185456, 0.061472724669139424, 0.14745574234467484, 0.026021601590236736, 0.17347734393491157, 0.017347734393491156, 0.017347734393491156, 0.03469546878698231, 0.008673867196745578, 0.026021601590236736, 0.14745574234467484, 0.008673867196745578, 0.008673867196745578, 0.017347734393491156, 0.008673867196745578, 0.060717070377219046, 0.008673867196745578, 0.008673867196745578, 0.03469546878698231, 0.017347734393491156, 0.05204320318047347, 0.026021601590236736, 0.13010800795118369, 0.016217121288809262, 0.09730272773285557, 0.0810856064440463, 0.07297704579964168, 0.016217121288809262, 0.04054280322202315, 0.016217121288809262, 0.04054280322202315, 0.09730272773285557, 0.04054280322202315, 0.07297704579964168, 0.032434242577618524, 0.056759924510832414, 0.04054280322202315, 0.008108560644404631, 0.032434242577618524, 0.09730272773285557, 0.016217121288809262, 0.016217121288809262, 0.032434242577618524, 0.016217121288809262, 0.04054280322202315, 0.03494574415865998, 0.08736436039664995, 0.03494574415865998, 0.06989148831731996, 0.12231010455530994, 0.06989148831731996, 0.08736436039664995, 0.08736436039664995, 0.08736436039664995, 0.01747287207932999, 0.12231010455530994, 0.01747287207932999, 0.03494574415865998, 0.12231010455530994, 0.11507734247419213, 0.057538671237096065, 0.07671822831612808, 0.03835911415806404, 0.11507734247419213, 0.057538671237096065, 0.057538671237096065, 0.03835911415806404, 0.24933424202741628, 0.03835911415806404, 0.03835911415806404, 0.03835911415806404, 0.01917955707903202, 0.01917955707903202, 0.01579669073572033, 0.09478014441432198, 0.007898345367860165, 0.055288417575021155, 0.023695036103580494, 0.023695036103580494, 0.03159338147144066, 0.03159338147144066, 0.04739007220716099, 0.08688179904646182, 0.06318676294288132, 0.007898345367860165, 0.007898345367860165, 0.03159338147144066, 0.023695036103580494, 0.023695036103580494, 0.03159338147144066, 0.16586525272506344, 0.007898345367860165, 0.023695036103580494, 0.01579669073572033, 0.023695036103580494, 0.023695036103580494, 0.023695036103580494, 0.10267848978218214, 0.012106003091226808, 0.04842401236490723, 0.012106003091226808, 0.09684802472981446, 0.036318009273680424, 0.06053001545613404, 0.06053001545613404, 0.012106003091226808, 0.09684802472981446, 0.012106003091226808, 0.20580205255085574, 0.036318009273680424, 0.012106003091226808, 0.024212006182453616, 0.012106003091226808, 0.04842401236490723, 0.024212006182453616, 0.024212006182453616, 0.024212006182453616, 0.024212006182453616, 0.024212006182453616, 0.036318009273680424, 0.06053001545613404, 0.055613863725793536, 0.11122772745158707, 0.027806931862896768, 0.055613863725793536, 0.19464852304027735, 0.027806931862896768, 0.30587625049186445, 0.11122772745158707, 0.13903465931448383, 0.049550229064152615, 0.019180733831284883, 0.0687309628954375, 0.07832132981107995, 0.028771100746927326, 0.030369495232867732, 0.030369495232867732, 0.039959862148510175, 0.055943807007914244, 0.01598394485940407, 0.007991972429702034, 0.009590366915642442, 0.025574311775046512, 0.012787155887523256, 0.036763073176629364, 0.009590366915642442, 0.012787155887523256, 0.011188761401582849, 0.019180733831284883, 0.019180733831284883, 0.027172706260986917, 0.007991972429702034, 0.020779128317225292, 0.003196788971880814, 0.054345412521973835, 0.017582339345344478, 0.01598394485940407, 0.038361467662569766, 0.03196788971880814, 0.028771100746927326, 0.014385550373463663, 0.006393577943761628, 0.028771100746927326, 0.009590366915642442, 0.036763073176629364, 0.051148623550093024, 0.028771100746927326, 0.04827345815790748, 0.06436461087720997, 0.09654691631581495, 0.04827345815790748, 0.03218230543860499, 0.016091152719302493, 0.03218230543860499, 0.04827345815790748, 0.04827345815790748, 0.03218230543860499, 0.03218230543860499, 0.03218230543860499, 0.016091152719302493, 0.016091152719302493, 0.016091152719302493, 0.08045576359651246, 0.12872922175441995, 0.14482037447372245, 0.06436461087720997, 0.1536052097935417, 0.03072104195870834, 0.18432625175225004, 0.03072104195870834, 0.06144208391741668, 0.03072104195870834, 0.21504729371095838, 0.03072104195870834, 0.1536052097935417, 0.06144208391741668, 0.35601174438414274, 0.019778430243563485, 0.11867058146138092, 0.09889215121781743, 0.07911372097425394, 0.019778430243563485, 0.11867058146138092, 0.05933529073069046, 0.03955686048712697, 0.019778430243563485, 0.03955686048712697, 0.1367840823172553, 0.03419602057931383, 0.03419602057931383, 0.056993367632189706, 0.06839204115862765, 0.04559469410575177, 0.04559469410575177, 0.03419602057931383, 0.011398673526437942, 0.03419602057931383, 0.03419602057931383, 0.07979071468506559, 0.011398673526437942, 0.011398673526437942, 0.03419602057931383, 0.011398673526437942, 0.03419602057931383, 0.14818275584369323, 0.022797347052875883, 0.011398673526437942, 0.11398673526437941, 0.020516437135705026, 0.020516437135705026, 0.020516437135705026, 0.020516437135705026, 0.03077465570355754, 0.03077465570355754, 0.051291092839262566, 0.09232396711067262, 0.010258218567852513, 0.020516437135705026, 0.020516437135705026, 0.17438971565349273, 0.020516437135705026, 0.17438971565349273, 0.0718075299749676, 0.13335684138208267, 0.020516437135705026, 0.010258218567852513, 0.04103287427141005, 0.03295464472325853, 0.03295464472325853, 0.06590928944651706, 0.08238661180814633, 0.06590928944651706, 0.03295464472325853, 0.06590928944651706, 0.04943196708488779, 0.09886393416977558, 0.08238661180814633, 0.03295464472325853, 0.06590928944651706, 0.04943196708488779, 0.04943196708488779, 0.016477322361629264, 0.03295464472325853, 0.03295464472325853, 0.06590928944651706, 0.03295464472325853, 0.020471965827626486, 0.0018610878025114986, 0.01488870242009199, 0.06699916089041395, 0.3833840873173687, 0.005583263407534496, 0.005583263407534496, 0.016749790222603488, 0.0018610878025114986, 0.011166526815068993, 0.024194141432649485, 0.020471965827626486, 0.0018610878025114986, 0.016749790222603488, 0.005583263407534496, 0.020471965827626486, 0.016749790222603488, 0.01488870242009199, 0.009305439012557494, 0.02791631703767248, 0.0037221756050229973, 0.01302761461758049, 0.2679966435616558, 0.02977740484018398, 0.043900136139140296, 0.3731511571826925, 0.021950068069570148, 0.06585020420871045, 0.4609514294609731, 0.20652123999173774, 0.0064537887497418045, 0.06453788749741805, 0.019361366249225416, 0.03226894374870903, 0.019361366249225416, 0.0064537887497418045, 0.012907577499483609, 0.04517652124819263, 0.0064537887497418045, 0.0064537887497418045, 0.012907577499483609, 0.04517652124819263, 0.0064537887497418045, 0.06453788749741805, 0.03872273249845083, 0.019361366249225416, 0.051630309997934436, 0.012907577499483609, 0.1226219862450943, 0.012907577499483609, 0.012907577499483609, 0.0064537887497418045, 0.03226894374870903, 0.04517652124819263, 0.019361366249225416, 0.05808409874767624, 0.012907577499483609, 0.17492705967837108, 0.03498541193567421, 0.03498541193567421, 0.017492705967837106, 0.13994164774269685, 0.03498541193567421, 0.03498541193567421, 0.2798832954853937, 0.10495623580702265, 0.017492705967837106, 0.06997082387134843, 0.03498541193567421, 0.18759826821365755, 0.03751965364273151, 0.11255896092819452, 0.18759826821365755, 0.03751965364273151, 0.03751965364273151, 0.18759826821365755, 0.03751965364273151, 0.03751965364273151, 0.03751965364273151, 0.07503930728546301, 0.03751965364273151, 0.07759720749371461, 0.038798603746857306, 0.19399301873428654, 0.07759720749371461, 0.038798603746857306, 0.11639581124057193, 0.11639581124057193, 0.07759720749371461, 0.038798603746857306, 0.038798603746857306, 0.11639581124057193, 0.22545392509204193, 0.03757565418200699, 0.07515130836401399, 0.11272696254602096, 0.03757565418200699, 0.03757565418200699, 0.03757565418200699, 0.07515130836401399, 0.03757565418200699, 0.18787827091003495, 0.07515130836401399], \"Term\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"according\", \"according\", \"according\", \"according\", \"according\", \"according\", \"according\", \"according\", \"according\", \"according\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieve\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"action\", \"action\", \"action\", \"action\", \"action\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"additional\", \"additional\", \"additional\", \"additional\", \"additional\", \"additional\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"adjoining\", \"adjoining\", \"adjoining\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"advantage\", \"after\", \"after\", \"after\", \"after\", \"after\", \"after\", \"after\", \"after\", \"after\", \"after\", \"after\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"aim\", \"aim\", \"aim\", \"aim\", \"aim\", \"aim\", \"aim\", \"aim\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"along\", \"along\", \"along\", \"along\", \"along\", \"along\", \"along\", \"along\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"ambiguous\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"analyze\", \"anaphora\", \"anaphora\", \"anaphora\", \"anaphora\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"applicable\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"applied\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"apply\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argue\", \"argument\", \"argument\", \"argument\", \"argument\", \"argument\", \"argument\", \"argument\", \"argument\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatic\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automaton\", \"automaton\", \"automaton\", \"automaton\", \"automaton\", \"automaton\", \"automaton\", \"automaton\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"based_on\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be_used\", \"be_used\", \"be_used\", \"be_used\", \"be_used\", \"be_used\", \"be_used\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"because\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"between\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"both\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"built\", \"built\", \"built\", \"built\", \"built\", \"built\", \"built\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"but\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"can_be\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"cannot\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"categorial\", \"categorial\", \"categorial\", \"categorial\", \"categorial_grammar\", \"categorial_grammar\", \"categorial_grammar\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"centering\", \"centering\", \"centering\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combination\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combine\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combined\", \"combining\", \"combining\", \"combining\", \"combining\", \"combining\", \"combining\", \"combining\", \"combining\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"compared\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparing\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computationally\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"computing\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"conceptual\", \"conceptual\", \"conceptual\", \"conceptual\", \"conceptual\", \"conceptual\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"considered\", \"consists\", \"consists\", \"consists\", \"consists\", \"consists\", \"consists\", \"consists\", \"consists\", \"consists\", \"consists\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"construct\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"constructed\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"containing\", \"containing\", \"containing\", \"containing\", \"containing\", \"containing\", \"containing\", \"containing\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context_free\", \"context_free\", \"context_free\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correction\", \"correction\", \"correction\", \"correction\", \"correction\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"corresponding\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"criterion\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"definition\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependency\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"dependent\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"describe\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"described\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"describes\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"designed\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"detailed\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determine\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"determining\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"dialogue\", \"dialogue\", \"dialogue\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"difference\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficulty\", \"difficulty\", \"difficulty\", \"difficulty\", \"difficulty\", \"difficulty\", \"difficulty\", \"difficulty\", \"difficulty\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"directly\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"disambiguation\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discourse\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discus\", \"discussed\", \"discussed\", \"discussed\", \"discussed\", \"discussed\", \"discussed\", \"discussed\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinction\", \"distinguish\", \"distinguish\", \"distinguish\", \"distinguish\", \"distinguish\", \"distinguish\", \"distinguish\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"doe_not\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"during\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"each\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"efficiently\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"either\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"employ\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluated\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluating\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"experimental_result\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expressed\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extended\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature_structure\", \"feature_structure\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"finally\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite_state\", \"finite_state\", \"finite_state\", \"finite_state\", \"finite_state\", \"finite_state\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"form\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formula\", \"formula\", \"formula\", \"formula\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"french\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"from\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"functional\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"further\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"furthermore\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generalized\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generated\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generating\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gram\", \"gram\", \"gram\", \"gram\", \"gram\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"grammatical\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"ha_been\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"handle\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"have_been\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"higher\", \"highly\", \"highly\", \"highly\", \"highly\", \"highly\", \"highly\", \"highly\", \"highly\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"how\", \"hpsg\", \"hpsg\", \"hpsg\", \"hpsg\", \"hpsg\", \"hpsg\", \"hpsg\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improve\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improved\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improving\", \"improving\", \"improving\", \"improving\", \"improving\", \"improving\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"including\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"incremental\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"indicate\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"induction\", \"induction\", \"induction\", \"induction\", \"induction\", \"induction\", \"induction\", \"induction\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"integrated\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"interpretation\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"it\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge_base\", \"knowledge_base\", \"knowledge_base\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language_processing\", \"language_processing\", \"language_processing\", \"language_processing\", \"language_processing\", \"language_processing\", \"language_processing\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"lead\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexical\", \"lexicalized\", \"lexicalized\", \"lexicalized\", \"lexicalized\", \"lexicalized\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"lexicon\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"limited\", \"limited\", \"limited\", \"limited\", \"limited\", \"limited\", \"limited\", \"limited\", \"limited\", \"limited\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistically\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"linguistics\", \"literature\", \"literature\", \"literature\", \"literature\", \"literature\", \"literature\", \"literature\", \"literature\", \"literature\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logic\", \"logical\", \"logical\", \"logical\", \"logical\", \"logical\", \"logical\", \"logical\", \"logical\", \"logical\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"machine_translation\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"making\", \"making\", \"making\", \"making\", \"making\", \"making\", \"making\", \"making\", \"making\", \"making\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"markov\", \"markov\", \"markov\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"may_be\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphological\", \"morphology\", \"morphology\", \"morphology\", \"morphology\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"natural_language\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"naturally\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"nlp\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"notion\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun_phrase\", \"noun_phrase\", \"noun_phrase\", \"noun_phrase\", \"noun_phrase\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"occurrence\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"off\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"others\", \"others\", \"others\", \"others\", \"others\", \"others\", \"others\", \"others\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"our\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"out\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parse\", \"parse\", \"parse\", \"parse\", \"parse\", \"parse\", \"parse\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"partial\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"perform\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performed\", \"performed\", \"performed\", \"performed\", \"performed\", \"performed\", \"performed\", \"performed\", \"performed\", \"performed\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"perspective\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phenomenon\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phonological\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"practical\", \"precision\", \"precision\", \"precision\", \"precision\", \"precision\", \"precision\", \"precision\", \"precision\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"preference\", \"preference\", \"preference\", \"preference\", \"preference\", \"preference\", \"preference\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"preliminary\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"previously\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processor\", \"processor\", \"processor\", \"processor\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"produced\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"prolog\", \"prolog\", \"prolog\", \"prolog\", \"prolog\", \"prolog\", \"prolog\", \"pronoun\", \"pronoun\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"proposes\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"provided\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"rather_than\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"reasoning\", \"recall\", \"recall\", \"recall\", \"recall\", \"recall\", \"recall\", \"recall\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recent\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"regular\", \"regular\", \"regular\", \"regular\", \"regular\", \"regular\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"relative\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relies\", \"relies\", \"relies\", \"relies\", \"relies\", \"relies\", \"relies\", \"relies\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"report\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"represented\", \"represented\", \"represented\", \"represented\", \"represented\", \"represented\", \"represented\", \"represented\", \"represented\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requirement\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"requires\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resolution\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"restriction\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"resulting\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"scheme\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_how\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"show_that\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similar\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"situation\", \"situation\", \"situation\", \"situation\", \"situation\", \"situation\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech_recognition\", \"speech_recognition\", \"speech_recognition\", \"speech_recognition\", \"speech_recognition\", \"speech_recognition\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken_language\", \"spoken_language\", \"spoken_language\", \"spoken_language\", \"spoken_language\", \"spoken_language\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structural\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"such_a\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggest\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"suggests\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntactic\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"syntax\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tagger\", \"tagger\", \"tagger\", \"tagger\", \"tagging\", \"tagging\", \"tagging\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"tested\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"than\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"there\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"therefore\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"these\", \"thesis\", \"thesis\", \"thesis\", \"thesis\", \"thesis\", \"thesis\", \"thesis\", \"thesis\", \"thesis\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"this_paper\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"towards\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"traditional\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree_adjoining\", \"tree_adjoining\", \"tree_adjoining\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"typed\", \"typed\", \"typed\", \"typed\", \"typed\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"under\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"underlying\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"unification\", \"unification\", \"unification\", \"unification\", \"unification\", \"unification\", \"unification\", \"unification\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"up\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"utterance\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variant\", \"variant\", \"variant\", \"variant\", \"variant\", \"variant\", \"variant\", \"variation\", \"variation\", \"variation\", \"variation\", \"variation\", \"variation\", \"variation\", \"variation\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"various\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"via\", \"via\", \"via\", \"via\", \"via\", \"via\", \"via\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_argue\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_describe\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_discus\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_present\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_propose\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"we_show\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"where\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"which\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"wide\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word_sense\", \"word_sense\", \"word_sense\", \"word_sense\", \"word_sense\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"written\", \"written\", \"written\", \"written\", \"written\", \"written\", \"written\", \"written\", \"written\", \"written\", \"written\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\", \"yield\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el201221403474645047049529299960\", ldavis_el201221403474645047049529299960_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el201221403474645047049529299960\", ldavis_el201221403474645047049529299960_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el201221403474645047049529299960\", ldavis_el201221403474645047049529299960_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df_train_1000['Abstract'].tolist()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "#!pip3 install gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "#only ones that appear 20 times or more.\n",
    "bigram = Phrases(docs, min_count=25)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "#training parameters.\n",
    "NUM_TOPICS = 40\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None \n",
    "\n",
    "temp = dictionary[0]  \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "outputfile = f'model{NUM_TOPICS}.gensim'\n",
    "print(\"Saving model in \" + outputfile)\n",
    "print(\"\")\n",
    "model.save(outputfile)\n",
    "\n",
    "top_topics = model.top_topics(corpus)\n",
    "model.num_topics\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / NUM_TOPICS\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "model.print_topics( num_words=20)\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-roads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5212ass1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c4947af61a40608bc41f2975e557ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c66acd0f36d474193fe4bc20a6ef9ca",
       "IPY_MODEL_a2fc0d2232ba4b07b43d136c5a0c341f"
      ],
      "layout": "IPY_MODEL_d35b61e91ba74271b21d068441cdf0fe"
     }
    },
    "0c66acd0f36d474193fe4bc20a6ef9ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa290d3d093d40c2ab0204869f8184a1",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2376c8ca1f454327b8a0abb521c5c1b9",
      "value": 231508
     }
    },
    "169c148e00e743ed951321231a4ee165": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84b17e634bad48238e462ec05527dbb4",
      "placeholder": "",
      "style": "IPY_MODEL_cf6042b9870e4aa1a10f14ecd42b90ca",
      "value": " 433/433 [00:00&lt;00:00, 1.80kB/s]"
     }
    },
    "2376c8ca1f454327b8a0abb521c5c1b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2563f0dd6c404fa4af0b403c77c6da7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91f3f1fd5e35402685489d74bcd441d3",
       "IPY_MODEL_c06fb95a45514f3fbb990bf5c38ed2cf"
      ],
      "layout": "IPY_MODEL_88b6978b8daf4183b60c7e963aba5c7c"
     }
    },
    "2899efa168cf4c1da278861ee31a7b01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "303f68b0545f4f6dbd847b8466ad9884": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_313a2a7cc85d4f2bae319610a7fe6b1e",
       "IPY_MODEL_169c148e00e743ed951321231a4ee165"
      ],
      "layout": "IPY_MODEL_653fc0c1edb9494f9cc14f23460b1a3f"
     }
    },
    "313a2a7cc85d4f2bae319610a7fe6b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6bf06e49cb364b91a5cc58d9d135707c",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_edad10fc1a4345cab0417dd8e7466fdc",
      "value": 433
     }
    },
    "39362aafbb4a47669d3998ac28548d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "653fc0c1edb9494f9cc14f23460b1a3f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bf06e49cb364b91a5cc58d9d135707c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84b17e634bad48238e462ec05527dbb4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88b6978b8daf4183b60c7e963aba5c7c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f3f1fd5e35402685489d74bcd441d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2899efa168cf4c1da278861ee31a7b01",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39362aafbb4a47669d3998ac28548d20",
      "value": 440473133
     }
    },
    "a07dec1b19ce4ff1a4cf4c6f5e902663": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2fc0d2232ba4b07b43d136c5a0c341f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a07dec1b19ce4ff1a4cf4c6f5e902663",
      "placeholder": "",
      "style": "IPY_MODEL_b1b12dbaab0343268e03088d08a0645e",
      "value": " 232k/232k [00:00&lt;00:00, 588kB/s]"
     }
    },
    "b1b12dbaab0343268e03088d08a0645e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba3e86ff23a949108148016e8a6bf91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c06fb95a45514f3fbb990bf5c38ed2cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb3d2b08ecf64defb9d819dda8334a9d",
      "placeholder": "",
      "style": "IPY_MODEL_ba3e86ff23a949108148016e8a6bf91b",
      "value": " 440M/440M [00:15&lt;00:00, 28.7MB/s]"
     }
    },
    "cb3d2b08ecf64defb9d819dda8334a9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf6042b9870e4aa1a10f14ecd42b90ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d35b61e91ba74271b21d068441cdf0fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edad10fc1a4345cab0417dd8e7466fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fa290d3d093d40c2ab0204869f8184a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
